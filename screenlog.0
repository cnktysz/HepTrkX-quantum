Account expires						: Sep 01, 2020
kctuysuz@culture-plate-sm:~/HepTrkX-quantum\[ctuysuz@culture-plate-sm HepTrkX-quantum]$ [K[ctuysuz@culture-plate-sm HepTrkX-quantum]$ [K[ctuysuz@culture-plate-sm HepTrkX-quantum]$ /storage/group/gpu/software/gpuservers/singularity/run.sh /storage/group/gpu/software/singu larity/ibanks/over.simg 
+ binding=
+ '[' -d /nfshome ']'
+ '[' -d /etc/grid-security/letsencrypt ']'
+ binding=' -B /etc/grid-security/letsencrypt'
+ '[' -d /data ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data'
+ '[' -d /bigdata ']'
+ '[' -d /imdata ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata'
+ '[' -d /mnt/hadoop ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop'
+ '[' -d /storage ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage'
+ ex=
+ img=/storage/group/gpu/software/singularity/ibanks/edge.simg
+ '[' '!' -z /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ '[' -r /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ echo using /storage/group/gpu/software/singularity/ibanks/over.simg as image
using /storage/group/gpu/software/singularity/ibanks/over.simg as image
+ img=/storage/group/gpu/software/singularity/ibanks/over.simg
+ ex=
+ set -x
+ '[' '!' -z '' ']'
+ XDG_RUNTIME_DIR=
+ JUPYTER_DATA_DIR=/tmp/ctuysuz/jupyter
+ LC_ALL=C
+ singularity shell -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage --nv /storage/group/gpu/software/singularity/ibanks/over.simg
[33mWARNING:[0m underlay of /etc/grid-security/letsencrypt required more than 50 (125) bind mounts
Singularity> python3 train.py configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:100: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 1
n_train: 1400
lr: 0.01
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
2020-04-24 04:43:03.877971 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_params_NN.csv
2020-04-24 04:43:03.878238 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/summary.csv
2020-04-24 04:43:03.878531 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_params_IN.csv
2020-04-24 04:43:03.878892 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_grads_EN.csv
2020-04-24 04:43:03.879149 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_grads_NN.csv
2020-04-24 04:43:03.879379 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_params_EN.csv
2020-04-24 04:43:03.879620 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_grads_IN.csv
2020-04-24 04:43:03.879856 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_validation.csv
2020-04-24 04:43:03.880093 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_loss.csv
Starting testing the validation set with 1 subgraphs!
2020-04-24 04:43:11.488285: Validation Test:  Loss: 0.6368,  Acc: 65.1724, AUC: 0.6744, Precision: 0.7925 -- Elapsed: 0m6s
2020-04-24 04:43:11.488364: Training is starting!
^[:wqAccount expires						: Sep 01, 2020
kctuysuz@culture-plate-sm:~/HepTrkX-quantum\[ctuysuz@culture-plate-sm HepTrkX-quantum]$ /storage/group/gpu/software/gpuservers/singularity/run.sh /storage/group/gpu/software/singu larity/ibanks/over.simg 
+ binding=
+ '[' -d /nfshome ']'
+ '[' -d /etc/grid-security/letsencrypt ']'
+ binding=' -B /etc/grid-security/letsencrypt'
+ '[' -d /data ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data'
+ '[' -d /bigdata ']'
+ '[' -d /imdata ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata'
+ '[' -d /mnt/hadoop ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop'
+ '[' -d /storage ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage'
+ ex=
+ img=/storage/group/gpu/software/singularity/ibanks/edge.simg
+ '[' '!' -z /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ '[' -r /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ echo using /storage/group/gpu/software/singularity/ibanks/over.simg as image
using /storage/group/gpu/software/singularity/ibanks/over.simg as image
+ img=/storage/group/gpu/software/singularity/ibanks/over.simg
+ ex=
+ set -x
+ '[' '!' -z '' ']'
+ XDG_RUNTIME_DIR=
+ JUPYTER_DATA_DIR=/tmp/ctuysuz/jupyter
+ LC_ALL=C
+ singularity shell -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage --nv /storage/group/gpu/software/singularity/ibanks/over.simg
[33mWARNING:[0m underlay of /etc/grid-security/letsencrypt required more than 50 (125) bind mounts
Singularity> python3 train.py configs/q comparisons/qgnn/iteration_comparison/                     learning_rate_comparison/lr_2e-2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:100: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/learning_rate_comparison/lr_2e-2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.02
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/learning_rate_comparison/lr_2e-2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
^CTraceback (most recent call last):
  File "train.py", line 80, in <module>
    loss, grads = gradient(graph_array,labels)
  File "train.py", line 22, in gradient
    return loss, tape.gradient(loss,block.trainable_variables)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 1029, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 439, in actual_grad_fn
    input_grads = grad_fn(*result_grads)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/interfaces/tf.py", line 92, in grad
    jacobian = qnode.jacobian(args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 918, in jacobian
    grad[:, i] = self._pd_analytic(flat_params, k, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 1047, in _pd_analytic
    y1 = np.asarray(self.evaluate(shift_p2, **circuit_kwargs))
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/autograd/tracer.py", line 48, in f_wrapped
    return f_raw(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 682, in evaluate
    ret = self.device.execute(self.circuit.operations, self.circuit.observables, self.variable_deps)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/_device.py", line 179, in execute
    results.append(self.expval(obs.name, obs.wires, obs.parameters))
  File "/usr/local/lib/python3.6/site-packages/pennylane_qulacs/qulacs_device.py", line 196, in expval
    expectation = inner_product(bra, self._state)
KeyboardInterrupt
Singularity> vim configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml 
[?1049h[?1h=[1;73r[34l[34h[?25h[23m[24m[0m[H[J[?25l[73;1H"configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml" 18L, 423C[1;1H[33m  1 [0m[36mtrain_dir[0m[35m    :[0m [31m'data/graph_data/train'[0m
[33m  2 [0m[36mvalid_dir[0m[35m    :[0m [31m'data/graph_data/valid'[0m
[33m  3 [0m[36mparam_dir[0m[35m    :[0m [31m'params/'[0m
[33m  4 [0m[36mlog_dir[0m[35m      :[0m [31m'logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/'[0m
[33m  5 [0m[36mrun_type[0m[35m     :[0m [31m'new_run'[0m
[33m  6 [0m[36mgpu[0m[35m          :[0m [31m'7'[0m
[33m  7 [0m[36mn_files[0m[35m      :[0m [31m1600[0m
[33m  8 [0m[36mn_valid[0m[35m      :[0m [31m1[0m
[33m  9 [0m[36mn_train[0m[35m      :[0m [31m1400[0m
[33m 10 [0m[36mlr[0m[35m           :[0m [31m0.01[0m
[33m 11 [0m[36mn_iters[0m[35m      :[0m [31m2[0m
[33m 12 [0m[36mn_epoch[0m[35m      :[0m [31m1[0m
[33m 13 [0m[36mTEST_every[0m[35m   :[0m [31m50[0m
[33m 14 [0m[36mhid_dim[0m[35m      :[0m [31m1[0m
[33m 15 [0m[36mnetwork[0m[35m      :[0m [31m'QGNN'[0m
[33m 16 [0m[36mn_thread[0m[35m     :[0m [31m4[0m
[33m 17 [0m[36mlog_verbosity[0m[35m:[0m [31m2[0m
[33m 18 [0m
[1m[34m~                                                                                                                                      [20;1H~                                                                                                                                      [21;1H~                                                                                                                                      [22;1H~                                                                                                                                      [23;1H~                                                                                                                                      [24;1H~                                                                                                                                      [25;1H~                                                                                                                                      [26;1H~                                                                                                                                      [27;1H~                                                                                                                                      [28;1H~                                                                                                                                      [29;1H~                                                                                                                                      [30;1H~                                                                                                                                      [31;1H~                                                                                                                                      [32;1H~                                                                                                                                      [33;1H~                                                                                                                                      [34;1H~                                                                                                                                      [35;1H~                                                                                                                                      [36;1H~                                                                                                                                      [37;1H~                                                                                                                                      [38;1H~                                                                                                                                      [39;1H~                                                                                                                                      [40;1H~                                                                                                                                      [41;1H~                                                                                                                                      [42;1H~                                                                                                                                      [43;1H~                                                                                                                                      [44;1H~                                                                                                                                      [45;1H~                                                                                                                                      [46;1H~                                                                                                                                      [47;1H~                                                                                                                                      [48;1H~                                                                                                                                      [49;1H~                                                                                                                                      [50;1H~                                                                                                                                      [51;1H~                                                                                                                                      [52;1H~                                                                                                                                      [53;1H~                                                                                                                                      [54;1H~                                                                                                                                      [55;1H~                                                                                                                                      [56;1H~                                                                                                                                      [57;1H~                                                                                                                                      [58;1H~                                                                                                                                      [59;1H~                                                                                                                                      [60;1H~                                                                                                                                      [61;1H~                                                                                                                                      [62;1H~                                                                                                                                      [63;1H~                                                                                                                                      [64;1H~                                                                                                                                      [65;1H~                                                                                                                                      [66;1H~                                                                                                                                      [67;1H~                                                                                                                                      [68;1H~                                                                                                                                      [69;1H~                                                                                                                                      [70;1H~                                                                                                                                      [71;1H~                                                                                                                                      [72;1H~                                                                                                                                      [0m[73;118H8,1[11CAll[8;5H[34h[?25h[?25l[73;118H9[9;5H[34h[?25h[?25l[73;120H2[9;6H[34h[?25h[?25l[73;118H8[8;6H[34h[?25h[?25l[73;120H3[8;7H[34h[?25h[?25l[73;120H4[8;8H[34h[?25h[?25l[73;120H5[8;9H[34h[?25h[?25l[73;120H6[8;10H[34h[?25h[?25l[73;120H7[8;11H[34h[?25h[?25l[73;120H8[8;12H[34h[?25h[?25l[73;120H9[8;13H[34h[?25h[?25l[73;120H10[8;14H[34h[?25h[?25l[73;121H1[8;15H[34h[?25h[?25l[73;121H2[8;16H[34h[?25h[?25l[73;121H3[8;17H[34h[?25h[?25l[73;121H4[8;18H[34h[?25h[?25l[73;121H5[8;19H[34h[?25h[?25l[73;121H6[8;20H[34h[?25h[?25l[73;1H[1m-- INSERT --[0m[73;13H[K[73;118H8,16[10CAll[8;20H[34h[?25h[?25l[73;121H7[8;21H[34h[?25h[?25l[8;20H[K[73;121H6[8;20H[34h[?25h[?25l[31m2[0m[73;121H7[8;21H[34h[?25h[?25l[31m0[0m[73;121H8[8;22H[34h[?25h[?25l[31m0[0m[73;121H9[8;23H[34h[?25h[73;1H[K[?25l[73;118H8,18[10CAll[8;22H[34h[?25h[?25l[73;118H[K[73;1H:[34h[?25hwq[?25l"configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml" 18L, 425C written
[?1l>[34h[?25h[?1049lSingularity> vim configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml Singularity> [13@python3 train.py configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:100: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.01
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
2020-04-24 04:44:32.964701 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_validation.csv
Starting testing the validation set with 200 subgraphs!
Account expires						: Sep 01, 2020
kctuysuz@culture-plate-sm:~/HepTrkX-quantum\[ctuysuz@culture-plate-sm HepTrkX-quantum]$ /storage/group/gpu/software/gpuservers/singularity/run.sh /storage/group/gpu/software/singu larity/ibanks/over.simg 
+ binding=
+ '[' -d /nfshome ']'
+ '[' -d /etc/grid-security/letsencrypt ']'
+ binding=' -B /etc/grid-security/letsencrypt'
+ '[' -d /data ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data'
+ '[' -d /bigdata ']'
+ '[' -d /imdata ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata'
+ '[' -d /mnt/hadoop ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop'
+ '[' -d /storage ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage'
+ ex=
+ img=/storage/group/gpu/software/singularity/ibanks/edge.simg
+ '[' '!' -z /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ '[' -r /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ echo using /storage/group/gpu/software/singularity/ibanks/over.simg as image
using /storage/group/gpu/software/singularity/ibanks/over.simg as image
+ img=/storage/group/gpu/software/singularity/ibanks/over.simg
+ ex=
+ set -x
+ '[' '!' -z '' ']'
+ XDG_RUNTIME_DIR=
+ JUPYTER_DATA_DIR=/tmp/ctuysuz/jupyter
+ LC_ALL=C
+ singularity shell -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage --nv /storage/group/gpu/software/singularity/ibanks/over.simg
[33mWARNING:[0m underlay of /etc/grid-security/letsencrypt required more than 50 (125) bind mounts
Singularity> python3 train.py configs/comparisons/qgnn/L 
dimension_comparison/     iteration_comparison/     learning_rate_comparison/ 
Singularity> python3 train.py configs/comparisons/qgnn/ler arning_rate_comparison/lr_2e-2.yaml           3e-2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:100: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/learning_rate_comparison/lr_3e-2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/learning_rate_comparison/lr_3e-2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
2020-04-24 05:18:39.134229: Validation Test:  Loss: 0.7859,  Acc: 48.4221, AUC: 0.4778, Precision: 0.5517 -- Elapsed: 34m45s
2020-04-24 05:18:39.134316: Training is starting!
2020-04-24 05:19:27.408295: Validation Test:  Loss: 0.7393,  Acc: 47.1277, AUC: 0.4563, Precision: 0.5364 -- Elapsed: 34m53s
2020-04-24 05:19:27.408377: Training is starting!
2020-04-24 05:20:46.835994: Validation Test:  Loss: 0.7514,  Acc: 52.4676, AUC: 0.5324, Precision: 0.5954 -- Elapsed: 35m26s
2020-04-24 05:20:46.836079: Training is starting!
2020-04-24 05:23:36.751152: Epoch: 1, Batch: 1, Loss: 0.8325, Elapsed: 2m49s
2020-04-24 05:24:13.949662: Epoch: 1, Batch: 1, Loss: 0.7195, Elapsed: 4m46s
2020-04-24 05:25:48.522282: Epoch: 1, Batch: 1, Loss: 0.7716, Elapsed: 7m9s
2020-04-24 05:29:35.562508: Epoch: 1, Batch: 2, Loss: 0.7191, Elapsed: 5m58s
2020-04-24 05:30:29.543834: Epoch: 1, Batch: 2, Loss: 0.7318, Elapsed: 6m15s
2020-04-24 05:31:20.192177: Epoch: 1, Batch: 2, Loss: 0.7396, Elapsed: 5m31s
2020-04-24 05:33:10.035429: Epoch: 1, Batch: 3, Loss: 0.8022, Elapsed: 3m34s
2020-04-24 05:34:34.144971: Epoch: 1, Batch: 3, Loss: 0.7407, Elapsed: 3m13s
2020-04-24 05:35:45.392533: Epoch: 1, Batch: 3, Loss: 0.7082, Elapsed: 5m15s
2020-04-24 05:36:57.330429: Epoch: 1, Batch: 4, Loss: 0.7492, Elapsed: 3m47s
2020-04-24 05:39:45.315336: Epoch: 1, Batch: 4, Loss: 0.7519, Elapsed: 5m11s
2020-04-24 05:41:45.839605: Epoch: 1, Batch: 5, Loss: 0.7393, Elapsed: 4m48s
2020-04-24 05:44:23.164240: Epoch: 1, Batch: 5, Loss: 0.7876, Elapsed: 4m37s
2020-04-24 05:45:53.490824: Epoch: 1, Batch: 4, Loss: 0.7346, Elapsed: 10m8s
2020-04-24 05:46:47.509320: Epoch: 1, Batch: 6, Loss: 0.7463, Elapsed: 5m1s
2020-04-24 05:49:18.875460: Epoch: 1, Batch: 6, Loss: 0.7802, Elapsed: 4m55s
2020-04-24 05:52:16.052237: Epoch: 1, Batch: 7, Loss: 0.7022, Elapsed: 5m28s
2020-04-24 05:52:21.394841: Epoch: 1, Batch: 5, Loss: 0.7210, Elapsed: 6m27s
2020-04-24 05:55:25.679334: Epoch: 1, Batch: 7, Loss: 0.8148, Elapsed: 6m6s
2020-04-24 05:58:56.216621: Epoch: 1, Batch: 6, Loss: 0.6972, Elapsed: 6m34s
2020-04-24 05:59:27.339924: Epoch: 1, Batch: 8, Loss: 0.6953, Elapsed: 7m11s
2020-04-24 06:04:22.108126: Epoch: 1, Batch: 7, Loss: 0.7250, Elapsed: 5m25s
2020-04-24 06:05:05.377672: Epoch: 1, Batch: 9, Loss: 0.7035, Elapsed: 5m38s
2020-04-24 06:06:01.117626: Epoch: 1, Batch: 8, Loss: 0.7889, Elapsed: 10m35s
2020-04-24 06:09:39.660706: Epoch: 1, Batch: 10, Loss: 0.7375, Elapsed: 4m34s
2020-04-24 06:10:33.315886: Epoch: 1, Batch: 8, Loss: 0.6934, Elapsed: 6m11s
2020-04-24 06:12:02.325033: Epoch: 1, Batch: 9, Loss: 0.7224, Elapsed: 6m1s
2020-04-24 06:15:30.290418: Epoch: 1, Batch: 9, Loss: 0.7321, Elapsed: 4m56s
2020-04-24 06:16:23.123921: Epoch: 1, Batch: 10, Loss: 0.7924, Elapsed: 4m20s
2020-04-24 06:17:09.613944: Epoch: 1, Batch: 11, Loss: 0.7334, Elapsed: 7m29s
2020-04-24 06:22:24.229452: Epoch: 1, Batch: 11, Loss: 0.7488, Elapsed: 6m1s
2020-04-24 06:23:29.488972: Epoch: 1, Batch: 10, Loss: 0.7429, Elapsed: 7m59s
2020-04-24 06:26:21.262633: Epoch: 1, Batch: 12, Loss: 0.6928, Elapsed: 9m11s
2020-04-24 06:26:50.664630: Epoch: 1, Batch: 12, Loss: 0.7210, Elapsed: 4m26s
2020-04-24 06:32:22.998624: Epoch: 1, Batch: 11, Loss: 0.7369, Elapsed: 8m53s
2020-04-24 06:32:58.568499: Epoch: 1, Batch: 13, Loss: 0.7076, Elapsed: 6m37s
2020-04-24 06:35:02.763729: Epoch: 1, Batch: 13, Loss: 0.7802, Elapsed: 8m12s
2020-04-24 06:38:49.481900: Epoch: 1, Batch: 14, Loss: 0.6997, Elapsed: 3m46s
2020-04-24 06:39:02.095125: Epoch: 1, Batch: 14, Loss: 0.7100, Elapsed: 6m3s
2020-04-24 06:40:02.262929: Epoch: 1, Batch: 12, Loss: 0.7393, Elapsed: 7m39s
2020-04-24 06:43:09.550632: Epoch: 1, Batch: 15, Loss: 0.7260, Elapsed: 4m20s
2020-04-24 06:46:12.560796: Epoch: 1, Batch: 15, Loss: 0.6981, Elapsed: 7m10s
2020-04-24 06:48:09.798018: Epoch: 1, Batch: 13, Loss: 0.7313, Elapsed: 8m7s
2020-04-24 06:48:16.262655: Epoch: 1, Batch: 16, Loss: 0.7265, Elapsed: 5m6s
2020-04-24 06:51:41.376603: Epoch: 1, Batch: 17, Loss: 0.7300, Elapsed: 3m25s
2020-04-24 06:53:00.370470: Epoch: 1, Batch: 16, Loss: 0.6909, Elapsed: 6m47s
2020-04-24 06:56:49.833264: Epoch: 1, Batch: 17, Loss: 0.7061, Elapsed: 3m49s
2020-04-24 06:57:35.199187: Epoch: 1, Batch: 18, Loss: 0.7112, Elapsed: 5m53s
2020-04-24 06:57:47.081821: Epoch: 1, Batch: 14, Loss: 0.7105, Elapsed: 9m37s
2020-04-24 07:02:04.374583: Epoch: 1, Batch: 19, Loss: 0.6935, Elapsed: 4m29s
2020-04-24 07:06:01.836560: Epoch: 1, Batch: 18, Loss: 0.6963, Elapsed: 9m11s
2020-04-24 07:06:27.251073: Epoch: 1, Batch: 20, Loss: 0.7127, Elapsed: 4m22s
2020-04-24 07:09:18.201636: Epoch: 1, Batch: 15, Loss: 0.7257, Elapsed: 11m31s
2020-04-24 07:09:22.175540: Epoch: 1, Batch: 19, Loss: 0.7202, Elapsed: 3m20s
2020-04-24 07:15:27.148055: Epoch: 1, Batch: 21, Loss: 0.7112, Elapsed: 8m59s
2020-04-24 07:15:30.597570: Epoch: 1, Batch: 20, Loss: 0.6990, Elapsed: 6m8s
2020-04-24 07:15:50.288806: Epoch: 1, Batch: 16, Loss: 0.7048, Elapsed: 6m32s
2020-04-24 07:21:23.886103: Epoch: 1, Batch: 22, Loss: 0.7502, Elapsed: 5m56s
2020-04-24 07:24:39.953115: Epoch: 1, Batch: 17, Loss: 0.7265, Elapsed: 8m49s
2020-04-24 07:25:10.108890: Epoch: 1, Batch: 21, Loss: 0.7189, Elapsed: 9m39s
2020-04-24 07:28:00.329942: Epoch: 1, Batch: 23, Loss: 0.7168, Elapsed: 6m36s
2020-04-24 07:30:40.828874: Epoch: 1, Batch: 18, Loss: 0.7151, Elapsed: 6m0s
2020-04-24 07:30:46.245976: Epoch: 1, Batch: 22, Loss: 0.7083, Elapsed: 5m36s
2020-04-24 07:33:38.078365: Epoch: 1, Batch: 24, Loss: 0.7074, Elapsed: 5m37s
2020-04-24 07:35:33.061941: Epoch: 1, Batch: 19, Loss: 0.6681, Elapsed: 4m52s
2020-04-24 07:40:23.045561: Epoch: 1, Batch: 25, Loss: 0.7033, Elapsed: 6m44s
2020-04-24 07:41:32.971985: Epoch: 1, Batch: 23, Loss: 0.6972, Elapsed: 10m46s
2020-04-24 07:46:00.526206: Epoch: 1, Batch: 20, Loss: 0.7317, Elapsed: 10m27s
2020-04-24 07:49:10.942664: Epoch: 1, Batch: 26, Loss: 0.7191, Elapsed: 8m47s
2020-04-24 07:50:58.987772: Epoch: 1, Batch: 24, Loss: 0.6937, Elapsed: 9m25s
2020-04-24 07:52:34.249037: Epoch: 1, Batch: 21, Loss: 0.6700, Elapsed: 6m33s
2020-04-24 07:54:18.495511: Epoch: 1, Batch: 27, Loss: 0.6894, Elapsed: 5m7s
2020-04-24 07:54:38.033267: Epoch: 1, Batch: 25, Loss: 0.6996, Elapsed: 3m39s
2020-04-24 07:57:21.504765: Epoch: 1, Batch: 22, Loss: 0.7418, Elapsed: 4m47s
2020-04-24 08:00:51.851217: Epoch: 1, Batch: 28, Loss: 0.7199, Elapsed: 6m33s
2020-04-24 08:00:58.405077: Epoch: 1, Batch: 26, Loss: 0.6993, Elapsed: 6m20s
2020-04-24 08:03:49.669814: Epoch: 1, Batch: 23, Loss: 0.7201, Elapsed: 6m28s
2020-04-24 08:06:06.554272: Epoch: 1, Batch: 27, Loss: 0.6992, Elapsed: 5m8s
2020-04-24 08:07:29.183826: Epoch: 1, Batch: 29, Loss: 0.7053, Elapsed: 6m37s
2020-04-24 08:09:05.420587: Epoch: 1, Batch: 24, Loss: 0.6739, Elapsed: 5m15s
2020-04-24 08:11:13.185557: Epoch: 1, Batch: 30, Loss: 0.6795, Elapsed: 3m43s
2020-04-24 08:12:39.248919: Epoch: 1, Batch: 28, Loss: 0.6888, Elapsed: 6m32s
2020-04-24 08:15:42.396396: Epoch: 1, Batch: 31, Loss: 0.6902, Elapsed: 4m29s
2020-04-24 08:17:42.163330: Epoch: 1, Batch: 25, Loss: 0.7233, Elapsed: 8m36s
2020-04-24 08:18:25.901840: Epoch: 1, Batch: 29, Loss: 0.6960, Elapsed: 5m46s
2020-04-24 08:20:09.599885: Epoch: 1, Batch: 32, Loss: 0.7199, Elapsed: 4m27s
2020-04-24 08:25:50.449464: Epoch: 1, Batch: 26, Loss: 0.7116, Elapsed: 8m8s
2020-04-24 08:26:12.832519: Epoch: 1, Batch: 33, Loss: 0.7055, Elapsed: 6m3s
2020-04-24 08:26:22.038931: Epoch: 1, Batch: 30, Loss: 0.7062, Elapsed: 7m56s
2020-04-24 08:32:12.927776: Epoch: 1, Batch: 34, Loss: 0.6741, Elapsed: 6m0s
2020-04-24 08:32:42.197440: Epoch: 1, Batch: 27, Loss: 0.7212, Elapsed: 6m51s
2020-04-24 08:32:41.685541: Epoch: 1, Batch: 31, Loss: 0.6932, Elapsed: 6m19s
2020-04-24 08:38:58.836369: Epoch: 1, Batch: 32, Loss: 0.6973, Elapsed: 6m17s
2020-04-24 08:40:11.085555: Epoch: 1, Batch: 28, Loss: 0.6893, Elapsed: 7m28s
2020-04-24 08:42:28.268846: Epoch: 1, Batch: 35, Loss: 0.7045, Elapsed: 10m15s
2020-04-24 08:44:43.517387: Epoch: 1, Batch: 33, Loss: 0.6942, Elapsed: 5m44s
2020-04-24 08:46:03.857336: Epoch: 1, Batch: 29, Loss: 0.6822, Elapsed: 5m52s
2020-04-24 08:49:35.009485: Epoch: 1, Batch: 36, Loss: 0.7036, Elapsed: 7m6s
2020-04-24 08:52:47.695837: Epoch: 1, Batch: 30, Loss: 0.7076, Elapsed: 6m43s
2020-04-24 08:53:52.227237: Epoch: 1, Batch: 34, Loss: 0.6884, Elapsed: 9m8s
2020-04-24 08:55:33.873304: Epoch: 1, Batch: 37, Loss: 0.7045, Elapsed: 5m58s
2020-04-24 08:56:20.057113: Epoch: 1, Batch: 31, Loss: 0.7288, Elapsed: 3m32s
2020-04-24 08:58:45.820614: Epoch: 1, Batch: 35, Loss: 0.6963, Elapsed: 4m53s
2020-04-24 09:01:05.624758: Epoch: 1, Batch: 38, Loss: 0.7024, Elapsed: 5m31s
2020-04-24 09:03:29.403001: Epoch: 1, Batch: 36, Loss: 0.6898, Elapsed: 4m43s
2020-04-24 09:03:39.460672: Epoch: 1, Batch: 32, Loss: 0.6816, Elapsed: 7m19s
2020-04-24 09:08:53.978821: Epoch: 1, Batch: 37, Loss: 0.6920, Elapsed: 5m24s
2020-04-24 09:09:24.240232: Epoch: 1, Batch: 33, Loss: 0.7288, Elapsed: 5m44s
2020-04-24 09:10:29.061931: Epoch: 1, Batch: 39, Loss: 0.6901, Elapsed: 9m23s
2020-04-24 09:14:27.530504: Epoch: 1, Batch: 38, Loss: 0.6812, Elapsed: 5m33s
2020-04-24 09:15:15.102801: Epoch: 1, Batch: 40, Loss: 0.6833, Elapsed: 4m46s
2020-04-24 09:16:55.210416: Epoch: 1, Batch: 34, Loss: 0.7049, Elapsed: 7m30s
2020-04-24 09:19:28.131107: Epoch: 1, Batch: 41, Loss: 0.6807, Elapsed: 4m13s
2020-04-24 09:21:19.362821: Epoch: 1, Batch: 39, Loss: 0.7037, Elapsed: 6m51s
2020-04-24 09:24:09.437149: Epoch: 1, Batch: 35, Loss: 0.7001, Elapsed: 7m14s
2020-04-24 09:25:25.555088: Epoch: 1, Batch: 42, Loss: 0.6793, Elapsed: 5m57s
2020-04-24 09:29:32.316394: Epoch: 1, Batch: 40, Loss: 0.6924, Elapsed: 8m12s
2020-04-24 09:34:02.453948: Epoch: 1, Batch: 43, Loss: 0.7075, Elapsed: 8m36s
2020-04-24 09:35:19.893048: Epoch: 1, Batch: 41, Loss: 0.7015, Elapsed: 5m47s
2020-04-24 09:35:30.582745: Epoch: 1, Batch: 36, Loss: 0.6947, Elapsed: 11m21s
2020-04-24 09:38:32.765430: Epoch: 1, Batch: 37, Loss: 0.6369, Elapsed: 3m2s
2020-04-24 09:39:45.440852: Epoch: 1, Batch: 42, Loss: 0.6788, Elapsed: 4m25s
2020-04-24 09:41:15.797415: Epoch: 1, Batch: 44, Loss: 0.6705, Elapsed: 7m13s
2020-04-24 09:45:32.827462: Epoch: 1, Batch: 38, Loss: 0.7079, Elapsed: 7m0s
2020-04-24 09:46:07.767445: Epoch: 1, Batch: 43, Loss: 0.6863, Elapsed: 6m22s
2020-04-24 09:51:52.579106: Epoch: 1, Batch: 39, Loss: 0.6737, Elapsed: 6m19s
2020-04-24 09:53:58.348862: Epoch: 1, Batch: 45, Loss: 0.6921, Elapsed: 12m42s
2020-04-24 09:54:25.567115: Epoch: 1, Batch: 44, Loss: 0.6970, Elapsed: 8m17s
2020-04-24 09:56:23.460840: Epoch: 1, Batch: 40, Loss: 0.6651, Elapsed: 4m30s
2020-04-24 10:00:31.157230: Epoch: 1, Batch: 45, Loss: 0.6920, Elapsed: 6m5s
2020-04-24 10:04:09.614923: Epoch: 1, Batch: 46, Loss: 0.6761, Elapsed: 10m11s
2020-04-24 10:05:37.604908: Epoch: 1, Batch: 46, Loss: 0.6948, Elapsed: 5m6s
2020-04-24 10:07:26.159179: Epoch: 1, Batch: 41, Loss: 0.7162, Elapsed: 11m2s
2020-04-24 10:09:33.225393: Epoch: 1, Batch: 47, Loss: 0.6794, Elapsed: 5m23s
2020-04-24 10:12:24.564130: Epoch: 1, Batch: 42, Loss: 0.7056, Elapsed: 4m58s
2020-04-24 10:13:52.595236: Epoch: 1, Batch: 47, Loss: 0.6803, Elapsed: 8m14s
2020-04-24 10:18:37.294174: Epoch: 1, Batch: 48, Loss: 0.6908, Elapsed: 4m44s
2020-04-24 10:18:52.703281: Epoch: 1, Batch: 43, Loss: 0.6679, Elapsed: 6m28s
2020-04-24 10:20:32.984648: Epoch: 1, Batch: 48, Loss: 0.6841, Elapsed: 10m59s
2020-04-24 10:22:14.382632: Epoch: 1, Batch: 49, Loss: 0.6783, Elapsed: 3m37s
2020-04-24 10:26:19.499398: Epoch: 1, Batch: 44, Loss: 0.6816, Elapsed: 7m26s
2020-04-24 10:27:29.816572: Epoch: 1, Batch: 50, Loss: 0.6904, Elapsed: 5m15s
Starting testing the validation set with 200 subgraphs!
2020-04-24 10:29:27.093129: Epoch: 1, Batch: 49, Loss: 0.6947, Elapsed: 8m54s
2020-04-24 10:30:23.502302: Epoch: 1, Batch: 45, Loss: 0.6549, Elapsed: 4m3s
2020-04-24 10:34:37.727365: Epoch: 1, Batch: 50, Loss: 0.6749, Elapsed: 5m10s
Starting testing the validation set with 200 subgraphs!
2020-04-24 10:38:17.646062: Epoch: 1, Batch: 46, Loss: 0.7025, Elapsed: 7m54s
2020-04-24 10:44:27.785661: Epoch: 1, Batch: 47, Loss: 0.7040, Elapsed: 6m10s
2020-04-24 10:49:04.721385: Epoch: 1, Batch: 48, Loss: 0.6877, Elapsed: 4m36s
2020-04-24 10:55:26.268082: Epoch: 1, Batch: 49, Loss: 0.7023, Elapsed: 6m21s
2020-04-24 11:01:06.785581: Epoch: 1, Batch: 50, Loss: 0.7004, Elapsed: 5m40s
Starting testing the validation set with 200 subgraphs!
2020-04-24 11:06:13.530044: Validation Test:  Loss: 0.6877,  Acc: 54.3950, AUC: 0.5649, Precision: 0.6232 -- Elapsed: 38m43s
2020-04-24 11:11:27.928462: Epoch: 1, Batch: 51, Loss: 0.6963, Elapsed: 5m14s
2020-04-24 11:12:22.122519: Validation Test:  Loss: 0.6858,  Acc: 56.3284, AUC: 0.5826, Precision: 0.6404 -- Elapsed: 37m44s
2020-04-24 11:16:48.434059: Epoch: 1, Batch: 51, Loss: 0.6833, Elapsed: 4m26s
2020-04-24 11:17:06.969930: Epoch: 1, Batch: 52, Loss: 0.6927, Elapsed: 5m39s
2020-04-24 11:21:54.661281: Epoch: 1, Batch: 53, Loss: 0.6883, Elapsed: 4m47s
2020-04-24 11:22:05.723791: Epoch: 1, Batch: 52, Loss: 0.6940, Elapsed: 5m17s
2020-04-24 11:25:41.305009: Epoch: 1, Batch: 53, Loss: 0.6840, Elapsed: 3m35s
2020-04-24 11:28:19.315428: Epoch: 1, Batch: 54, Loss: 0.6794, Elapsed: 6m24s
2020-04-24 11:30:31.024560: Epoch: 1, Batch: 54, Loss: 0.6803, Elapsed: 4m49s
2020-04-24 11:33:18.183241: Epoch: 1, Batch: 55, Loss: 0.6815, Elapsed: 4m58s
2020-04-24 11:38:29.688820: Epoch: 1, Batch: 55, Loss: 0.6900, Elapsed: 7m58s
2020-04-24 11:39:20.733463: Epoch: 1, Batch: 56, Loss: 0.6855, Elapsed: 6m2s
2020-04-24 11:41:26.779917: Validation Test:  Loss: 0.6954,  Acc: 51.8133, AUC: 0.5151, Precision: 0.6008 -- Elapsed: 40m19s
2020-04-24 11:43:26.250904: Epoch: 1, Batch: 57, Loss: 0.6741, Elapsed: 4m5s
2020-04-24 11:44:29.922712: Epoch: 1, Batch: 56, Loss: 0.6847, Elapsed: 6m0s
2020-04-24 11:46:54.688216: Epoch: 1, Batch: 51, Loss: 0.7098, Elapsed: 5m27s
2020-04-24 11:50:01.816316: Epoch: 1, Batch: 57, Loss: 0.6858, Elapsed: 5m31s
2020-04-24 11:50:10.689146: Epoch: 1, Batch: 58, Loss: 0.6804, Elapsed: 6m44s
2020-04-24 11:53:23.041312: Epoch: 1, Batch: 52, Loss: 0.7044, Elapsed: 6m28s
2020-04-24 11:55:04.549527: Epoch: 1, Batch: 58, Loss: 0.6802, Elapsed: 5m2s
2020-04-24 11:58:08.706511: Epoch: 1, Batch: 59, Loss: 0.6785, Elapsed: 7m57s
2020-04-24 11:59:08.442841: Epoch: 1, Batch: 53, Loss: 0.6989, Elapsed: 5m45s
2020-04-24 12:00:23.318445: Epoch: 1, Batch: 59, Loss: 0.6799, Elapsed: 5m18s
2020-04-24 12:02:14.133705: Epoch: 1, Batch: 60, Loss: 0.6953, Elapsed: 4m5s
2020-04-24 12:03:42.612299: Epoch: 1, Batch: 60, Loss: 0.6646, Elapsed: 3m19s
2020-04-24 12:08:23.466730: Epoch: 1, Batch: 54, Loss: 0.7011, Elapsed: 9m14s
2020-04-24 12:09:46.019516: Epoch: 1, Batch: 61, Loss: 0.6814, Elapsed: 7m31s
2020-04-24 12:11:02.558128: Epoch: 1, Batch: 61, Loss: 0.6881, Elapsed: 7m19s
2020-04-24 12:12:06.408402: Epoch: 1, Batch: 55, Loss: 0.6892, Elapsed: 3m42s
2020-04-24 12:15:01.607802: Epoch: 1, Batch: 62, Loss: 0.6993, Elapsed: 5m15s
2020-04-24 12:17:32.365593: Epoch: 1, Batch: 56, Loss: 0.6705, Elapsed: 5m25s
2020-04-24 12:18:32.639896: Epoch: 1, Batch: 62, Loss: 0.6718, Elapsed: 7m30s
2020-04-24 12:22:59.439692: Epoch: 1, Batch: 63, Loss: 0.6872, Elapsed: 7m57s
2020-04-24 12:24:07.524692: Epoch: 1, Batch: 57, Loss: 0.6804, Elapsed: 6m35s
2020-04-24 12:25:49.629095: Epoch: 1, Batch: 63, Loss: 0.6865, Elapsed: 7m16s
2020-04-24 12:29:25.791811: Epoch: 1, Batch: 64, Loss: 0.6623, Elapsed: 6m26s
2020-04-24 12:29:39.365931: Epoch: 1, Batch: 58, Loss: 0.6898, Elapsed: 5m31s
2020-04-24 12:33:18.452900: Epoch: 1, Batch: 64, Loss: 0.6752, Elapsed: 7m28s
2020-04-24 12:34:40.357820: Epoch: 1, Batch: 65, Loss: 0.6703, Elapsed: 5m14s
2020-04-24 12:36:15.046476: Epoch: 1, Batch: 59, Loss: 0.7015, Elapsed: 6m35s
2020-04-24 12:41:11.842386: Epoch: 1, Batch: 65, Loss: 0.6849, Elapsed: 7m53s
2020-04-24 12:41:12.387867: Epoch: 1, Batch: 60, Loss: 0.6741, Elapsed: 4m57s
2020-04-24 12:42:21.680109: Epoch: 1, Batch: 66, Loss: 0.6775, Elapsed: 7m41s
2020-04-24 12:44:51.788988: Epoch: 1, Batch: 61, Loss: 0.6869, Elapsed: 3m39s
2020-04-24 12:46:52.335406: Epoch: 1, Batch: 66, Loss: 0.6673, Elapsed: 5m40s
2020-04-24 12:47:58.377444: Epoch: 1, Batch: 67, Loss: 0.6803, Elapsed: 5m36s
2020-04-24 12:52:03.116439: Epoch: 1, Batch: 67, Loss: 0.6795, Elapsed: 5m10s
2020-04-24 12:52:16.510702: Epoch: 1, Batch: 62, Loss: 0.6951, Elapsed: 7m24s
2020-04-24 12:52:37.494083: Epoch: 1, Batch: 68, Loss: 0.6612, Elapsed: 4m39s
2020-04-24 12:57:16.621605: Epoch: 1, Batch: 68, Loss: 0.6493, Elapsed: 5m13s
2020-04-24 12:57:58.872099: Epoch: 1, Batch: 69, Loss: 0.6826, Elapsed: 5m21s
2020-04-24 12:59:09.567407: Epoch: 1, Batch: 63, Loss: 0.6738, Elapsed: 6m53s
2020-04-24 13:05:15.959551: Epoch: 1, Batch: 69, Loss: 0.6898, Elapsed: 7m59s
2020-04-24 13:05:45.992226: Epoch: 1, Batch: 64, Loss: 0.6714, Elapsed: 6m36s
2020-04-24 13:06:18.378273: Epoch: 1, Batch: 70, Loss: 0.6926, Elapsed: 8m19s
2020-04-24 13:12:30.939740: Epoch: 1, Batch: 71, Loss: 0.6778, Elapsed: 6m12s
2020-04-24 13:13:01.784927: Epoch: 1, Batch: 70, Loss: 0.6688, Elapsed: 7m45s
2020-04-24 13:15:48.250924: Epoch: 1, Batch: 65, Loss: 0.6955, Elapsed: 10m2s
2020-04-24 13:19:38.833233: Epoch: 1, Batch: 72, Loss: 0.6722, Elapsed: 7m7s
2020-04-24 13:22:24.509225: Epoch: 1, Batch: 66, Loss: 0.6911, Elapsed: 6m36s
2020-04-24 13:22:50.337031: Epoch: 1, Batch: 71, Loss: 0.6804, Elapsed: 9m48s
2020-04-24 13:26:06.604451: Epoch: 1, Batch: 73, Loss: 0.6993, Elapsed: 6m27s
2020-04-24 13:27:54.871730: Epoch: 1, Batch: 67, Loss: 0.6992, Elapsed: 5m30s
2020-04-24 13:28:52.854177: Epoch: 1, Batch: 72, Loss: 0.6760, Elapsed: 6m2s
2020-04-24 13:30:18.083526: Epoch: 1, Batch: 74, Loss: 0.6733, Elapsed: 4m11s
2020-04-24 13:34:50.865802: Epoch: 1, Batch: 75, Loss: 0.6681, Elapsed: 4m32s
2020-04-24 13:35:43.419138: Epoch: 1, Batch: 73, Loss: 0.6857, Elapsed: 6m50s
2020-04-24 13:35:58.483258: Epoch: 1, Batch: 68, Loss: 0.6770, Elapsed: 8m3s
2020-04-24 13:39:13.771841: Epoch: 1, Batch: 76, Loss: 0.6749, Elapsed: 4m22s
2020-04-24 13:43:44.698496: Epoch: 1, Batch: 74, Loss: 0.6741, Elapsed: 8m1s
2020-04-24 13:46:00.862949: Epoch: 1, Batch: 69, Loss: 0.6953, Elapsed: 10m2s
2020-04-24 13:46:44.782093: Epoch: 1, Batch: 77, Loss: 0.6902, Elapsed: 7m30s
2020-04-24 13:49:16.972973: Epoch: 1, Batch: 75, Loss: 0.6623, Elapsed: 5m32s
2020-04-24 13:56:15.541359: Epoch: 1, Batch: 78, Loss: 0.6837, Elapsed: 9m30s
2020-04-24 13:58:43.243684: Epoch: 1, Batch: 70, Loss: 0.6840, Elapsed: 12m42s
2020-04-24 14:01:14.360939: Epoch: 1, Batch: 76, Loss: 0.6703, Elapsed: 11m57s
2020-04-24 14:02:11.623484: Epoch: 1, Batch: 79, Loss: 0.6667, Elapsed: 5m56s
2020-04-24 14:02:32.671885: Epoch: 1, Batch: 71, Loss: 0.6251, Elapsed: 3m49s
2020-04-24 14:07:05.652283: Epoch: 1, Batch: 77, Loss: 0.6760, Elapsed: 5m51s
2020-04-24 14:07:59.033873: Epoch: 1, Batch: 72, Loss: 0.6778, Elapsed: 5m26s
2020-04-24 14:09:08.955701: Epoch: 1, Batch: 80, Loss: 0.6695, Elapsed: 6m57s
2020-04-24 14:13:14.360425: Epoch: 1, Batch: 78, Loss: 0.6509, Elapsed: 6m8s
2020-04-24 14:14:39.540751: Epoch: 1, Batch: 73, Loss: 0.6832, Elapsed: 6m40s
2020-04-24 14:16:54.191193: Epoch: 1, Batch: 81, Loss: 0.6868, Elapsed: 7m45s
2020-04-24 14:23:40.071754: Epoch: 1, Batch: 74, Loss: 0.6940, Elapsed: 9m0s
2020-04-24 14:23:44.083714: Epoch: 1, Batch: 82, Loss: 0.6781, Elapsed: 6m49s
2020-04-24 14:24:03.482071: Epoch: 1, Batch: 79, Loss: 0.6582, Elapsed: 10m49s
2020-04-24 14:28:24.607089: Epoch: 1, Batch: 80, Loss: 0.6922, Elapsed: 4m21s
2020-04-24 14:29:09.188270: Epoch: 1, Batch: 83, Loss: 0.6806, Elapsed: 5m25s
2020-04-24 14:31:22.747321: Epoch: 1, Batch: 75, Loss: 0.6901, Elapsed: 7m42s
2020-04-24 14:34:33.273067: Epoch: 1, Batch: 81, Loss: 0.6597, Elapsed: 6m8s
2020-04-24 14:35:22.437197: Epoch: 1, Batch: 84, Loss: 0.6643, Elapsed: 6m13s
2020-04-24 14:38:10.573831: Epoch: 1, Batch: 76, Loss: 0.6884, Elapsed: 6m47s
2020-04-24 14:40:08.828756: Epoch: 1, Batch: 82, Loss: 0.6849, Elapsed: 5m35s
2020-04-24 14:42:06.062628: Epoch: 1, Batch: 85, Loss: 0.6671, Elapsed: 6m43s
2020-04-24 14:44:07.183329: Epoch: 1, Batch: 83, Loss: 0.6803, Elapsed: 3m58s
2020-04-24 14:46:37.245381: Epoch: 1, Batch: 86, Loss: 0.6664, Elapsed: 4m31s
2020-04-24 14:47:02.516582: Epoch: 1, Batch: 77, Loss: 0.7043, Elapsed: 8m51s
2020-04-24 14:49:10.784359: Epoch: 1, Batch: 84, Loss: 0.6746, Elapsed: 5m3s
2020-04-24 14:49:58.948899: Epoch: 1, Batch: 87, Loss: 0.6546, Elapsed: 3m21s
2020-04-24 14:50:43.268864: Epoch: 1, Batch: 78, Loss: 0.6810, Elapsed: 3m40s
2020-04-24 14:52:35.349937: Epoch: 1, Batch: 85, Loss: 0.6772, Elapsed: 3m24s
2020-04-24 14:54:54.558756: Epoch: 1, Batch: 79, Loss: 0.6659, Elapsed: 4m11s
2020-04-24 14:55:08.259673: Epoch: 1, Batch: 88, Loss: 0.6472, Elapsed: 5m9s
2020-04-24 14:59:19.727719: Epoch: 1, Batch: 86, Loss: 0.6730, Elapsed: 6m44s
2020-04-24 14:59:45.230828: Epoch: 1, Batch: 80, Loss: 0.6849, Elapsed: 4m50s
2020-04-24 14:59:49.930840: Epoch: 1, Batch: 89, Loss: 0.6856, Elapsed: 4m41s
2020-04-24 15:05:04.427645: Epoch: 1, Batch: 87, Loss: 0.6858, Elapsed: 5m44s
2020-04-24 15:05:42.773965: Epoch: 1, Batch: 90, Loss: 0.6620, Elapsed: 5m52s
2020-04-24 15:10:40.783010: Epoch: 1, Batch: 91, Loss: 0.6679, Elapsed: 4m57s
2020-04-24 15:11:01.929295: Epoch: 1, Batch: 81, Loss: 0.6843, Elapsed: 11m16s
2020-04-24 15:14:47.554322: Epoch: 1, Batch: 82, Loss: 0.6649, Elapsed: 3m45s
2020-04-24 15:15:18.263502: Epoch: 1, Batch: 88, Loss: 0.6761, Elapsed: 10m13s
2020-04-24 15:19:11.698780: Epoch: 1, Batch: 92, Loss: 0.6724, Elapsed: 8m30s
2020-04-24 15:19:56.140573: Epoch: 1, Batch: 83, Loss: 0.6529, Elapsed: 5m8s
2020-04-24 15:22:58.905586: Epoch: 1, Batch: 93, Loss: 0.6113, Elapsed: 3m47s
2020-04-24 15:23:15.110178: Epoch: 1, Batch: 84, Loss: 0.6539, Elapsed: 3m18s
2020-04-24 15:23:20.876864: Epoch: 1, Batch: 89, Loss: 0.6623, Elapsed: 8m2s
2020-04-24 15:27:47.688751: Epoch: 1, Batch: 94, Loss: 0.6507, Elapsed: 4m48s
2020-04-24 15:28:47.607952: Epoch: 1, Batch: 85, Loss: 0.6690, Elapsed: 5m32s
2020-04-24 15:30:16.146130: Epoch: 1, Batch: 90, Loss: 0.6872, Elapsed: 6m55s
2020-04-24 15:33:06.181337: Epoch: 1, Batch: 91, Loss: 0.6934, Elapsed: 2m50s
2020-04-24 15:34:32.735720: Epoch: 1, Batch: 95, Loss: 0.6545, Elapsed: 6m45s
2020-04-24 15:37:49.600227: Epoch: 1, Batch: 86, Loss: 0.6986, Elapsed: 9m1s
2020-04-24 15:38:14.462073: Epoch: 1, Batch: 96, Loss: 0.6527, Elapsed: 3m41s
2020-04-24 15:40:32.737202: Epoch: 1, Batch: 92, Loss: 0.6626, Elapsed: 7m26s
2020-04-24 15:47:24.213969: Epoch: 1, Batch: 93, Loss: 0.6909, Elapsed: 6m51s
2020-04-24 15:48:03.929324: Epoch: 1, Batch: 87, Loss: 0.6937, Elapsed: 10m14s
2020-04-24 15:49:47.711265: Epoch: 1, Batch: 97, Loss: 0.6528, Elapsed: 11m33s
2020-04-24 15:51:29.335315: Epoch: 1, Batch: 88, Loss: 0.6259, Elapsed: 3m25s
2020-04-24 15:53:52.340755: Epoch: 1, Batch: 94, Loss: 0.6693, Elapsed: 6m28s
2020-04-24 15:56:11.775596: Epoch: 1, Batch: 95, Loss: 0.6442, Elapsed: 2m19s
2020-04-24 15:56:14.409267: Epoch: 1, Batch: 89, Loss: 0.6413, Elapsed: 4m45s
2020-04-24 15:57:21.164071: Epoch: 1, Batch: 98, Loss: 0.6772, Elapsed: 7m33s
2020-04-24 16:02:02.247864: Epoch: 1, Batch: 96, Loss: 0.6754, Elapsed: 5m50s
2020-04-24 16:03:00.986868: Epoch: 1, Batch: 90, Loss: 0.6816, Elapsed: 6m46s
2020-04-24 16:04:55.953858: Epoch: 1, Batch: 99, Loss: 0.6693, Elapsed: 7m34s
2020-04-24 16:06:56.707064: Epoch: 1, Batch: 91, Loss: 0.6687, Elapsed: 3m55s
2020-04-24 16:07:34.288446: Epoch: 1, Batch: 97, Loss: 0.6634, Elapsed: 5m32s
2020-04-24 16:11:02.568880: Epoch: 1, Batch: 100, Loss: 0.6627, Elapsed: 6m6s
Starting testing the validation set with 200 subgraphs!
2020-04-24 16:11:27.415232: Epoch: 1, Batch: 92, Loss: 0.6708, Elapsed: 4m30s
2020-04-24 16:14:55.139745: Epoch: 1, Batch: 93, Loss: 0.6610, Elapsed: 3m27s
2020-04-24 16:16:40.383973: Epoch: 1, Batch: 98, Loss: 0.6652, Elapsed: 9m6s
2020-04-24 16:22:47.583885: Epoch: 1, Batch: 99, Loss: 0.6751, Elapsed: 6m7s
2020-04-24 16:24:18.621835: Epoch: 1, Batch: 94, Loss: 0.6803, Elapsed: 9m23s
2020-04-24 16:26:45.313713: Epoch: 1, Batch: 100, Loss: 0.6668, Elapsed: 3m57s
Starting testing the validation set with 200 subgraphs!
2020-04-24 16:28:23.716997: Epoch: 1, Batch: 95, Loss: 0.6688, Elapsed: 4m5s
2020-04-24 16:33:04.815331: Epoch: 1, Batch: 96, Loss: 0.6525, Elapsed: 4m41s
2020-04-24 16:37:22.729963: Epoch: 1, Batch: 97, Loss: 0.6569, Elapsed: 4m17s
2020-04-24 16:41:28.608978: Epoch: 1, Batch: 98, Loss: 0.6608, Elapsed: 4m5s
2020-04-24 16:47:18.259469: Epoch: 1, Batch: 99, Loss: 0.6473, Elapsed: 5m49s
2020-04-24 16:51:06.350419: Validation Test:  Loss: 0.6548,  Acc: 61.7708, AUC: 0.6572, Precision: 0.7076 -- Elapsed: 40m3s
2020-04-24 16:52:44.598258: Epoch: 1, Batch: 100, Loss: 0.6737, Elapsed: 5m26s
Starting testing the validation set with 200 subgraphs!
2020-04-24 16:59:04.886449: Epoch: 1, Batch: 101, Loss: 0.6577, Elapsed: 7m58s
2020-04-24 17:05:19.624869: Validation Test:  Loss: 0.6723,  Acc: 58.6900, AUC: 0.6231, Precision: 0.6682 -- Elapsed: 38m34s
2020-04-24 17:06:30.249473: Epoch: 1, Batch: 102, Loss: 0.6620, Elapsed: 7m25s
2020-04-24 17:10:05.553929: Epoch: 1, Batch: 103, Loss: 0.6597, Elapsed: 3m35s
2020-04-24 17:10:52.281414: Epoch: 1, Batch: 101, Loss: 0.6899, Elapsed: 5m32s
2020-04-24 17:17:14.054054: Epoch: 1, Batch: 104, Loss: 0.6537, Elapsed: 7m8s
2020-04-24 17:17:21.141901: Epoch: 1, Batch: 102, Loss: 0.6723, Elapsed: 6m28s
2020-04-24 17:21:06.307815: Epoch: 1, Batch: 103, Loss: 0.6654, Elapsed: 3m45s
2020-04-24 17:22:39.173623: Epoch: 1, Batch: 105, Loss: 0.6518, Elapsed: 5m25s
2020-04-24 17:26:53.346252: Epoch: 1, Batch: 104, Loss: 0.6543, Elapsed: 5m47s
2020-04-24 17:27:59.473187: Epoch: 1, Batch: 106, Loss: 0.6275, Elapsed: 5m20s
2020-04-24 17:31:24.602374: Epoch: 1, Batch: 105, Loss: 0.6738, Elapsed: 4m31s
2020-04-24 17:33:04.527494: Validation Test:  Loss: 0.6742,  Acc: 57.1126, AUC: 0.5943, Precision: 0.6572 -- Elapsed: 40m19s
2020-04-24 17:34:03.683309: Epoch: 1, Batch: 107, Loss: 0.6564, Elapsed: 6m4s
2020-04-24 17:35:22.945429: Epoch: 1, Batch: 106, Loss: 0.6556, Elapsed: 3m58s
2020-04-24 17:39:51.114936: Epoch: 1, Batch: 101, Loss: 0.6629, Elapsed: 6m46s
2020-04-24 17:40:08.016720: Epoch: 1, Batch: 108, Loss: 0.6429, Elapsed: 6m4s
2020-04-24 17:40:48.176484: Epoch: 1, Batch: 107, Loss: 0.6682, Elapsed: 5m25s
2020-04-24 17:43:25.136019: Epoch: 1, Batch: 102, Loss: 0.6430, Elapsed: 3m33s
2020-04-24 17:46:31.156233: Epoch: 1, Batch: 109, Loss: 0.6493, Elapsed: 6m23s
2020-04-24 17:48:27.430580: Epoch: 1, Batch: 108, Loss: 0.6845, Elapsed: 7m39s
2020-04-24 17:48:45.288988: Epoch: 1, Batch: 103, Loss: 0.6406, Elapsed: 5m20s
2020-04-24 17:53:05.838743: Epoch: 1, Batch: 110, Loss: 0.6651, Elapsed: 6m34s
2020-04-24 17:53:17.423266: Epoch: 1, Batch: 104, Loss: 0.6558, Elapsed: 4m32s
2020-04-24 17:53:26.960951: Epoch: 1, Batch: 109, Loss: 0.6708, Elapsed: 4m59s
2020-04-24 17:58:26.046783: Epoch: 1, Batch: 105, Loss: 0.6771, Elapsed: 5m8s
2020-04-24 17:59:06.621260: Epoch: 1, Batch: 110, Loss: 0.6666, Elapsed: 5m39s
2020-04-24 17:59:21.281675: Epoch: 1, Batch: 111, Loss: 0.6664, Elapsed: 6m15s
2020-04-24 18:01:46.752016: Epoch: 1, Batch: 106, Loss: 0.6755, Elapsed: 3m20s
2020-04-24 18:04:00.325812: Epoch: 1, Batch: 112, Loss: 0.6223, Elapsed: 4m39s
2020-04-24 18:05:13.959187: Epoch: 1, Batch: 111, Loss: 0.6806, Elapsed: 6m7s
2020-04-24 18:06:05.084413: Epoch: 1, Batch: 107, Loss: 0.6252, Elapsed: 4m18s
2020-04-24 18:12:03.006686: Epoch: 1, Batch: 113, Loss: 0.6409, Elapsed: 8m2s
2020-04-24 18:13:55.157262: Epoch: 1, Batch: 108, Loss: 0.6703, Elapsed: 7m50s
2020-04-24 18:15:41.027948: Epoch: 1, Batch: 112, Loss: 0.7101, Elapsed: 10m27s
2020-04-24 18:20:49.894075: Epoch: 1, Batch: 109, Loss: 0.6823, Elapsed: 6m54s
2020-04-24 18:23:24.657379: Epoch: 1, Batch: 113, Loss: 0.6758, Elapsed: 7m43s
2020-04-24 18:23:55.846536: Epoch: 1, Batch: 114, Loss: 0.6576, Elapsed: 11m52s
2020-04-24 18:24:54.055381: Epoch: 1, Batch: 110, Loss: 0.6647, Elapsed: 4m4s
2020-04-24 18:31:26.254840: Epoch: 1, Batch: 114, Loss: 0.6974, Elapsed: 8m1s
2020-04-24 18:32:22.901217: Epoch: 1, Batch: 115, Loss: 0.6447, Elapsed: 8m27s
2020-04-24 18:33:15.617910: Epoch: 1, Batch: 111, Loss: 0.6774, Elapsed: 8m21s
2020-04-24 18:37:30.396343: Epoch: 1, Batch: 115, Loss: 0.6867, Elapsed: 6m4s
2020-04-24 18:39:23.225460: Epoch: 1, Batch: 116, Loss: 0.6476, Elapsed: 7m0s
2020-04-24 18:41:46.480160: Epoch: 1, Batch: 116, Loss: 0.6903, Elapsed: 4m16s
2020-04-24 18:42:38.997037: Epoch: 1, Batch: 112, Loss: 0.6604, Elapsed: 9m23s
2020-04-24 18:43:40.728501: Epoch: 1, Batch: 117, Loss: 0.6259, Elapsed: 4m17s
2020-04-24 18:49:07.286694: Epoch: 1, Batch: 117, Loss: 0.6730, Elapsed: 7m20s
2020-04-24 18:49:48.047681: Epoch: 1, Batch: 118, Loss: 0.6613, Elapsed: 6m7s
2020-04-24 18:50:34.754188: Epoch: 1, Batch: 113, Loss: 0.6896, Elapsed: 7m55s
2020-04-24 18:56:03.258544: Epoch: 1, Batch: 114, Loss: 0.6885, Elapsed: 5m28s
2020-04-24 18:56:18.215099: Epoch: 1, Batch: 119, Loss: 0.6465, Elapsed: 6m30s
2020-04-24 18:57:49.048330: Epoch: 1, Batch: 118, Loss: 0.6686, Elapsed: 8m41s
2020-04-24 19:00:19.947087: Epoch: 1, Batch: 120, Loss: 0.5781, Elapsed: 4m1s
2020-04-24 19:03:04.265135: Epoch: 1, Batch: 115, Loss: 0.6569, Elapsed: 7m0s
2020-04-24 19:06:24.801660: Epoch: 1, Batch: 121, Loss: 0.6612, Elapsed: 6m4s
2020-04-24 19:07:05.424372: Epoch: 1, Batch: 119, Loss: 0.6734, Elapsed: 9m16s
2020-04-24 19:10:05.965928: Epoch: 1, Batch: 122, Loss: 0.5904, Elapsed: 3m41s
2020-04-24 19:13:54.371525: Epoch: 1, Batch: 116, Loss: 0.7002, Elapsed: 10m50s
2020-04-24 19:14:18.217335: Epoch: 1, Batch: 120, Loss: 0.6758, Elapsed: 7m12s
2020-04-24 19:17:12.068377: Epoch: 1, Batch: 123, Loss: 0.6639, Elapsed: 7m6s
2020-04-24 19:19:17.435952: Epoch: 1, Batch: 121, Loss: 0.6644, Elapsed: 4m59s
2020-04-24 19:21:13.598518: Epoch: 1, Batch: 117, Loss: 0.6872, Elapsed: 7m19s
2020-04-24 19:23:04.530008: Epoch: 1, Batch: 122, Loss: 0.6584, Elapsed: 3m47s
2020-04-24 19:23:24.699789: Epoch: 1, Batch: 124, Loss: 0.6102, Elapsed: 6m12s
2020-04-24 19:26:00.714277: Epoch: 1, Batch: 125, Loss: 0.5827, Elapsed: 2m35s
2020-04-24 19:28:11.507924: Epoch: 1, Batch: 118, Loss: 0.6717, Elapsed: 6m57s
2020-04-24 19:31:06.179191: Epoch: 1, Batch: 123, Loss: 0.6537, Elapsed: 8m1s
2020-04-24 19:31:42.184343: Epoch: 1, Batch: 126, Loss: 0.6220, Elapsed: 5m41s
2020-04-24 19:33:47.311014: Epoch: 1, Batch: 119, Loss: 0.6817, Elapsed: 5m35s
2020-04-24 19:35:48.877567: Epoch: 1, Batch: 127, Loss: 0.5870, Elapsed: 4m6s
2020-04-24 19:36:21.609242: Epoch: 1, Batch: 124, Loss: 0.6664, Elapsed: 5m15s
2020-04-24 19:41:42.437581: Epoch: 1, Batch: 120, Loss: 0.7061, Elapsed: 7m55s
2020-04-24 19:43:22.576229: Epoch: 1, Batch: 125, Loss: 0.6581, Elapsed: 7m0s
2020-04-24 19:45:22.385896: Epoch: 1, Batch: 128, Loss: 0.6734, Elapsed: 9m33s
2020-04-24 19:48:46.432849: Epoch: 1, Batch: 126, Loss: 0.6641, Elapsed: 5m23s
2020-04-24 19:48:55.476430: Epoch: 1, Batch: 121, Loss: 0.6634, Elapsed: 7m13s
2020-04-24 19:55:12.574300: Epoch: 1, Batch: 129, Loss: 0.6763, Elapsed: 9m50s
2020-04-24 19:55:27.372239: Epoch: 1, Batch: 127, Loss: 0.6737, Elapsed: 6m40s
2020-04-24 19:55:56.918006: Epoch: 1, Batch: 122, Loss: 0.6796, Elapsed: 7m1s
2020-04-24 20:01:08.871884: Epoch: 1, Batch: 130, Loss: 0.6415, Elapsed: 5m56s
2020-04-24 20:01:47.357960: Epoch: 1, Batch: 123, Loss: 0.6942, Elapsed: 5m50s
2020-04-24 20:02:47.780737: Epoch: 1, Batch: 128, Loss: 0.6567, Elapsed: 7m20s
2020-04-24 20:07:17.318243: Epoch: 1, Batch: 124, Loss: 0.6641, Elapsed: 5m29s
2020-04-24 20:08:03.416477: Epoch: 1, Batch: 129, Loss: 0.6584, Elapsed: 5m15s
2020-04-24 20:08:45.081917: Epoch: 1, Batch: 131, Loss: 0.6500, Elapsed: 7m36s
2020-04-24 20:12:05.574875: Epoch: 1, Batch: 125, Loss: 0.6543, Elapsed: 4m48s
2020-04-24 20:15:17.394225: Epoch: 1, Batch: 130, Loss: 0.6641, Elapsed: 7m13s
2020-04-24 20:17:11.142342: Epoch: 1, Batch: 126, Loss: 0.6592, Elapsed: 5m5s
2020-04-24 20:18:08.008595: Epoch: 1, Batch: 132, Loss: 0.6746, Elapsed: 9m22s
2020-04-24 20:23:06.970088: Epoch: 1, Batch: 133, Loss: 0.6433, Elapsed: 4m58s
2020-04-24 20:23:40.006134: Epoch: 1, Batch: 127, Loss: 0.6624, Elapsed: 6m28s
2020-04-24 20:25:02.365517: Epoch: 1, Batch: 131, Loss: 0.7382, Elapsed: 9m44s
2020-04-24 20:29:10.019785: Epoch: 1, Batch: 132, Loss: 0.6547, Elapsed: 4m7s
2020-04-24 20:30:26.004198: Epoch: 1, Batch: 134, Loss: 0.6645, Elapsed: 7m19s
2020-04-24 20:31:30.492263: Epoch: 1, Batch: 128, Loss: 0.6630, Elapsed: 7m50s
2020-04-24 20:37:31.727162: Epoch: 1, Batch: 129, Loss: 0.6455, Elapsed: 6m1s
2020-04-24 20:40:14.813232: Epoch: 1, Batch: 135, Loss: 0.6583, Elapsed: 9m48s
2020-04-24 20:43:23.801681: Epoch: 1, Batch: 133, Loss: 0.6586, Elapsed: 14m13s
2020-04-24 20:46:54.050011: Epoch: 1, Batch: 130, Loss: 0.6681, Elapsed: 9m22s
2020-04-24 20:50:11.237484: Epoch: 1, Batch: 136, Loss: 0.6566, Elapsed: 9m56s
2020-04-24 20:50:19.932551: Epoch: 1, Batch: 134, Loss: 0.6586, Elapsed: 6m56s
2020-04-24 20:53:03.370838: Epoch: 1, Batch: 131, Loss: 0.6426, Elapsed: 6m9s
2020-04-24 20:54:39.904133: Epoch: 1, Batch: 135, Loss: 0.6586, Elapsed: 4m19s
2020-04-24 20:57:15.964271: Epoch: 1, Batch: 137, Loss: 0.6455, Elapsed: 7m4s
2020-04-24 21:00:42.215000: Epoch: 1, Batch: 132, Loss: 0.6735, Elapsed: 7m38s
2020-04-24 21:01:46.707475: Epoch: 1, Batch: 136, Loss: 0.6791, Elapsed: 7m6s
2020-04-24 21:02:40.595006: Epoch: 1, Batch: 138, Loss: 0.6371, Elapsed: 5m24s
2020-04-24 21:07:34.274461: Epoch: 1, Batch: 133, Loss: 0.6531, Elapsed: 6m52s
2020-04-24 21:10:36.029707: Epoch: 1, Batch: 139, Loss: 0.6524, Elapsed: 7m55s
2020-04-24 21:12:09.019867: Epoch: 1, Batch: 134, Loss: 0.6682, Elapsed: 4m34s
2020-04-24 21:15:04.750001: Epoch: 1, Batch: 140, Loss: 0.6113, Elapsed: 4m28s
2020-04-24 21:16:04.623858: Epoch: 1, Batch: 137, Loss: 0.6711, Elapsed: 14m17s
2020-04-24 21:19:22.547551: Epoch: 1, Batch: 141, Loss: 0.6049, Elapsed: 4m17s
2020-04-24 21:20:14.224275: Epoch: 1, Batch: 135, Loss: 0.6758, Elapsed: 8m5s
2020-04-24 21:21:57.250948: Epoch: 1, Batch: 138, Loss: 0.6530, Elapsed: 5m52s
2020-04-24 21:24:15.936613: Epoch: 1, Batch: 136, Loss: 0.6513, Elapsed: 4m1s
2020-04-24 21:28:49.972919: Epoch: 1, Batch: 139, Loss: 0.6603, Elapsed: 6m52s
2020-04-24 21:29:05.950364: Epoch: 1, Batch: 137, Loss: 0.6539, Elapsed: 4m49s
2020-04-24 21:29:50.257426: Epoch: 1, Batch: 142, Loss: 0.6380, Elapsed: 10m27s
2020-04-24 21:36:07.319964: Epoch: 1, Batch: 140, Loss: 0.6678, Elapsed: 7m17s
2020-04-24 21:37:02.011607: Epoch: 1, Batch: 138, Loss: 0.6769, Elapsed: 7m56s
2020-04-24 21:40:01.096140: Epoch: 1, Batch: 143, Loss: 0.6569, Elapsed: 10m10s
2020-04-24 21:41:59.468282: Epoch: 1, Batch: 141, Loss: 0.6553, Elapsed: 5m52s
2020-04-24 21:43:01.852321: Epoch: 1, Batch: 139, Loss: 0.6438, Elapsed: 5m59s
2020-04-24 21:44:11.514162: Epoch: 1, Batch: 144, Loss: 0.6097, Elapsed: 4m10s
2020-04-24 21:46:53.957089: Epoch: 1, Batch: 140, Loss: 0.6462, Elapsed: 3m52s
2020-04-24 21:47:01.716289: Epoch: 1, Batch: 142, Loss: 0.6576, Elapsed: 5m2s
2020-04-24 21:49:23.465171: Epoch: 1, Batch: 145, Loss: 0.6348, Elapsed: 5m11s
2020-04-24 21:52:57.909208: Epoch: 1, Batch: 143, Loss: 0.6628, Elapsed: 5m56s
2020-04-24 21:53:24.323599: Epoch: 1, Batch: 141, Loss: 0.6519, Elapsed: 6m30s
2020-04-24 21:55:38.426349: Epoch: 1, Batch: 146, Loss: 0.6388, Elapsed: 6m14s
2020-04-24 21:57:09.050259: Epoch: 1, Batch: 144, Loss: 0.6472, Elapsed: 4m11s
2020-04-24 21:58:24.311883: Epoch: 1, Batch: 142, Loss: 0.6523, Elapsed: 4m59s
2020-04-24 22:02:27.296572: Epoch: 1, Batch: 143, Loss: 0.6532, Elapsed: 4m2s
2020-04-24 22:03:08.108734: Epoch: 1, Batch: 147, Loss: 0.6364, Elapsed: 7m29s
2020-04-24 22:06:10.614472: Epoch: 1, Batch: 145, Loss: 0.6473, Elapsed: 9m1s
2020-04-24 22:06:25.023620: Epoch: 1, Batch: 144, Loss: 0.6858, Elapsed: 3m57s
2020-04-24 22:07:24.156115: Epoch: 1, Batch: 148, Loss: 0.5887, Elapsed: 4m16s
2020-04-24 22:09:12.700627: Epoch: 1, Batch: 146, Loss: 0.6348, Elapsed: 3m2s
2020-04-24 22:12:32.145605: Epoch: 1, Batch: 145, Loss: 0.6565, Elapsed: 6m7s
2020-04-24 22:12:49.089410: Epoch: 1, Batch: 149, Loss: 0.5854, Elapsed: 5m24s
2020-04-24 22:17:28.299380: Epoch: 1, Batch: 150, Loss: 0.6127, Elapsed: 4m39s
Starting testing the validation set with 200 subgraphs!
2020-04-24 22:18:13.798620: Epoch: 1, Batch: 147, Loss: 0.6671, Elapsed: 9m1s
2020-04-24 22:19:17.057395: Epoch: 1, Batch: 146, Loss: 0.6515, Elapsed: 6m44s
2020-04-24 22:23:57.416064: Epoch: 1, Batch: 148, Loss: 0.6519, Elapsed: 5m43s
2020-04-24 22:27:22.931085: Epoch: 1, Batch: 147, Loss: 0.6928, Elapsed: 8m5s
2020-04-24 22:31:56.600284: Epoch: 1, Batch: 149, Loss: 0.6683, Elapsed: 7m59s
2020-04-24 22:34:00.502338: Epoch: 1, Batch: 148, Loss: 0.6525, Elapsed: 6m37s
2020-04-24 22:37:18.493635: Epoch: 1, Batch: 150, Loss: 0.6518, Elapsed: 5m21s
Starting testing the validation set with 200 subgraphs!
2020-04-24 22:40:14.846728: Epoch: 1, Batch: 149, Loss: 0.6963, Elapsed: 6m14s
2020-04-24 22:44:10.564257: Epoch: 1, Batch: 150, Loss: 0.6360, Elapsed: 3m55s
Starting testing the validation set with 200 subgraphs!
2020-04-24 22:56:23.849313: Validation Test:  Loss: 0.6281,  Acc: 66.8540, AUC: 0.7027, Precision: 0.7321 -- Elapsed: 38m55s
2020-04-24 23:01:06.531113: Epoch: 1, Batch: 151, Loss: 0.5837, Elapsed: 4m42s
2020-04-24 23:04:32.197209: Epoch: 1, Batch: 152, Loss: 0.6495, Elapsed: 3m25s
2020-04-24 23:10:20.662386: Epoch: 1, Batch: 153, Loss: 0.6623, Elapsed: 5m48s
2020-04-24 23:14:06.389493: Validation Test:  Loss: 0.6553,  Acc: 61.2646, AUC: 0.6572, Precision: 0.7044 -- Elapsed: 36m47s
2020-04-24 23:14:13.960068: Epoch: 1, Batch: 154, Loss: 0.6621, Elapsed: 3m53s
2020-04-24 23:21:35.456826: Validation Test:  Loss: 0.6629,  Acc: 60.4226, AUC: 0.6261, Precision: 0.6833 -- Elapsed: 37m24s
2020-04-24 23:21:36.751827: Epoch: 1, Batch: 151, Loss: 0.6542, Elapsed: 7m30s
2020-04-24 23:23:21.098668: Epoch: 1, Batch: 155, Loss: 0.6443, Elapsed: 9m7s
2020-04-24 23:25:08.739075: Epoch: 1, Batch: 152, Loss: 0.6402, Elapsed: 3m31s
2020-04-24 23:26:33.508293: Epoch: 1, Batch: 151, Loss: 0.6592, Elapsed: 4m58s
2020-04-24 23:30:42.136950: Epoch: 1, Batch: 156, Loss: 0.6293, Elapsed: 7m21s
2020-04-24 23:32:55.081999: Epoch: 1, Batch: 152, Loss: 0.6469, Elapsed: 6m21s
2020-04-24 23:33:05.226380: Epoch: 1, Batch: 153, Loss: 0.6551, Elapsed: 7m56s
2020-04-24 23:35:56.209338: Epoch: 1, Batch: 157, Loss: 0.6258, Elapsed: 5m14s
2020-04-24 23:37:35.729103: Epoch: 1, Batch: 153, Loss: 0.6542, Elapsed: 4m40s
2020-04-24 23:38:02.368647: Epoch: 1, Batch: 154, Loss: 0.6588, Elapsed: 4m57s
2020-04-24 23:40:08.339849: Epoch: 1, Batch: 154, Loss: 0.5877, Elapsed: 2m32s
2020-04-24 23:45:23.165194: Epoch: 1, Batch: 155, Loss: 0.6675, Elapsed: 5m14s
2020-04-24 23:46:16.132363: Epoch: 1, Batch: 155, Loss: 0.6633, Elapsed: 8m13s
2020-04-24 23:51:07.159887: Epoch: 1, Batch: 156, Loss: 0.6535, Elapsed: 4m51s
2020-04-24 23:51:33.220322: Epoch: 1, Batch: 158, Loss: 0.6362, Elapsed: 15m36s
2020-04-24 23:51:53.752478: Epoch: 1, Batch: 156, Loss: 0.6794, Elapsed: 6m30s
2020-04-24 23:57:01.766804: Epoch: 1, Batch: 159, Loss: 0.6241, Elapsed: 5m28s
2020-04-24 23:59:39.079769: Epoch: 1, Batch: 157, Loss: 0.6634, Elapsed: 8m31s
2020-04-25 00:01:42.618194: Epoch: 1, Batch: 157, Loss: 0.6757, Elapsed: 9m48s
2020-04-25 00:04:31.534297: Epoch: 1, Batch: 158, Loss: 0.6220, Elapsed: 4m52s
2020-04-25 00:05:57.728351: Epoch: 1, Batch: 160, Loss: 0.6360, Elapsed: 8m55s
2020-04-25 00:10:31.371882: Epoch: 1, Batch: 158, Loss: 0.6945, Elapsed: 8m48s
2020-04-25 00:11:00.203312: Epoch: 1, Batch: 161, Loss: 0.6129, Elapsed: 5m2s
2020-04-25 00:11:14.634002: Epoch: 1, Batch: 159, Loss: 0.6920, Elapsed: 6m43s
2020-04-25 00:14:50.204849: Epoch: 1, Batch: 160, Loss: 0.6205, Elapsed: 3m35s
2020-04-25 00:16:18.373756: Epoch: 1, Batch: 159, Loss: 0.6504, Elapsed: 5m46s
2020-04-25 00:16:53.290337: Epoch: 1, Batch: 162, Loss: 0.6109, Elapsed: 5m53s
2020-04-25 00:22:26.070757: Epoch: 1, Batch: 161, Loss: 0.6658, Elapsed: 7m35s
2020-04-25 00:22:27.511924: Epoch: 1, Batch: 163, Loss: 0.6243, Elapsed: 5m34s
2020-04-25 00:27:16.483396: Epoch: 1, Batch: 160, Loss: 0.7017, Elapsed: 10m58s
2020-04-25 00:29:00.839669: Epoch: 1, Batch: 164, Loss: 0.6178, Elapsed: 6m33s
2020-04-25 00:30:20.241340: Epoch: 1, Batch: 162, Loss: 0.6765, Elapsed: 7m54s
2020-04-25 00:31:04.270486: Epoch: 1, Batch: 161, Loss: 0.6168, Elapsed: 3m47s
2020-04-25 00:33:40.132366: Epoch: 1, Batch: 165, Loss: 0.6059, Elapsed: 4m39s
2020-04-25 00:37:28.073341: Epoch: 1, Batch: 163, Loss: 0.6648, Elapsed: 7m7s
2020-04-25 00:37:50.344054: Epoch: 1, Batch: 162, Loss: 0.6671, Elapsed: 6m46s
2020-04-25 00:39:51.887092: Epoch: 1, Batch: 166, Loss: 0.6595, Elapsed: 6m11s
2020-04-25 00:43:14.903219: Epoch: 1, Batch: 164, Loss: 0.6419, Elapsed: 5m46s
2020-04-25 00:44:25.516634: Epoch: 1, Batch: 163, Loss: 0.6614, Elapsed: 6m35s
2020-04-25 00:46:58.820727: Epoch: 1, Batch: 167, Loss: 0.6112, Elapsed: 7m6s
2020-04-25 00:48:22.011646: Epoch: 1, Batch: 165, Loss: 0.6613, Elapsed: 5m7s
2020-04-25 00:49:17.761776: Epoch: 1, Batch: 164, Loss: 0.6527, Elapsed: 4m52s
2020-04-25 00:52:35.659725: Epoch: 1, Batch: 168, Loss: 0.6179, Elapsed: 5m36s
2020-04-25 00:55:29.692014: Epoch: 1, Batch: 166, Loss: 0.6626, Elapsed: 7m7s
2020-04-25 00:55:42.548042: Epoch: 1, Batch: 165, Loss: 0.6734, Elapsed: 6m24s
2020-04-25 00:57:19.018808: Epoch: 1, Batch: 169, Loss: 0.6336, Elapsed: 4m43s
2020-04-25 01:00:06.487231: Epoch: 1, Batch: 167, Loss: 0.6500, Elapsed: 4m36s
2020-04-25 01:01:54.016916: Epoch: 1, Batch: 166, Loss: 0.6528, Elapsed: 6m11s
2020-04-25 01:02:24.641609: Epoch: 1, Batch: 170, Loss: 0.5894, Elapsed: 5m5s
2020-04-25 01:06:35.151677: Epoch: 1, Batch: 171, Loss: 0.5934, Elapsed: 4m10s
2020-04-25 01:07:58.786506: Epoch: 1, Batch: 167, Loss: 0.6417, Elapsed: 6m4s
2020-04-25 01:09:09.211234: Epoch: 1, Batch: 168, Loss: 0.6528, Elapsed: 9m2s
2020-04-25 01:11:37.634025: Epoch: 1, Batch: 169, Loss: 0.6071, Elapsed: 2m28s
2020-04-25 01:13:04.543487: Epoch: 1, Batch: 172, Loss: 0.6355, Elapsed: 6m29s
2020-04-25 01:14:50.326559: Epoch: 1, Batch: 168, Loss: 0.6869, Elapsed: 6m51s
2020-04-25 01:17:50.045037: Epoch: 1, Batch: 173, Loss: 0.6034, Elapsed: 4m45s
2020-04-25 01:18:53.604479: Epoch: 1, Batch: 170, Loss: 0.6335, Elapsed: 7m15s
2020-04-25 01:22:38.999689: Epoch: 1, Batch: 169, Loss: 0.6550, Elapsed: 7m48s
2020-04-25 01:26:32.126807: Epoch: 1, Batch: 174, Loss: 0.6151, Elapsed: 8m42s
2020-04-25 01:29:17.913040: Epoch: 1, Batch: 170, Loss: 0.6937, Elapsed: 6m38s
2020-04-25 01:30:58.164643: Epoch: 1, Batch: 171, Loss: 0.6902, Elapsed: 12m4s
2020-04-25 01:32:22.875712: Epoch: 1, Batch: 175, Loss: 0.5912, Elapsed: 5m50s
2020-04-25 01:36:19.648523: Epoch: 1, Batch: 171, Loss: 0.6544, Elapsed: 7m1s
2020-04-25 01:38:19.954414: Epoch: 1, Batch: 172, Loss: 0.6598, Elapsed: 7m21s
2020-04-25 01:39:15.882713: Epoch: 1, Batch: 176, Loss: 0.6053, Elapsed: 6m52s
2020-04-25 01:42:50.501469: Epoch: 1, Batch: 172, Loss: 0.6592, Elapsed: 6m30s
2020-04-25 01:44:24.954283: Epoch: 1, Batch: 177, Loss: 0.5887, Elapsed: 5m9s
2020-04-25 01:47:42.894102: Epoch: 1, Batch: 173, Loss: 0.6622, Elapsed: 9m22s
2020-04-25 01:49:17.830379: Epoch: 1, Batch: 178, Loss: 0.5963, Elapsed: 4m52s
2020-04-25 01:52:06.321705: Epoch: 1, Batch: 173, Loss: 0.6797, Elapsed: 9m15s
2020-04-25 01:57:38.284382: Epoch: 1, Batch: 179, Loss: 0.6384, Elapsed: 8m20s
2020-04-25 01:57:48.181341: Epoch: 1, Batch: 174, Loss: 0.6831, Elapsed: 10m5s
2020-04-25 01:57:59.842077: Epoch: 1, Batch: 174, Loss: 0.6532, Elapsed: 5m53s
2020-04-25 02:00:07.678845: Epoch: 1, Batch: 180, Loss: 0.5899, Elapsed: 2m29s
2020-04-25 02:03:50.874928: Epoch: 1, Batch: 181, Loss: 0.5952, Elapsed: 3m43s
2020-04-25 02:03:59.723714: Epoch: 1, Batch: 175, Loss: 0.6690, Elapsed: 5m59s
2020-04-25 02:04:35.538133: Epoch: 1, Batch: 175, Loss: 0.6535, Elapsed: 6m47s
2020-04-25 02:06:31.535918: Epoch: 1, Batch: 176, Loss: 0.6347, Elapsed: 2m31s
2020-04-25 02:10:39.856128: Epoch: 1, Batch: 176, Loss: 0.6645, Elapsed: 6m4s
2020-04-25 02:13:36.469685: Epoch: 1, Batch: 182, Loss: 0.6282, Elapsed: 9m45s
2020-04-25 02:14:05.689810: Epoch: 1, Batch: 177, Loss: 0.6542, Elapsed: 7m34s
2020-04-25 02:16:02.169142: Epoch: 1, Batch: 177, Loss: 0.6249, Elapsed: 5m22s
2020-04-25 02:20:49.417921: Epoch: 1, Batch: 183, Loss: 0.6468, Elapsed: 7m12s
2020-04-25 02:23:14.968609: Epoch: 1, Batch: 178, Loss: 0.6288, Elapsed: 7m12s
2020-04-25 02:24:14.144280: Epoch: 1, Batch: 178, Loss: 0.6797, Elapsed: 10m8s
2020-04-25 02:27:49.971095: Epoch: 1, Batch: 179, Loss: 0.6664, Elapsed: 4m34s
2020-04-25 02:27:55.810733: Epoch: 1, Batch: 184, Loss: 0.6248, Elapsed: 7m6s
2020-04-25 02:31:12.537978: Epoch: 1, Batch: 180, Loss: 0.6110, Elapsed: 3m22s
2020-04-25 02:31:41.653925: Epoch: 1, Batch: 179, Loss: 0.6836, Elapsed: 7m27s
2020-04-25 02:34:34.895821: Epoch: 1, Batch: 185, Loss: 0.6154, Elapsed: 6m39s
2020-04-25 02:36:44.730534: Epoch: 1, Batch: 181, Loss: 0.6283, Elapsed: 5m32s
2020-04-25 02:38:31.780370: Epoch: 1, Batch: 180, Loss: 0.6632, Elapsed: 6m50s
2020-04-25 02:43:52.524659: Epoch: 1, Batch: 182, Loss: 0.6430, Elapsed: 7m7s
2020-04-25 02:44:33.420940: Epoch: 1, Batch: 186, Loss: 0.6410, Elapsed: 9m58s
2020-04-25 02:45:48.845385: Epoch: 1, Batch: 181, Loss: 0.6595, Elapsed: 7m17s
2020-04-25 02:50:27.989797: Epoch: 1, Batch: 182, Loss: 0.6563, Elapsed: 4m39s
2020-04-25 02:50:24.089691: Epoch: 1, Batch: 187, Loss: 0.6014, Elapsed: 5m50s
2020-04-25 02:51:57.771567: Epoch: 1, Batch: 183, Loss: 0.6572, Elapsed: 8m5s
2020-04-25 02:56:21.132788: Epoch: 1, Batch: 184, Loss: 0.6314, Elapsed: 4m23s
2020-04-25 02:57:08.362950: Epoch: 1, Batch: 188, Loss: 0.6545, Elapsed: 6m44s
2020-04-25 02:57:47.956545: Epoch: 1, Batch: 183, Loss: 0.6661, Elapsed: 7m19s
2020-04-25 03:02:38.688997: Epoch: 1, Batch: 185, Loss: 0.6761, Elapsed: 6m17s
2020-04-25 03:03:45.465608: Epoch: 1, Batch: 184, Loss: 0.6591, Elapsed: 5m57s
2020-04-25 03:04:20.091889: Epoch: 1, Batch: 189, Loss: 0.6098, Elapsed: 7m11s
2020-04-25 03:09:47.857284: Epoch: 1, Batch: 186, Loss: 0.6624, Elapsed: 7m9s
2020-04-25 03:11:55.560706: Epoch: 1, Batch: 190, Loss: 0.6395, Elapsed: 7m35s
2020-04-25 03:17:25.378173: Epoch: 1, Batch: 185, Loss: 0.6781, Elapsed: 13m39s
2020-04-25 03:19:36.460260: Epoch: 1, Batch: 187, Loss: 0.6333, Elapsed: 9m48s
2020-04-25 03:20:40.837568: Epoch: 1, Batch: 191, Loss: 0.6191, Elapsed: 8m45s
2020-04-25 03:22:53.869003: Epoch: 1, Batch: 186, Loss: 0.6426, Elapsed: 5m28s
2020-04-25 03:25:52.455573: Epoch: 1, Batch: 188, Loss: 0.6828, Elapsed: 6m15s
2020-04-25 03:26:03.389676: Epoch: 1, Batch: 192, Loss: 0.6183, Elapsed: 5m22s
2020-04-25 03:27:28.543312: Epoch: 1, Batch: 187, Loss: 0.6308, Elapsed: 4m34s
2020-04-25 03:31:04.563451: Epoch: 1, Batch: 189, Loss: 0.6482, Elapsed: 5m12s
2020-04-25 03:33:59.192914: Epoch: 1, Batch: 188, Loss: 0.6463, Elapsed: 6m30s
2020-04-25 03:34:49.245052: Epoch: 1, Batch: 193, Loss: 0.6245, Elapsed: 8m45s
2020-04-25 03:38:13.603176: Epoch: 1, Batch: 190, Loss: 0.6728, Elapsed: 7m9s
2020-04-25 03:41:35.631390: Epoch: 1, Batch: 194, Loss: 0.6422, Elapsed: 6m46s
2020-04-25 03:43:36.520324: Epoch: 1, Batch: 189, Loss: 0.6659, Elapsed: 9m37s
2020-04-25 03:44:28.151023: Epoch: 1, Batch: 191, Loss: 0.6818, Elapsed: 6m14s
2020-04-25 03:46:24.282373: Epoch: 1, Batch: 195, Loss: 0.5756, Elapsed: 4m48s
2020-04-25 03:50:51.524921: Epoch: 1, Batch: 190, Loss: 0.6684, Elapsed: 7m14s
2020-04-25 03:52:39.302205: Epoch: 1, Batch: 192, Loss: 0.6498, Elapsed: 8m11s
2020-04-25 03:54:44.011895: Epoch: 1, Batch: 196, Loss: 0.6019, Elapsed: 8m19s
2020-04-25 03:57:26.857674: Epoch: 1, Batch: 191, Loss: 0.6697, Elapsed: 6m35s
2020-04-25 03:57:42.881226: Epoch: 1, Batch: 197, Loss: 0.5555, Elapsed: 2m58s
2020-04-25 03:58:00.559227: Epoch: 1, Batch: 193, Loss: 0.6602, Elapsed: 5m21s
2020-04-25 04:00:39.557350: Epoch: 1, Batch: 192, Loss: 0.6140, Elapsed: 3m12s
2020-04-25 04:04:08.651258: Epoch: 1, Batch: 194, Loss: 0.6238, Elapsed: 6m8s
2020-04-25 04:04:17.372417: Epoch: 1, Batch: 198, Loss: 0.6496, Elapsed: 6m34s
2020-04-25 04:04:58.043259: Epoch: 1, Batch: 193, Loss: 0.6417, Elapsed: 4m18s
2020-04-25 04:10:32.944274: Epoch: 1, Batch: 199, Loss: 0.6262, Elapsed: 6m15s
2020-04-25 04:11:03.569557: Epoch: 1, Batch: 195, Loss: 0.6308, Elapsed: 6m54s
2020-04-25 04:12:25.217778: Epoch: 1, Batch: 194, Loss: 0.6636, Elapsed: 7m27s
2020-04-25 04:13:22.950756: Epoch: 1, Batch: 200, Loss: 0.6006, Elapsed: 2m49s
Starting testing the validation set with 200 subgraphs!
2020-04-25 04:18:25.984297: Epoch: 1, Batch: 195, Loss: 0.6512, Elapsed: 6m0s
2020-04-25 04:19:24.671637: Epoch: 1, Batch: 196, Loss: 0.6406, Elapsed: 8m21s
2020-04-25 04:23:18.939789: Epoch: 1, Batch: 196, Loss: 0.6783, Elapsed: 4m52s
2020-04-25 04:23:29.810447: Epoch: 1, Batch: 197, Loss: 0.6584, Elapsed: 4m5s
2020-04-25 04:28:29.181990: Epoch: 1, Batch: 198, Loss: 0.6495, Elapsed: 4m59s
2020-04-25 04:28:44.307952: Epoch: 1, Batch: 197, Loss: 0.6849, Elapsed: 5m25s
2020-04-25 04:32:48.724398: Epoch: 1, Batch: 199, Loss: 0.6194, Elapsed: 4m19s
2020-04-25 04:35:15.720315: Epoch: 1, Batch: 198, Loss: 0.6516, Elapsed: 6m31s
2020-04-25 04:39:19.828982: Epoch: 1, Batch: 200, Loss: 0.6520, Elapsed: 6m31s
Starting testing the validation set with 200 subgraphs!
2020-04-25 04:40:37.786267: Epoch: 1, Batch: 199, Loss: 0.6454, Elapsed: 5m22s
2020-04-25 04:45:48.304366: Epoch: 1, Batch: 200, Loss: 0.6680, Elapsed: 5m10s
Starting testing the validation set with 200 subgraphs!
2020-04-25 04:53:36.939197: Validation Test:  Loss: 0.6218,  Acc: 66.0932, AUC: 0.7085, Precision: 0.7401 -- Elapsed: 40m13s
2020-04-25 04:58:37.921700: Epoch: 1, Batch: 201, Loss: 0.6228, Elapsed: 5m0s
2020-04-25 05:04:13.166028: Epoch: 1, Batch: 202, Loss: 0.6155, Elapsed: 5m35s
2020-04-25 05:09:32.433054: Epoch: 1, Batch: 203, Loss: 0.6055, Elapsed: 5m19s
2020-04-25 05:15:36.064546: Epoch: 1, Batch: 204, Loss: 0.6145, Elapsed: 6m3s
2020-04-25 05:16:44.940768: Validation Test:  Loss: 0.6459,  Acc: 63.1047, AUC: 0.6785, Precision: 0.7265 -- Elapsed: 37m25s
2020-04-25 05:22:57.643951: Epoch: 1, Batch: 201, Loss: 0.6324, Elapsed: 6m12s
2020-04-25 05:23:46.458446: Validation Test:  Loss: 0.6583,  Acc: 61.4614, AUC: 0.6395, Precision: 0.6949 -- Elapsed: 37m58s
2020-04-25 05:25:43.264497: Epoch: 1, Batch: 205, Loss: 0.6417, Elapsed: 10m7s
2020-04-25 05:29:06.849715: Epoch: 1, Batch: 202, Loss: 0.6429, Elapsed: 6m9s
2020-04-25 05:32:20.421014: Epoch: 1, Batch: 206, Loss: 0.6043, Elapsed: 6m37s
2020-04-25 05:32:29.152910: Epoch: 1, Batch: 203, Loss: 0.6192, Elapsed: 3m22s
2020-04-25 05:32:50.480806: Epoch: 1, Batch: 201, Loss: 0.6767, Elapsed: 9m4s
2020-04-25 05:36:20.136259: Epoch: 1, Batch: 202, Loss: 0.6399, Elapsed: 3m29s
2020-04-25 05:37:04.321410: Epoch: 1, Batch: 204, Loss: 0.6154, Elapsed: 4m35s
2020-04-25 05:39:53.063733: Epoch: 1, Batch: 207, Loss: 0.6212, Elapsed: 7m32s
2020-04-25 05:42:55.818699: Epoch: 1, Batch: 208, Loss: 0.5572, Elapsed: 3m2s
2020-04-25 05:43:06.857554: Epoch: 1, Batch: 205, Loss: 0.6502, Elapsed: 6m2s
2020-04-25 05:44:02.384769: Epoch: 1, Batch: 203, Loss: 0.6810, Elapsed: 7m42s
2020-04-25 05:46:57.245968: Epoch: 1, Batch: 209, Loss: 0.5890, Elapsed: 4m1s
2020-04-25 05:50:07.287929: Epoch: 1, Batch: 206, Loss: 0.6447, Elapsed: 7m0s
2020-04-25 05:51:27.482433: Epoch: 1, Batch: 204, Loss: 0.6565, Elapsed: 7m25s
2020-04-25 05:53:29.217251: Epoch: 1, Batch: 210, Loss: 0.6142, Elapsed: 6m31s
2020-04-25 05:56:00.699585: Epoch: 1, Batch: 205, Loss: 0.6368, Elapsed: 4m33s
2020-04-25 05:59:50.541766: Epoch: 1, Batch: 207, Loss: 0.6520, Elapsed: 9m43s
2020-04-25 06:03:45.390904: Epoch: 1, Batch: 211, Loss: 0.6388, Elapsed: 10m16s
2020-04-25 06:04:31.755173: Epoch: 1, Batch: 206, Loss: 0.6552, Elapsed: 8m31s
2020-04-25 06:06:34.110139: Epoch: 1, Batch: 208, Loss: 0.6293, Elapsed: 6m43s
2020-04-25 06:10:51.943007: Epoch: 1, Batch: 212, Loss: 0.6316, Elapsed: 7m6s
2020-04-25 06:13:52.347072: Epoch: 1, Batch: 207, Loss: 0.6672, Elapsed: 9m20s
2020-04-25 06:16:07.179124: Epoch: 1, Batch: 209, Loss: 0.6568, Elapsed: 9m33s
2020-04-25 06:17:25.506720: Epoch: 1, Batch: 213, Loss: 0.6180, Elapsed: 6m33s
2020-04-25 06:20:26.445044: Epoch: 1, Batch: 208, Loss: 0.6376, Elapsed: 6m34s
2020-04-25 06:21:40.655963: Epoch: 1, Batch: 210, Loss: 0.6221, Elapsed: 5m33s
2020-04-25 06:23:50.506087: Epoch: 1, Batch: 214, Loss: 0.6426, Elapsed: 6m24s
2020-04-25 06:25:49.822484: Epoch: 1, Batch: 211, Loss: 0.5944, Elapsed: 4m9s
2020-04-25 06:29:29.436118: Epoch: 1, Batch: 215, Loss: 0.6210, Elapsed: 5m38s
2020-04-25 06:29:54.211356: Epoch: 1, Batch: 209, Loss: 0.6946, Elapsed: 9m27s
2020-04-25 06:32:42.355596: Epoch: 1, Batch: 212, Loss: 0.6261, Elapsed: 6m52s
2020-04-25 06:35:45.475031: Epoch: 1, Batch: 210, Loss: 0.6510, Elapsed: 5m51s
2020-04-25 06:37:38.122615: Epoch: 1, Batch: 216, Loss: 0.6193, Elapsed: 8m8s
2020-04-25 06:39:03.597747: Epoch: 1, Batch: 213, Loss: 0.6295, Elapsed: 6m21s
2020-04-25 06:43:05.968190: Epoch: 1, Batch: 214, Loss: 0.6137, Elapsed: 4m2s
2020-04-25 06:43:41.375553: Epoch: 1, Batch: 217, Loss: 0.5968, Elapsed: 6m3s
2020-04-25 06:44:25.216199: Epoch: 1, Batch: 211, Loss: 0.6596, Elapsed: 8m39s
2020-04-25 06:48:15.844864: Epoch: 1, Batch: 218, Loss: 0.6077, Elapsed: 4m34s
2020-04-25 06:52:12.340294: Epoch: 1, Batch: 215, Loss: 0.6514, Elapsed: 9m6s
2020-04-25 06:52:22.152259: Epoch: 1, Batch: 212, Loss: 0.6745, Elapsed: 7m56s
2020-04-25 06:52:30.867273: Epoch: 1, Batch: 219, Loss: 0.6050, Elapsed: 4m14s
2020-04-25 06:57:17.198020: Epoch: 1, Batch: 216, Loss: 0.6399, Elapsed: 5m4s
2020-04-25 06:57:29.098026: Epoch: 1, Batch: 213, Loss: 0.6209, Elapsed: 5m6s
2020-04-25 07:03:39.966342: Epoch: 1, Batch: 217, Loss: 0.6239, Elapsed: 6m22s
2020-04-25 07:04:26.034864: Epoch: 1, Batch: 214, Loss: 0.6489, Elapsed: 6m56s
2020-04-25 07:04:26.541938: Epoch: 1, Batch: 220, Loss: 0.6220, Elapsed: 11m55s
2020-04-25 07:08:39.371750: Epoch: 1, Batch: 218, Loss: 0.6285, Elapsed: 4m59s
2020-04-25 07:10:07.022740: Epoch: 1, Batch: 215, Loss: 0.6509, Elapsed: 5m40s
2020-04-25 07:12:49.704291: Epoch: 1, Batch: 221, Loss: 0.6333, Elapsed: 8m23s
2020-04-25 07:17:59.538525: Epoch: 1, Batch: 219, Loss: 0.6700, Elapsed: 9m20s
2020-04-25 07:19:41.563142: Epoch: 1, Batch: 216, Loss: 0.6604, Elapsed: 9m34s
2020-04-25 07:20:07.041454: Epoch: 1, Batch: 222, Loss: 0.6044, Elapsed: 7m17s
2020-04-25 07:24:56.033397: Epoch: 1, Batch: 223, Loss: 0.6103, Elapsed: 4m48s
2020-04-25 07:26:34.415841: Epoch: 1, Batch: 217, Loss: 0.6573, Elapsed: 6m52s
2020-04-25 07:26:38.300531: Epoch: 1, Batch: 220, Loss: 0.6642, Elapsed: 8m38s
2020-04-25 07:30:55.975696: Epoch: 1, Batch: 224, Loss: 0.5894, Elapsed: 5m59s
2020-04-25 07:31:18.511975: Epoch: 1, Batch: 218, Loss: 0.6367, Elapsed: 4m44s
2020-04-25 07:33:11.825758: Epoch: 1, Batch: 221, Loss: 0.6449, Elapsed: 6m33s
2020-04-25 07:37:22.105178: Epoch: 1, Batch: 225, Loss: 0.5973, Elapsed: 6m26s
2020-04-25 07:39:58.396105: Epoch: 1, Batch: 219, Loss: 0.6699, Elapsed: 8m39s
2020-04-25 07:42:04.288727: Epoch: 1, Batch: 222, Loss: 0.6344, Elapsed: 8m52s
2020-04-25 07:43:24.193463: Epoch: 1, Batch: 226, Loss: 0.6050, Elapsed: 6m2s
2020-04-25 07:47:24.402197: Epoch: 1, Batch: 220, Loss: 0.6546, Elapsed: 7m25s
2020-04-25 07:47:27.642062: Epoch: 1, Batch: 227, Loss: 0.5965, Elapsed: 4m3s
2020-04-25 07:53:29.562081: Epoch: 1, Batch: 228, Loss: 0.6324, Elapsed: 6m1s
2020-04-25 07:54:20.998123: Epoch: 1, Batch: 223, Loss: 0.6658, Elapsed: 12m16s
2020-04-25 07:55:36.922125: Epoch: 1, Batch: 221, Loss: 0.6577, Elapsed: 8m12s
2020-04-25 07:57:24.878803: Epoch: 1, Batch: 229, Loss: 0.5706, Elapsed: 3m55s
2020-04-25 08:01:53.046949: Epoch: 1, Batch: 224, Loss: 0.6424, Elapsed: 7m32s
2020-04-25 08:03:54.520741: Epoch: 1, Batch: 230, Loss: 0.5935, Elapsed: 6m29s
2020-04-25 08:06:54.084610: Epoch: 1, Batch: 222, Loss: 0.6784, Elapsed: 11m17s
2020-04-25 08:09:44.916955: Epoch: 1, Batch: 231, Loss: 0.5992, Elapsed: 5m50s
2020-04-25 08:09:53.468291: Epoch: 1, Batch: 225, Loss: 0.6259, Elapsed: 8m0s
2020-04-25 08:13:30.770280: Epoch: 1, Batch: 223, Loss: 0.6510, Elapsed: 6m36s
2020-04-25 08:13:50.694699: Epoch: 1, Batch: 226, Loss: 0.6334, Elapsed: 3m57s
2020-04-25 08:14:17.948047: Epoch: 1, Batch: 232, Loss: 0.5804, Elapsed: 4m33s
2020-04-25 08:21:25.020974: Epoch: 1, Batch: 227, Loss: 0.6275, Elapsed: 7m34s
2020-04-25 08:22:36.515529: Epoch: 1, Batch: 224, Loss: 0.6414, Elapsed: 9m5s
2020-04-25 08:22:47.147985: Epoch: 1, Batch: 233, Loss: 0.6226, Elapsed: 8m29s
2020-04-25 08:25:31.113714: Epoch: 1, Batch: 228, Loss: 0.6103, Elapsed: 4m6s
2020-04-25 08:28:11.441335: Epoch: 1, Batch: 234, Loss: 0.6137, Elapsed: 5m24s
2020-04-25 08:28:54.528018: Epoch: 1, Batch: 225, Loss: 0.6519, Elapsed: 6m17s
2020-04-25 08:31:00.884842: Epoch: 1, Batch: 229, Loss: 0.6160, Elapsed: 5m29s
2020-04-25 08:34:43.864113: Epoch: 1, Batch: 235, Loss: 0.6105, Elapsed: 6m32s
2020-04-25 08:35:58.636995: Epoch: 1, Batch: 230, Loss: 0.6301, Elapsed: 4m57s
2020-04-25 08:36:28.711759: Epoch: 1, Batch: 226, Loss: 0.6524, Elapsed: 7m34s
2020-04-25 08:39:04.289321: Epoch: 1, Batch: 236, Loss: 0.6126, Elapsed: 4m20s
2020-04-25 08:40:59.008814: Epoch: 1, Batch: 227, Loss: 0.6312, Elapsed: 4m30s
2020-04-25 08:41:07.104332: Epoch: 1, Batch: 231, Loss: 0.6287, Elapsed: 5m8s
2020-04-25 08:43:14.613359: Epoch: 1, Batch: 237, Loss: 0.5978, Elapsed: 4m10s
2020-04-25 08:45:56.479973: Epoch: 1, Batch: 232, Loss: 0.6559, Elapsed: 4m49s
2020-04-25 08:49:29.501020: Epoch: 1, Batch: 228, Loss: 0.6425, Elapsed: 8m30s
2020-04-25 08:51:35.456295: Epoch: 1, Batch: 238, Loss: 0.6056, Elapsed: 8m20s
2020-04-25 08:54:49.644614: Epoch: 1, Batch: 229, Loss: 0.6387, Elapsed: 5m20s
2020-04-25 08:55:17.140849: Epoch: 1, Batch: 233, Loss: 0.6379, Elapsed: 9m20s
2020-04-25 08:56:35.067976: Epoch: 1, Batch: 239, Loss: 0.5825, Elapsed: 4m59s
2020-04-25 08:59:17.382793: Epoch: 1, Batch: 230, Loss: 0.6238, Elapsed: 4m27s
2020-04-25 09:01:12.705114: Epoch: 1, Batch: 234, Loss: 0.6215, Elapsed: 5m55s
2020-04-25 09:01:46.105041: Epoch: 1, Batch: 240, Loss: 0.6075, Elapsed: 5m11s
2020-04-25 09:04:21.428878: Epoch: 1, Batch: 231, Loss: 0.6436, Elapsed: 5m4s
2020-04-25 09:08:09.158910: Epoch: 1, Batch: 241, Loss: 0.5851, Elapsed: 6m23s
2020-04-25 09:10:07.950726: Epoch: 1, Batch: 235, Loss: 0.6536, Elapsed: 8m55s
2020-04-25 09:12:57.962523: Epoch: 1, Batch: 232, Loss: 0.6630, Elapsed: 8m36s
2020-04-25 09:16:23.516493: Epoch: 1, Batch: 236, Loss: 0.6161, Elapsed: 6m15s
2020-04-25 09:16:43.084201: Epoch: 1, Batch: 233, Loss: 0.6496, Elapsed: 3m45s
2020-04-25 09:17:38.720004: Epoch: 1, Batch: 242, Loss: 0.6685, Elapsed: 9m29s
2020-04-25 09:21:39.849187: Epoch: 1, Batch: 237, Loss: 0.6425, Elapsed: 5m16s
2020-04-25 09:25:14.285180: Epoch: 1, Batch: 234, Loss: 0.6485, Elapsed: 8m31s
2020-04-25 09:27:01.780306: Epoch: 1, Batch: 243, Loss: 0.6258, Elapsed: 9m23s
2020-04-25 09:29:38.613350: Epoch: 1, Batch: 238, Loss: 0.6460, Elapsed: 7m58s
2020-04-25 09:31:50.105164: Epoch: 1, Batch: 244, Loss: 0.5731, Elapsed: 4m48s
2020-04-25 09:35:17.826681: Epoch: 1, Batch: 239, Loss: 0.6290, Elapsed: 5m39s
2020-04-25 09:35:35.301440: Epoch: 1, Batch: 235, Loss: 0.7283, Elapsed: 10m20s
2020-04-25 09:38:55.070557: Epoch: 1, Batch: 245, Loss: 0.6115, Elapsed: 7m4s
2020-04-25 09:41:45.679640: Epoch: 1, Batch: 240, Loss: 0.6536, Elapsed: 6m27s
2020-04-25 09:42:53.228067: Epoch: 1, Batch: 236, Loss: 0.6401, Elapsed: 7m17s
2020-04-25 09:44:11.122736: Epoch: 1, Batch: 246, Loss: 0.5941, Elapsed: 5m16s
2020-04-25 09:45:42.070280: Epoch: 1, Batch: 241, Loss: 0.6086, Elapsed: 3m56s
2020-04-25 09:50:15.838737: Epoch: 1, Batch: 237, Loss: 0.6511, Elapsed: 7m22s
2020-04-25 09:50:59.965442: Epoch: 1, Batch: 247, Loss: 0.6486, Elapsed: 6m48s
2020-04-25 09:52:07.390978: Epoch: 1, Batch: 242, Loss: 0.6174, Elapsed: 6m25s
2020-04-25 09:56:07.824895: Epoch: 1, Batch: 248, Loss: 0.5470, Elapsed: 5m7s
2020-04-25 09:58:11.936287: Epoch: 1, Batch: 238, Loss: 0.6663, Elapsed: 7m56s
2020-04-25 09:59:26.817618: Epoch: 1, Batch: 243, Loss: 0.6292, Elapsed: 7m19s
2020-04-25 10:02:20.452794: Epoch: 1, Batch: 239, Loss: 0.6829, Elapsed: 4m8s
2020-04-25 10:03:50.773365: Epoch: 1, Batch: 249, Loss: 0.6068, Elapsed: 7m42s
2020-04-25 10:05:27.028973: Epoch: 1, Batch: 244, Loss: 0.6204, Elapsed: 6m0s
2020-04-25 10:09:48.239460: Epoch: 1, Batch: 240, Loss: 0.6641, Elapsed: 7m27s
2020-04-25 10:10:00.468774: Epoch: 1, Batch: 245, Loss: 0.6321, Elapsed: 4m33s
2020-04-25 10:10:56.487305: Epoch: 1, Batch: 250, Loss: 0.5920, Elapsed: 7m5s
Starting testing the validation set with 200 subgraphs!
2020-04-25 10:14:28.710581: Epoch: 1, Batch: 241, Loss: 0.6360, Elapsed: 4m40s
2020-04-25 10:17:05.169467: Epoch: 1, Batch: 246, Loss: 0.6674, Elapsed: 7m4s
2020-04-25 10:20:57.755402: Epoch: 1, Batch: 242, Loss: 0.6457, Elapsed: 6m29s
2020-04-25 10:23:10.083769: Epoch: 1, Batch: 247, Loss: 0.6570, Elapsed: 6m4s
2020-04-25 10:26:35.439904: Epoch: 1, Batch: 248, Loss: 0.5836, Elapsed: 3m25s
2020-04-25 10:26:43.626600: Epoch: 1, Batch: 243, Loss: 0.6342, Elapsed: 5m45s
2020-04-25 10:31:10.394495: Epoch: 1, Batch: 249, Loss: 0.6379, Elapsed: 4m34s
2020-04-25 10:35:27.164448: Epoch: 1, Batch: 244, Loss: 0.6490, Elapsed: 8m43s
2020-04-25 10:36:53.942384: Epoch: 1, Batch: 250, Loss: 0.6275, Elapsed: 5m43s
Starting testing the validation set with 200 subgraphs!
2020-04-25 10:42:10.690414: Epoch: 1, Batch: 245, Loss: 0.6663, Elapsed: 6m43s
2020-04-25 10:47:23.745119: Epoch: 1, Batch: 246, Loss: 0.6634, Elapsed: 5m13s
2020-04-25 10:51:56.507326: Validation Test:  Loss: 0.6128,  Acc: 66.5466, AUC: 0.7194, Precision: 0.7558 -- Elapsed: 40m59s
2020-04-25 10:55:05.571420: Epoch: 1, Batch: 247, Loss: 0.6757, Elapsed: 7m41s
2020-04-25 10:57:55.994903: Epoch: 1, Batch: 251, Loss: 0.6156, Elapsed: 5m59s
2020-04-25 11:01:32.313482: Epoch: 1, Batch: 248, Loss: 0.6374, Elapsed: 6m26s
2020-04-25 11:08:10.846043: Epoch: 1, Batch: 252, Loss: 0.7101, Elapsed: 10m14s
2020-04-25 11:08:17.642276: Epoch: 1, Batch: 249, Loss: 0.6543, Elapsed: 6m45s
2020-04-25 11:12:22.544539: Epoch: 1, Batch: 250, Loss: 0.6494, Elapsed: 4m4s
Starting testing the validation set with 200 subgraphs!
2020-04-25 11:13:24.901437: Epoch: 1, Batch: 253, Loss: 0.5962, Elapsed: 5m14s
2020-04-25 11:16:47.068476: Validation Test:  Loss: 0.6326,  Acc: 65.6174, AUC: 0.7023, Precision: 0.7485 -- Elapsed: 39m53s
2020-04-25 11:21:24.836046: Epoch: 1, Batch: 254, Loss: 0.6153, Elapsed: 7m59s
2020-04-25 11:23:42.147780: Epoch: 1, Batch: 251, Loss: 0.6201, Elapsed: 6m55s
2020-04-25 11:27:05.547618: Epoch: 1, Batch: 255, Loss: 0.5792, Elapsed: 5m40s
2020-04-25 11:29:38.693784: Epoch: 1, Batch: 252, Loss: 0.6243, Elapsed: 5m56s
2020-04-25 11:32:40.713769: Epoch: 1, Batch: 256, Loss: 0.5954, Elapsed: 5m35s
2020-04-25 11:37:02.180273: Epoch: 1, Batch: 253, Loss: 0.6681, Elapsed: 7m23s
2020-04-25 11:39:58.661133: Epoch: 1, Batch: 257, Loss: 0.6384, Elapsed: 7m17s
2020-04-25 11:45:22.224546: Epoch: 1, Batch: 258, Loss: 0.6048, Elapsed: 5m23s
2020-04-25 11:45:39.870786: Epoch: 1, Batch: 254, Loss: 0.6429, Elapsed: 8m37s
2020-04-25 11:51:28.257036: Epoch: 1, Batch: 255, Loss: 0.6453, Elapsed: 5m48s
2020-04-25 11:52:52.241092: Epoch: 1, Batch: 259, Loss: 0.6382, Elapsed: 7m29s
2020-04-25 11:53:34.296216: Validation Test:  Loss: 0.6491,  Acc: 62.4464, AUC: 0.6717, Precision: 0.7105 -- Elapsed: 41m11s
2020-04-25 11:57:43.312571: Epoch: 1, Batch: 251, Loss: 0.6263, Elapsed: 4m9s
2020-04-25 11:59:47.836164: Epoch: 1, Batch: 260, Loss: 0.6025, Elapsed: 6m55s
2020-04-25 12:02:35.975246: Epoch: 1, Batch: 256, Loss: 0.6420, Elapsed: 11m7s
2020-04-25 12:07:01.253191: Epoch: 1, Batch: 252, Loss: 0.6649, Elapsed: 9m17s
2020-04-25 12:08:21.394965: Epoch: 1, Batch: 261, Loss: 0.6128, Elapsed: 8m33s
2020-04-25 12:10:24.571563: Epoch: 1, Batch: 257, Loss: 0.6191, Elapsed: 7m48s
2020-04-25 12:11:50.333386: Epoch: 1, Batch: 253, Loss: 0.6220, Elapsed: 4m49s
2020-04-25 12:15:30.737600: Epoch: 1, Batch: 262, Loss: 0.6128, Elapsed: 7m9s
2020-04-25 12:18:07.993339: Epoch: 1, Batch: 258, Loss: 0.6436, Elapsed: 7m43s
2020-04-25 12:18:09.824236: Epoch: 1, Batch: 254, Loss: 0.6511, Elapsed: 6m19s
2020-04-25 12:22:42.647483: Epoch: 1, Batch: 259, Loss: 0.6123, Elapsed: 4m34s
2020-04-25 12:23:50.319201: Epoch: 1, Batch: 255, Loss: 0.6345, Elapsed: 5m40s
2020-04-25 12:25:33.069726: Epoch: 1, Batch: 263, Loss: 0.6587, Elapsed: 10m2s
2020-04-25 12:30:26.794116: Epoch: 1, Batch: 256, Loss: 0.6471, Elapsed: 6m36s
2020-04-25 12:30:35.236718: Epoch: 1, Batch: 260, Loss: 0.6323, Elapsed: 7m52s
2020-04-25 12:31:38.148068: Epoch: 1, Batch: 264, Loss: 0.6245, Elapsed: 6m5s
2020-04-25 12:35:29.492478: Epoch: 1, Batch: 261, Loss: 0.6017, Elapsed: 4m54s
2020-04-25 12:37:02.568050: Epoch: 1, Batch: 257, Loss: 0.6367, Elapsed: 6m35s
2020-04-25 12:39:37.633454: Epoch: 1, Batch: 265, Loss: 0.6252, Elapsed: 7m59s
2020-04-25 12:42:21.022246: Epoch: 1, Batch: 262, Loss: 0.6252, Elapsed: 6m51s
2020-04-25 12:42:48.540888: Epoch: 1, Batch: 258, Loss: 0.6111, Elapsed: 5m45s
2020-04-25 12:46:27.432145: Epoch: 1, Batch: 266, Loss: 0.6220, Elapsed: 6m49s
2020-04-25 12:48:23.615264: Epoch: 1, Batch: 263, Loss: 0.6144, Elapsed: 6m2s
2020-04-25 12:49:43.381011: Epoch: 1, Batch: 259, Loss: 0.6591, Elapsed: 6m54s
2020-04-25 12:53:25.322038: Epoch: 1, Batch: 264, Loss: 0.6384, Elapsed: 5m1s
2020-04-25 12:53:37.727550: Epoch: 1, Batch: 267, Loss: 0.6167, Elapsed: 7m10s
2020-04-25 12:54:30.270604: Epoch: 1, Batch: 260, Loss: 0.6236, Elapsed: 4m46s
2020-04-25 12:57:14.441336: Epoch: 1, Batch: 261, Loss: 0.6211, Elapsed: 2m44s
2020-04-25 13:00:08.250386: Epoch: 1, Batch: 265, Loss: 0.6279, Elapsed: 6m42s
2020-04-25 13:01:08.002993: Epoch: 1, Batch: 268, Loss: 0.6315, Elapsed: 7m30s
2020-04-25 13:03:10.222040: Epoch: 1, Batch: 262, Loss: 0.6332, Elapsed: 5m55s
2020-04-25 13:04:50.678611: Epoch: 1, Batch: 266, Loss: 0.6266, Elapsed: 4m42s
2020-04-25 13:08:57.756200: Epoch: 1, Batch: 267, Loss: 0.6336, Elapsed: 4m7s
2020-04-25 13:09:18.525356: Epoch: 1, Batch: 269, Loss: 0.6363, Elapsed: 8m10s
2020-04-25 13:10:44.720094: Epoch: 1, Batch: 263, Loss: 0.6388, Elapsed: 7m34s
2020-04-25 13:13:10.960111: Epoch: 1, Batch: 268, Loss: 0.5871, Elapsed: 4m13s
2020-04-25 13:15:36.949361: Epoch: 1, Batch: 270, Loss: 0.6198, Elapsed: 6m18s
2020-04-25 13:17:40.099541: Epoch: 1, Batch: 269, Loss: 0.6328, Elapsed: 4m29s
2020-04-25 13:21:30.549805: Epoch: 1, Batch: 271, Loss: 0.5902, Elapsed: 5m53s
2020-04-25 13:21:42.294189: Epoch: 1, Batch: 270, Loss: 0.5575, Elapsed: 4m2s
2020-04-25 13:25:57.244838: Epoch: 1, Batch: 271, Loss: 0.6222, Elapsed: 4m14s
2020-04-25 13:28:40.095938: Epoch: 1, Batch: 272, Loss: 0.6097, Elapsed: 7m9s
2020-04-25 13:32:01.296751: Epoch: 1, Batch: 272, Loss: 0.6088, Elapsed: 6m4s
2020-04-25 13:35:00.241480: Epoch: 1, Batch: 273, Loss: 0.6051, Elapsed: 6m20s
2020-04-25 13:36:07.647259: Epoch: 1, Batch: 273, Loss: 0.5897, Elapsed: 4m6s
2020-04-25 13:38:14.738618: Epoch: 1, Batch: 264, Loss: 0.7080, Elapsed: 27m29s
2020-04-25 13:40:59.332577: Epoch: 1, Batch: 274, Loss: 0.6109, Elapsed: 4m51s
2020-04-25 13:41:48.448134: Epoch: 1, Batch: 274, Loss: 0.6016, Elapsed: 6m48s
2020-04-25 13:46:47.473054: Epoch: 1, Batch: 275, Loss: 0.5840, Elapsed: 4m59s
2020-04-25 13:47:20.605532: Epoch: 1, Batch: 275, Loss: 0.6250, Elapsed: 6m21s
2020-04-25 13:48:16.883803: Epoch: 1, Batch: 265, Loss: 0.6766, Elapsed: 10m2s
2020-04-25 13:52:54.029148: Epoch: 1, Batch: 276, Loss: 0.6092, Elapsed: 5m33s
2020-04-25 13:54:10.233899: Epoch: 1, Batch: 266, Loss: 0.6539, Elapsed: 5m53s
2020-04-25 13:55:03.129549: Epoch: 1, Batch: 276, Loss: 0.6311, Elapsed: 8m15s
2020-04-25 13:56:52.005967: Epoch: 1, Batch: 277, Loss: 0.5890, Elapsed: 3m57s
2020-04-25 13:59:22.841850: Epoch: 1, Batch: 267, Loss: 0.6365, Elapsed: 5m12s
2020-04-25 14:03:17.204282: Epoch: 1, Batch: 277, Loss: 0.6089, Elapsed: 8m14s
2020-04-25 14:04:28.076224: Epoch: 1, Batch: 278, Loss: 0.6331, Elapsed: 7m36s
2020-04-25 14:08:37.906789: Epoch: 1, Batch: 268, Loss: 0.6961, Elapsed: 9m15s
2020-04-25 14:10:30.251322: Epoch: 1, Batch: 279, Loss: 0.6346, Elapsed: 6m2s
2020-04-25 14:13:07.276147: Epoch: 1, Batch: 278, Loss: 0.6184, Elapsed: 9m50s
2020-04-25 14:17:17.992787: Epoch: 1, Batch: 279, Loss: 0.5804, Elapsed: 4m10s
2020-04-25 14:19:46.526424: Epoch: 1, Batch: 269, Loss: 0.6714, Elapsed: 11m8s
2020-04-25 14:22:32.015830: Epoch: 1, Batch: 280, Loss: 0.6428, Elapsed: 5m14s
2020-04-25 14:22:36.121039: Epoch: 1, Batch: 280, Loss: 0.6275, Elapsed: 12m5s
2020-04-25 14:23:55.123097: Epoch: 1, Batch: 270, Loss: 0.6451, Elapsed: 4m8s
2020-04-25 14:26:45.362190: Epoch: 1, Batch: 281, Loss: 0.5741, Elapsed: 4m13s
2020-04-25 14:28:40.066932: Epoch: 1, Batch: 281, Loss: 0.6174, Elapsed: 6m3s
2020-04-25 14:32:18.991801: Epoch: 1, Batch: 271, Loss: 0.6481, Elapsed: 8m23s
2020-04-25 14:33:48.720855: Epoch: 1, Batch: 282, Loss: 0.6060, Elapsed: 7m3s
2020-04-25 14:34:31.526971: Epoch: 1, Batch: 282, Loss: 0.6190, Elapsed: 5m51s
2020-04-25 14:38:21.883788: Epoch: 1, Batch: 272, Loss: 0.6320, Elapsed: 6m2s
2020-04-25 14:43:04.000314: Epoch: 1, Batch: 283, Loss: 0.6415, Elapsed: 8m32s
2020-04-25 14:44:43.529424: Epoch: 1, Batch: 283, Loss: 0.6619, Elapsed: 10m54s
2020-04-25 14:46:04.110925: Epoch: 1, Batch: 273, Loss: 0.6563, Elapsed: 7m42s
2020-04-25 14:49:51.799854: Epoch: 1, Batch: 274, Loss: 0.6251, Elapsed: 3m47s
2020-04-25 14:50:36.625024: Epoch: 1, Batch: 284, Loss: 0.6422, Elapsed: 7m32s
2020-04-25 14:51:23.224948: Epoch: 1, Batch: 284, Loss: 0.6066, Elapsed: 6m39s
2020-04-25 14:54:19.966398: Epoch: 1, Batch: 275, Loss: 0.6720, Elapsed: 4m28s
2020-04-25 14:54:15.835259: Epoch: 1, Batch: 285, Loss: 0.6199, Elapsed: 3m39s
2020-04-25 14:58:19.427042: Epoch: 1, Batch: 285, Loss: 0.6144, Elapsed: 6m56s
2020-04-25 14:58:31.468475: Epoch: 1, Batch: 276, Loss: 0.6470, Elapsed: 4m11s
2020-04-25 15:01:18.215222: Epoch: 1, Batch: 286, Loss: 0.6200, Elapsed: 7m2s
2020-04-25 15:03:13.372389: Epoch: 1, Batch: 277, Loss: 0.6301, Elapsed: 4m41s
2020-04-25 15:05:08.835519: Epoch: 1, Batch: 286, Loss: 0.6092, Elapsed: 6m49s
2020-04-25 15:09:34.622317: Epoch: 1, Batch: 278, Loss: 0.6423, Elapsed: 6m21s
2020-04-25 15:09:53.886172: Epoch: 1, Batch: 287, Loss: 0.6228, Elapsed: 8m35s
2020-04-25 15:10:25.011573: Epoch: 1, Batch: 287, Loss: 0.5482, Elapsed: 5m16s
2020-04-25 15:14:53.298660: Epoch: 1, Batch: 279, Loss: 0.6298, Elapsed: 5m18s
2020-04-25 15:15:34.618693: Epoch: 1, Batch: 288, Loss: 0.5718, Elapsed: 5m9s
2020-04-25 15:15:48.009549: Epoch: 1, Batch: 288, Loss: 0.6406, Elapsed: 5m54s
2020-04-25 15:19:27.510201: Epoch: 1, Batch: 280, Loss: 0.6340, Elapsed: 4m34s
2020-04-25 15:21:39.288253: Epoch: 1, Batch: 289, Loss: 0.6091, Elapsed: 6m4s
2020-04-25 15:23:16.932142: Epoch: 1, Batch: 289, Loss: 0.6229, Elapsed: 7m28s
2020-04-25 15:24:38.896749: Epoch: 1, Batch: 281, Loss: 0.6406, Elapsed: 5m11s
2020-04-25 15:28:35.644647: Epoch: 1, Batch: 290, Loss: 0.6069, Elapsed: 5m18s
2020-04-25 15:30:17.297610: Epoch: 1, Batch: 290, Loss: 0.6555, Elapsed: 8m37s
2020-04-25 15:33:24.544164: Epoch: 1, Batch: 291, Loss: 0.6149, Elapsed: 4m48s
2020-04-25 15:38:10.249183: Epoch: 1, Batch: 292, Loss: 0.5630, Elapsed: 4m45s
2020-04-25 15:39:09.741239: Epoch: 1, Batch: 282, Loss: 0.6623, Elapsed: 14m30s
2020-04-25 15:41:00.218713: Epoch: 1, Batch: 291, Loss: 0.6584, Elapsed: 10m42s
2020-04-25 15:45:06.495852: Epoch: 1, Batch: 293, Loss: 0.6298, Elapsed: 6m56s
2020-04-25 15:45:20.045897: Epoch: 1, Batch: 283, Loss: 0.6290, Elapsed: 6m10s
2020-04-25 15:49:31.795075: Epoch: 1, Batch: 292, Loss: 0.6270, Elapsed: 8m31s
2020-04-25 15:50:06.718652: Epoch: 1, Batch: 284, Loss: 0.6485, Elapsed: 4m46s
2020-04-25 15:50:47.188279: Epoch: 1, Batch: 294, Loss: 0.6280, Elapsed: 5m40s
2020-04-25 15:53:52.761232: Epoch: 1, Batch: 293, Loss: 0.5940, Elapsed: 4m20s
2020-04-25 15:55:43.270093: Epoch: 1, Batch: 285, Loss: 0.6469, Elapsed: 5m36s
2020-04-25 15:56:19.116758: Epoch: 1, Batch: 295, Loss: 0.5751, Elapsed: 5m31s
2020-04-25 16:00:03.235162: Epoch: 1, Batch: 296, Loss: 0.5880, Elapsed: 3m44s
2020-04-25 16:00:54.958513: Epoch: 1, Batch: 294, Loss: 0.5831, Elapsed: 7m2s
2020-04-25 16:04:32.343230: Epoch: 1, Batch: 297, Loss: 0.5978, Elapsed: 4m29s
2020-04-25 16:04:52.341824: Epoch: 1, Batch: 286, Loss: 0.6351, Elapsed: 9m9s
2020-04-25 16:08:24.146733: Epoch: 1, Batch: 295, Loss: 0.6132, Elapsed: 7m29s
2020-04-25 16:10:23.109094: Epoch: 1, Batch: 298, Loss: 0.6335, Elapsed: 5m50s
2020-04-25 16:10:56.174004: Epoch: 1, Batch: 296, Loss: 0.5679, Elapsed: 2m32s
2020-04-25 16:12:01.453018: Epoch: 1, Batch: 287, Loss: 0.6382, Elapsed: 7m9s
2020-04-25 16:15:51.959393: Epoch: 1, Batch: 299, Loss: 0.5945, Elapsed: 5m28s
2020-04-25 16:17:03.154292: Epoch: 1, Batch: 288, Loss: 0.6447, Elapsed: 5m1s
2020-04-25 16:19:02.084742: Epoch: 1, Batch: 297, Loss: 0.6479, Elapsed: 8m5s
2020-04-25 16:23:52.605348: Epoch: 1, Batch: 300, Loss: 0.6109, Elapsed: 8m0s
Starting testing the validation set with 200 subgraphs!
2020-04-25 16:24:43.906529: Epoch: 1, Batch: 298, Loss: 0.6224, Elapsed: 5m41s
2020-04-25 16:26:13.251342: Epoch: 1, Batch: 289, Loss: 0.6757, Elapsed: 9m10s
2020-04-25 16:28:28.584888: Epoch: 1, Batch: 299, Loss: 0.5753, Elapsed: 3m44s
2020-04-25 16:31:53.417947: Epoch: 1, Batch: 290, Loss: 0.6549, Elapsed: 5m40s
2020-04-25 16:38:20.465625: Epoch: 1, Batch: 291, Loss: 0.6451, Elapsed: 6m27s
2020-04-25 16:38:31.627483: Epoch: 1, Batch: 300, Loss: 0.6178, Elapsed: 10m3s
Starting testing the validation set with 200 subgraphs!
2020-04-25 16:45:56.517286: Epoch: 1, Batch: 292, Loss: 0.6521, Elapsed: 7m36s
2020-04-25 16:49:28.372299: Epoch: 1, Batch: 293, Loss: 0.6301, Elapsed: 3m31s
2020-04-25 16:56:44.660993: Epoch: 1, Batch: 294, Loss: 0.6695, Elapsed: 7m16s
2020-04-25 17:01:16.477396: Epoch: 1, Batch: 295, Loss: 0.6450, Elapsed: 4m31s
2020-04-25 17:03:14.998898: Validation Test:  Loss: 0.6201,  Acc: 66.5121, AUC: 0.7107, Precision: 0.7447 -- Elapsed: 39m22s
2020-04-25 17:06:39.744751: Epoch: 1, Batch: 301, Loss: 0.5980, Elapsed: 3m24s
2020-04-25 17:07:41.061725: Epoch: 1, Batch: 296, Loss: 0.6409, Elapsed: 6m24s
2020-04-25 17:10:52.615653: Epoch: 1, Batch: 302, Loss: 0.5867, Elapsed: 4m12s
2020-04-25 17:11:51.060110: Epoch: 1, Batch: 297, Loss: 0.6231, Elapsed: 4m9s
2020-04-25 17:18:24.840136: Validation Test:  Loss: 0.6020,  Acc: 68.6384, AUC: 0.7374, Precision: 0.7783 -- Elapsed: 39m53s
2020-04-25 17:19:01.130520: Epoch: 1, Batch: 298, Loss: 0.6528, Elapsed: 7m10s
2020-04-25 17:21:22.467170: Epoch: 1, Batch: 303, Loss: 0.6192, Elapsed: 10m29s
2020-04-25 17:26:25.318957: Epoch: 1, Batch: 304, Loss: 0.5992, Elapsed: 5m2s
2020-04-25 17:26:29.856737: Epoch: 1, Batch: 299, Loss: 0.6609, Elapsed: 7m28s
2020-04-25 17:26:52.927029: Epoch: 1, Batch: 301, Loss: 0.5931, Elapsed: 8m28s
2020-04-25 17:33:56.557401: Epoch: 1, Batch: 300, Loss: 0.6577, Elapsed: 7m26s
Starting testing the validation set with 200 subgraphs!
2020-04-25 17:33:54.699153: Epoch: 1, Batch: 302, Loss: 0.6133, Elapsed: 7m1s
2020-04-25 17:35:52.336142: Epoch: 1, Batch: 305, Loss: 0.6349, Elapsed: 9m26s
2020-04-25 17:41:53.610335: Epoch: 1, Batch: 303, Loss: 0.6433, Elapsed: 7m58s
2020-04-25 17:44:31.982584: Epoch: 1, Batch: 306, Loss: 0.6251, Elapsed: 8m39s
2020-04-25 17:45:44.262524: Epoch: 1, Batch: 304, Loss: 0.5302, Elapsed: 3m50s
2020-04-25 17:53:02.116506: Epoch: 1, Batch: 307, Loss: 0.6556, Elapsed: 8m30s
2020-04-25 17:54:28.028970: Epoch: 1, Batch: 305, Loss: 0.6157, Elapsed: 8m43s
2020-04-25 17:57:56.537184: Epoch: 1, Batch: 308, Loss: 0.6040, Elapsed: 4m54s
2020-04-25 18:00:43.232800: Epoch: 1, Batch: 306, Loss: 0.5693, Elapsed: 6m15s
2020-04-25 18:03:15.029800: Epoch: 1, Batch: 309, Loss: 0.6096, Elapsed: 5m18s
2020-04-25 18:05:45.736639: Epoch: 1, Batch: 307, Loss: 0.5886, Elapsed: 5m2s
2020-04-25 18:09:55.457619: Epoch: 1, Batch: 310, Loss: 0.5978, Elapsed: 6m40s
2020-04-25 18:10:15.157968: Epoch: 1, Batch: 308, Loss: 0.6033, Elapsed: 4m29s
2020-04-25 18:15:36.792145: Validation Test:  Loss: 0.6452,  Acc: 62.5853, AUC: 0.6795, Precision: 0.7230 -- Elapsed: 41m40s
2020-04-25 18:16:02.367628: Epoch: 1, Batch: 309, Loss: 0.5659, Elapsed: 5m47s
2020-04-25 18:16:49.046305: Epoch: 1, Batch: 311, Loss: 0.6244, Elapsed: 6m53s
2020-04-25 18:20:12.176669: Epoch: 1, Batch: 310, Loss: 0.5486, Elapsed: 4m9s
2020-04-25 18:22:14.136167: Epoch: 1, Batch: 301, Loss: 0.6411, Elapsed: 6m37s
2020-04-25 18:23:44.587384: Epoch: 1, Batch: 312, Loss: 0.6025, Elapsed: 6m55s
2020-04-25 18:28:15.738912: Epoch: 1, Batch: 313, Loss: 0.6292, Elapsed: 4m31s
2020-04-25 18:28:26.566475: Epoch: 1, Batch: 302, Loss: 0.6286, Elapsed: 6m12s
2020-04-25 18:30:07.454488: Epoch: 1, Batch: 311, Loss: 0.6122, Elapsed: 9m55s
2020-04-25 18:33:29.961029: Epoch: 1, Batch: 312, Loss: 0.4987, Elapsed: 3m22s
2020-04-25 18:34:17.549995: Epoch: 1, Batch: 303, Loss: 0.6210, Elapsed: 5m50s
2020-04-25 18:35:19.938838: Epoch: 1, Batch: 314, Loss: 0.6145, Elapsed: 7m4s
2020-04-25 18:38:36.740781: Epoch: 1, Batch: 313, Loss: 0.5945, Elapsed: 5m6s
2020-04-25 18:41:10.127905: Epoch: 1, Batch: 304, Loss: 0.6459, Elapsed: 6m52s
2020-04-25 18:41:36.136768: Epoch: 1, Batch: 315, Loss: 0.5902, Elapsed: 6m16s
2020-04-25 18:45:16.470288: Epoch: 1, Batch: 314, Loss: 0.5878, Elapsed: 6m39s
2020-04-25 18:46:48.781901: Epoch: 1, Batch: 305, Loss: 0.6509, Elapsed: 5m38s
2020-04-25 18:51:33.408076: Epoch: 1, Batch: 316, Loss: 0.6221, Elapsed: 9m57s
2020-04-25 18:53:44.183915: Epoch: 1, Batch: 315, Loss: 0.5933, Elapsed: 8m27s
2020-04-25 18:55:14.980359: Epoch: 1, Batch: 306, Loss: 0.6434, Elapsed: 8m26s
2020-04-25 18:57:19.187841: Epoch: 1, Batch: 317, Loss: 0.6117, Elapsed: 5m45s
2020-04-25 18:58:39.816968: Epoch: 1, Batch: 307, Loss: 0.6286, Elapsed: 3m24s
2020-04-25 18:58:44.892507: Epoch: 1, Batch: 316, Loss: 0.5799, Elapsed: 5m0s
2020-04-25 19:03:15.244722: Epoch: 1, Batch: 318, Loss: 0.6089, Elapsed: 5m56s
2020-04-25 19:04:43.043189: Epoch: 1, Batch: 308, Loss: 0.6508, Elapsed: 6m3s
2020-04-25 19:06:23.200687: Epoch: 1, Batch: 317, Loss: 0.6338, Elapsed: 7m38s
2020-04-25 19:09:20.468052: Epoch: 1, Batch: 309, Loss: 0.6287, Elapsed: 4m37s
2020-04-25 19:12:38.736829: Epoch: 1, Batch: 319, Loss: 0.6467, Elapsed: 9m23s
2020-04-25 19:12:51.605131: Epoch: 1, Batch: 318, Loss: 0.5848, Elapsed: 6m28s
2020-04-25 19:16:26.256133: Epoch: 1, Batch: 310, Loss: 0.6466, Elapsed: 7m5s
2020-04-25 19:17:03.909776: Epoch: 1, Batch: 320, Loss: 0.5424, Elapsed: 4m25s
2020-04-25 19:17:14.990120: Epoch: 1, Batch: 319, Loss: 0.5880, Elapsed: 4m23s
2020-04-25 19:21:12.404534: Epoch: 1, Batch: 311, Loss: 0.6628, Elapsed: 4m46s
2020-04-25 19:23:18.740415: Epoch: 1, Batch: 321, Loss: 0.6063, Elapsed: 6m14s
2020-04-25 19:26:01.593094: Epoch: 1, Batch: 312, Loss: 0.6263, Elapsed: 4m49s
2020-04-25 19:26:24.132282: Epoch: 1, Batch: 320, Loss: 0.6189, Elapsed: 9m9s
2020-04-25 19:29:56.642380: Epoch: 1, Batch: 322, Loss: 0.6412, Elapsed: 6m37s
2020-04-25 19:34:11.120295: Epoch: 1, Batch: 313, Loss: 0.6422, Elapsed: 8m9s
2020-04-25 19:34:58.102693: Epoch: 1, Batch: 323, Loss: 0.6323, Elapsed: 5m1s
2020-04-25 19:36:33.179166: Epoch: 1, Batch: 321, Loss: 0.6227, Elapsed: 10m9s
2020-04-25 19:40:05.610022: Epoch: 1, Batch: 314, Loss: 0.6458, Elapsed: 5m54s
2020-04-25 19:41:03.061108: Epoch: 1, Batch: 324, Loss: 0.6181, Elapsed: 6m4s
2020-04-25 19:43:10.973066: Epoch: 1, Batch: 322, Loss: 0.5975, Elapsed: 6m37s
2020-04-25 19:45:05.474201: Epoch: 1, Batch: 325, Loss: 0.5731, Elapsed: 4m2s
2020-04-25 19:47:06.286070: Epoch: 1, Batch: 323, Loss: 0.5608, Elapsed: 3m55s
2020-04-25 19:47:59.199320: Epoch: 1, Batch: 315, Loss: 0.6478, Elapsed: 7m53s
2020-04-25 19:49:19.732765: Epoch: 1, Batch: 326, Loss: 0.5644, Elapsed: 4m14s
2020-04-25 19:51:16.390654: Epoch: 1, Batch: 324, Loss: 0.5938, Elapsed: 4m10s
2020-04-25 19:54:29.147544: Epoch: 1, Batch: 316, Loss: 0.6424, Elapsed: 6m29s
2020-04-25 19:55:26.326870: Epoch: 1, Batch: 327, Loss: 0.6265, Elapsed: 6m6s
2020-04-25 19:59:43.836322: Epoch: 1, Batch: 317, Loss: 0.6322, Elapsed: 5m14s
2020-04-25 20:00:44.959080: Epoch: 1, Batch: 325, Loss: 0.6156, Elapsed: 9m28s
2020-04-25 20:04:19.107599: Epoch: 1, Batch: 328, Loss: 0.6465, Elapsed: 8m52s
2020-04-25 20:05:16.206540: Epoch: 1, Batch: 318, Loss: 0.6348, Elapsed: 5m32s
2020-04-25 20:05:33.356310: Epoch: 1, Batch: 326, Loss: 0.5623, Elapsed: 4m48s
2020-04-25 20:12:36.312356: Epoch: 1, Batch: 327, Loss: 0.6375, Elapsed: 7m2s
2020-04-25 20:13:02.096872: Epoch: 1, Batch: 319, Loss: 0.6608, Elapsed: 7m45s
2020-04-25 20:13:16.838313: Epoch: 1, Batch: 329, Loss: 0.6722, Elapsed: 8m57s
2020-04-25 20:17:39.960418: Epoch: 1, Batch: 328, Loss: 0.6603, Elapsed: 5m3s
2020-04-25 20:18:55.551336: Epoch: 1, Batch: 330, Loss: 0.5679, Elapsed: 5m38s
2020-04-25 20:22:19.555673: Epoch: 1, Batch: 320, Loss: 0.6418, Elapsed: 9m17s
2020-04-25 20:23:06.874749: Epoch: 1, Batch: 331, Loss: 0.5658, Elapsed: 4m11s
2020-04-25 20:24:26.520158: Epoch: 1, Batch: 329, Loss: 0.5965, Elapsed: 6m46s
2020-04-25 20:26:15.203003: Epoch: 1, Batch: 321, Loss: 0.6279, Elapsed: 3m55s
2020-04-25 20:30:03.788720: Epoch: 1, Batch: 332, Loss: 0.6177, Elapsed: 6m56s
2020-04-25 20:30:52.061572: Epoch: 1, Batch: 330, Loss: 0.5913, Elapsed: 6m25s
2020-04-25 20:31:58.541279: Epoch: 1, Batch: 322, Loss: 0.6294, Elapsed: 5m43s
2020-04-25 20:37:03.443271: Epoch: 1, Batch: 333, Loss: 0.6058, Elapsed: 6m59s
2020-04-25 20:37:18.937936: Epoch: 1, Batch: 323, Loss: 0.6371, Elapsed: 5m20s
2020-04-25 20:41:13.939874: Epoch: 1, Batch: 331, Loss: 0.6010, Elapsed: 10m21s
2020-04-25 20:43:38.877398: Epoch: 1, Batch: 334, Loss: 0.6276, Elapsed: 6m35s
2020-04-25 20:44:55.196199: Epoch: 1, Batch: 324, Loss: 0.6466, Elapsed: 7m36s
2020-04-25 20:47:11.415304: Epoch: 1, Batch: 332, Loss: 0.5716, Elapsed: 5m57s
2020-04-25 20:49:01.056580: Epoch: 1, Batch: 335, Loss: 0.6282, Elapsed: 5m22s
2020-04-25 20:51:30.602681: Epoch: 1, Batch: 325, Loss: 0.6187, Elapsed: 6m35s
2020-04-25 20:51:43.813330: Epoch: 1, Batch: 333, Loss: 0.5843, Elapsed: 4m32s
2020-04-25 20:53:40.982547: Epoch: 1, Batch: 336, Loss: 0.5846, Elapsed: 4m39s
2020-04-25 20:55:50.401494: Epoch: 1, Batch: 326, Loss: 0.6475, Elapsed: 4m19s
2020-04-25 20:56:12.415447: Epoch: 1, Batch: 334, Loss: 0.6091, Elapsed: 4m28s
2020-04-25 20:58:33.007558: Epoch: 1, Batch: 337, Loss: 0.5834, Elapsed: 4m52s
2020-04-25 21:00:41.301261: Epoch: 1, Batch: 335, Loss: 0.5810, Elapsed: 4m28s
2020-04-25 21:00:44.833745: Epoch: 1, Batch: 327, Loss: 0.6200, Elapsed: 4m54s
2020-04-25 21:03:07.491147: Epoch: 1, Batch: 338, Loss: 0.6092, Elapsed: 4m34s
2020-04-25 21:07:48.924261: Epoch: 1, Batch: 328, Loss: 0.6394, Elapsed: 7m4s
2020-04-25 21:09:19.852280: Epoch: 1, Batch: 336, Loss: 0.5944, Elapsed: 8m38s
2020-04-25 21:10:12.071328: Epoch: 1, Batch: 339, Loss: 0.6014, Elapsed: 7m4s
2020-04-25 21:12:18.536631: Epoch: 1, Batch: 329, Loss: 0.6341, Elapsed: 4m29s
2020-04-25 21:14:53.786085: Epoch: 1, Batch: 337, Loss: 0.5871, Elapsed: 5m33s
2020-04-25 21:15:06.952126: Epoch: 1, Batch: 340, Loss: 0.6125, Elapsed: 4m54s
2020-04-25 21:16:36.899255: Epoch: 1, Batch: 330, Loss: 0.6217, Elapsed: 4m18s
2020-04-25 21:19:46.973906: Epoch: 1, Batch: 338, Loss: 0.5734, Elapsed: 4m53s
2020-04-25 21:22:26.005449: Epoch: 1, Batch: 331, Loss: 0.6235, Elapsed: 5m49s
2020-04-25 21:22:26.372516: Epoch: 1, Batch: 341, Loss: 0.6153, Elapsed: 7m19s
2020-04-25 21:25:13.291780: Epoch: 1, Batch: 339, Loss: 0.5829, Elapsed: 5m26s
2020-04-25 21:26:03.978877: Epoch: 1, Batch: 332, Loss: 0.6227, Elapsed: 3m37s
2020-04-25 21:28:29.029802: Epoch: 1, Batch: 342, Loss: 0.6359, Elapsed: 6m2s
2020-04-25 21:30:46.806437: Epoch: 1, Batch: 343, Loss: 0.5732, Elapsed: 2m17s
2020-04-25 21:32:17.734719: Epoch: 1, Batch: 333, Loss: 0.6577, Elapsed: 6m13s
2020-04-25 21:33:36.292703: Epoch: 1, Batch: 340, Loss: 0.6434, Elapsed: 8m22s
2020-04-25 21:37:13.568453: Epoch: 1, Batch: 334, Loss: 0.6402, Elapsed: 4m55s
2020-04-25 21:40:08.827697: Epoch: 1, Batch: 344, Loss: 0.6670, Elapsed: 9m21s
2020-04-25 21:41:43.810546: Epoch: 1, Batch: 341, Loss: 0.5769, Elapsed: 8m7s
2020-04-25 21:44:39.909474: Epoch: 1, Batch: 335, Loss: 0.6563, Elapsed: 7m26s
2020-04-25 21:46:19.175129: Epoch: 1, Batch: 342, Loss: 0.5177, Elapsed: 4m35s
2020-04-25 21:49:14.120848: Epoch: 1, Batch: 345, Loss: 0.6628, Elapsed: 9m5s
2020-04-25 21:51:18.561668: Epoch: 1, Batch: 336, Loss: 0.6296, Elapsed: 6m38s
2020-04-25 21:53:59.253569: Epoch: 1, Batch: 346, Loss: 0.5439, Elapsed: 4m45s
2020-04-25 21:55:35.976940: Epoch: 1, Batch: 343, Loss: 0.5707, Elapsed: 9m16s
2020-04-25 21:57:03.745959: Epoch: 1, Batch: 337, Loss: 0.6307, Elapsed: 5m45s
2020-04-25 22:00:56.999655: Epoch: 1, Batch: 347, Loss: 0.6077, Elapsed: 6m57s
2020-04-25 22:01:33.179350: Epoch: 1, Batch: 338, Loss: 0.6456, Elapsed: 4m29s
2020-04-25 22:05:24.316136: Epoch: 1, Batch: 348, Loss: 0.6084, Elapsed: 4m27s
2020-04-25 22:07:31.996400: Epoch: 1, Batch: 344, Loss: 0.6108, Elapsed: 11m55s
2020-04-25 22:11:10.740215: Epoch: 1, Batch: 349, Loss: 0.6012, Elapsed: 5m46s
2020-04-25 22:11:41.027481: Epoch: 1, Batch: 345, Loss: 0.5645, Elapsed: 4m9s
2020-04-25 22:11:52.035433: Epoch: 1, Batch: 339, Loss: 0.6671, Elapsed: 10m18s
2020-04-25 22:15:12.871116: Epoch: 1, Batch: 350, Loss: 0.5455, Elapsed: 4m2s
Starting testing the validation set with 200 subgraphs!
2020-04-25 22:15:11.258550: Epoch: 1, Batch: 346, Loss: 0.5139, Elapsed: 3m30s
2020-04-25 22:15:24.669135: Epoch: 1, Batch: 340, Loss: 0.5912, Elapsed: 3m32s
2020-04-25 22:18:40.672311: Epoch: 1, Batch: 341, Loss: 0.6647, Elapsed: 3m15s
2020-04-25 22:22:13.668104: Epoch: 1, Batch: 347, Loss: 0.6095, Elapsed: 7m2s
2020-04-25 22:23:13.355681: Epoch: 1, Batch: 342, Loss: 0.6405, Elapsed: 4m32s
2020-04-25 22:27:56.948129: Epoch: 1, Batch: 343, Loss: 0.6168, Elapsed: 4m43s
2020-04-25 22:29:53.507207: Epoch: 1, Batch: 348, Loss: 0.6439, Elapsed: 7m39s
2020-04-25 22:35:01.240756: Epoch: 1, Batch: 349, Loss: 0.5493, Elapsed: 5m7s
2020-04-25 22:39:30.106465: Epoch: 1, Batch: 344, Loss: 0.6999, Elapsed: 11m33s
2020-04-25 22:43:43.808055: Epoch: 1, Batch: 350, Loss: 0.5723, Elapsed: 8m42s
Starting testing the validation set with 200 subgraphs!
2020-04-25 22:46:49.152857: Epoch: 1, Batch: 345, Loss: 0.6299, Elapsed: 7m19s
2020-04-25 22:55:44.543443: Validation Test:  Loss: 0.6109,  Acc: 67.2516, AUC: 0.7230, Precision: 0.7685 -- Elapsed: 40m31s
2020-04-25 22:56:35.348156: Epoch: 1, Batch: 346, Loss: 0.6691, Elapsed: 9m46s
2020-04-25 23:00:36.957101: Epoch: 1, Batch: 347, Loss: 0.6461, Elapsed: 4m1s
2020-04-25 23:03:10.424812: Epoch: 1, Batch: 351, Loss: 0.6185, Elapsed: 7m25s
2020-04-25 23:05:47.589531: Epoch: 1, Batch: 348, Loss: 0.6491, Elapsed: 5m10s
2020-04-25 23:10:20.831726: Epoch: 1, Batch: 349, Loss: 0.6364, Elapsed: 4m33s
2020-04-25 23:11:48.704496: Epoch: 1, Batch: 352, Loss: 0.6219, Elapsed: 8m38s
2020-04-25 23:15:09.534118: Epoch: 1, Batch: 350, Loss: 0.6412, Elapsed: 4m48s
Starting testing the validation set with 200 subgraphs!
2020-04-25 23:16:24.349794: Epoch: 1, Batch: 353, Loss: 0.5706, Elapsed: 4m35s
2020-04-25 23:20:54.131467: Epoch: 1, Batch: 354, Loss: 0.6053, Elapsed: 4m29s
2020-04-25 23:24:06.512438: Validation Test:  Loss: 0.5962,  Acc: 67.8390, AUC: 0.7402, Precision: 0.7839 -- Elapsed: 40m22s
2020-04-25 23:25:37.092512: Epoch: 1, Batch: 355, Loss: 0.5871, Elapsed: 4m42s
2020-04-25 23:31:35.061878: Epoch: 1, Batch: 356, Loss: 0.5662, Elapsed: 5m57s
2020-04-25 23:31:43.486324: Epoch: 1, Batch: 351, Loss: 0.6172, Elapsed: 7m36s
2020-04-25 23:37:23.309141: Epoch: 1, Batch: 357, Loss: 0.5962, Elapsed: 5m48s
2020-04-25 23:38:39.844014: Epoch: 1, Batch: 352, Loss: 0.6299, Elapsed: 6m56s
2020-04-25 23:41:47.889236: Epoch: 1, Batch: 353, Loss: 0.4911, Elapsed: 3m8s
2020-04-25 23:44:59.075581: Epoch: 1, Batch: 358, Loss: 0.6147, Elapsed: 7m35s
2020-04-25 23:51:11.304333: Epoch: 1, Batch: 354, Loss: 0.6238, Elapsed: 9m23s
2020-04-25 23:54:48.207936: Epoch: 1, Batch: 359, Loss: 0.6497, Elapsed: 9m49s
2020-04-25 23:55:27.723671: Epoch: 1, Batch: 355, Loss: 0.5682, Elapsed: 4m16s
2020-04-25 23:56:05.946628: Validation Test:  Loss: 0.6434,  Acc: 61.9848, AUC: 0.6722, Precision: 0.7209 -- Elapsed: 40m56s
2020-04-26 00:00:15.810970: Epoch: 1, Batch: 356, Loss: 0.6261, Elapsed: 4m48s
2020-04-26 00:00:25.725295: Epoch: 1, Batch: 360, Loss: 0.5692, Elapsed: 5m37s
2020-04-26 00:00:30.996397: Epoch: 1, Batch: 351, Loss: 0.6483, Elapsed: 4m25s
2020-04-26 00:05:07.420918: Epoch: 1, Batch: 361, Loss: 0.5690, Elapsed: 4m41s
2020-04-26 00:07:18.185368: Epoch: 1, Batch: 352, Loss: 0.6587, Elapsed: 6m47s
2020-04-26 00:09:14.198461: Epoch: 1, Batch: 357, Loss: 0.6037, Elapsed: 8m58s
2020-04-26 00:11:35.066045: Epoch: 1, Batch: 362, Loss: 0.5964, Elapsed: 6m27s
2020-04-26 00:17:51.681032: Epoch: 1, Batch: 353, Loss: 0.6578, Elapsed: 10m33s
2020-04-26 00:18:55.835376: Epoch: 1, Batch: 358, Loss: 0.5953, Elapsed: 9m41s
2020-04-26 00:22:19.403895: Epoch: 1, Batch: 363, Loss: 0.6293, Elapsed: 10m44s
2020-04-26 00:25:06.865661: Epoch: 1, Batch: 359, Loss: 0.5490, Elapsed: 6m11s
2020-04-26 00:27:56.213386: Epoch: 1, Batch: 354, Loss: 0.6839, Elapsed: 10m4s
2020-04-26 00:29:35.439951: Epoch: 1, Batch: 364, Loss: 0.6330, Elapsed: 7m16s
2020-04-26 00:31:50.977330: Epoch: 1, Batch: 355, Loss: 0.5971, Elapsed: 3m54s
2020-04-26 00:33:05.384496: Epoch: 1, Batch: 360, Loss: 0.5807, Elapsed: 7m58s
2020-04-26 00:34:17.044311: Epoch: 1, Batch: 365, Loss: 0.5463, Elapsed: 4m41s
2020-04-26 00:39:40.459814: Epoch: 1, Batch: 356, Loss: 0.6586, Elapsed: 7m49s
2020-04-26 00:40:03.141556: Epoch: 1, Batch: 366, Loss: 0.5668, Elapsed: 5m46s
2020-04-26 00:45:06.880788: Epoch: 1, Batch: 367, Loss: 0.5628, Elapsed: 5m3s
2020-04-26 00:45:21.243146: Epoch: 1, Batch: 361, Loss: 0.6065, Elapsed: 12m15s
2020-04-26 00:49:46.317074: Epoch: 1, Batch: 368, Loss: 0.5726, Elapsed: 4m39s
2020-04-26 00:50:53.382612: Epoch: 1, Batch: 357, Loss: 0.6477, Elapsed: 11m12s
2020-04-26 00:54:08.072833: Epoch: 1, Batch: 369, Loss: 0.5686, Elapsed: 4m21s
2020-04-26 00:54:13.812582: Epoch: 1, Batch: 362, Loss: 0.6136, Elapsed: 8m52s
2020-04-26 00:58:00.317828: Epoch: 1, Batch: 358, Loss: 0.6432, Elapsed: 7m6s
2020-04-26 01:00:17.737330: Epoch: 1, Batch: 363, Loss: 0.5927, Elapsed: 6m3s
2020-04-26 01:01:48.057216: Epoch: 1, Batch: 370, Loss: 0.7464, Elapsed: 7m39s
2020-04-26 01:04:21.132731: Epoch: 1, Batch: 364, Loss: 0.5272, Elapsed: 4m3s
2020-04-26 01:06:17.746217: Epoch: 1, Batch: 359, Loss: 0.6387, Elapsed: 8m17s
2020-04-26 01:10:14.285282: Epoch: 1, Batch: 371, Loss: 0.6362, Elapsed: 8m26s
2020-04-26 01:11:03.324942: Epoch: 1, Batch: 365, Loss: 0.5700, Elapsed: 6m42s
2020-04-26 01:13:06.267714: Epoch: 1, Batch: 360, Loss: 0.6454, Elapsed: 6m48s
2020-04-26 01:15:27.603101: Epoch: 1, Batch: 366, Loss: 0.5299, Elapsed: 4m24s
2020-04-26 01:19:37.136531: Epoch: 1, Batch: 372, Loss: 0.5965, Elapsed: 9m22s
2020-04-26 01:20:15.942605: Epoch: 1, Batch: 361, Loss: 0.6459, Elapsed: 7m9s
2020-04-26 01:20:22.846675: Epoch: 1, Batch: 367, Loss: 0.5646, Elapsed: 4m55s
2020-04-26 01:24:20.017707: Epoch: 1, Batch: 368, Loss: 0.5603, Elapsed: 3m57s
2020-04-26 01:25:22.487822: Epoch: 1, Batch: 362, Loss: 0.6144, Elapsed: 5m6s
2020-04-26 01:26:15.105267: Epoch: 1, Batch: 373, Loss: 0.6188, Elapsed: 6m37s
2020-04-26 01:30:49.464742: Epoch: 1, Batch: 374, Loss: 0.5758, Elapsed: 4m34s
2020-04-26 01:32:18.214519: Epoch: 1, Batch: 369, Loss: 0.5967, Elapsed: 7m58s
2020-04-26 01:34:22.054998: Epoch: 1, Batch: 363, Loss: 0.7196, Elapsed: 8m59s
2020-04-26 01:37:42.596503: Epoch: 1, Batch: 375, Loss: 0.5864, Elapsed: 6m53s
2020-04-26 01:38:24.901709: Epoch: 1, Batch: 370, Loss: 0.5874, Elapsed: 6m6s
2020-04-26 01:41:36.378762: Epoch: 1, Batch: 376, Loss: 0.5667, Elapsed: 3m53s
2020-04-26 01:43:47.223261: Epoch: 1, Batch: 364, Loss: 0.6435, Elapsed: 9m25s
2020-04-26 01:48:17.856948: Epoch: 1, Batch: 377, Loss: 0.6107, Elapsed: 6m41s
2020-04-26 01:48:21.933599: Epoch: 1, Batch: 371, Loss: 0.6138, Elapsed: 9m57s
2020-04-26 01:48:40.253719: Epoch: 1, Batch: 365, Loss: 0.6225, Elapsed: 4m53s
2020-04-26 01:53:02.016432: Epoch: 1, Batch: 366, Loss: 0.6244, Elapsed: 4m21s
2020-04-26 01:52:58.220744: Epoch: 1, Batch: 372, Loss: 0.5730, Elapsed: 4m36s
2020-04-26 01:53:46.878209: Epoch: 1, Batch: 378, Loss: 0.6086, Elapsed: 5m28s
2020-04-26 01:59:04.714979: Epoch: 1, Batch: 373, Loss: 0.5838, Elapsed: 6m6s
2020-04-26 02:00:01.401411: Epoch: 1, Batch: 367, Loss: 0.6618, Elapsed: 6m59s
2020-04-26 02:01:58.161305: Epoch: 1, Batch: 379, Loss: 0.5809, Elapsed: 8m11s
2020-04-26 02:03:29.308684: Epoch: 1, Batch: 374, Loss: 0.5429, Elapsed: 4m24s
2020-04-26 02:05:15.063283: Epoch: 1, Batch: 368, Loss: 0.6384, Elapsed: 5m13s
2020-04-26 02:09:20.790709: Epoch: 1, Batch: 380, Loss: 0.5956, Elapsed: 7m22s
2020-04-26 02:10:34.931159: Epoch: 1, Batch: 375, Loss: 0.5853, Elapsed: 7m5s
2020-04-26 02:11:48.921172: Epoch: 1, Batch: 369, Loss: 0.6511, Elapsed: 6m33s
2020-04-26 02:15:46.833132: Epoch: 1, Batch: 381, Loss: 0.5779, Elapsed: 6m26s
2020-04-26 02:17:48.882405: Epoch: 1, Batch: 376, Loss: 0.5995, Elapsed: 7m13s
2020-04-26 02:18:13.069438: Epoch: 1, Batch: 370, Loss: 0.6390, Elapsed: 6m24s
2020-04-26 02:19:20.777388: Epoch: 1, Batch: 382, Loss: 0.5736, Elapsed: 3m33s
2020-04-26 02:23:22.066613: Epoch: 1, Batch: 377, Loss: 0.5975, Elapsed: 5m33s
2020-04-26 02:24:39.858800: Epoch: 1, Batch: 371, Loss: 0.6584, Elapsed: 6m26s
2020-04-26 02:26:24.574440: Epoch: 1, Batch: 383, Loss: 0.6355, Elapsed: 7m3s
2020-04-26 02:28:36.956267: Epoch: 1, Batch: 378, Loss: 0.5879, Elapsed: 5m14s
2020-04-26 02:29:21.852977: Epoch: 1, Batch: 372, Loss: 0.6299, Elapsed: 4m41s
2020-04-26 02:33:41.417533: Epoch: 1, Batch: 384, Loss: 0.6165, Elapsed: 7m16s
2020-04-26 02:36:08.591690: Epoch: 1, Batch: 379, Loss: 0.6195, Elapsed: 7m31s
2020-04-26 02:36:16.229177: Epoch: 1, Batch: 373, Loss: 0.6247, Elapsed: 6m54s
2020-04-26 02:39:28.004713: Epoch: 1, Batch: 385, Loss: 0.6190, Elapsed: 5m46s
2020-04-26 02:40:29.202319: Epoch: 1, Batch: 380, Loss: 0.5683, Elapsed: 4m20s
2020-04-26 02:40:53.431437: Epoch: 1, Batch: 374, Loss: 0.6708, Elapsed: 4m37s
2020-04-26 02:44:40.852608: Epoch: 1, Batch: 375, Loss: 0.6114, Elapsed: 3m47s
2020-04-26 02:45:08.794481: Epoch: 1, Batch: 381, Loss: 0.5475, Elapsed: 4m39s
2020-04-26 02:48:19.165635: Epoch: 1, Batch: 386, Loss: 0.6032, Elapsed: 8m51s
2020-04-26 02:48:52.498136: Epoch: 1, Batch: 376, Loss: 0.6128, Elapsed: 4m11s
2020-04-26 02:51:53.665187: Epoch: 1, Batch: 382, Loss: 0.6097, Elapsed: 6m44s
2020-04-26 02:53:44.890872: Epoch: 1, Batch: 387, Loss: 0.5770, Elapsed: 5m25s
2020-04-26 02:55:54.561171: Epoch: 1, Batch: 377, Loss: 0.6353, Elapsed: 7m2s
2020-04-26 02:58:06.003402: Epoch: 1, Batch: 388, Loss: 0.5841, Elapsed: 4m21s
2020-04-26 03:00:36.159145: Epoch: 1, Batch: 383, Loss: 0.6298, Elapsed: 8m42s
2020-04-26 03:02:12.946731: Epoch: 1, Batch: 389, Loss: 0.5818, Elapsed: 4m6s
2020-04-26 03:03:47.026664: Epoch: 1, Batch: 378, Loss: 0.6360, Elapsed: 7m52s
2020-04-26 03:05:37.921339: Epoch: 1, Batch: 384, Loss: 0.5648, Elapsed: 5m1s
2020-04-26 03:10:18.326524: Epoch: 1, Batch: 390, Loss: 0.6166, Elapsed: 8m5s
2020-04-26 03:12:32.185964: Epoch: 1, Batch: 379, Loss: 0.6487, Elapsed: 8m45s
2020-04-26 03:12:53.266729: Epoch: 1, Batch: 385, Loss: 0.5823, Elapsed: 7m15s
2020-04-26 03:17:34.998680: Epoch: 1, Batch: 391, Loss: 0.6190, Elapsed: 7m16s
2020-04-26 03:19:49.885639: Epoch: 1, Batch: 380, Loss: 0.6443, Elapsed: 7m17s
2020-04-26 03:21:12.569399: Epoch: 1, Batch: 386, Loss: 0.5768, Elapsed: 8m19s
2020-04-26 03:25:04.928594: Epoch: 1, Batch: 392, Loss: 0.6097, Elapsed: 7m29s
2020-04-26 03:27:35.020832: Epoch: 1, Batch: 387, Loss: 0.5541, Elapsed: 6m22s
2020-04-26 03:31:55.592796: Epoch: 1, Batch: 381, Loss: 0.6433, Elapsed: 12m5s
2020-04-26 03:32:21.915911: Epoch: 1, Batch: 393, Loss: 0.6277, Elapsed: 7m16s
2020-04-26 03:36:40.855676: Epoch: 1, Batch: 388, Loss: 0.5951, Elapsed: 9m5s
2020-04-26 03:36:52.681831: Epoch: 1, Batch: 382, Loss: 0.6456, Elapsed: 4m57s
2020-04-26 03:40:49.175352: Epoch: 1, Batch: 394, Loss: 0.6794, Elapsed: 8m27s
2020-04-26 03:41:04.948823: Epoch: 1, Batch: 389, Loss: 0.6093, Elapsed: 4m24s
2020-04-26 03:44:25.265631: Epoch: 1, Batch: 383, Loss: 0.6538, Elapsed: 7m32s
2020-04-26 03:44:30.103718: Epoch: 1, Batch: 390, Loss: 0.5266, Elapsed: 3m25s
2020-04-26 03:45:01.926831: Epoch: 1, Batch: 395, Loss: 0.5784, Elapsed: 4m12s
2020-04-26 03:50:11.281199: Epoch: 1, Batch: 384, Loss: 0.6506, Elapsed: 5m45s
2020-04-26 03:50:17.390199: Epoch: 1, Batch: 391, Loss: 0.5807, Elapsed: 5m47s
2020-04-26 03:52:33.035913: Epoch: 1, Batch: 396, Loss: 0.6089, Elapsed: 7m31s
2020-04-26 03:55:52.720661: Epoch: 1, Batch: 385, Loss: 0.6171, Elapsed: 5m41s
2020-04-26 03:56:33.054645: Epoch: 1, Batch: 392, Loss: 0.5751, Elapsed: 6m15s
2020-04-26 03:58:05.801460: Epoch: 1, Batch: 397, Loss: 0.5982, Elapsed: 5m32s
2020-04-26 04:00:33.813181: Epoch: 1, Batch: 393, Loss: 0.5467, Elapsed: 4m0s
2020-04-26 04:01:26.622395: Epoch: 1, Batch: 386, Loss: 0.6037, Elapsed: 5m33s
2020-04-26 04:04:55.985134: Epoch: 1, Batch: 398, Loss: 0.6514, Elapsed: 6m50s
2020-04-26 04:06:53.647543: Epoch: 1, Batch: 387, Loss: 0.6631, Elapsed: 5m27s
2020-04-26 04:07:55.112292: Epoch: 1, Batch: 394, Loss: 0.5944, Elapsed: 7m21s
2020-04-26 04:14:54.677056: Epoch: 1, Batch: 395, Loss: 0.6050, Elapsed: 6m59s
2020-04-26 04:16:24.483706: Epoch: 1, Batch: 399, Loss: 0.6349, Elapsed: 11m28s
2020-04-26 04:20:25.679281: Epoch: 1, Batch: 388, Loss: 0.7130, Elapsed: 13m32s
2020-04-26 04:22:25.162969: Epoch: 1, Batch: 400, Loss: 0.5615, Elapsed: 6m0s
Starting testing the validation set with 200 subgraphs!
2020-04-26 04:22:58.699802: Epoch: 1, Batch: 396, Loss: 0.5996, Elapsed: 8m3s
2020-04-26 04:28:26.866121: Epoch: 1, Batch: 397, Loss: 0.6057, Elapsed: 5m28s
2020-04-26 04:29:53.415248: Epoch: 1, Batch: 389, Loss: 0.6592, Elapsed: 9m27s
2020-04-26 04:35:56.690796: Epoch: 1, Batch: 390, Loss: 0.6400, Elapsed: 6m3s
2020-04-26 04:37:20.596541: Epoch: 1, Batch: 398, Loss: 0.6259, Elapsed: 8m53s
2020-04-26 04:40:53.025883: Epoch: 1, Batch: 391, Loss: 0.6024, Elapsed: 4m56s
2020-04-26 04:43:52.081856: Epoch: 1, Batch: 399, Loss: 0.6059, Elapsed: 6m31s
2020-04-26 04:46:39.830582: Epoch: 1, Batch: 392, Loss: 0.6379, Elapsed: 5m46s
2020-04-26 04:51:23.737304: Epoch: 1, Batch: 393, Loss: 0.6231, Elapsed: 4m43s
2020-04-26 04:52:13.436051: Epoch: 1, Batch: 400, Loss: 0.6192, Elapsed: 8m21s
Starting testing the validation set with 200 subgraphs!
2020-04-26 05:00:42.309583: Epoch: 1, Batch: 394, Loss: 0.6469, Elapsed: 9m18s
2020-04-26 05:02:52.612639: Validation Test:  Loss: 0.6046,  Acc: 68.2569, AUC: 0.7288, Precision: 0.7687 -- Elapsed: 40m27s
2020-04-26 05:05:22.441502: Epoch: 1, Batch: 395, Loss: 0.6427, Elapsed: 4m40s
2020-04-26 05:09:39.928926: Epoch: 1, Batch: 396, Loss: 0.6190, Elapsed: 4m17s
2020-04-26 05:11:07.250917: Epoch: 1, Batch: 401, Loss: 0.6421, Elapsed: 8m14s
2020-04-26 05:14:30.353450: Epoch: 1, Batch: 402, Loss: 0.5588, Elapsed: 3m23s
2020-04-26 05:18:23.572012: Epoch: 1, Batch: 397, Loss: 0.6515, Elapsed: 8m43s
2020-04-26 05:21:30.559400: Epoch: 1, Batch: 403, Loss: 0.6038, Elapsed: 7m0s
2020-04-26 05:22:59.101761: Epoch: 1, Batch: 398, Loss: 0.6454, Elapsed: 4m35s
2020-04-26 05:28:22.524687: Epoch: 1, Batch: 399, Loss: 0.6573, Elapsed: 5m23s
2020-04-26 05:28:46.023769: Epoch: 1, Batch: 404, Loss: 0.5841, Elapsed: 7m15s
2020-04-26 05:32:43.311176: Epoch: 1, Batch: 400, Loss: 0.6462, Elapsed: 4m20s
Starting testing the validation set with 200 subgraphs!
2020-04-26 05:33:14.884248: Validation Test:  Loss: 0.5939,  Acc: 69.7187, AUC: 0.7510, Precision: 0.7983 -- Elapsed: 41m1s
2020-04-26 05:34:00.075738: Epoch: 1, Batch: 405, Loss: 0.5821, Elapsed: 5m14s
2020-04-26 05:41:40.742536: Epoch: 1, Batch: 401, Loss: 0.5797, Elapsed: 8m25s
2020-04-26 05:42:08.027319: Epoch: 1, Batch: 406, Loss: 0.6179, Elapsed: 8m7s
2020-04-26 05:47:17.597823: Epoch: 1, Batch: 407, Loss: 0.5824, Elapsed: 5m9s
2020-04-26 05:47:43.793768: Epoch: 1, Batch: 402, Loss: 0.5728, Elapsed: 6m3s
2020-04-26 05:53:42.689851: Epoch: 1, Batch: 408, Loss: 0.5494, Elapsed: 6m25s
2020-04-26 05:55:11.789158: Epoch: 1, Batch: 403, Loss: 0.5846, Elapsed: 7m27s
2020-04-26 05:57:17.205658: Epoch: 1, Batch: 409, Loss: 0.5351, Elapsed: 3m34s
2020-04-26 05:59:41.422097: Epoch: 1, Batch: 404, Loss: 0.6310, Elapsed: 4m29s
2020-04-26 06:01:58.200422: Epoch: 1, Batch: 410, Loss: 0.5826, Elapsed: 4m40s
2020-04-26 06:05:28.529454: Epoch: 1, Batch: 405, Loss: 0.5761, Elapsed: 5m47s
2020-04-26 06:08:09.219428: Epoch: 1, Batch: 411, Loss: 0.5880, Elapsed: 6m10s
2020-04-26 06:11:20.264015: Epoch: 1, Batch: 406, Loss: 0.5738, Elapsed: 5m51s
2020-04-26 06:14:04.496796: Validation Test:  Loss: 0.6421,  Acc: 62.4910, AUC: 0.6792, Precision: 0.7237 -- Elapsed: 41m21s
2020-04-26 06:14:37.228568: Epoch: 1, Batch: 412, Loss: 0.6353, Elapsed: 6m27s
2020-04-26 06:17:01.733785: Epoch: 1, Batch: 407, Loss: 0.5648, Elapsed: 5m41s
2020-04-26 06:20:00.808669: Epoch: 1, Batch: 401, Loss: 0.6301, Elapsed: 5m56s
2020-04-26 06:22:12.887691: Epoch: 1, Batch: 413, Loss: 0.6527, Elapsed: 7m35s
2020-04-26 06:25:36.552379: Epoch: 1, Batch: 408, Loss: 0.5862, Elapsed: 8m34s
2020-04-26 06:25:39.361453: Epoch: 1, Batch: 402, Loss: 0.6230, Elapsed: 5m38s
2020-04-26 06:26:07.580542: Epoch: 1, Batch: 414, Loss: 0.5446, Elapsed: 3m54s
2020-04-26 06:30:04.181627: Epoch: 1, Batch: 415, Loss: 0.5930, Elapsed: 3m56s
2020-04-26 06:31:21.567359: Epoch: 1, Batch: 403, Loss: 0.6172, Elapsed: 5m42s
2020-04-26 06:32:59.308237: Epoch: 1, Batch: 409, Loss: 0.5663, Elapsed: 7m22s
2020-04-26 06:35:04.001726: Epoch: 1, Batch: 404, Loss: 0.6443, Elapsed: 3m42s
2020-04-26 06:35:59.606397: Epoch: 1, Batch: 416, Loss: 0.5702, Elapsed: 5m55s
2020-04-26 06:37:03.374329: Epoch: 1, Batch: 410, Loss: 0.5231, Elapsed: 4m4s
2020-04-26 06:41:19.928994: Epoch: 1, Batch: 405, Loss: 0.6487, Elapsed: 6m15s
2020-04-26 06:45:17.853163: Epoch: 1, Batch: 411, Loss: 0.5849, Elapsed: 8m14s
2020-04-26 06:49:29.789607: Epoch: 1, Batch: 417, Loss: 0.6396, Elapsed: 13m30s
2020-04-26 06:51:13.058463: Epoch: 1, Batch: 406, Loss: 0.6316, Elapsed: 9m53s
2020-04-26 06:56:14.273729: Epoch: 1, Batch: 412, Loss: 0.6317, Elapsed: 10m56s
2020-04-26 06:58:59.525764: Epoch: 1, Batch: 407, Loss: 0.6455, Elapsed: 7m46s
2020-04-26 06:59:24.869537: Epoch: 1, Batch: 418, Loss: 0.5879, Elapsed: 9m55s
2020-04-26 07:00:59.448562: Epoch: 1, Batch: 413, Loss: 0.5843, Elapsed: 4m45s
2020-04-26 07:03:50.057920: Epoch: 1, Batch: 419, Loss: 0.5934, Elapsed: 4m25s
2020-04-26 07:06:14.407711: Epoch: 1, Batch: 408, Loss: 0.6467, Elapsed: 7m14s
2020-04-26 07:06:24.329218: Epoch: 1, Batch: 414, Loss: 0.6299, Elapsed: 5m24s
2020-04-26 07:09:01.276966: Epoch: 1, Batch: 420, Loss: 0.5487, Elapsed: 5m11s
2020-04-26 07:12:16.802567: Epoch: 1, Batch: 409, Loss: 0.6366, Elapsed: 6m2s
2020-04-26 07:12:25.550909: Epoch: 1, Batch: 415, Loss: 0.5815, Elapsed: 6m1s
2020-04-26 07:15:48.000826: Epoch: 1, Batch: 421, Loss: 0.6367, Elapsed: 6m46s
2020-04-26 07:19:03.800507: Epoch: 1, Batch: 410, Loss: 0.6352, Elapsed: 6m46s
2020-04-26 07:19:34.347187: Epoch: 1, Batch: 416, Loss: 0.5825, Elapsed: 7m8s
2020-04-26 07:20:43.218971: Epoch: 1, Batch: 422, Loss: 0.5633, Elapsed: 4m55s
2020-04-26 07:22:10.387042: Epoch: 1, Batch: 411, Loss: 0.6205, Elapsed: 3m6s
2020-04-26 07:23:29.367939: Epoch: 1, Batch: 417, Loss: 0.5360, Elapsed: 3m54s
2020-04-26 07:27:43.234466: Epoch: 1, Batch: 423, Loss: 0.6146, Elapsed: 6m59s
2020-04-26 07:29:15.228994: Epoch: 1, Batch: 412, Loss: 0.6557, Elapsed: 7m4s
2020-04-26 07:29:50.755765: Epoch: 1, Batch: 418, Loss: 0.5895, Elapsed: 6m21s
2020-04-26 07:32:07.549032: Epoch: 1, Batch: 424, Loss: 0.5740, Elapsed: 4m24s
2020-04-26 07:34:31.957535: Epoch: 1, Batch: 419, Loss: 0.6091, Elapsed: 4m41s
2020-04-26 07:36:24.270223: Epoch: 1, Batch: 413, Loss: 0.6293, Elapsed: 7m9s
2020-04-26 07:39:38.977969: Epoch: 1, Batch: 420, Loss: 0.6012, Elapsed: 5m6s
2020-04-26 07:40:05.134080: Epoch: 1, Batch: 425, Loss: 0.6181, Elapsed: 7m57s
2020-04-26 07:43:14.297774: Epoch: 1, Batch: 426, Loss: 0.5005, Elapsed: 3m9s
2020-04-26 07:45:17.268143: Epoch: 1, Batch: 414, Loss: 0.6659, Elapsed: 8m52s
2020-04-26 07:46:20.377357: Epoch: 1, Batch: 421, Loss: 0.5776, Elapsed: 6m41s
2020-04-26 07:50:25.314385: Epoch: 1, Batch: 415, Loss: 0.6391, Elapsed: 5m8s
2020-04-26 07:50:47.582136: Epoch: 1, Batch: 427, Loss: 0.6119, Elapsed: 7m33s
2020-04-26 07:53:00.467474: Epoch: 1, Batch: 422, Loss: 0.5836, Elapsed: 6m40s
2020-04-26 07:58:42.521665: Epoch: 1, Batch: 428, Loss: 0.6388, Elapsed: 7m54s
2020-04-26 08:00:01.589992: Epoch: 1, Batch: 416, Loss: 0.6518, Elapsed: 9m36s
2020-04-26 08:01:09.727295: Epoch: 1, Batch: 423, Loss: 0.6122, Elapsed: 8m9s
2020-04-26 08:05:51.789482: Epoch: 1, Batch: 424, Loss: 0.5112, Elapsed: 4m42s
2020-04-26 08:05:46.398820: Epoch: 1, Batch: 429, Loss: 0.6015, Elapsed: 7m3s
2020-04-26 08:06:14.067750: Epoch: 1, Batch: 417, Loss: 0.6272, Elapsed: 6m12s
2020-04-26 08:11:25.423376: Epoch: 1, Batch: 425, Loss: 0.5473, Elapsed: 5m33s
2020-04-26 08:14:24.240650: Epoch: 1, Batch: 418, Loss: 0.6612, Elapsed: 8m10s
2020-04-26 08:17:39.766787: Epoch: 1, Batch: 430, Loss: 0.6413, Elapsed: 11m53s
2020-04-26 08:19:38.247689: Epoch: 1, Batch: 419, Loss: 0.6541, Elapsed: 5m13s
2020-04-26 08:20:40.202820: Epoch: 1, Batch: 426, Loss: 0.6359, Elapsed: 9m14s
2020-04-26 08:27:10.999663: Epoch: 1, Batch: 420, Loss: 0.6330, Elapsed: 7m32s
2020-04-26 08:27:56.078485: Epoch: 1, Batch: 427, Loss: 0.6357, Elapsed: 7m15s
2020-04-26 08:28:01.586399: Epoch: 1, Batch: 431, Loss: 0.6422, Elapsed: 10m21s
2020-04-26 08:32:40.282929: Epoch: 1, Batch: 421, Loss: 0.6347, Elapsed: 5m29s
2020-04-26 08:35:02.282180: Epoch: 1, Batch: 432, Loss: 0.6403, Elapsed: 7m0s
2020-04-26 08:35:47.378962: Epoch: 1, Batch: 428, Loss: 0.6274, Elapsed: 7m51s
2020-04-26 08:38:38.807033: Epoch: 1, Batch: 422, Loss: 0.6589, Elapsed: 5m58s
2020-04-26 08:41:52.562740: Epoch: 1, Batch: 429, Loss: 0.5734, Elapsed: 6m5s
2020-04-26 08:43:03.619337: Epoch: 1, Batch: 433, Loss: 0.6449, Elapsed: 8m1s
2020-04-26 08:43:47.115368: Epoch: 1, Batch: 423, Loss: 0.6186, Elapsed: 5m8s
2020-04-26 08:48:50.522334: Epoch: 1, Batch: 434, Loss: 0.6088, Elapsed: 5m46s
2020-04-26 08:49:46.335152: Epoch: 1, Batch: 424, Loss: 0.6517, Elapsed: 5m59s
2020-04-26 08:51:36.859286: Epoch: 1, Batch: 430, Loss: 0.5841, Elapsed: 9m44s
2020-04-26 08:53:47.584922: Epoch: 1, Batch: 435, Loss: 0.5697, Elapsed: 4m57s
2020-04-26 08:58:07.846296: Epoch: 1, Batch: 431, Loss: 0.5735, Elapsed: 6m30s
2020-04-26 08:58:17.813597: Epoch: 1, Batch: 425, Loss: 0.6389, Elapsed: 8m31s
2020-04-26 08:58:33.295110: Epoch: 1, Batch: 436, Loss: 0.5872, Elapsed: 4m45s
2020-04-26 09:02:52.421389: Epoch: 1, Batch: 432, Loss: 0.5624, Elapsed: 4m44s
2020-04-26 09:03:06.033195: Epoch: 1, Batch: 426, Loss: 0.6448, Elapsed: 4m48s
2020-04-26 09:03:18.052616: Epoch: 1, Batch: 437, Loss: 0.5745, Elapsed: 4m44s
2020-04-26 09:07:52.700317: Epoch: 1, Batch: 438, Loss: 0.5358, Elapsed: 4m34s
2020-04-26 09:08:03.169157: Epoch: 1, Batch: 427, Loss: 0.6322, Elapsed: 4m57s
2020-04-26 09:10:02.971531: Epoch: 1, Batch: 433, Loss: 0.6444, Elapsed: 7m10s
2020-04-26 09:14:10.806160: Epoch: 1, Batch: 439, Loss: 0.6008, Elapsed: 6m18s
2020-04-26 09:16:25.571197: Epoch: 1, Batch: 428, Loss: 0.6567, Elapsed: 8m22s
2020-04-26 09:20:30.816162: Epoch: 1, Batch: 429, Loss: 0.6211, Elapsed: 4m5s
2020-04-26 09:24:25.450172: Epoch: 1, Batch: 440, Loss: 0.6589, Elapsed: 10m14s
2020-04-26 09:25:04.517418: Epoch: 1, Batch: 434, Loss: 0.6196, Elapsed: 15m1s
2020-04-26 09:26:25.943548: Epoch: 1, Batch: 430, Loss: 0.6256, Elapsed: 5m55s
2020-04-26 09:29:01.462607: Epoch: 1, Batch: 441, Loss: 0.5322, Elapsed: 4m35s
2020-04-26 09:30:26.657746: Epoch: 1, Batch: 431, Loss: 0.6467, Elapsed: 4m0s
2020-04-26 09:34:30.296290: Epoch: 1, Batch: 442, Loss: 0.6059, Elapsed: 5m28s
2020-04-26 09:36:01.345396: Epoch: 1, Batch: 432, Loss: 0.6589, Elapsed: 5m34s
2020-04-26 09:39:01.688852: Epoch: 1, Batch: 435, Loss: 0.6315, Elapsed: 13m57s
2020-04-26 09:39:33.758404: Epoch: 1, Batch: 443, Loss: 0.5469, Elapsed: 5m3s
2020-04-26 09:43:33.154139: Epoch: 1, Batch: 444, Loss: 0.5175, Elapsed: 3m59s
2020-04-26 09:43:54.595447: Epoch: 1, Batch: 433, Loss: 0.6296, Elapsed: 7m53s
2020-04-26 09:47:09.482776: Epoch: 1, Batch: 436, Loss: 0.5996, Elapsed: 8m7s
2020-04-26 09:49:51.357106: Epoch: 1, Batch: 434, Loss: 0.6506, Elapsed: 5m56s
2020-04-26 09:51:13.109456: Epoch: 1, Batch: 445, Loss: 0.6371, Elapsed: 7m39s
2020-04-26 09:52:58.489240: Epoch: 1, Batch: 437, Loss: 0.5901, Elapsed: 5m48s
2020-04-26 09:55:11.941755: Epoch: 1, Batch: 435, Loss: 0.6115, Elapsed: 5m20s
2020-04-26 09:56:16.634638: Epoch: 1, Batch: 446, Loss: 0.5772, Elapsed: 5m3s
2020-04-26 09:59:20.189864: Epoch: 1, Batch: 438, Loss: 0.5653, Elapsed: 6m21s
2020-04-26 10:00:49.181251: Epoch: 1, Batch: 436, Loss: 0.6450, Elapsed: 5m37s
2020-04-26 10:01:01.735492: Epoch: 1, Batch: 447, Loss: 0.5803, Elapsed: 4m45s
2020-04-26 10:05:06.359113: Epoch: 1, Batch: 439, Loss: 0.5983, Elapsed: 5m46s
2020-04-26 10:07:02.593443: Epoch: 1, Batch: 448, Loss: 0.5808, Elapsed: 6m0s
2020-04-26 10:10:11.479022: Epoch: 1, Batch: 437, Loss: 0.6682, Elapsed: 9m22s
2020-04-26 10:11:00.782072: Epoch: 1, Batch: 440, Loss: 0.5482, Elapsed: 5m54s
2020-04-26 10:13:58.207705: Epoch: 1, Batch: 441, Loss: 0.5202, Elapsed: 2m57s
2020-04-26 10:15:50.890887: Epoch: 1, Batch: 449, Loss: 0.6141, Elapsed: 8m48s
2020-04-26 10:17:11.391470: Epoch: 1, Batch: 438, Loss: 0.6159, Elapsed: 6m59s
2020-04-26 10:21:29.386946: Epoch: 1, Batch: 439, Loss: 0.6344, Elapsed: 4m17s
2020-04-26 10:23:16.751358: Epoch: 1, Batch: 442, Loss: 0.5738, Elapsed: 9m18s
2020-04-26 10:23:57.608412: Epoch: 1, Batch: 450, Loss: 0.5950, Elapsed: 8m6s
Starting testing the validation set with 200 subgraphs!
2020-04-26 10:27:17.173575: Epoch: 1, Batch: 440, Loss: 0.6390, Elapsed: 5m47s
2020-04-26 10:28:48.962619: Epoch: 1, Batch: 443, Loss: 0.5151, Elapsed: 5m32s
2020-04-26 10:34:35.401622: Epoch: 1, Batch: 441, Loss: 0.6469, Elapsed: 7m18s
2020-04-26 10:34:50.576136: Epoch: 1, Batch: 444, Loss: 0.6162, Elapsed: 6m1s
2020-04-26 10:41:00.066961: Epoch: 1, Batch: 445, Loss: 0.5978, Elapsed: 6m9s
2020-04-26 10:41:23.265469: Epoch: 1, Batch: 442, Loss: 0.6450, Elapsed: 6m47s
2020-04-26 10:45:45.149445: Epoch: 1, Batch: 443, Loss: 0.6255, Elapsed: 4m21s
2020-04-26 10:46:41.947027: Epoch: 1, Batch: 446, Loss: 0.5614, Elapsed: 5m41s
2020-04-26 10:50:59.676530: Epoch: 1, Batch: 447, Loss: 0.5644, Elapsed: 4m17s
2020-04-26 10:51:29.573972: Epoch: 1, Batch: 444, Loss: 0.6569, Elapsed: 5m44s
2020-04-26 10:56:36.448609: Epoch: 1, Batch: 448, Loss: 0.6039, Elapsed: 5m36s
2020-04-26 10:58:01.956960: Epoch: 1, Batch: 445, Loss: 0.6593, Elapsed: 6m32s
2020-04-26 11:04:17.540769: Epoch: 1, Batch: 449, Loss: 0.6126, Elapsed: 7m41s
2020-04-26 11:05:05.599070: Epoch: 1, Batch: 446, Loss: 0.6491, Elapsed: 7m3s
2020-04-26 11:05:40.827549: Validation Test:  Loss: 0.6069,  Acc: 68.0794, AUC: 0.7296, Precision: 0.7755 -- Elapsed: 41m43s
2020-04-26 11:10:41.814537: Epoch: 1, Batch: 450, Loss: 0.5574, Elapsed: 6m24s
Starting testing the validation set with 200 subgraphs!
2020-04-26 11:15:30.648844: Epoch: 1, Batch: 447, Loss: 0.6590, Elapsed: 10m25s
2020-04-26 11:18:08.825690: Epoch: 1, Batch: 451, Loss: 0.6570, Elapsed: 12m27s
2020-04-26 11:21:24.806456: Epoch: 1, Batch: 448, Loss: 0.6530, Elapsed: 5m54s
2020-04-26 11:22:51.779987: Epoch: 1, Batch: 452, Loss: 0.6318, Elapsed: 4m42s
2020-04-26 11:28:06.550146: Epoch: 1, Batch: 449, Loss: 0.6335, Elapsed: 6m41s
2020-04-26 11:32:04.086721: Epoch: 1, Batch: 450, Loss: 0.6108, Elapsed: 3m57s
Starting testing the validation set with 200 subgraphs!
2020-04-26 11:32:32.088236: Epoch: 1, Batch: 453, Loss: 0.5977, Elapsed: 9m40s
2020-04-26 11:36:33.693894: Epoch: 1, Batch: 454, Loss: 0.5922, Elapsed: 4m1s
2020-04-26 11:40:12.035283: Epoch: 1, Batch: 455, Loss: 0.5517, Elapsed: 3m38s
2020-04-26 11:47:00.812413: Epoch: 1, Batch: 456, Loss: 0.6040, Elapsed: 6m48s
2020-04-26 11:51:30.371762: Validation Test:  Loss: 0.5905,  Acc: 68.7652, AUC: 0.7508, Precision: 0.8042 -- Elapsed: 40m48s
2020-04-26 11:59:02.602494: Epoch: 1, Batch: 451, Loss: 0.6131, Elapsed: 7m32s
2020-04-26 11:59:06.114867: Epoch: 1, Batch: 457, Loss: 0.6382, Elapsed: 12m5s
2020-04-26 12:03:42.564872: Epoch: 1, Batch: 458, Loss: 0.6433, Elapsed: 4m36s
2020-04-26 12:05:59.782162: Epoch: 1, Batch: 452, Loss: 0.5940, Elapsed: 6m57s
2020-04-26 12:10:24.951163: Epoch: 1, Batch: 459, Loss: 0.6291, Elapsed: 6m42s
2020-04-26 12:12:00.758113: Validation Test:  Loss: 0.6421,  Acc: 62.4951, AUC: 0.6762, Precision: 0.7228 -- Elapsed: 39m56s
2020-04-26 12:15:22.533289: Epoch: 1, Batch: 453, Loss: 0.6163, Elapsed: 9m22s
2020-04-26 12:17:48.820739: Epoch: 1, Batch: 460, Loss: 0.6102, Elapsed: 7m23s
2020-04-26 12:19:48.759195: Epoch: 1, Batch: 451, Loss: 0.6420, Elapsed: 7m48s
2020-04-26 12:23:38.421819: Epoch: 1, Batch: 454, Loss: 0.5819, Elapsed: 8m15s
2020-04-26 12:24:02.140896: Epoch: 1, Batch: 461, Loss: 0.5755, Elapsed: 6m13s
2020-04-26 12:27:23.471319: Epoch: 1, Batch: 452, Loss: 0.6515, Elapsed: 7m34s
2020-04-26 12:27:38.727598: Epoch: 1, Batch: 455, Loss: 0.5194, Elapsed: 4m0s
2020-04-26 12:31:02.783400: Epoch: 1, Batch: 456, Loss: 0.5732, Elapsed: 3m24s
2020-04-26 12:31:48.338729: Epoch: 1, Batch: 462, Loss: 0.6031, Elapsed: 7m46s
2020-04-26 12:38:51.595318: Epoch: 1, Batch: 457, Loss: 0.5667, Elapsed: 7m48s
2020-04-26 12:39:51.730303: Epoch: 1, Batch: 453, Loss: 0.6597, Elapsed: 12m28s
2020-04-26 12:40:55.508020: Epoch: 1, Batch: 463, Loss: 0.6055, Elapsed: 9m7s
2020-04-26 12:44:11.407352: Epoch: 1, Batch: 458, Loss: 0.5666, Elapsed: 5m19s
2020-04-26 12:46:22.630566: Epoch: 1, Batch: 464, Loss: 0.6176, Elapsed: 5m27s
2020-04-26 12:49:59.479257: Epoch: 1, Batch: 459, Loss: 0.5956, Elapsed: 5m48s
2020-04-26 12:50:48.095236: Epoch: 1, Batch: 465, Loss: 0.5889, Elapsed: 4m25s
2020-04-26 12:51:29.008002: Epoch: 1, Batch: 454, Loss: 0.6562, Elapsed: 11m37s
2020-04-26 12:56:50.592019: Epoch: 1, Batch: 466, Loss: 0.5929, Elapsed: 6m2s
2020-04-26 13:00:11.241863: Epoch: 1, Batch: 455, Loss: 0.6213, Elapsed: 8m42s
2020-04-26 13:01:27.441139: Epoch: 1, Batch: 460, Loss: 0.6730, Elapsed: 11m27s
2020-04-26 13:04:35.264553: Epoch: 1, Batch: 467, Loss: 0.5952, Elapsed: 7m44s
2020-04-26 13:07:47.097261: Epoch: 1, Batch: 461, Loss: 0.5883, Elapsed: 6m19s
2020-04-26 13:09:41.755688: Epoch: 1, Batch: 456, Loss: 0.7657, Elapsed: 9m30s
2020-04-26 13:12:32.394920: Epoch: 1, Batch: 468, Loss: 0.6218, Elapsed: 7m57s
2020-04-26 13:15:43.511228: Epoch: 1, Batch: 457, Loss: 0.6538, Elapsed: 6m1s
2020-04-26 13:16:01.102985: Epoch: 1, Batch: 462, Loss: 0.6097, Elapsed: 8m13s
2020-04-26 13:17:37.458155: Epoch: 1, Batch: 469, Loss: 0.6181, Elapsed: 5m5s
2020-04-26 13:20:04.704894: Epoch: 1, Batch: 458, Loss: 0.6086, Elapsed: 4m21s
2020-04-26 13:20:29.621039: Epoch: 1, Batch: 463, Loss: 0.5477, Elapsed: 4m28s
2020-04-26 13:23:47.547434: Epoch: 1, Batch: 470, Loss: 0.6175, Elapsed: 6m10s
2020-04-26 13:25:20.494472: Epoch: 1, Batch: 464, Loss: 0.5740, Elapsed: 4m50s
2020-04-26 13:30:10.190802: Epoch: 1, Batch: 465, Loss: 0.5306, Elapsed: 4m49s
2020-04-26 13:30:48.021188: Epoch: 1, Batch: 471, Loss: 0.6267, Elapsed: 7m0s
2020-04-26 13:32:05.523081: Epoch: 1, Batch: 459, Loss: 0.6280, Elapsed: 12m0s
2020-04-26 13:34:37.508677: Epoch: 1, Batch: 466, Loss: 0.5987, Elapsed: 4m27s
2020-04-26 13:37:15.282990: Epoch: 1, Batch: 460, Loss: 0.6200, Elapsed: 5m9s
2020-04-26 13:38:02.750404: Epoch: 1, Batch: 472, Loss: 0.6107, Elapsed: 7m14s
2020-04-26 13:42:24.139007: Epoch: 1, Batch: 461, Loss: 0.6438, Elapsed: 5m8s
2020-04-26 13:42:24.967737: Epoch: 1, Batch: 473, Loss: 0.5550, Elapsed: 4m22s
2020-04-26 13:43:16.223071: Epoch: 1, Batch: 467, Loss: 0.6195, Elapsed: 8m38s
2020-04-26 13:48:16.775310: Epoch: 1, Batch: 468, Loss: 0.5467, Elapsed: 5m0s
2020-04-26 13:48:58.847371: Epoch: 1, Batch: 462, Loss: 0.6557, Elapsed: 6m34s
2020-04-26 13:55:00.660526: Epoch: 1, Batch: 469, Loss: 0.5618, Elapsed: 6m43s
2020-04-26 13:57:24.262117: Epoch: 1, Batch: 463, Loss: 0.6601, Elapsed: 8m25s
2020-04-26 13:57:21.284394: Epoch: 1, Batch: 474, Loss: 0.5749, Elapsed: 14m56s
2020-04-26 14:00:38.603416: Epoch: 1, Batch: 470, Loss: 0.5971, Elapsed: 5m37s
2020-04-26 14:01:58.087709: Epoch: 1, Batch: 464, Loss: 0.6145, Elapsed: 4m33s
2020-04-26 14:06:13.336701: Epoch: 1, Batch: 475, Loss: 0.5898, Elapsed: 8m52s
2020-04-26 14:06:32.939361: Epoch: 1, Batch: 465, Loss: 0.6608, Elapsed: 4m34s
2020-04-26 14:07:08.073305: Epoch: 1, Batch: 471, Loss: 0.6151, Elapsed: 6m29s
2020-04-26 14:11:26.268498: Epoch: 1, Batch: 466, Loss: 0.6326, Elapsed: 4m53s
2020-04-26 14:13:35.611329: Epoch: 1, Batch: 472, Loss: 0.6074, Elapsed: 6m27s
2020-04-26 14:16:07.550135: Epoch: 1, Batch: 476, Loss: 0.5882, Elapsed: 9m54s
2020-04-26 14:17:55.830097: Epoch: 1, Batch: 473, Loss: 0.5180, Elapsed: 4m20s
2020-04-26 14:18:10.803508: Epoch: 1, Batch: 467, Loss: 0.6336, Elapsed: 6m44s
2020-04-26 14:22:41.687177: Epoch: 1, Batch: 477, Loss: 0.5886, Elapsed: 6m34s
2020-04-26 14:24:10.845522: Epoch: 1, Batch: 474, Loss: 0.5686, Elapsed: 6m14s
2020-04-26 14:27:47.297177: Epoch: 1, Batch: 468, Loss: 0.6547, Elapsed: 9m36s
2020-04-26 14:28:21.651642: Epoch: 1, Batch: 478, Loss: 0.6005, Elapsed: 5m39s
2020-04-26 14:30:15.313572: Epoch: 1, Batch: 475, Loss: 0.5845, Elapsed: 6m4s
2020-04-26 14:33:54.213489: Epoch: 1, Batch: 469, Loss: 0.6637, Elapsed: 6m6s
2020-04-26 14:35:58.444093: Epoch: 1, Batch: 479, Loss: 0.6303, Elapsed: 7m36s
2020-04-26 14:36:39.791500: Epoch: 1, Batch: 476, Loss: 0.5675, Elapsed: 6m24s
2020-04-26 14:38:59.401439: Epoch: 1, Batch: 470, Loss: 0.6177, Elapsed: 5m5s
2020-04-26 14:42:38.114816: Epoch: 1, Batch: 480, Loss: 0.5875, Elapsed: 6m39s
2020-04-26 14:44:27.808800: Epoch: 1, Batch: 477, Loss: 0.5918, Elapsed: 7m47s
2020-04-26 14:45:12.130459: Epoch: 1, Batch: 471, Loss: 0.6210, Elapsed: 6m12s
2020-04-26 14:48:25.077430: Epoch: 1, Batch: 481, Loss: 0.6303, Elapsed: 5m46s
2020-04-26 14:50:08.329767: Epoch: 1, Batch: 472, Loss: 0.6352, Elapsed: 4m56s
2020-04-26 14:52:57.737700: Epoch: 1, Batch: 478, Loss: 0.6495, Elapsed: 8m29s
2020-04-26 14:55:01.240003: Epoch: 1, Batch: 473, Loss: 0.6199, Elapsed: 4m52s
2020-04-26 14:55:24.773832: Epoch: 1, Batch: 482, Loss: 0.6123, Elapsed: 6m59s
2020-04-26 14:57:57.730351: Epoch: 1, Batch: 479, Loss: 0.5832, Elapsed: 4m59s
2020-04-26 14:59:37.242412: Epoch: 1, Batch: 474, Loss: 0.6115, Elapsed: 4m35s
2020-04-26 14:59:59.383287: Epoch: 1, Batch: 483, Loss: 0.6115, Elapsed: 4m34s
2020-04-26 15:05:16.714329: Epoch: 1, Batch: 480, Loss: 0.6142, Elapsed: 7m18s
2020-04-26 15:05:21.846934: Epoch: 1, Batch: 475, Loss: 0.6455, Elapsed: 5m44s
2020-04-26 15:06:07.053878: Epoch: 1, Batch: 484, Loss: 0.5680, Elapsed: 6m7s
2020-04-26 15:09:42.583899: Epoch: 1, Batch: 481, Loss: 0.6175, Elapsed: 4m25s
2020-04-26 15:11:02.771351: Epoch: 1, Batch: 476, Loss: 0.6377, Elapsed: 5m40s
2020-04-26 15:11:54.839057: Epoch: 1, Batch: 485, Loss: 0.5850, Elapsed: 5m47s
2020-04-26 15:13:22.718219: Epoch: 1, Batch: 482, Loss: 0.5104, Elapsed: 3m40s
2020-04-26 15:16:57.280249: Epoch: 1, Batch: 486, Loss: 0.6129, Elapsed: 5m2s
2020-04-26 15:17:17.865381: Epoch: 1, Batch: 477, Loss: 0.6586, Elapsed: 6m15s
2020-04-26 15:20:31.813129: Epoch: 1, Batch: 483, Loss: 0.6011, Elapsed: 7m9s
2020-04-26 15:22:06.780327: Epoch: 1, Batch: 487, Loss: 0.5921, Elapsed: 5m9s
2020-04-26 15:22:25.839226: Epoch: 1, Batch: 478, Loss: 0.6491, Elapsed: 5m7s
2020-04-26 15:28:03.373759: Epoch: 1, Batch: 484, Loss: 0.5941, Elapsed: 7m31s
2020-04-26 15:29:25.012528: Epoch: 1, Batch: 488, Loss: 0.5677, Elapsed: 7m18s
2020-04-26 15:32:00.089251: Epoch: 1, Batch: 479, Loss: 0.6464, Elapsed: 9m34s
2020-04-26 15:35:31.711361: Epoch: 1, Batch: 480, Loss: 0.5891, Elapsed: 3m31s
2020-04-26 15:36:08.993453: Epoch: 1, Batch: 485, Loss: 0.5966, Elapsed: 8m5s
2020-04-26 15:41:05.873279: Epoch: 1, Batch: 489, Loss: 0.6238, Elapsed: 11m40s
2020-04-26 15:43:21.780157: Epoch: 1, Batch: 481, Loss: 0.6363, Elapsed: 7m50s
2020-04-26 15:45:40.362372: Epoch: 1, Batch: 486, Loss: 0.5885, Elapsed: 9m31s
2020-04-26 15:47:30.825801: Epoch: 1, Batch: 482, Loss: 0.5964, Elapsed: 4m9s
2020-04-26 15:50:57.956928: Epoch: 1, Batch: 490, Loss: 0.5992, Elapsed: 9m52s
2020-04-26 15:51:20.295457: Epoch: 1, Batch: 487, Loss: 0.5595, Elapsed: 5m39s
2020-04-26 15:54:50.048228: Epoch: 1, Batch: 483, Loss: 0.6691, Elapsed: 7m19s
2020-04-26 15:57:05.068534: Epoch: 1, Batch: 491, Loss: 0.6094, Elapsed: 6m7s
2020-04-26 16:02:07.303080: Epoch: 1, Batch: 488, Loss: 0.6021, Elapsed: 10m46s
2020-04-26 16:02:21.215134: Epoch: 1, Batch: 492, Loss: 0.5659, Elapsed: 5m16s
2020-04-26 16:04:58.011015: Epoch: 1, Batch: 484, Loss: 0.6428, Elapsed: 10m7s
2020-04-26 16:06:15.244659: Epoch: 1, Batch: 489, Loss: 0.5580, Elapsed: 4m7s
2020-04-26 16:06:20.910578: Epoch: 1, Batch: 493, Loss: 0.5300, Elapsed: 3m59s
2020-04-26 16:10:11.050097: Epoch: 1, Batch: 494, Loss: 0.5673, Elapsed: 3m50s
2020-04-26 16:11:46.547958: Epoch: 1, Batch: 490, Loss: 0.5553, Elapsed: 5m31s
2020-04-26 16:11:53.542925: Epoch: 1, Batch: 485, Loss: 0.6501, Elapsed: 6m55s
2020-04-26 16:15:33.703621: Epoch: 1, Batch: 486, Loss: 0.6034, Elapsed: 3m40s
2020-04-26 16:16:50.907963: Epoch: 1, Batch: 495, Loss: 0.6234, Elapsed: 6m39s
2020-04-26 16:17:50.151979: Epoch: 1, Batch: 491, Loss: 0.5830, Elapsed: 6m3s
2020-04-26 16:21:47.132620: Epoch: 1, Batch: 487, Loss: 0.6356, Elapsed: 6m13s
2020-04-26 16:23:05.486585: Epoch: 1, Batch: 496, Loss: 0.5976, Elapsed: 6m14s
2020-04-26 16:24:48.805080: Epoch: 1, Batch: 492, Loss: 0.5584, Elapsed: 6m58s
2020-04-26 16:28:53.180406: Epoch: 1, Batch: 497, Loss: 0.6040, Elapsed: 5m47s
2020-04-26 16:30:30.928060: Epoch: 1, Batch: 488, Loss: 0.6390, Elapsed: 8m43s
2020-04-26 16:36:00.381151: Epoch: 1, Batch: 498, Loss: 0.5853, Elapsed: 7m7s
2020-04-26 16:36:51.838383: Epoch: 1, Batch: 493, Loss: 0.6079, Elapsed: 12m3s
2020-04-26 16:37:27.405422: Epoch: 1, Batch: 489, Loss: 0.6371, Elapsed: 6m56s
2020-04-26 16:43:15.332866: Epoch: 1, Batch: 499, Loss: 0.6031, Elapsed: 7m14s
2020-04-26 16:44:40.541258: Epoch: 1, Batch: 490, Loss: 0.6320, Elapsed: 7m13s
2020-04-26 16:46:05.763260: Epoch: 1, Batch: 494, Loss: 0.5937, Elapsed: 9m13s
2020-04-26 16:48:29.785520: Epoch: 1, Batch: 500, Loss: 0.6191, Elapsed: 5m14s
Starting testing the validation set with 200 subgraphs!
2020-04-26 16:52:08.106918: Epoch: 1, Batch: 495, Loss: 0.6009, Elapsed: 6m2s
2020-04-26 16:54:48.439208: Epoch: 1, Batch: 491, Loss: 0.6706, Elapsed: 10m7s
2020-04-26 16:56:46.314683: Epoch: 1, Batch: 496, Loss: 0.5836, Elapsed: 4m38s
2020-04-26 17:01:32.643155: Epoch: 1, Batch: 497, Loss: 0.5911, Elapsed: 4m46s
2020-04-26 17:03:43.193401: Epoch: 1, Batch: 492, Loss: 0.6984, Elapsed: 8m54s
2020-04-26 17:08:54.264784: Epoch: 1, Batch: 493, Loss: 0.6489, Elapsed: 5m11s
2020-04-26 17:10:08.330780: Epoch: 1, Batch: 498, Loss: 0.6055, Elapsed: 8m35s
2020-04-26 17:15:49.473086: Epoch: 1, Batch: 494, Loss: 0.6417, Elapsed: 6m55s
2020-04-26 17:17:38.743512: Epoch: 1, Batch: 499, Loss: 0.5970, Elapsed: 7m30s
2020-04-26 17:21:04.276095: Epoch: 1, Batch: 495, Loss: 0.6272, Elapsed: 5m14s
2020-04-26 17:22:08.879152: Epoch: 1, Batch: 500, Loss: 0.5538, Elapsed: 4m30s
Starting testing the validation set with 200 subgraphs!
2020-04-26 17:27:28.654297: Epoch: 1, Batch: 496, Loss: 0.6353, Elapsed: 6m24s
2020-04-26 17:29:56.118782: Epoch: 1, Batch: 497, Loss: 0.6384, Elapsed: 2m27s
2020-04-26 17:30:38.778561: Validation Test:  Loss: 0.5904,  Acc: 69.8475, AUC: 0.7478, Precision: 0.7885 -- Elapsed: 42m8s
2020-04-26 17:36:32.593822: Epoch: 1, Batch: 498, Loss: 0.6407, Elapsed: 6m36s
2020-04-26 17:37:01.175118: Epoch: 1, Batch: 501, Loss: 0.6191, Elapsed: 6m22s
2020-04-26 17:42:44.269419: Epoch: 1, Batch: 502, Loss: 0.6154, Elapsed: 5m43s
2020-04-26 17:43:33.451974: Epoch: 1, Batch: 499, Loss: 0.6433, Elapsed: 7m0s
2020-04-26 17:49:14.058598: Epoch: 1, Batch: 500, Loss: 0.6591, Elapsed: 5m40s
Starting testing the validation set with 200 subgraphs!
2020-04-26 17:50:07.354082: Epoch: 1, Batch: 503, Loss: 0.5838, Elapsed: 7m23s
2020-04-26 17:56:27.239562: Epoch: 1, Batch: 504, Loss: 0.5861, Elapsed: 6m19s
2020-04-26 18:03:29.361255: Epoch: 1, Batch: 505, Loss: 0.6432, Elapsed: 7m2s
2020-04-26 18:03:38.677190: Validation Test:  Loss: 0.5829,  Acc: 69.8009, AUC: 0.7586, Precision: 0.8076 -- Elapsed: 41m29s
2020-04-26 18:07:22.116726: Epoch: 1, Batch: 501, Loss: 0.5626, Elapsed: 3m43s
2020-04-26 18:10:43.736259: Epoch: 1, Batch: 506, Loss: 0.5908, Elapsed: 7m14s
2020-04-26 18:12:17.175032: Epoch: 1, Batch: 502, Loss: 0.5356, Elapsed: 4m55s
2020-04-26 18:18:32.481217: Epoch: 1, Batch: 507, Loss: 0.5595, Elapsed: 7m48s
2020-04-26 18:21:11.077372: Epoch: 1, Batch: 503, Loss: 0.5716, Elapsed: 8m53s
2020-04-26 18:25:49.075497: Epoch: 1, Batch: 508, Loss: 0.5814, Elapsed: 7m16s
2020-04-26 18:28:50.152242: Epoch: 1, Batch: 504, Loss: 0.5792, Elapsed: 7m39s
2020-04-26 18:31:09.591854: Epoch: 1, Batch: 509, Loss: 0.5891, Elapsed: 5m20s
2020-04-26 18:33:10.284010: Validation Test:  Loss: 0.6418,  Acc: 62.6107, AUC: 0.6819, Precision: 0.7245 -- Elapsed: 43m56s
2020-04-26 18:34:03.078453: Epoch: 1, Batch: 505, Loss: 0.5909, Elapsed: 5m12s
2020-04-26 18:39:11.368377: Epoch: 1, Batch: 506, Loss: 0.5603, Elapsed: 5m8s
2020-04-26 18:41:10.548596: Epoch: 1, Batch: 510, Loss: 0.6044, Elapsed: 10m0s
2020-04-26 18:42:59.194421: Epoch: 1, Batch: 507, Loss: 0.4683, Elapsed: 3m47s
2020-04-26 18:43:35.508383: Epoch: 1, Batch: 501, Loss: 0.6313, Elapsed: 10m25s
2020-04-26 18:48:54.831388: Epoch: 1, Batch: 511, Loss: 0.5851, Elapsed: 7m44s
2020-04-26 18:49:46.776714: Epoch: 1, Batch: 502, Loss: 0.6262, Elapsed: 6m11s
2020-04-26 18:49:56.686715: Epoch: 1, Batch: 508, Loss: 0.5624, Elapsed: 6m57s
2020-04-26 18:54:23.077008: Epoch: 1, Batch: 512, Loss: 0.5908, Elapsed: 5m28s
2020-04-26 18:54:47.755131: Epoch: 1, Batch: 509, Loss: 0.5552, Elapsed: 4m51s
2020-04-26 18:56:40.478173: Epoch: 1, Batch: 503, Loss: 0.6031, Elapsed: 6m53s
2020-04-26 19:01:58.774590: Epoch: 1, Batch: 504, Loss: 0.5991, Elapsed: 5m18s
2020-04-26 19:02:00.772291: Epoch: 1, Batch: 513, Loss: 0.6057, Elapsed: 7m37s
2020-04-26 19:04:49.595639: Epoch: 1, Batch: 510, Loss: 0.6044, Elapsed: 10m1s
2020-04-26 19:07:15.135893: Epoch: 1, Batch: 505, Loss: 0.6311, Elapsed: 5m16s
2020-04-26 19:10:30.879441: Epoch: 1, Batch: 514, Loss: 0.5914, Elapsed: 8m30s
2020-04-26 19:12:57.335922: Epoch: 1, Batch: 511, Loss: 0.6229, Elapsed: 8m7s
2020-04-26 19:15:06.342764: Epoch: 1, Batch: 515, Loss: 0.5418, Elapsed: 4m35s
2020-04-26 19:15:49.214509: Epoch: 1, Batch: 506, Loss: 0.6381, Elapsed: 8m34s
2020-04-26 19:17:12.455406: Epoch: 1, Batch: 512, Loss: 0.5134, Elapsed: 4m15s
2020-04-26 19:19:06.171086: Epoch: 1, Batch: 507, Loss: 0.6217, Elapsed: 3m16s
2020-04-26 19:19:46.106355: Epoch: 1, Batch: 516, Loss: 0.5556, Elapsed: 4m39s
2020-04-26 19:24:18.282895: Epoch: 1, Batch: 513, Loss: 0.6074, Elapsed: 7m5s
2020-04-26 19:29:20.674672: Epoch: 1, Batch: 508, Loss: 0.6760, Elapsed: 10m14s
2020-04-26 19:32:09.318261: Epoch: 1, Batch: 517, Loss: 0.6110, Elapsed: 12m23s
2020-04-26 19:32:05.953861: Epoch: 1, Batch: 514, Loss: 0.5899, Elapsed: 7m47s
2020-04-26 19:32:28.228954: Epoch: 1, Batch: 509, Loss: 0.6272, Elapsed: 3m7s
2020-04-26 19:36:58.218834: Epoch: 1, Batch: 510, Loss: 0.6205, Elapsed: 4m29s
2020-04-26 19:37:20.123620: Epoch: 1, Batch: 518, Loss: 0.5466, Elapsed: 5m10s
2020-04-26 19:40:42.435613: Epoch: 1, Batch: 511, Loss: 0.6152, Elapsed: 3m44s
2020-04-26 19:41:15.010579: Epoch: 1, Batch: 515, Loss: 0.5893, Elapsed: 9m9s
2020-04-26 19:41:59.730808: Epoch: 1, Batch: 519, Loss: 0.5994, Elapsed: 4m39s
2020-04-26 19:46:02.720590: Epoch: 1, Batch: 512, Loss: 0.6299, Elapsed: 5m20s
2020-04-26 19:46:17.658705: Epoch: 1, Batch: 516, Loss: 0.5507, Elapsed: 5m2s
2020-04-26 19:50:49.803793: Epoch: 1, Batch: 520, Loss: 0.5841, Elapsed: 8m50s
2020-04-26 19:52:22.700335: Epoch: 1, Batch: 517, Loss: 0.5601, Elapsed: 6m5s
2020-04-26 19:53:10.449350: Epoch: 1, Batch: 513, Loss: 0.6343, Elapsed: 7m7s
2020-04-26 19:58:07.398569: Epoch: 1, Batch: 518, Loss: 0.5838, Elapsed: 5m44s
2020-04-26 19:58:38.317683: Epoch: 1, Batch: 514, Loss: 0.6246, Elapsed: 5m27s
2020-04-26 19:58:48.303285: Epoch: 1, Batch: 521, Loss: 0.5873, Elapsed: 7m58s
2020-04-26 20:02:43.544753: Epoch: 1, Batch: 522, Loss: 0.5556, Elapsed: 3m55s
2020-04-26 20:04:27.009314: Epoch: 1, Batch: 515, Loss: 0.6295, Elapsed: 5m48s
2020-04-26 20:05:57.501721: Epoch: 1, Batch: 519, Loss: 0.5753, Elapsed: 7m50s
2020-04-26 20:08:53.036732: Epoch: 1, Batch: 523, Loss: 0.5789, Elapsed: 6m9s
2020-04-26 20:10:46.993651: Epoch: 1, Batch: 520, Loss: 0.5616, Elapsed: 4m49s
2020-04-26 20:11:45.304296: Epoch: 1, Batch: 516, Loss: 0.6526, Elapsed: 7m18s
2020-04-26 20:14:46.729440: Epoch: 1, Batch: 524, Loss: 0.5564, Elapsed: 5m53s
2020-04-26 20:15:41.053503: Epoch: 1, Batch: 521, Loss: 0.5961, Elapsed: 4m54s
2020-04-26 20:20:21.631949: Epoch: 1, Batch: 525, Loss: 0.5516, Elapsed: 5m34s
2020-04-26 20:20:40.567400: Epoch: 1, Batch: 517, Loss: 0.6643, Elapsed: 8m55s
2020-04-26 20:23:16.137867: Epoch: 1, Batch: 522, Loss: 0.6029, Elapsed: 7m35s
2020-04-26 20:24:06.362744: Epoch: 1, Batch: 526, Loss: 0.5020, Elapsed: 3m44s
2020-04-26 20:25:58.585781: Epoch: 1, Batch: 518, Loss: 0.6125, Elapsed: 5m17s
2020-04-26 20:31:21.237388: Epoch: 1, Batch: 523, Loss: 0.5989, Elapsed: 8m5s
2020-04-26 20:31:51.534499: Epoch: 1, Batch: 519, Loss: 0.6371, Elapsed: 5m52s
2020-04-26 20:35:56.597664: Epoch: 1, Batch: 527, Loss: 0.6142, Elapsed: 11m50s
2020-04-26 20:38:59.433166: Epoch: 1, Batch: 520, Loss: 0.6391, Elapsed: 7m7s
2020-04-26 20:39:02.331244: Epoch: 1, Batch: 524, Loss: 0.6296, Elapsed: 7m41s
2020-04-26 20:44:01.835273: Epoch: 1, Batch: 521, Loss: 0.6072, Elapsed: 5m2s
2020-04-26 20:44:14.570713: Epoch: 1, Batch: 525, Loss: 0.5604, Elapsed: 5m12s
2020-04-26 20:45:15.746153: Epoch: 1, Batch: 528, Loss: 0.5971, Elapsed: 9m19s
2020-04-26 20:50:22.589898: Epoch: 1, Batch: 522, Loss: 0.6320, Elapsed: 6m20s
2020-04-26 20:50:40.970040: Epoch: 1, Batch: 529, Loss: 0.5422, Elapsed: 5m25s
2020-04-26 20:50:44.369915: Epoch: 1, Batch: 526, Loss: 0.5718, Elapsed: 6m29s
2020-04-26 20:54:33.475988: Epoch: 1, Batch: 523, Loss: 0.6481, Elapsed: 4m10s
2020-04-26 20:55:45.426825: Epoch: 1, Batch: 527, Loss: 0.5406, Elapsed: 5m1s
2020-04-26 20:59:19.694600: Epoch: 1, Batch: 530, Loss: 0.6070, Elapsed: 8m38s
2020-04-26 21:01:37.333555: Epoch: 1, Batch: 528, Loss: 0.5804, Elapsed: 5m51s
2020-04-26 21:04:14.174295: Epoch: 1, Batch: 524, Loss: 0.6566, Elapsed: 9m40s
2020-04-26 21:05:09.459425: Epoch: 1, Batch: 531, Loss: 0.5563, Elapsed: 5m49s
2020-04-26 21:07:52.661888: Epoch: 1, Batch: 529, Loss: 0.6004, Elapsed: 6m15s
2020-04-26 21:08:09.831270: Epoch: 1, Batch: 525, Loss: 0.6330, Elapsed: 3m55s
2020-04-26 21:13:02.097189: Epoch: 1, Batch: 526, Loss: 0.6280, Elapsed: 4m52s
2020-04-26 21:13:24.716012: Epoch: 1, Batch: 532, Loss: 0.5764, Elapsed: 8m15s
2020-04-26 21:18:17.851167: Epoch: 1, Batch: 530, Loss: 0.6099, Elapsed: 10m25s
2020-04-26 21:18:43.948740: Epoch: 1, Batch: 527, Loss: 0.6346, Elapsed: 5m41s
2020-04-26 21:21:36.351230: Epoch: 1, Batch: 533, Loss: 0.5864, Elapsed: 8m11s
2020-04-26 21:26:35.631828: Epoch: 1, Batch: 531, Loss: 0.6211, Elapsed: 8m17s
2020-04-26 21:27:25.877189: Epoch: 1, Batch: 534, Loss: 0.5679, Elapsed: 5m49s
2020-04-26 21:27:28.070448: Epoch: 1, Batch: 528, Loss: 0.6386, Elapsed: 8m44s
2020-04-26 21:30:34.791462: Epoch: 1, Batch: 532, Loss: 0.5786, Elapsed: 3m59s
2020-04-26 21:33:46.565355: Epoch: 1, Batch: 535, Loss: 0.5950, Elapsed: 6m20s
2020-04-26 21:34:28.056746: Epoch: 1, Batch: 529, Loss: 0.6517, Elapsed: 6m59s
2020-04-26 21:37:49.208864: Epoch: 1, Batch: 536, Loss: 0.5018, Elapsed: 4m2s
2020-04-26 21:38:28.244316: Epoch: 1, Batch: 533, Loss: 0.6082, Elapsed: 7m53s
2020-04-26 21:41:59.951912: Epoch: 1, Batch: 530, Loss: 0.6425, Elapsed: 7m31s
2020-04-26 21:42:32.912348: Epoch: 1, Batch: 537, Loss: 0.5499, Elapsed: 4m43s
2020-04-26 21:46:01.113092: Epoch: 1, Batch: 534, Loss: 0.6088, Elapsed: 7m32s
2020-04-26 21:47:14.782047: Epoch: 1, Batch: 531, Loss: 0.6169, Elapsed: 5m14s
2020-04-26 21:50:13.237842: Epoch: 1, Batch: 538, Loss: 0.6152, Elapsed: 7m40s
2020-04-26 21:50:34.739762: Epoch: 1, Batch: 535, Loss: 0.5484, Elapsed: 4m33s
2020-04-26 21:55:25.737483: Epoch: 1, Batch: 539, Loss: 0.6125, Elapsed: 5m12s
2020-04-26 21:55:35.812990: Epoch: 1, Batch: 532, Loss: 0.6502, Elapsed: 8m21s
2020-04-26 21:55:32.697222: Epoch: 1, Batch: 536, Loss: 0.5675, Elapsed: 4m57s
2020-04-26 22:00:13.200118: Epoch: 1, Batch: 540, Loss: 0.5838, Elapsed: 4m47s
2020-04-26 22:02:40.489776: Epoch: 1, Batch: 533, Loss: 0.6252, Elapsed: 7m4s
2020-04-26 22:05:16.944867: Epoch: 1, Batch: 541, Loss: 0.5720, Elapsed: 5m3s
2020-04-26 22:08:11.085498: Epoch: 1, Batch: 534, Loss: 0.6334, Elapsed: 5m30s
2020-04-26 22:08:28.242407: Epoch: 1, Batch: 537, Loss: 0.6162, Elapsed: 12m55s
2020-04-26 22:12:01.204261: Epoch: 1, Batch: 542, Loss: 0.6310, Elapsed: 6m44s
2020-04-26 22:15:43.872551: Epoch: 1, Batch: 543, Loss: 0.5529, Elapsed: 3m42s
2020-04-26 22:16:36.561294: Epoch: 1, Batch: 538, Loss: 0.5850, Elapsed: 8m8s
2020-04-26 22:18:51.899897: Epoch: 1, Batch: 535, Loss: 0.6761, Elapsed: 10m40s
2020-04-26 22:19:57.529467: Epoch: 1, Batch: 539, Loss: 0.5698, Elapsed: 3m20s
2020-04-26 22:26:31.109912: Epoch: 1, Batch: 544, Loss: 0.5752, Elapsed: 10m47s
2020-04-26 22:27:20.966726: Epoch: 1, Batch: 540, Loss: 0.6136, Elapsed: 7m23s
2020-04-26 22:29:15.265035: Epoch: 1, Batch: 536, Loss: 0.6520, Elapsed: 10m23s
2020-04-26 22:31:08.041064: Epoch: 1, Batch: 541, Loss: 0.5558, Elapsed: 3m47s
2020-04-26 22:33:28.522334: Epoch: 1, Batch: 545, Loss: 0.5707, Elapsed: 6m57s
2020-04-26 22:36:36.954569: Epoch: 1, Batch: 537, Loss: 0.6767, Elapsed: 7m21s
2020-04-26 22:37:12.613261: Epoch: 1, Batch: 542, Loss: 0.5257, Elapsed: 6m4s
2020-04-26 22:39:15.552770: Epoch: 1, Batch: 546, Loss: 0.5649, Elapsed: 5m46s
2020-04-26 22:40:34.704806: Epoch: 1, Batch: 538, Loss: 0.6304, Elapsed: 3m57s
2020-04-26 22:42:52.627934: Epoch: 1, Batch: 543, Loss: 0.5421, Elapsed: 5m39s
2020-04-26 22:46:36.557662: Epoch: 1, Batch: 539, Loss: 0.6447, Elapsed: 6m1s
2020-04-26 22:47:53.003725: Epoch: 1, Batch: 547, Loss: 0.6430, Elapsed: 8m37s
2020-04-26 22:48:29.301670: Epoch: 1, Batch: 544, Loss: 0.5366, Elapsed: 5m36s
2020-04-26 22:53:31.331372: Epoch: 1, Batch: 548, Loss: 0.5873, Elapsed: 5m38s
2020-04-26 22:54:41.627733: Epoch: 1, Batch: 540, Loss: 0.6512, Elapsed: 8m5s
2020-04-26 22:59:08.412830: Epoch: 1, Batch: 545, Loss: 0.6134, Elapsed: 10m39s
2020-04-26 22:59:48.213333: Epoch: 1, Batch: 541, Loss: 0.6204, Elapsed: 5m6s
2020-04-26 23:00:13.620537: Epoch: 1, Batch: 549, Loss: 0.6060, Elapsed: 6m42s
2020-04-26 23:06:51.070934: Epoch: 1, Batch: 546, Loss: 0.5881, Elapsed: 7m42s
2020-04-26 23:07:35.432563: Epoch: 1, Batch: 542, Loss: 0.6486, Elapsed: 7m47s
2020-04-26 23:08:12.554204: Epoch: 1, Batch: 550, Loss: 0.5878, Elapsed: 7m58s
Starting testing the validation set with 200 subgraphs!
2020-04-26 23:12:22.508788: Epoch: 1, Batch: 543, Loss: 0.6570, Elapsed: 4m47s
2020-04-26 23:17:15.935657: Epoch: 1, Batch: 547, Loss: 0.5950, Elapsed: 10m24s
2020-04-26 23:17:49.830355: Epoch: 1, Batch: 544, Loss: 0.6403, Elapsed: 5m27s
2020-04-26 23:22:11.580884: Epoch: 1, Batch: 548, Loss: 0.5580, Elapsed: 4m55s
2020-04-26 23:26:23.278292: Epoch: 1, Batch: 545, Loss: 0.6928, Elapsed: 8m33s
2020-04-26 23:27:41.167826: Epoch: 1, Batch: 549, Loss: 0.5676, Elapsed: 5m29s
2020-04-26 23:33:04.990159: Epoch: 1, Batch: 546, Loss: 0.6505, Elapsed: 6m41s
2020-04-26 23:39:09.101832: Epoch: 1, Batch: 550, Loss: 0.6197, Elapsed: 11m27s
Starting testing the validation set with 200 subgraphs!
2020-04-26 23:40:48.808765: Epoch: 1, Batch: 547, Loss: 0.6454, Elapsed: 7m43s
2020-04-26 23:46:23.701113: Epoch: 1, Batch: 548, Loss: 0.6557, Elapsed: 5m34s
2020-04-26 23:49:08.540595: Validation Test:  Loss: 0.5861,  Acc: 70.0220, AUC: 0.7539, Precision: 0.8004 -- Elapsed: 40m55s
2020-04-26 23:53:59.582693: Epoch: 1, Batch: 551, Loss: 0.6028, Elapsed: 4m51s
2020-04-26 23:54:21.239703: Epoch: 1, Batch: 549, Loss: 0.6465, Elapsed: 7m57s
2020-04-26 23:59:47.039017: Epoch: 1, Batch: 550, Loss: 0.6318, Elapsed: 5m25s
Starting testing the validation set with 200 subgraphs!
2020-04-27 00:00:28.289982: Epoch: 1, Batch: 552, Loss: 0.6054, Elapsed: 6m28s
2020-04-27 00:06:33.748448: Epoch: 1, Batch: 553, Loss: 0.5914, Elapsed: 6m5s
2020-04-27 00:14:36.404136: Epoch: 1, Batch: 554, Loss: 0.5872, Elapsed: 8m2s
2020-04-27 00:19:03.480244: Validation Test:  Loss: 0.5845,  Acc: 69.6761, AUC: 0.7553, Precision: 0.8031 -- Elapsed: 39m54s
2020-04-27 00:21:45.864492: Epoch: 1, Batch: 555, Loss: 0.5936, Elapsed: 7m9s
2020-04-27 00:25:09.866109: Epoch: 1, Batch: 551, Loss: 0.5712, Elapsed: 6m6s
2020-04-27 00:26:23.699818: Epoch: 1, Batch: 556, Loss: 0.5632, Elapsed: 4m37s
2020-04-27 00:29:55.481917: Epoch: 1, Batch: 552, Loss: 0.5771, Elapsed: 4m45s
2020-04-27 00:31:19.354204: Epoch: 1, Batch: 557, Loss: 0.5562, Elapsed: 4m55s
2020-04-27 00:37:33.659640: Epoch: 1, Batch: 553, Loss: 0.5911, Elapsed: 7m38s
2020-04-27 00:38:56.135953: Epoch: 1, Batch: 558, Loss: 0.5891, Elapsed: 7m36s
2020-04-27 00:40:03.059816: Validation Test:  Loss: 0.6434,  Acc: 62.8410, AUC: 0.6803, Precision: 0.7169 -- Elapsed: 40m15s
2020-04-27 00:42:55.969157: Epoch: 1, Batch: 554, Loss: 0.5430, Elapsed: 5m22s
2020-04-27 00:44:03.465555: Epoch: 1, Batch: 559, Loss: 0.6030, Elapsed: 5m7s
2020-04-27 00:47:39.295650: Epoch: 1, Batch: 551, Loss: 0.6526, Elapsed: 7m36s
2020-04-27 00:49:09.642151: Epoch: 1, Batch: 555, Loss: 0.5813, Elapsed: 6m13s
2020-04-27 00:52:06.821994: Epoch: 1, Batch: 552, Loss: 0.6122, Elapsed: 4m27s
2020-04-27 00:52:16.983021: Epoch: 1, Batch: 560, Loss: 0.6077, Elapsed: 8m13s
2020-04-27 00:55:53.333041: Epoch: 1, Batch: 556, Loss: 0.5664, Elapsed: 6m43s
2020-04-27 00:57:17.472327: Epoch: 1, Batch: 553, Loss: 0.6390, Elapsed: 5m10s
2020-04-27 00:57:50.224874: Epoch: 1, Batch: 561, Loss: 0.5788, Elapsed: 5m33s
2020-04-27 01:01:32.215058: Epoch: 1, Batch: 557, Loss: 0.5506, Elapsed: 5m38s
2020-04-27 01:05:07.047513: Epoch: 1, Batch: 554, Loss: 0.6357, Elapsed: 7m49s
2020-04-27 01:06:51.943538: Epoch: 1, Batch: 562, Loss: 0.6403, Elapsed: 9m1s
2020-04-27 01:10:51.098739: Epoch: 1, Batch: 558, Loss: 0.6122, Elapsed: 9m18s
2020-04-27 01:16:18.601594: Epoch: 1, Batch: 563, Loss: 0.6060, Elapsed: 9m26s
2020-04-27 01:16:55.017695: Epoch: 1, Batch: 555, Loss: 0.6798, Elapsed: 11m47s
2020-04-27 01:17:04.844411: Epoch: 1, Batch: 559, Loss: 0.5499, Elapsed: 6m13s
2020-04-27 01:23:24.230718: Epoch: 1, Batch: 564, Loss: 0.6146, Elapsed: 7m5s
2020-04-27 01:24:19.596445: Epoch: 1, Batch: 556, Loss: 0.6389, Elapsed: 7m24s
2020-04-27 01:26:34.798006: Epoch: 1, Batch: 565, Loss: 0.4800, Elapsed: 3m10s
2020-04-27 01:28:28.263389: Epoch: 1, Batch: 560, Loss: 0.6221, Elapsed: 11m23s
2020-04-27 01:31:50.929100: Epoch: 1, Batch: 557, Loss: 0.6615, Elapsed: 7m31s
2020-04-27 01:32:39.013128: Epoch: 1, Batch: 566, Loss: 0.5667, Elapsed: 6m4s
2020-04-27 01:36:34.578785: Epoch: 1, Batch: 561, Loss: 0.6119, Elapsed: 8m6s
2020-04-27 01:39:14.928734: Epoch: 1, Batch: 558, Loss: 0.6617, Elapsed: 7m23s
2020-04-27 01:40:48.394435: Epoch: 1, Batch: 567, Loss: 0.5846, Elapsed: 8m9s
2020-04-27 01:45:44.980774: Epoch: 1, Batch: 559, Loss: 0.6402, Elapsed: 6m30s
2020-04-27 01:46:11.320243: Epoch: 1, Batch: 562, Loss: 0.6315, Elapsed: 9m36s
2020-04-27 01:50:36.301771: Epoch: 1, Batch: 563, Loss: 0.5594, Elapsed: 4m24s
2020-04-27 01:52:17.484880: Epoch: 1, Batch: 560, Loss: 0.6307, Elapsed: 6m32s
2020-04-27 01:52:15.467429: Epoch: 1, Batch: 568, Loss: 0.5932, Elapsed: 11m27s
2020-04-27 01:57:35.202293: Epoch: 1, Batch: 564, Loss: 0.6017, Elapsed: 6m58s
2020-04-27 02:00:00.189576: Epoch: 1, Batch: 561, Loss: 0.6459, Elapsed: 7m42s
2020-04-27 02:02:51.599734: Epoch: 1, Batch: 565, Loss: 0.5477, Elapsed: 5m16s
2020-04-27 02:05:25.067638: Epoch: 1, Batch: 562, Loss: 0.6231, Elapsed: 5m24s
2020-04-27 02:05:29.091235: Epoch: 1, Batch: 569, Loss: 0.6301, Elapsed: 13m13s
2020-04-27 02:07:40.879810: Epoch: 1, Batch: 566, Loss: 0.5396, Elapsed: 4m49s
2020-04-27 02:10:49.654015: Epoch: 1, Batch: 563, Loss: 0.6344, Elapsed: 5m24s
2020-04-27 02:11:56.134505: Epoch: 1, Batch: 570, Loss: 0.6241, Elapsed: 6m27s
2020-04-27 02:15:17.358514: Epoch: 1, Batch: 567, Loss: 0.5948, Elapsed: 7m36s
2020-04-27 02:18:28.055921: Epoch: 1, Batch: 571, Loss: 0.5559, Elapsed: 6m31s
2020-04-27 02:18:54.212425: Epoch: 1, Batch: 568, Loss: 0.4585, Elapsed: 3m36s
2020-04-27 02:18:51.068539: Epoch: 1, Batch: 564, Loss: 0.6496, Elapsed: 8m1s
2020-04-27 02:25:36.825975: Epoch: 1, Batch: 565, Loss: 0.6544, Elapsed: 6m45s
2020-04-27 02:26:36.156200: Epoch: 1, Batch: 572, Loss: 0.5875, Elapsed: 8m8s
2020-04-27 02:28:59.057935: Epoch: 1, Batch: 569, Loss: 0.5804, Elapsed: 10m4s
2020-04-27 02:31:14.840010: Epoch: 1, Batch: 573, Loss: 0.5569, Elapsed: 4m38s
2020-04-27 02:32:38.755345: Epoch: 1, Batch: 566, Loss: 0.5989, Elapsed: 7m1s
2020-04-27 02:35:19.678678: Epoch: 1, Batch: 570, Loss: 0.6115, Elapsed: 6m20s
2020-04-27 02:39:27.873052: Epoch: 1, Batch: 574, Loss: 0.5900, Elapsed: 8m13s
2020-04-27 02:42:10.868094: Epoch: 1, Batch: 567, Loss: 0.6189, Elapsed: 9m32s
2020-04-27 02:43:29.465142: Epoch: 1, Batch: 571, Loss: 0.6087, Elapsed: 8m9s
2020-04-27 02:46:02.859228: Epoch: 1, Batch: 568, Loss: 0.6517, Elapsed: 3m51s
2020-04-27 02:47:10.977228: Epoch: 1, Batch: 575, Loss: 0.5764, Elapsed: 7m43s
2020-04-27 02:49:15.066486: Epoch: 1, Batch: 572, Loss: 0.5847, Elapsed: 5m45s
2020-04-27 02:52:57.391524: Epoch: 1, Batch: 576, Loss: 0.5725, Elapsed: 5m46s
2020-04-27 02:53:15.536166: Epoch: 1, Batch: 569, Loss: 0.6367, Elapsed: 7m12s
2020-04-27 02:53:35.000303: Epoch: 1, Batch: 573, Loss: 0.5150, Elapsed: 4m19s
2020-04-27 02:59:14.772790: Epoch: 1, Batch: 574, Loss: 0.5727, Elapsed: 5m39s
2020-04-27 03:00:07.170724: Epoch: 1, Batch: 577, Loss: 0.5948, Elapsed: 7m9s
2020-04-27 03:01:35.693931: Epoch: 1, Batch: 570, Loss: 0.6626, Elapsed: 8m20s
2020-04-27 03:05:30.208312: Epoch: 1, Batch: 575, Loss: 0.5745, Elapsed: 6m15s
2020-04-27 03:06:18.727018: Epoch: 1, Batch: 578, Loss: 0.5588, Elapsed: 6m11s
2020-04-27 03:07:21.677784: Epoch: 1, Batch: 571, Loss: 0.6737, Elapsed: 5m45s
2020-04-27 03:11:03.070956: Epoch: 1, Batch: 579, Loss: 0.5249, Elapsed: 4m44s
2020-04-27 03:14:35.735478: Epoch: 1, Batch: 576, Loss: 0.6039, Elapsed: 9m5s
2020-04-27 03:15:11.990624: Epoch: 1, Batch: 572, Loss: 0.6521, Elapsed: 7m50s
2020-04-27 03:16:49.642569: Epoch: 1, Batch: 580, Loss: 0.5833, Elapsed: 5m46s
2020-04-27 03:19:41.579856: Epoch: 1, Batch: 577, Loss: 0.5709, Elapsed: 5m5s
2020-04-27 03:20:50.859928: Epoch: 1, Batch: 573, Loss: 0.6414, Elapsed: 5m38s
2020-04-27 03:22:27.962783: Epoch: 1, Batch: 581, Loss: 0.5606, Elapsed: 5m38s
2020-04-27 03:23:48.439303: Epoch: 1, Batch: 578, Loss: 0.5119, Elapsed: 4m6s
2020-04-27 03:26:57.120250: Epoch: 1, Batch: 574, Loss: 0.6533, Elapsed: 6m6s
2020-04-27 03:27:02.789142: Epoch: 1, Batch: 582, Loss: 0.5635, Elapsed: 4m34s
2020-04-27 03:30:32.102293: Epoch: 1, Batch: 579, Loss: 0.5851, Elapsed: 6m43s
2020-04-27 03:31:28.335103: Epoch: 1, Batch: 575, Loss: 0.6466, Elapsed: 4m31s
2020-04-27 03:33:19.471078: Epoch: 1, Batch: 583, Loss: 0.5713, Elapsed: 6m16s
2020-04-27 03:36:13.785000: Epoch: 1, Batch: 580, Loss: 0.5546, Elapsed: 5m41s
2020-04-27 03:39:03.075291: Epoch: 1, Batch: 576, Loss: 0.6786, Elapsed: 7m34s
2020-04-27 03:39:49.643539: Epoch: 1, Batch: 584, Loss: 0.6058, Elapsed: 6m30s
2020-04-27 03:42:06.043756: Epoch: 1, Batch: 581, Loss: 0.5558, Elapsed: 5m52s
2020-04-27 03:44:32.462813: Epoch: 1, Batch: 577, Loss: 0.6689, Elapsed: 5m29s
2020-04-27 03:46:27.191017: Epoch: 1, Batch: 585, Loss: 0.5771, Elapsed: 6m37s
2020-04-27 03:49:38.168736: Epoch: 1, Batch: 578, Loss: 0.6574, Elapsed: 5m5s
2020-04-27 03:51:15.552811: Epoch: 1, Batch: 586, Loss: 0.5448, Elapsed: 4m48s
2020-04-27 03:51:37.283953: Epoch: 1, Batch: 582, Loss: 0.5912, Elapsed: 9m31s
2020-04-27 03:55:24.060993: Epoch: 1, Batch: 579, Loss: 0.6433, Elapsed: 5m45s
2020-04-27 03:56:19.213487: Epoch: 1, Batch: 587, Loss: 0.5504, Elapsed: 5m3s
2020-04-27 03:57:55.606007: Epoch: 1, Batch: 583, Loss: 0.5818, Elapsed: 6m18s
2020-04-27 04:02:05.296342: Epoch: 1, Batch: 588, Loss: 0.5835, Elapsed: 5m46s
2020-04-27 04:05:09.530151: Epoch: 1, Batch: 584, Loss: 0.5910, Elapsed: 7m13s
2020-04-27 04:05:59.482795: Epoch: 1, Batch: 580, Loss: 0.6368, Elapsed: 10m35s
2020-04-27 04:08:23.631497: Epoch: 1, Batch: 589, Loss: 0.5900, Elapsed: 6m18s
2020-04-27 04:11:27.449149: Epoch: 1, Batch: 585, Loss: 0.5778, Elapsed: 6m17s
2020-04-27 04:14:52.800068: Epoch: 1, Batch: 590, Loss: 0.5488, Elapsed: 6m29s
2020-04-27 04:15:29.026394: Epoch: 1, Batch: 581, Loss: 0.6423, Elapsed: 9m29s
2020-04-27 04:20:02.435794: Epoch: 1, Batch: 586, Loss: 0.5954, Elapsed: 8m34s
2020-04-27 04:22:47.320961: Epoch: 1, Batch: 591, Loss: 0.6160, Elapsed: 7m54s
2020-04-27 04:24:06.548079: Epoch: 1, Batch: 582, Loss: 0.6251, Elapsed: 8m37s
2020-04-27 04:24:22.202531: Epoch: 1, Batch: 587, Loss: 0.5358, Elapsed: 4m19s
2020-04-27 04:27:38.114082: Epoch: 1, Batch: 592, Loss: 0.5780, Elapsed: 4m50s
2020-04-27 04:30:24.537349: Epoch: 1, Batch: 588, Loss: 0.5684, Elapsed: 6m2s
2020-04-27 04:30:48.103476: Epoch: 1, Batch: 583, Loss: 0.6770, Elapsed: 6m41s
2020-04-27 04:35:40.967142: Epoch: 1, Batch: 589, Loss: 0.5358, Elapsed: 5m16s
2020-04-27 04:36:14.073949: Epoch: 1, Batch: 584, Loss: 0.6482, Elapsed: 5m25s
2020-04-27 04:36:19.744379: Epoch: 1, Batch: 593, Loss: 0.5891, Elapsed: 8m41s
2020-04-27 04:39:16.352838: Epoch: 1, Batch: 590, Loss: 0.5472, Elapsed: 3m35s
2020-04-27 04:40:39.222616: Epoch: 1, Batch: 585, Loss: 0.6312, Elapsed: 4m25s
2020-04-27 04:43:04.080089: Epoch: 1, Batch: 594, Loss: 0.5504, Elapsed: 6m44s
2020-04-27 04:47:06.426651: Epoch: 1, Batch: 586, Loss: 0.6490, Elapsed: 6m27s
2020-04-27 04:48:38.256553: Epoch: 1, Batch: 595, Loss: 0.5462, Elapsed: 5m34s
2020-04-27 04:48:42.604436: Epoch: 1, Batch: 591, Loss: 0.6037, Elapsed: 9m26s
2020-04-27 04:49:47.290873: Epoch: 1, Batch: 587, Loss: 0.6040, Elapsed: 2m40s
2020-04-27 04:54:11.763111: Epoch: 1, Batch: 592, Loss: 0.6040, Elapsed: 5m29s
2020-04-27 04:57:48.610991: Epoch: 1, Batch: 588, Loss: 0.6293, Elapsed: 8m1s
2020-04-27 04:58:35.064644: Epoch: 1, Batch: 596, Loss: 0.6296, Elapsed: 9m56s
2020-04-27 05:01:41.665375: Epoch: 1, Batch: 593, Loss: 0.6072, Elapsed: 7m29s
2020-04-27 05:01:56.231861: Epoch: 1, Batch: 597, Loss: 0.5605, Elapsed: 3m21s
2020-04-27 05:05:30.741822: Epoch: 1, Batch: 589, Loss: 0.6193, Elapsed: 7m42s
2020-04-27 05:09:07.518767: Epoch: 1, Batch: 594, Loss: 0.6144, Elapsed: 7m25s
2020-04-27 05:11:58.583639: Epoch: 1, Batch: 598, Loss: 0.6025, Elapsed: 10m2s
2020-04-27 05:12:14.239575: Epoch: 1, Batch: 590, Loss: 0.6228, Elapsed: 6m43s
2020-04-27 05:15:23.410091: Epoch: 1, Batch: 595, Loss: 0.5091, Elapsed: 6m15s
2020-04-27 05:17:46.472129: Epoch: 1, Batch: 599, Loss: 0.6095, Elapsed: 5m47s
2020-04-27 05:19:43.680618: Epoch: 1, Batch: 596, Loss: 0.4894, Elapsed: 4m20s
2020-04-27 05:21:36.483176: Epoch: 1, Batch: 591, Loss: 0.6442, Elapsed: 9m22s
2020-04-27 05:25:26.179605: Epoch: 1, Batch: 600, Loss: 0.5593, Elapsed: 7m39s
Starting testing the validation set with 200 subgraphs!
2020-04-27 05:29:11.128508: Epoch: 1, Batch: 592, Loss: 0.6387, Elapsed: 7m34s
2020-04-27 05:29:58.998801: Epoch: 1, Batch: 597, Loss: 0.6323, Elapsed: 10m15s
2020-04-27 05:33:55.944053: Epoch: 1, Batch: 593, Loss: 0.6618, Elapsed: 4m44s
2020-04-27 05:39:15.749068: Epoch: 1, Batch: 598, Loss: 0.5881, Elapsed: 9m16s
2020-04-27 05:42:17.577471: Epoch: 1, Batch: 594, Loss: 0.6359, Elapsed: 8m21s
2020-04-27 05:46:01.373924: Epoch: 1, Batch: 599, Loss: 0.5578, Elapsed: 6m45s
2020-04-27 05:48:47.276861: Epoch: 1, Batch: 595, Loss: 0.6247, Elapsed: 6m29s
2020-04-27 05:52:54.970202: Epoch: 1, Batch: 600, Loss: 0.5515, Elapsed: 6m53s
Starting testing the validation set with 200 subgraphs!
2020-04-27 05:53:47.615909: Epoch: 1, Batch: 596, Loss: 0.6229, Elapsed: 5m0s
2020-04-27 06:00:50.181873: Epoch: 1, Batch: 597, Loss: 0.6252, Elapsed: 7m2s
2020-04-27 06:05:55.905030: Epoch: 1, Batch: 598, Loss: 0.6394, Elapsed: 5m5s
2020-04-27 06:08:31.167313: Validation Test:  Loss: 0.5840,  Acc: 69.6731, AUC: 0.7529, Precision: 0.7978 -- Elapsed: 43m4s
2020-04-27 06:11:09.060025: Epoch: 1, Batch: 599, Loss: 0.6214, Elapsed: 5m13s
2020-04-27 06:14:04.101858: Epoch: 1, Batch: 601, Loss: 0.5892, Elapsed: 5m32s
2020-04-27 06:18:00.847732: Epoch: 1, Batch: 602, Loss: 0.5437, Elapsed: 3m56s
2020-04-27 06:18:32.730091: Epoch: 1, Batch: 600, Loss: 0.6276, Elapsed: 7m23s
Starting testing the validation set with 200 subgraphs!
2020-04-27 06:21:44.549712: Epoch: 1, Batch: 603, Loss: 0.5450, Elapsed: 3m43s
2020-04-27 06:29:52.101721: Epoch: 1, Batch: 604, Loss: 0.6013, Elapsed: 8m7s
2020-04-27 06:34:42.850882: Epoch: 1, Batch: 605, Loss: 0.5696, Elapsed: 4m50s
2020-04-27 06:34:51.383587: Validation Test:  Loss: 0.5844,  Acc: 69.4235, AUC: 0.7520, Precision: 0.8000 -- Elapsed: 41m56s
2020-04-27 06:41:26.681732: Epoch: 1, Batch: 606, Loss: 0.6118, Elapsed: 6m43s
2020-04-27 06:42:52.231607: Epoch: 1, Batch: 601, Loss: 0.5726, Elapsed: 8m0s
2020-04-27 06:47:37.432320: Epoch: 1, Batch: 607, Loss: 0.5767, Elapsed: 6m10s
2020-04-27 06:51:32.924560: Epoch: 1, Batch: 602, Loss: 0.6086, Elapsed: 8m40s
2020-04-27 06:54:24.955522: Epoch: 1, Batch: 608, Loss: 0.5529, Elapsed: 6m47s
2020-04-27 06:57:50.426867: Epoch: 1, Batch: 609, Loss: 0.5079, Elapsed: 3m25s
2020-04-27 06:59:02.997817: Epoch: 1, Batch: 603, Loss: 0.5775, Elapsed: 7m30s
2020-04-27 07:01:37.902255: Validation Test:  Loss: 0.6414,  Acc: 62.6046, AUC: 0.6794, Precision: 0.7221 -- Elapsed: 43m5s
2020-04-27 07:05:01.241802: Epoch: 1, Batch: 610, Loss: 0.5620, Elapsed: 7m10s
2020-04-27 07:06:05.088886: Epoch: 1, Batch: 604, Loss: 0.6325, Elapsed: 7m2s
2020-04-27 07:09:09.643324: Epoch: 1, Batch: 601, Loss: 0.6286, Elapsed: 7m31s
2020-04-27 07:12:12.472690: Epoch: 1, Batch: 611, Loss: 0.5724, Elapsed: 7m11s
2020-04-27 07:13:44.136390: Epoch: 1, Batch: 605, Loss: 0.5853, Elapsed: 7m39s
2020-04-27 07:14:40.143488: Epoch: 1, Batch: 602, Loss: 0.6178, Elapsed: 5m30s
2020-04-27 07:18:49.248747: Epoch: 1, Batch: 612, Loss: 0.5540, Elapsed: 6m36s
2020-04-27 07:19:25.959540: Epoch: 1, Batch: 606, Loss: 0.5420, Elapsed: 5m41s
2020-04-27 07:24:22.766107: Epoch: 1, Batch: 607, Loss: 0.5453, Elapsed: 4m56s
2020-04-27 07:24:55.312057: Epoch: 1, Batch: 603, Loss: 0.6612, Elapsed: 10m15s
2020-04-27 07:26:33.273388: Epoch: 1, Batch: 613, Loss: 0.5679, Elapsed: 7m43s
2020-04-27 07:29:56.634345: Epoch: 1, Batch: 608, Loss: 0.5754, Elapsed: 5m33s
2020-04-27 07:30:51.215303: Epoch: 1, Batch: 604, Loss: 0.6202, Elapsed: 5m55s
2020-04-27 07:31:16.623588: Epoch: 1, Batch: 614, Loss: 0.5274, Elapsed: 4m43s
2020-04-27 07:36:12.794223: Epoch: 1, Batch: 615, Loss: 0.6139, Elapsed: 4m56s
2020-04-27 07:36:18.054853: Epoch: 1, Batch: 609, Loss: 0.5670, Elapsed: 6m21s
2020-04-27 07:38:07.472501: Epoch: 1, Batch: 605, Loss: 0.6502, Elapsed: 7m16s
2020-04-27 07:42:10.410798: Epoch: 1, Batch: 616, Loss: 0.5955, Elapsed: 5m57s
2020-04-27 07:44:07.956456: Epoch: 1, Batch: 610, Loss: 0.5596, Elapsed: 7m49s
2020-04-27 07:44:30.521473: Epoch: 1, Batch: 606, Loss: 0.6229, Elapsed: 6m23s
2020-04-27 07:49:30.947641: Epoch: 1, Batch: 617, Loss: 0.5800, Elapsed: 7m20s
2020-04-27 07:51:20.032068: Epoch: 1, Batch: 611, Loss: 0.5956, Elapsed: 7m12s
2020-04-27 07:51:19.959543: Epoch: 1, Batch: 607, Loss: 0.6347, Elapsed: 6m49s
2020-04-27 07:56:55.915279: Epoch: 1, Batch: 618, Loss: 0.5581, Elapsed: 7m24s
2020-04-27 07:58:05.011949: Epoch: 1, Batch: 612, Loss: 0.5341, Elapsed: 6m44s
2020-04-27 07:59:52.232601: Epoch: 1, Batch: 608, Loss: 0.6532, Elapsed: 8m32s
2020-04-27 08:00:47.524611: Epoch: 1, Batch: 619, Loss: 0.5653, Elapsed: 3m51s
2020-04-27 08:05:18.535408: Epoch: 1, Batch: 609, Loss: 0.6397, Elapsed: 5m26s
2020-04-27 08:06:08.822851: Epoch: 1, Batch: 613, Loss: 0.5852, Elapsed: 8m3s
2020-04-27 08:06:39.184720: Epoch: 1, Batch: 620, Loss: 0.6123, Elapsed: 5m51s
2020-04-27 08:10:01.962775: Epoch: 1, Batch: 614, Loss: 0.5425, Elapsed: 3m53s
2020-04-27 08:10:30.847242: Epoch: 1, Batch: 610, Loss: 0.6405, Elapsed: 5m12s
2020-04-27 08:10:51.875723: Epoch: 1, Batch: 621, Loss: 0.5256, Elapsed: 4m12s
2020-04-27 08:17:32.580371: Epoch: 1, Batch: 622, Loss: 0.6055, Elapsed: 6m40s
2020-04-27 08:18:20.122620: Epoch: 1, Batch: 611, Loss: 0.6541, Elapsed: 7m49s
2020-04-27 08:19:09.662046: Epoch: 1, Batch: 615, Loss: 0.5777, Elapsed: 9m7s
2020-04-27 08:24:10.891681: Epoch: 1, Batch: 623, Loss: 0.5701, Elapsed: 6m38s
2020-04-27 08:24:31.879885: Epoch: 1, Batch: 612, Loss: 0.6193, Elapsed: 6m11s
2020-04-27 08:27:02.540022: Epoch: 1, Batch: 616, Loss: 0.5972, Elapsed: 7m52s
2020-04-27 08:28:17.039633: Epoch: 1, Batch: 624, Loss: 0.5856, Elapsed: 4m6s
2020-04-27 08:30:19.142664: Epoch: 1, Batch: 613, Loss: 0.6203, Elapsed: 5m47s
2020-04-27 08:31:28.514615: Epoch: 1, Batch: 617, Loss: 0.5501, Elapsed: 4m25s
2020-04-27 08:34:25.139202: Epoch: 1, Batch: 614, Loss: 0.6393, Elapsed: 4m5s
2020-04-27 08:36:33.270427: Epoch: 1, Batch: 625, Loss: 0.6115, Elapsed: 8m16s
2020-04-27 08:37:11.845191: Epoch: 1, Batch: 618, Loss: 0.5607, Elapsed: 5m43s
2020-04-27 08:40:47.602965: Epoch: 1, Batch: 615, Loss: 0.6504, Elapsed: 6m22s
2020-04-27 08:43:05.983946: Epoch: 1, Batch: 626, Loss: 0.6089, Elapsed: 6m32s
2020-04-27 08:43:55.602820: Epoch: 1, Batch: 616, Loss: 0.5959, Elapsed: 3m7s
2020-04-27 08:44:27.942052: Epoch: 1, Batch: 619, Loss: 0.5895, Elapsed: 7m16s
2020-04-27 08:49:53.451188: Epoch: 1, Batch: 627, Loss: 0.5417, Elapsed: 6m47s
2020-04-27 08:52:09.305282: Epoch: 1, Batch: 620, Loss: 0.5735, Elapsed: 7m41s
2020-04-27 08:52:55.812533: Epoch: 1, Batch: 617, Loss: 0.6387, Elapsed: 9m0s
2020-04-27 08:54:26.586968: Epoch: 1, Batch: 628, Loss: 0.5646, Elapsed: 4m33s
2020-04-27 08:58:30.411554: Epoch: 1, Batch: 618, Loss: 0.6265, Elapsed: 5m34s
2020-04-27 09:00:35.038514: Epoch: 1, Batch: 621, Loss: 0.5983, Elapsed: 8m25s
2020-04-27 09:04:52.296247: Epoch: 1, Batch: 629, Loss: 0.5857, Elapsed: 10m25s
2020-04-27 09:05:14.165870: Epoch: 1, Batch: 619, Loss: 0.6199, Elapsed: 6m43s
2020-04-27 09:08:50.000334: Epoch: 1, Batch: 622, Loss: 0.5701, Elapsed: 8m14s
2020-04-27 09:11:54.325344: Epoch: 1, Batch: 620, Loss: 0.6194, Elapsed: 6m40s
2020-04-27 09:12:44.768116: Epoch: 1, Batch: 630, Loss: 0.5955, Elapsed: 7m52s
2020-04-27 09:16:26.262069: Epoch: 1, Batch: 621, Loss: 0.6326, Elapsed: 4m31s
2020-04-27 09:17:14.464641: Epoch: 1, Batch: 623, Loss: 0.5860, Elapsed: 8m24s
2020-04-27 09:21:33.389724: Epoch: 1, Batch: 631, Loss: 0.5865, Elapsed: 8m48s
2020-04-27 09:22:39.943659: Epoch: 1, Batch: 624, Loss: 0.5286, Elapsed: 5m25s
2020-04-27 09:25:07.456882: Epoch: 1, Batch: 622, Loss: 0.6769, Elapsed: 8m41s
2020-04-27 09:29:19.450914: Epoch: 1, Batch: 632, Loss: 0.5945, Elapsed: 7m46s
2020-04-27 09:30:05.399822: Epoch: 1, Batch: 625, Loss: 0.5774, Elapsed: 7m25s
2020-04-27 09:32:15.186932: Epoch: 1, Batch: 623, Loss: 0.6297, Elapsed: 7m7s
2020-04-27 09:37:02.557070: Epoch: 1, Batch: 624, Loss: 0.6253, Elapsed: 4m47s
2020-04-27 09:37:09.586514: Epoch: 1, Batch: 633, Loss: 0.5687, Elapsed: 7m50s
2020-04-27 09:37:45.504032: Epoch: 1, Batch: 626, Loss: 0.5780, Elapsed: 7m40s
2020-04-27 09:45:08.041633: Epoch: 1, Batch: 627, Loss: 0.6123, Elapsed: 7m22s
2020-04-27 09:45:59.171771: Epoch: 1, Batch: 625, Loss: 0.6675, Elapsed: 8m56s
2020-04-27 09:47:42.025552: Epoch: 1, Batch: 634, Loss: 0.6038, Elapsed: 10m32s
2020-04-27 09:52:36.090453: Epoch: 1, Batch: 635, Loss: 0.5369, Elapsed: 4m54s
2020-04-27 09:52:55.051188: Epoch: 1, Batch: 628, Loss: 0.5937, Elapsed: 7m46s
2020-04-27 09:54:41.300741: Epoch: 1, Batch: 626, Loss: 0.6464, Elapsed: 8m42s
2020-04-27 09:57:57.343250: Epoch: 1, Batch: 629, Loss: 0.5335, Elapsed: 5m2s
2020-04-27 09:58:46.279474: Epoch: 1, Batch: 627, Loss: 0.6121, Elapsed: 4m4s
2020-04-27 10:01:50.777579: Epoch: 1, Batch: 636, Loss: 0.6123, Elapsed: 9m14s
2020-04-27 10:04:27.056147: Epoch: 1, Batch: 630, Loss: 0.5407, Elapsed: 6m29s
2020-04-27 10:06:11.733345: Epoch: 1, Batch: 637, Loss: 0.5559, Elapsed: 4m20s
2020-04-27 10:06:59.652152: Epoch: 1, Batch: 628, Loss: 0.6287, Elapsed: 8m13s
2020-04-27 10:09:29.669880: Epoch: 1, Batch: 631, Loss: 0.5712, Elapsed: 5m2s
2020-04-27 10:12:36.044892: Epoch: 1, Batch: 629, Loss: 0.6329, Elapsed: 5m36s
2020-04-27 10:13:21.603957: Epoch: 1, Batch: 638, Loss: 0.6014, Elapsed: 7m9s
2020-04-27 10:19:32.540302: Epoch: 1, Batch: 632, Loss: 0.6091, Elapsed: 10m2s
2020-04-27 10:20:33.180206: Epoch: 1, Batch: 630, Loss: 0.6415, Elapsed: 7m57s
2020-04-27 10:21:51.569450: Epoch: 1, Batch: 639, Loss: 0.5763, Elapsed: 8m29s
2020-04-27 10:25:28.966267: Epoch: 1, Batch: 633, Loss: 0.5590, Elapsed: 5m56s
2020-04-27 10:25:50.239126: Epoch: 1, Batch: 631, Loss: 0.6214, Elapsed: 5m17s
2020-04-27 10:25:59.628688: Epoch: 1, Batch: 640, Loss: 0.5755, Elapsed: 4m8s
2020-04-27 10:31:24.016367: Epoch: 1, Batch: 641, Loss: 0.5954, Elapsed: 5m24s
2020-04-27 10:31:32.766554: Epoch: 1, Batch: 632, Loss: 0.6470, Elapsed: 5m42s
2020-04-27 10:32:13.899881: Epoch: 1, Batch: 634, Loss: 0.5686, Elapsed: 6m44s
2020-04-27 10:38:40.508803: Epoch: 1, Batch: 642, Loss: 0.5873, Elapsed: 7m16s
2020-04-27 10:38:59.875686: Epoch: 1, Batch: 633, Loss: 0.6404, Elapsed: 7m27s
2020-04-27 10:42:42.351489: Epoch: 1, Batch: 635, Loss: 0.6282, Elapsed: 10m28s
2020-04-27 10:44:18.937154: Epoch: 1, Batch: 634, Loss: 0.6311, Elapsed: 5m19s
2020-04-27 10:45:38.925998: Epoch: 1, Batch: 643, Loss: 0.5956, Elapsed: 6m58s
2020-04-27 10:49:58.323479: Epoch: 1, Batch: 636, Loss: 0.6083, Elapsed: 7m15s
2020-04-27 10:50:38.909069: Epoch: 1, Batch: 635, Loss: 0.6291, Elapsed: 6m19s
2020-04-27 10:55:29.564904: Epoch: 1, Batch: 637, Loss: 0.4852, Elapsed: 5m31s
2020-04-27 10:55:47.926510: Epoch: 1, Batch: 636, Loss: 0.6335, Elapsed: 5m8s
2020-04-27 10:55:51.618796: Epoch: 1, Batch: 644, Loss: 0.6359, Elapsed: 10m12s
2020-04-27 11:03:37.188615: Epoch: 1, Batch: 638, Loss: 0.5984, Elapsed: 8m7s
2020-04-27 11:05:06.976457: Epoch: 1, Batch: 645, Loss: 0.5966, Elapsed: 9m15s
2020-04-27 11:07:18.708113: Epoch: 1, Batch: 637, Loss: 0.6779, Elapsed: 11m30s
2020-04-27 11:12:28.926673: Epoch: 1, Batch: 638, Loss: 0.6238, Elapsed: 5m10s
2020-04-27 11:13:49.725451: Epoch: 1, Batch: 639, Loss: 0.5826, Elapsed: 10m12s
2020-04-27 11:15:42.813753: Epoch: 1, Batch: 646, Loss: 0.6168, Elapsed: 10m35s
2020-04-27 11:17:24.210091: Epoch: 1, Batch: 639, Loss: 0.6085, Elapsed: 4m55s
2020-04-27 11:21:20.034345: Epoch: 1, Batch: 647, Loss: 0.6049, Elapsed: 5m37s
2020-04-27 11:23:32.015865: Epoch: 1, Batch: 640, Loss: 0.6274, Elapsed: 6m7s
2020-04-27 11:24:33.607072: Epoch: 1, Batch: 640, Loss: 0.6509, Elapsed: 10m43s
2020-04-27 11:27:02.742347: Epoch: 1, Batch: 648, Loss: 0.5611, Elapsed: 5m42s
2020-04-27 11:30:09.928647: Epoch: 1, Batch: 641, Loss: 0.5586, Elapsed: 5m36s
2020-04-27 11:31:20.050909: Epoch: 1, Batch: 641, Loss: 0.6511, Elapsed: 7m47s
2020-04-27 11:32:56.759441: Epoch: 1, Batch: 649, Loss: 0.5701, Elapsed: 5m53s
2020-04-27 11:36:08.300841: Epoch: 1, Batch: 642, Loss: 0.5597, Elapsed: 5m58s
2020-04-27 11:37:15.268860: Epoch: 1, Batch: 650, Loss: 0.5418, Elapsed: 4m18s
Starting testing the validation set with 200 subgraphs!
2020-04-27 11:38:26.844065: Epoch: 1, Batch: 642, Loss: 0.6499, Elapsed: 7m6s
2020-04-27 11:41:57.843330: Epoch: 1, Batch: 643, Loss: 0.5430, Elapsed: 5m49s
2020-04-27 11:42:43.417196: Epoch: 1, Batch: 643, Loss: 0.6225, Elapsed: 4m16s
2020-04-27 11:48:58.816819: Epoch: 1, Batch: 644, Loss: 0.6324, Elapsed: 6m15s
2020-04-27 11:49:20.307192: Epoch: 1, Batch: 644, Loss: 0.5848, Elapsed: 7m22s
2020-04-27 11:55:32.566673: Epoch: 1, Batch: 645, Loss: 0.6497, Elapsed: 6m33s
2020-04-27 11:55:39.034519: Epoch: 1, Batch: 645, Loss: 0.5680, Elapsed: 6m18s
2020-04-27 12:00:26.442037: Epoch: 1, Batch: 646, Loss: 0.6340, Elapsed: 4m53s
2020-04-27 12:03:27.410539: Epoch: 1, Batch: 646, Loss: 0.5939, Elapsed: 7m48s
2020-04-27 12:04:08.593495: Epoch: 1, Batch: 647, Loss: 0.6330, Elapsed: 3m42s
2020-04-27 12:06:17.493898: Epoch: 1, Batch: 647, Loss: 0.4943, Elapsed: 2m50s
2020-04-27 12:09:30.838353: Epoch: 1, Batch: 648, Loss: 0.6481, Elapsed: 5m22s
2020-04-27 12:14:51.328689: Epoch: 1, Batch: 648, Loss: 0.6080, Elapsed: 8m33s
2020-04-27 12:15:02.270685: Epoch: 1, Batch: 649, Loss: 0.6320, Elapsed: 5m31s
2020-04-27 12:18:52.260621: Epoch: 1, Batch: 650, Loss: 0.6386, Elapsed: 3m49s
Starting testing the validation set with 200 subgraphs!
2020-04-27 12:19:45.340830: Validation Test:  Loss: 0.5877,  Acc: 69.5483, AUC: 0.7492, Precision: 0.7914 -- Elapsed: 42m30s
2020-04-27 12:20:53.266360: Epoch: 1, Batch: 649, Loss: 0.5686, Elapsed: 6m1s
2020-04-27 12:25:02.966087: Epoch: 1, Batch: 651, Loss: 0.5944, Elapsed: 5m17s
2020-04-27 12:29:31.828074: Epoch: 1, Batch: 652, Loss: 0.4823, Elapsed: 4m28s
2020-04-27 12:30:12.786448: Epoch: 1, Batch: 650, Loss: 0.6026, Elapsed: 9m19s
Starting testing the validation set with 200 subgraphs!
2020-04-27 12:37:10.666628: Epoch: 1, Batch: 653, Loss: 0.6054, Elapsed: 7m38s
2020-04-27 12:41:33.946834: Epoch: 1, Batch: 654, Loss: 0.5742, Elapsed: 4m23s
2020-04-27 12:47:06.748773: Epoch: 1, Batch: 655, Loss: 0.5762, Elapsed: 5m32s
2020-04-27 12:52:21.854752: Epoch: 1, Batch: 656, Loss: 0.5736, Elapsed: 5m15s
2020-04-27 12:58:12.331551: Epoch: 1, Batch: 657, Loss: 0.5527, Elapsed: 5m50s
2020-04-27 12:59:01.132855: Validation Test:  Loss: 0.6426,  Acc: 62.1806, AUC: 0.6717, Precision: 0.7175 -- Elapsed: 40m8s
2020-04-27 13:01:50.193923: Epoch: 1, Batch: 658, Loss: 0.5484, Elapsed: 3m37s
2020-04-27 13:05:47.057133: Epoch: 1, Batch: 659, Loss: 0.5465, Elapsed: 3m56s
2020-04-27 13:09:00.308674: Epoch: 1, Batch: 651, Loss: 0.7322, Elapsed: 9m59s
2020-04-27 13:10:54.889983: Validation Test:  Loss: 0.5822,  Acc: 69.6923, AUC: 0.7549, Precision: 0.8047 -- Elapsed: 40m42s
2020-04-27 13:12:42.788095: Epoch: 1, Batch: 660, Loss: 0.5838, Elapsed: 6m55s
2020-04-27 13:12:53.100189: Epoch: 1, Batch: 652, Loss: 0.6427, Elapsed: 3m52s
2020-04-27 13:17:04.546714: Epoch: 1, Batch: 651, Loss: 0.5508, Elapsed: 6m9s
2020-04-27 13:17:56.496929: Epoch: 1, Batch: 661, Loss: 0.5481, Elapsed: 5m13s
2020-04-27 13:19:34.214893: Epoch: 1, Batch: 653, Loss: 0.6622, Elapsed: 6m41s
2020-04-27 13:24:41.559687: Epoch: 1, Batch: 652, Loss: 0.6067, Elapsed: 7m36s
2020-04-27 13:25:53.858820: Epoch: 1, Batch: 662, Loss: 0.5974, Elapsed: 7m57s
2020-04-27 13:26:12.092944: Epoch: 1, Batch: 654, Loss: 0.6305, Elapsed: 6m37s
2020-04-27 13:29:34.334727: Epoch: 1, Batch: 653, Loss: 0.5360, Elapsed: 4m52s
2020-04-27 13:30:45.012640: Epoch: 1, Batch: 663, Loss: 0.5385, Elapsed: 4m51s
2020-04-27 13:34:23.386451: Epoch: 1, Batch: 655, Loss: 0.6946, Elapsed: 8m11s
2020-04-27 13:34:35.655229: Epoch: 1, Batch: 654, Loss: 0.5767, Elapsed: 5m1s
2020-04-27 13:37:51.843139: Epoch: 1, Batch: 664, Loss: 0.5937, Elapsed: 7m6s
2020-04-27 13:38:22.605614: Epoch: 1, Batch: 655, Loss: 0.5088, Elapsed: 3m46s
2020-04-27 13:38:50.036042: Epoch: 1, Batch: 656, Loss: 0.6250, Elapsed: 4m26s
2020-04-27 13:42:55.994850: Epoch: 1, Batch: 657, Loss: 0.6378, Elapsed: 4m5s
2020-04-27 13:45:31.219862: Epoch: 1, Batch: 656, Loss: 0.5981, Elapsed: 7m8s
2020-04-27 13:45:38.094874: Epoch: 1, Batch: 665, Loss: 0.5936, Elapsed: 7m46s
2020-04-27 13:48:36.070083: Epoch: 1, Batch: 658, Loss: 0.6242, Elapsed: 5m40s
2020-04-27 13:51:55.999956: Epoch: 1, Batch: 666, Loss: 0.6079, Elapsed: 6m17s
2020-04-27 13:52:05.242732: Epoch: 1, Batch: 657, Loss: 0.5541, Elapsed: 6m33s
2020-04-27 13:55:56.674105: Epoch: 1, Batch: 659, Loss: 0.6608, Elapsed: 7m20s
2020-04-27 13:59:53.873671: Epoch: 1, Batch: 667, Loss: 0.6229, Elapsed: 7m57s
2020-04-27 14:00:45.557015: Epoch: 1, Batch: 658, Loss: 0.6451, Elapsed: 8m40s
2020-04-27 14:04:40.503959: Epoch: 1, Batch: 660, Loss: 0.6661, Elapsed: 8m43s
2020-04-27 14:05:17.992128: Epoch: 1, Batch: 668, Loss: 0.5475, Elapsed: 5m24s
2020-04-27 14:07:03.929840: Epoch: 1, Batch: 659, Loss: 0.5818, Elapsed: 6m18s
2020-04-27 14:08:59.963917: Epoch: 1, Batch: 661, Loss: 0.6445, Elapsed: 4m19s
2020-04-27 14:14:05.804934: Epoch: 1, Batch: 669, Loss: 0.5967, Elapsed: 8m47s
2020-04-27 14:16:47.355813: Epoch: 1, Batch: 662, Loss: 0.6452, Elapsed: 7m47s
2020-04-27 14:17:23.557171: Epoch: 1, Batch: 660, Loss: 0.5824, Elapsed: 10m19s
2020-04-27 14:23:17.095912: Epoch: 1, Batch: 663, Loss: 0.6310, Elapsed: 6m29s
2020-04-27 14:26:54.430436: Epoch: 1, Batch: 664, Loss: 0.6192, Elapsed: 3m37s
2020-04-27 14:27:16.014547: Epoch: 1, Batch: 661, Loss: 0.6083, Elapsed: 9m52s
2020-04-27 14:30:12.197873: Epoch: 1, Batch: 670, Loss: 0.5993, Elapsed: 16m6s
2020-04-27 14:30:25.041745: Epoch: 1, Batch: 665, Loss: 0.6223, Elapsed: 3m30s
2020-04-27 14:33:21.971439: Epoch: 1, Batch: 662, Loss: 0.5963, Elapsed: 6m5s
2020-04-27 14:37:08.249622: Epoch: 1, Batch: 671, Loss: 0.5767, Elapsed: 6m56s
2020-04-27 14:39:29.971992: Epoch: 1, Batch: 666, Loss: 0.6772, Elapsed: 9m4s
2020-04-27 14:40:01.307979: Epoch: 1, Batch: 663, Loss: 0.5744, Elapsed: 6m39s
2020-04-27 14:44:29.003433: Epoch: 1, Batch: 672, Loss: 0.5839, Elapsed: 7m20s
2020-04-27 14:44:35.473397: Epoch: 1, Batch: 667, Loss: 0.6156, Elapsed: 5m5s
2020-04-27 14:47:26.818012: Epoch: 1, Batch: 664, Loss: 0.5910, Elapsed: 7m25s
2020-04-27 14:50:45.476010: Epoch: 1, Batch: 673, Loss: 0.5479, Elapsed: 6m16s
2020-04-27 14:53:18.982829: Epoch: 1, Batch: 668, Loss: 0.6367, Elapsed: 8m43s
2020-04-27 14:54:28.331192: Epoch: 1, Batch: 674, Loss: 0.5050, Elapsed: 3m42s
2020-04-27 14:55:01.252248: Epoch: 1, Batch: 665, Loss: 0.5767, Elapsed: 7m34s
2020-04-27 15:01:13.777883: Epoch: 1, Batch: 675, Loss: 0.5965, Elapsed: 6m45s
2020-04-27 15:04:46.511643: Epoch: 1, Batch: 669, Loss: 0.6973, Elapsed: 11m27s
2020-04-27 15:05:02.899136: Epoch: 1, Batch: 676, Loss: 0.5354, Elapsed: 3m49s
2020-04-27 15:08:57.492423: Epoch: 1, Batch: 666, Loss: 0.6252, Elapsed: 13m56s
2020-04-27 15:11:03.473933: Epoch: 1, Batch: 677, Loss: 0.5897, Elapsed: 6m0s
2020-04-27 15:13:43.560987: Epoch: 1, Batch: 670, Loss: 0.6451, Elapsed: 8m57s
2020-04-27 15:17:20.030378: Epoch: 1, Batch: 667, Loss: 0.5904, Elapsed: 8m22s
2020-04-27 15:17:57.745717: Epoch: 1, Batch: 678, Loss: 0.5653, Elapsed: 6m54s
2020-04-27 15:20:34.772073: Epoch: 1, Batch: 671, Loss: 0.6439, Elapsed: 6m51s
2020-04-27 15:23:16.228768: Epoch: 1, Batch: 668, Loss: 0.5877, Elapsed: 5m56s
2020-04-27 15:23:46.293612: Epoch: 1, Batch: 679, Loss: 0.6261, Elapsed: 5m48s
2020-04-27 15:25:31.306330: Epoch: 1, Batch: 672, Loss: 0.6366, Elapsed: 4m56s
2020-04-27 15:30:36.699050: Epoch: 1, Batch: 673, Loss: 0.6545, Elapsed: 5m5s
2020-04-27 15:30:45.189371: Epoch: 1, Batch: 669, Loss: 0.6061, Elapsed: 7m28s
2020-04-27 15:32:18.090737: Epoch: 1, Batch: 680, Loss: 0.5851, Elapsed: 8m31s
2020-04-27 15:37:34.065067: Epoch: 1, Batch: 670, Loss: 0.5895, Elapsed: 6m48s
2020-04-27 15:39:32.716977: Epoch: 1, Batch: 681, Loss: 0.6372, Elapsed: 7m14s
2020-04-27 15:43:22.278515: Epoch: 1, Batch: 671, Loss: 0.5611, Elapsed: 5m48s
2020-04-27 15:44:04.780707: Epoch: 1, Batch: 682, Loss: 0.5626, Elapsed: 4m32s
2020-04-27 15:46:36.544715: Epoch: 1, Batch: 674, Loss: 0.6633, Elapsed: 15m59s
2020-04-27 15:49:28.964077: Epoch: 1, Batch: 683, Loss: 0.5692, Elapsed: 5m24s
2020-04-27 15:50:18.625390: Epoch: 1, Batch: 672, Loss: 0.6379, Elapsed: 6m56s
2020-04-27 15:52:52.674411: Epoch: 1, Batch: 684, Loss: 0.5204, Elapsed: 3m23s
2020-04-27 15:53:36.105705: Epoch: 1, Batch: 675, Loss: 0.6336, Elapsed: 6m59s
2020-04-27 15:55:16.472778: Epoch: 1, Batch: 673, Loss: 0.5821, Elapsed: 4m57s
2020-04-27 15:55:41.024413: Epoch: 1, Batch: 685, Loss: 0.5228, Elapsed: 2m48s
2020-04-27 15:59:32.472832: Epoch: 1, Batch: 676, Loss: 0.6143, Elapsed: 5m56s
2020-04-27 16:00:31.372515: Epoch: 1, Batch: 686, Loss: 0.5763, Elapsed: 4m50s
2020-04-27 16:05:28.049146: Epoch: 1, Batch: 677, Loss: 0.6350, Elapsed: 5m55s
2020-04-27 16:06:19.911220: Epoch: 1, Batch: 687, Loss: 0.5401, Elapsed: 5m48s
2020-04-27 16:09:05.367367: Epoch: 1, Batch: 674, Loss: 0.6388, Elapsed: 13m48s
2020-04-27 16:09:49.258832: Epoch: 1, Batch: 688, Loss: 0.4702, Elapsed: 3m29s
2020-04-27 16:13:18.893790: Epoch: 1, Batch: 675, Loss: 0.5590, Elapsed: 4m13s
2020-04-27 16:14:46.103851: Epoch: 1, Batch: 689, Loss: 0.5323, Elapsed: 4m56s
2020-04-27 16:17:15.082583: Epoch: 1, Batch: 678, Loss: 0.6462, Elapsed: 11m47s
2020-04-27 16:23:05.254136: Epoch: 1, Batch: 690, Loss: 0.6003, Elapsed: 8m19s
2020-04-27 16:23:06.146639: Epoch: 1, Batch: 676, Loss: 0.6265, Elapsed: 9m47s
2020-04-27 16:23:57.196004: Epoch: 1, Batch: 679, Loss: 0.6246, Elapsed: 6m42s
2020-04-27 16:27:58.408710: Epoch: 1, Batch: 680, Loss: 0.6296, Elapsed: 4m1s
2020-04-27 16:29:16.279736: Epoch: 1, Batch: 677, Loss: 0.5850, Elapsed: 6m10s
2020-04-27 16:30:02.773098: Epoch: 1, Batch: 691, Loss: 0.5775, Elapsed: 6m57s
2020-04-27 16:37:03.245055: Epoch: 1, Batch: 681, Loss: 0.6515, Elapsed: 9m4s
2020-04-27 16:38:54.312695: Epoch: 1, Batch: 678, Loss: 0.6274, Elapsed: 9m38s
2020-04-27 16:39:54.276066: Epoch: 1, Batch: 692, Loss: 0.6703, Elapsed: 9m51s
2020-04-27 16:43:34.485174: Epoch: 1, Batch: 693, Loss: 0.4965, Elapsed: 3m40s
2020-04-27 16:44:44.477955: Epoch: 1, Batch: 682, Loss: 0.6877, Elapsed: 7m41s
2020-04-27 16:47:09.653588: Epoch: 1, Batch: 679, Loss: 0.5816, Elapsed: 8m15s
2020-04-27 16:49:43.788164: Epoch: 1, Batch: 694, Loss: 0.5361, Elapsed: 6m9s
2020-04-27 16:51:38.706830: Epoch: 1, Batch: 683, Loss: 0.6381, Elapsed: 6m54s
2020-04-27 16:52:22.692786: Epoch: 1, Batch: 680, Loss: 0.5524, Elapsed: 5m13s
2020-04-27 16:57:18.728673: Epoch: 1, Batch: 695, Loss: 0.5924, Elapsed: 7m34s
2020-04-27 16:59:04.700739: Epoch: 1, Batch: 681, Loss: 0.6059, Elapsed: 6m41s
2020-04-27 16:59:25.885539: Epoch: 1, Batch: 684, Loss: 0.6565, Elapsed: 7m47s
2020-04-27 17:03:56.420127: Epoch: 1, Batch: 685, Loss: 0.6288, Elapsed: 4m30s
2020-04-27 17:05:12.662884: Epoch: 1, Batch: 696, Loss: 0.6493, Elapsed: 7m53s
2020-04-27 17:10:07.371877: Epoch: 1, Batch: 682, Loss: 0.6471, Elapsed: 11m2s
2020-04-27 17:10:48.560244: Epoch: 1, Batch: 686, Loss: 0.6249, Elapsed: 6m52s
2020-04-27 17:11:10.101073: Epoch: 1, Batch: 697, Loss: 0.5586, Elapsed: 5m57s
2020-04-27 17:16:29.390501: Epoch: 1, Batch: 687, Loss: 0.6391, Elapsed: 5m40s
2020-04-27 17:19:12.955881: Epoch: 1, Batch: 698, Loss: 0.6212, Elapsed: 8m2s
2020-04-27 17:20:08.570582: Epoch: 1, Batch: 683, Loss: 0.6389, Elapsed: 10m1s
2020-04-27 17:21:25.930332: Epoch: 1, Batch: 688, Loss: 0.6299, Elapsed: 4m56s
2020-04-27 17:24:15.917086: Epoch: 1, Batch: 699, Loss: 0.5627, Elapsed: 5m2s
2020-04-27 17:26:11.006920: Epoch: 1, Batch: 689, Loss: 0.6201, Elapsed: 4m45s
2020-04-27 17:28:24.250879: Epoch: 1, Batch: 700, Loss: 0.6023, Elapsed: 4m8s
Starting testing the validation set with 200 subgraphs!
2020-04-27 17:30:16.367738: Epoch: 1, Batch: 684, Loss: 0.6006, Elapsed: 10m7s
2020-04-27 17:34:19.796121: Epoch: 1, Batch: 685, Loss: 0.5724, Elapsed: 4m3s
2020-04-27 17:37:57.198747: Epoch: 1, Batch: 686, Loss: 0.5934, Elapsed: 3m37s
2020-04-27 17:38:30.661484: Epoch: 1, Batch: 690, Loss: 0.6966, Elapsed: 12m19s
2020-04-27 17:43:00.864223: Epoch: 1, Batch: 687, Loss: 0.5883, Elapsed: 5m3s
2020-04-27 17:45:19.352339: Epoch: 1, Batch: 691, Loss: 0.6304, Elapsed: 6m48s
2020-04-27 17:49:30.062545: Epoch: 1, Batch: 688, Loss: 0.5713, Elapsed: 6m29s
2020-04-27 17:52:51.915181: Epoch: 1, Batch: 692, Loss: 0.6493, Elapsed: 7m32s
2020-04-27 17:55:31.822109: Epoch: 1, Batch: 689, Loss: 0.5794, Elapsed: 6m1s
2020-04-27 17:58:48.591240: Epoch: 1, Batch: 693, Loss: 0.6282, Elapsed: 5m56s
2020-04-27 18:00:20.878584: Epoch: 1, Batch: 690, Loss: 0.5677, Elapsed: 4m49s
2020-04-27 18:05:09.440529: Epoch: 1, Batch: 694, Loss: 0.6361, Elapsed: 6m20s
2020-04-27 18:05:47.027378: Epoch: 1, Batch: 691, Loss: 0.5842, Elapsed: 5m26s
2020-04-27 18:08:41.164259: Epoch: 1, Batch: 695, Loss: 0.5995, Elapsed: 3m31s
2020-04-27 18:09:44.471805: Validation Test:  Loss: 0.5937,  Acc: 68.5044, AUC: 0.7430, Precision: 0.7856 -- Elapsed: 41m20s
2020-04-27 18:13:59.090515: Epoch: 1, Batch: 696, Loss: 0.6208, Elapsed: 5m17s
2020-04-27 18:14:26.048125: Epoch: 1, Batch: 692, Loss: 0.5882, Elapsed: 8m38s
2020-04-27 18:15:23.841562: Epoch: 1, Batch: 701, Loss: 0.5673, Elapsed: 5m39s
2020-04-27 18:18:24.949455: Epoch: 1, Batch: 697, Loss: 0.6573, Elapsed: 4m25s
2020-04-27 18:18:35.708432: Epoch: 1, Batch: 693, Loss: 0.5926, Elapsed: 4m9s
2020-04-27 18:21:43.057100: Epoch: 1, Batch: 702, Loss: 0.5871, Elapsed: 6m19s
2020-04-27 18:23:59.793282: Epoch: 1, Batch: 698, Loss: 0.6471, Elapsed: 5m34s
2020-04-27 18:26:05.836281: Epoch: 1, Batch: 703, Loss: 0.4982, Elapsed: 4m22s
2020-04-27 18:28:02.976026: Epoch: 1, Batch: 699, Loss: 0.5974, Elapsed: 4m3s
2020-04-27 18:28:58.007846: Epoch: 1, Batch: 694, Loss: 0.6138, Elapsed: 10m22s
2020-04-27 18:31:27.044668: Epoch: 1, Batch: 704, Loss: 0.6250, Elapsed: 5m21s
2020-04-27 18:34:36.239594: Epoch: 1, Batch: 700, Loss: 0.6454, Elapsed: 6m33s
Starting testing the validation set with 200 subgraphs!
2020-04-27 18:35:15.002008: Epoch: 1, Batch: 695, Loss: 0.6209, Elapsed: 6m16s
2020-04-27 18:38:15.378171: Epoch: 1, Batch: 705, Loss: 0.5987, Elapsed: 6m48s
2020-04-27 18:41:08.042008: Epoch: 1, Batch: 696, Loss: 0.5921, Elapsed: 5m53s
2020-04-27 18:45:12.120312: Epoch: 1, Batch: 706, Loss: 0.6123, Elapsed: 6m56s
2020-04-27 18:49:48.759599: Epoch: 1, Batch: 707, Loss: 0.5310, Elapsed: 4m36s
2020-04-27 18:50:41.267491: Epoch: 1, Batch: 697, Loss: 0.6127, Elapsed: 9m33s
2020-04-27 18:56:11.278079: Epoch: 1, Batch: 708, Loss: 0.6138, Elapsed: 6m22s
2020-04-27 18:56:35.597082: Epoch: 1, Batch: 698, Loss: 0.5732, Elapsed: 5m54s
2020-04-27 19:02:36.418192: Epoch: 1, Batch: 699, Loss: 0.5952, Elapsed: 6m0s
2020-04-27 19:05:44.582028: Epoch: 1, Batch: 709, Loss: 0.6154, Elapsed: 9m33s
2020-04-27 19:08:43.789618: Epoch: 1, Batch: 700, Loss: 0.5835, Elapsed: 6m7s
Starting testing the validation set with 200 subgraphs!
2020-04-27 19:13:42.131351: Epoch: 1, Batch: 710, Loss: 0.5921, Elapsed: 7m57s
2020-04-27 19:15:40.449251: Validation Test:  Loss: 0.6420,  Acc: 62.3267, AUC: 0.6736, Precision: 0.7166 -- Elapsed: 41m4s
2020-04-27 19:16:50.101530: Epoch: 1, Batch: 711, Loss: 0.5299, Elapsed: 3m7s
2020-04-27 19:20:49.755109: Epoch: 1, Batch: 712, Loss: 0.6070, Elapsed: 3m59s
2020-04-27 19:23:01.812427: Epoch: 1, Batch: 701, Loss: 0.6343, Elapsed: 7m21s
2020-04-27 19:29:50.085366: Epoch: 1, Batch: 713, Loss: 0.6088, Elapsed: 9m0s
2020-04-27 19:31:54.387694: Epoch: 1, Batch: 702, Loss: 0.6345, Elapsed: 8m52s
2020-04-27 19:34:23.349165: Epoch: 1, Batch: 714, Loss: 0.5344, Elapsed: 4m33s
2020-04-27 19:36:42.057165: Epoch: 1, Batch: 703, Loss: 0.6292, Elapsed: 4m47s
2020-04-27 19:39:04.964925: Epoch: 1, Batch: 715, Loss: 0.5508, Elapsed: 4m41s
2020-04-27 19:40:38.046425: Epoch: 1, Batch: 704, Loss: 0.6150, Elapsed: 3m55s
2020-04-27 19:43:40.366852: Epoch: 1, Batch: 716, Loss: 0.5992, Elapsed: 4m35s
2020-04-27 19:48:11.906171: Epoch: 1, Batch: 717, Loss: 0.5579, Elapsed: 4m31s
2020-04-27 19:49:04.086808: Epoch: 1, Batch: 705, Loss: 0.6319, Elapsed: 8m26s
2020-04-27 19:50:28.044920: Validation Test:  Loss: 0.5828,  Acc: 69.8607, AUC: 0.7588, Precision: 0.8071 -- Elapsed: 41m44s
2020-04-27 19:53:53.823924: Epoch: 1, Batch: 718, Loss: 0.6033, Elapsed: 5m41s
2020-04-27 19:55:19.336125: Epoch: 1, Batch: 706, Loss: 0.6467, Elapsed: 6m15s
2020-04-27 19:57:06.521163: Epoch: 1, Batch: 701, Loss: 0.5449, Elapsed: 6m38s
2020-04-27 19:59:45.627942: Epoch: 1, Batch: 719, Loss: 0.6209, Elapsed: 5m51s
2020-04-27 20:00:24.281409: Epoch: 1, Batch: 707, Loss: 0.6291, Elapsed: 5m4s
2020-04-27 20:04:36.612043: Epoch: 1, Batch: 702, Loss: 0.5801, Elapsed: 7m30s
2020-04-27 20:07:27.460629: Epoch: 1, Batch: 708, Loss: 0.6615, Elapsed: 7m3s
2020-04-27 20:08:00.973855: Epoch: 1, Batch: 720, Loss: 0.5929, Elapsed: 8m15s
2020-04-27 20:10:49.480126: Epoch: 1, Batch: 703, Loss: 0.5800, Elapsed: 6m12s
2020-04-27 20:14:57.607851: Epoch: 1, Batch: 721, Loss: 0.5881, Elapsed: 6m56s
2020-04-27 20:15:18.341311: Epoch: 1, Batch: 709, Loss: 0.6418, Elapsed: 7m50s
2020-04-27 20:15:52.606945: Epoch: 1, Batch: 704, Loss: 0.5572, Elapsed: 5m3s
2020-04-27 20:19:28.707364: Epoch: 1, Batch: 722, Loss: 0.6048, Elapsed: 4m31s
2020-04-27 20:23:03.676593: Epoch: 1, Batch: 710, Loss: 0.6340, Elapsed: 7m45s
2020-04-27 20:25:56.687320: Epoch: 1, Batch: 705, Loss: 0.6093, Elapsed: 10m4s
2020-04-27 20:26:07.572749: Epoch: 1, Batch: 723, Loss: 0.5609, Elapsed: 6m38s
2020-04-27 20:29:01.622572: Epoch: 1, Batch: 711, Loss: 0.6475, Elapsed: 5m57s
2020-04-27 20:31:15.147376: Epoch: 1, Batch: 706, Loss: 0.5706, Elapsed: 5m18s
2020-04-27 20:32:14.852868: Epoch: 1, Batch: 724, Loss: 0.5983, Elapsed: 6m7s
2020-04-27 20:34:55.379333: Epoch: 1, Batch: 712, Loss: 0.6195, Elapsed: 5m53s
2020-04-27 20:36:23.969316: Epoch: 1, Batch: 725, Loss: 0.5676, Elapsed: 4m9s
2020-04-27 20:39:39.371959: Epoch: 1, Batch: 707, Loss: 0.6036, Elapsed: 8m24s
2020-04-27 20:41:52.652275: Epoch: 1, Batch: 726, Loss: 0.5661, Elapsed: 5m28s
2020-04-27 20:44:11.386112: Epoch: 1, Batch: 708, Loss: 0.5465, Elapsed: 4m31s
2020-04-27 20:44:23.423539: Epoch: 1, Batch: 713, Loss: 0.6503, Elapsed: 9m28s
2020-04-27 20:46:19.278829: Epoch: 1, Batch: 727, Loss: 0.5566, Elapsed: 4m26s
2020-04-27 20:48:50.043168: Epoch: 1, Batch: 709, Loss: 0.5813, Elapsed: 4m38s
2020-04-27 20:49:56.702612: Epoch: 1, Batch: 714, Loss: 0.6113, Elapsed: 5m33s
2020-04-27 20:50:31.628020: Epoch: 1, Batch: 728, Loss: 0.5127, Elapsed: 4m12s
2020-04-27 20:53:40.075135: Epoch: 1, Batch: 710, Loss: 0.5082, Elapsed: 4m50s
2020-04-27 20:56:33.906899: Epoch: 1, Batch: 729, Loss: 0.5805, Elapsed: 6m2s
2020-04-27 21:00:16.317913: Epoch: 1, Batch: 715, Loss: 0.6270, Elapsed: 10m19s
2020-04-27 21:00:27.534609: Epoch: 1, Batch: 711, Loss: 0.5882, Elapsed: 6m47s
2020-04-27 21:03:16.239525: Epoch: 1, Batch: 712, Loss: 0.4746, Elapsed: 2m48s
2020-04-27 21:05:40.428799: Epoch: 1, Batch: 730, Loss: 0.6159, Elapsed: 9m6s
2020-04-27 21:07:05.286713: Epoch: 1, Batch: 716, Loss: 0.6202, Elapsed: 6m48s
2020-04-27 21:09:56.877461: Epoch: 1, Batch: 713, Loss: 0.5847, Elapsed: 6m40s
2020-04-27 21:11:54.933744: Epoch: 1, Batch: 731, Loss: 0.5697, Elapsed: 6m14s
2020-04-27 21:14:02.332629: Epoch: 1, Batch: 717, Loss: 0.6523, Elapsed: 6m57s
2020-04-27 21:14:27.242113: Epoch: 1, Batch: 714, Loss: 0.5597, Elapsed: 4m30s
2020-04-27 21:17:17.286409: Epoch: 1, Batch: 732, Loss: 0.5546, Elapsed: 5m22s
2020-04-27 21:19:54.053658: Epoch: 1, Batch: 718, Loss: 0.6103, Elapsed: 5m51s
2020-04-27 21:21:16.866195: Epoch: 1, Batch: 715, Loss: 0.6111, Elapsed: 6m49s
2020-04-27 21:22:57.130987: Epoch: 1, Batch: 733, Loss: 0.5518, Elapsed: 5m39s
2020-04-27 21:26:09.377845: Epoch: 1, Batch: 716, Loss: 0.5169, Elapsed: 4m52s
2020-04-27 21:28:40.423311: Epoch: 1, Batch: 734, Loss: 0.5878, Elapsed: 5m43s
2020-04-27 21:29:35.699901: Epoch: 1, Batch: 719, Loss: 0.6742, Elapsed: 9m41s
2020-04-27 21:32:10.301328: Epoch: 1, Batch: 717, Loss: 0.6108, Elapsed: 6m0s
2020-04-27 21:33:26.350564: Epoch: 1, Batch: 735, Loss: 0.5923, Elapsed: 4m45s
2020-04-27 21:39:59.608601: Epoch: 1, Batch: 720, Loss: 0.6789, Elapsed: 10m23s
2020-04-27 21:40:54.434993: Epoch: 1, Batch: 718, Loss: 0.6436, Elapsed: 8m44s
2020-04-27 21:42:57.711014: Epoch: 1, Batch: 736, Loss: 0.6165, Elapsed: 9m31s
2020-04-27 21:45:25.002558: Epoch: 1, Batch: 721, Loss: 0.6220, Elapsed: 5m25s
2020-04-27 21:49:07.661065: Epoch: 1, Batch: 719, Loss: 0.6099, Elapsed: 8m13s
2020-04-27 21:50:11.448991: Epoch: 1, Batch: 722, Loss: 0.6420, Elapsed: 4m46s
2020-04-27 21:51:55.537444: Epoch: 1, Batch: 737, Loss: 0.6105, Elapsed: 8m57s
2020-04-27 21:56:18.862568: Epoch: 1, Batch: 723, Loss: 0.6374, Elapsed: 6m7s
2020-04-27 21:57:15.739377: Epoch: 1, Batch: 720, Loss: 0.6137, Elapsed: 8m8s
2020-04-27 22:01:25.413647: Epoch: 1, Batch: 738, Loss: 0.5982, Elapsed: 9m29s
2020-04-27 22:03:57.025097: Epoch: 1, Batch: 721, Loss: 0.6123, Elapsed: 6m41s
2020-04-27 22:05:27.857937: Epoch: 1, Batch: 724, Loss: 0.6336, Elapsed: 9m8s
2020-04-27 22:08:04.646996: Epoch: 1, Batch: 722, Loss: 0.5910, Elapsed: 4m7s
2020-04-27 22:10:00.175944: Epoch: 1, Batch: 739, Loss: 0.6302, Elapsed: 8m34s
2020-04-27 22:11:18.503934: Epoch: 1, Batch: 725, Loss: 0.6181, Elapsed: 5m50s
2020-04-27 22:14:09.060453: Epoch: 1, Batch: 723, Loss: 0.5823, Elapsed: 6m4s
2020-04-27 22:17:35.146757: Epoch: 1, Batch: 740, Loss: 0.5642, Elapsed: 7m34s
2020-04-27 22:19:28.484013: Epoch: 1, Batch: 726, Loss: 0.6238, Elapsed: 8m9s
2020-04-27 22:20:03.820266: Epoch: 1, Batch: 724, Loss: 0.5610, Elapsed: 5m54s
2020-04-27 22:21:25.920371: Epoch: 1, Batch: 741, Loss: 0.5266, Elapsed: 3m50s
2020-04-27 22:25:59.439720: Epoch: 1, Batch: 727, Loss: 0.6388, Elapsed: 6m30s
2020-04-27 22:28:14.645961: Epoch: 1, Batch: 725, Loss: 0.6130, Elapsed: 8m10s
2020-04-27 22:30:37.999327: Epoch: 1, Batch: 742, Loss: 0.5911, Elapsed: 9m12s
2020-04-27 22:32:39.333456: Epoch: 1, Batch: 728, Loss: 0.6303, Elapsed: 6m39s
2020-04-27 22:34:01.927102: Epoch: 1, Batch: 726, Loss: 0.5798, Elapsed: 5m47s
2020-04-27 22:39:18.824504: Epoch: 1, Batch: 727, Loss: 0.5483, Elapsed: 5m16s
2020-04-27 22:39:52.253819: Epoch: 1, Batch: 743, Loss: 0.5746, Elapsed: 9m14s
2020-04-27 22:40:06.090285: Epoch: 1, Batch: 729, Loss: 0.6819, Elapsed: 7m26s
2020-04-27 22:45:29.827603: Epoch: 1, Batch: 730, Loss: 0.6281, Elapsed: 5m23s
2020-04-27 22:46:03.074155: Epoch: 1, Batch: 728, Loss: 0.5857, Elapsed: 6m44s
2020-04-27 22:46:15.328544: Epoch: 1, Batch: 744, Loss: 0.6009, Elapsed: 6m23s
2020-04-27 22:51:00.692649: Epoch: 1, Batch: 729, Loss: 0.5534, Elapsed: 4m57s
2020-04-27 22:51:44.606687: Epoch: 1, Batch: 731, Loss: 0.6338, Elapsed: 6m14s
2020-04-27 22:54:46.197967: Epoch: 1, Batch: 730, Loss: 0.5293, Elapsed: 3m45s
2020-04-27 22:55:20.163207: Epoch: 1, Batch: 745, Loss: 0.5935, Elapsed: 9m4s
2020-04-27 22:55:55.973013: Epoch: 1, Batch: 732, Loss: 0.6331, Elapsed: 4m11s
2020-04-27 23:00:27.193427: Epoch: 1, Batch: 731, Loss: 0.5213, Elapsed: 5m40s
2020-04-27 23:01:34.768477: Epoch: 1, Batch: 733, Loss: 0.6284, Elapsed: 5m38s
2020-04-27 23:03:01.591300: Epoch: 1, Batch: 746, Loss: 0.5861, Elapsed: 7m41s
2020-04-27 23:05:22.171255: Epoch: 1, Batch: 732, Loss: 0.5334, Elapsed: 4m54s
2020-04-27 23:05:57.274977: Epoch: 1, Batch: 734, Loss: 0.6218, Elapsed: 4m22s
2020-04-27 23:11:50.025273: Epoch: 1, Batch: 735, Loss: 0.6281, Elapsed: 5m52s
2020-04-27 23:12:09.886881: Epoch: 1, Batch: 747, Loss: 0.5949, Elapsed: 9m8s
2020-04-27 23:14:33.717903: Epoch: 1, Batch: 733, Loss: 0.6158, Elapsed: 9m11s
2020-04-27 23:17:36.093523: Epoch: 1, Batch: 734, Loss: 0.4778, Elapsed: 3m2s
2020-04-27 23:19:21.317139: Epoch: 1, Batch: 736, Loss: 0.6346, Elapsed: 7m31s
2020-04-27 23:21:30.767575: Epoch: 1, Batch: 748, Loss: 0.5993, Elapsed: 9m20s
2020-04-27 23:24:19.208181: Epoch: 1, Batch: 735, Loss: 0.5708, Elapsed: 6m43s
2020-04-27 23:25:40.849835: Epoch: 1, Batch: 737, Loss: 0.6286, Elapsed: 6m19s
2020-04-27 23:25:47.712195: Epoch: 1, Batch: 749, Loss: 0.5158, Elapsed: 4m16s
2020-04-27 23:26:54.029744: Epoch: 1, Batch: 736, Loss: 0.4316, Elapsed: 2m34s
2020-04-27 23:31:32.813660: Epoch: 1, Batch: 738, Loss: 0.6557, Elapsed: 5m51s
2020-04-27 23:32:04.603869: Epoch: 1, Batch: 750, Loss: 0.5700, Elapsed: 6m16s
Starting testing the validation set with 200 subgraphs!
2020-04-27 23:32:10.046580: Epoch: 1, Batch: 737, Loss: 0.5837, Elapsed: 5m15s
2020-04-27 23:37:14.435539: Epoch: 1, Batch: 739, Loss: 0.6384, Elapsed: 5m41s
2020-04-27 23:38:53.017864: Epoch: 1, Batch: 738, Loss: 0.5461, Elapsed: 6m42s
2020-04-27 23:42:27.646006: Epoch: 1, Batch: 739, Loss: 0.4617, Elapsed: 3m34s
2020-04-27 23:44:48.912235: Epoch: 1, Batch: 740, Loss: 0.6300, Elapsed: 7m34s
2020-04-27 23:48:25.073404: Epoch: 1, Batch: 741, Loss: 0.6034, Elapsed: 3m36s
2020-04-27 23:48:28.174017: Epoch: 1, Batch: 740, Loss: 0.5591, Elapsed: 6m0s
2020-04-27 23:53:56.390572: Epoch: 1, Batch: 741, Loss: 0.6203, Elapsed: 5m28s
2020-04-27 23:54:56.449701: Epoch: 1, Batch: 742, Loss: 0.6337, Elapsed: 6m31s
2020-04-27 23:58:41.166105: Epoch: 1, Batch: 742, Loss: 0.5780, Elapsed: 4m44s
2020-04-27 23:59:30.457413: Epoch: 1, Batch: 743, Loss: 0.6760, Elapsed: 4m33s
2020-04-28 00:06:50.473302: Epoch: 1, Batch: 744, Loss: 0.6329, Elapsed: 7m19s
2020-04-28 00:06:52.767730: Epoch: 1, Batch: 743, Loss: 0.6220, Elapsed: 8m11s
2020-04-28 00:10:35.842072: Epoch: 1, Batch: 744, Loss: 0.5528, Elapsed: 3m43s
2020-04-28 00:13:50.360275: Epoch: 1, Batch: 745, Loss: 0.6414, Elapsed: 6m59s
2020-04-28 00:13:48.528111: Validation Test:  Loss: 0.5840,  Acc: 70.0616, AUC: 0.7551, Precision: 0.7958 -- Elapsed: 41m43s
2020-04-28 00:17:21.135001: Epoch: 1, Batch: 745, Loss: 0.5658, Elapsed: 6m45s
2020-04-28 00:20:22.476604: Epoch: 1, Batch: 751, Loss: 0.5679, Elapsed: 6m33s
2020-04-28 00:23:12.649756: Epoch: 1, Batch: 752, Loss: 0.4623, Elapsed: 2m50s
2020-04-28 00:25:11.996581: Epoch: 1, Batch: 746, Loss: 0.6681, Elapsed: 11m21s
2020-04-28 00:27:54.551301: Epoch: 1, Batch: 746, Loss: 0.6179, Elapsed: 10m33s
2020-04-28 00:29:20.862576: Epoch: 1, Batch: 753, Loss: 0.5890, Elapsed: 6m8s
2020-04-28 00:32:38.657990: Epoch: 1, Batch: 747, Loss: 0.5355, Elapsed: 4m44s
2020-04-28 00:32:52.881321: Epoch: 1, Batch: 747, Loss: 0.6580, Elapsed: 7m40s
2020-04-28 00:36:11.261942: Epoch: 1, Batch: 754, Loss: 0.6272, Elapsed: 6m50s
2020-04-28 00:39:44.845314: Epoch: 1, Batch: 748, Loss: 0.6151, Elapsed: 7m6s
2020-04-28 00:41:28.084476: Epoch: 1, Batch: 755, Loss: 0.5140, Elapsed: 5m16s
2020-04-28 00:41:44.512639: Epoch: 1, Batch: 748, Loss: 0.6622, Elapsed: 8m51s
2020-04-28 00:45:04.125880: Epoch: 1, Batch: 749, Loss: 0.5413, Elapsed: 5m19s
2020-04-28 00:46:05.801241: Epoch: 1, Batch: 756, Loss: 0.5464, Elapsed: 4m37s
2020-04-28 00:47:22.058250: Epoch: 1, Batch: 749, Loss: 0.6303, Elapsed: 5m37s
2020-04-28 00:50:10.420169: Epoch: 1, Batch: 750, Loss: 0.5305, Elapsed: 5m6s
Starting testing the validation set with 200 subgraphs!
2020-04-28 00:51:14.924299: Epoch: 1, Batch: 750, Loss: 0.6309, Elapsed: 3m52s
Starting testing the validation set with 200 subgraphs!
2020-04-28 00:52:45.476938: Epoch: 1, Batch: 757, Loss: 0.5649, Elapsed: 6m39s
2020-04-28 00:58:54.130759: Epoch: 1, Batch: 758, Loss: 0.6319, Elapsed: 6m8s
2020-04-28 01:04:51.688061: Epoch: 1, Batch: 759, Loss: 0.5972, Elapsed: 5m57s
2020-04-28 01:09:09.574113: Epoch: 1, Batch: 760, Loss: 0.5749, Elapsed: 4m17s
2020-04-28 01:12:44.379280: Epoch: 1, Batch: 761, Loss: 0.5649, Elapsed: 3m34s
2020-04-28 01:17:27.800710: Epoch: 1, Batch: 762, Loss: 0.5910, Elapsed: 4m43s
2020-04-28 01:23:15.728843: Epoch: 1, Batch: 763, Loss: 0.5861, Elapsed: 5m47s
2020-04-28 01:27:26.629166: Epoch: 1, Batch: 764, Loss: 0.5533, Elapsed: 4m10s
2020-04-28 01:28:53.479864: Validation Test:  Loss: 0.5822,  Acc: 70.0210, AUC: 0.7584, Precision: 0.8001 -- Elapsed: 38m43s
2020-04-28 01:29:40.216641: Validation Test:  Loss: 0.6406,  Acc: 62.3672, AUC: 0.6779, Precision: 0.7207 -- Elapsed: 38m25s
2020-04-28 01:32:10.050639: Epoch: 1, Batch: 765, Loss: 0.5371, Elapsed: 4m43s
2020-04-28 01:34:53.350775: Epoch: 1, Batch: 751, Loss: 0.5986, Elapsed: 5m59s
2020-04-28 01:35:34.755452: Epoch: 1, Batch: 751, Loss: 0.6410, Elapsed: 5m54s
2020-04-28 01:37:07.933857: Epoch: 1, Batch: 766, Loss: 0.5561, Elapsed: 4m57s
2020-04-28 01:39:52.043499: Epoch: 1, Batch: 767, Loss: 0.4771, Elapsed: 2m44s
2020-04-28 01:40:21.805688: Epoch: 1, Batch: 752, Loss: 0.5729, Elapsed: 5m28s
2020-04-28 01:46:52.275085: Epoch: 1, Batch: 752, Loss: 0.6437, Elapsed: 11m17s
2020-04-28 01:46:57.009120: Epoch: 1, Batch: 768, Loss: 0.5929, Elapsed: 7m4s
2020-04-28 01:47:39.516987: Epoch: 1, Batch: 753, Loss: 0.5981, Elapsed: 7m17s
2020-04-28 01:51:00.232717: Epoch: 1, Batch: 754, Loss: 0.5378, Elapsed: 3m20s
2020-04-28 01:51:28.648945: Epoch: 1, Batch: 753, Loss: 0.6287, Elapsed: 4m36s
2020-04-28 01:52:23.971137: Epoch: 1, Batch: 769, Loss: 0.5872, Elapsed: 5m26s
2020-04-28 01:56:43.630534: Epoch: 1, Batch: 754, Loss: 0.6315, Elapsed: 5m14s
2020-04-28 01:59:05.494526: Epoch: 1, Batch: 770, Loss: 0.5621, Elapsed: 6m41s
2020-04-28 01:59:44.685451: Epoch: 1, Batch: 755, Loss: 0.5861, Elapsed: 8m44s
2020-04-28 02:00:30.453867: Epoch: 1, Batch: 755, Loss: 0.6286, Elapsed: 3m46s
2020-04-28 02:03:53.931407: Epoch: 1, Batch: 756, Loss: 0.5769, Elapsed: 3m23s
2020-04-28 02:05:37.526708: Epoch: 1, Batch: 756, Loss: 0.5503, Elapsed: 5m52s
2020-04-28 02:06:18.494990: Epoch: 1, Batch: 771, Loss: 0.5551, Elapsed: 7m12s
2020-04-28 02:08:37.737872: Epoch: 1, Batch: 757, Loss: 0.6258, Elapsed: 4m43s
2020-04-28 02:12:02.004432: Epoch: 1, Batch: 757, Loss: 0.5893, Elapsed: 6m24s
2020-04-28 02:13:57.978135: Epoch: 1, Batch: 772, Loss: 0.5824, Elapsed: 7m39s
2020-04-28 02:14:38.164480: Epoch: 1, Batch: 758, Loss: 0.6098, Elapsed: 6m0s
2020-04-28 02:19:38.216275: Epoch: 1, Batch: 759, Loss: 0.6172, Elapsed: 5m0s
2020-04-28 02:19:41.719284: Epoch: 1, Batch: 758, Loss: 0.5864, Elapsed: 7m39s
2020-04-28 02:20:40.117429: Epoch: 1, Batch: 773, Loss: 0.5877, Elapsed: 6m42s
2020-04-28 02:25:23.126496: Epoch: 1, Batch: 760, Loss: 0.6432, Elapsed: 5m44s
2020-04-28 02:26:15.250324: Epoch: 1, Batch: 759, Loss: 0.6021, Elapsed: 6m33s
2020-04-28 02:26:58.418810: Epoch: 1, Batch: 774, Loss: 0.6313, Elapsed: 6m18s
2020-04-28 02:30:59.287927: Epoch: 1, Batch: 760, Loss: 0.5514, Elapsed: 4m44s
2020-04-28 02:31:30.645163: Epoch: 1, Batch: 775, Loss: 0.5037, Elapsed: 4m32s
2020-04-28 02:31:34.373723: Epoch: 1, Batch: 761, Loss: 0.6219, Elapsed: 6m11s
2020-04-28 02:36:15.099359: Epoch: 1, Batch: 762, Loss: 0.6130, Elapsed: 4m40s
2020-04-28 02:40:30.075642: Epoch: 1, Batch: 761, Loss: 0.5964, Elapsed: 9m30s
2020-04-28 02:41:03.556458: Epoch: 1, Batch: 776, Loss: 0.6001, Elapsed: 9m32s
2020-04-28 02:42:21.790003: Epoch: 1, Batch: 763, Loss: 0.6337, Elapsed: 6m6s
2020-04-28 02:47:43.263721: Epoch: 1, Batch: 777, Loss: 0.5751, Elapsed: 6m39s
2020-04-28 02:48:07.106615: Epoch: 1, Batch: 762, Loss: 0.5788, Elapsed: 7m37s
2020-04-28 02:49:46.283061: Epoch: 1, Batch: 764, Loss: 0.6443, Elapsed: 7m24s
2020-04-28 02:52:18.287387: Epoch: 1, Batch: 763, Loss: 0.4826, Elapsed: 4m11s
2020-04-28 02:52:44.263040: Epoch: 1, Batch: 778, Loss: 0.5595, Elapsed: 5m0s
2020-04-28 02:56:00.415191: Epoch: 1, Batch: 765, Loss: 0.6362, Elapsed: 6m14s
2020-04-28 02:57:03.882879: Epoch: 1, Batch: 764, Loss: 0.5089, Elapsed: 4m45s
2020-04-28 03:00:05.766805: Epoch: 1, Batch: 766, Loss: 0.6200, Elapsed: 4m5s
2020-04-28 03:00:23.994974: Epoch: 1, Batch: 779, Loss: 0.6316, Elapsed: 7m39s
2020-04-28 03:03:40.866534: Epoch: 1, Batch: 765, Loss: 0.6107, Elapsed: 6m36s
2020-04-28 03:07:20.984719: Epoch: 1, Batch: 780, Loss: 0.5904, Elapsed: 6m56s
2020-04-28 03:09:47.531601: Epoch: 1, Batch: 767, Loss: 0.6479, Elapsed: 9m41s
2020-04-28 03:11:08.951435: Epoch: 1, Batch: 766, Loss: 0.6331, Elapsed: 7m28s
2020-04-28 03:17:59.740779: Epoch: 1, Batch: 781, Loss: 0.6156, Elapsed: 10m38s
2020-04-28 03:18:13.548230: Epoch: 1, Batch: 767, Loss: 0.5394, Elapsed: 7m4s
2020-04-28 03:19:04.381636: Epoch: 1, Batch: 768, Loss: 0.6852, Elapsed: 9m16s
2020-04-28 03:22:59.547500: Epoch: 1, Batch: 782, Loss: 0.5673, Elapsed: 4m59s
2020-04-28 03:24:03.667841: Epoch: 1, Batch: 769, Loss: 0.6067, Elapsed: 4m59s
2020-04-28 03:27:09.901573: Epoch: 1, Batch: 768, Loss: 0.5851, Elapsed: 8m56s
2020-04-28 03:29:24.556068: Epoch: 1, Batch: 770, Loss: 0.6354, Elapsed: 5m20s
2020-04-28 03:30:31.427264: Epoch: 1, Batch: 783, Loss: 0.6131, Elapsed: 7m31s
2020-04-28 03:32:10.303563: Epoch: 1, Batch: 769, Loss: 0.5783, Elapsed: 5m0s
2020-04-28 03:35:47.722763: Epoch: 1, Batch: 771, Loss: 0.6184, Elapsed: 6m23s
2020-04-28 03:39:05.251972: Epoch: 1, Batch: 784, Loss: 0.5770, Elapsed: 8m33s
2020-04-28 03:40:33.832193: Epoch: 1, Batch: 770, Loss: 0.5903, Elapsed: 8m23s
2020-04-28 03:43:10.815140: Epoch: 1, Batch: 772, Loss: 0.6344, Elapsed: 7m23s
2020-04-28 03:43:47.133557: Epoch: 1, Batch: 785, Loss: 0.5361, Elapsed: 4m41s
2020-04-28 03:48:45.477741: Epoch: 1, Batch: 773, Loss: 0.6617, Elapsed: 5m34s
2020-04-28 03:48:54.107188: Epoch: 1, Batch: 771, Loss: 0.5754, Elapsed: 8m20s
2020-04-28 03:49:08.270068: Epoch: 1, Batch: 786, Loss: 0.5636, Elapsed: 5m21s
2020-04-28 03:51:56.419003: Epoch: 1, Batch: 787, Loss: 0.5106, Elapsed: 2m48s
2020-04-28 03:53:20.269054: Epoch: 1, Batch: 772, Loss: 0.5407, Elapsed: 4m26s
2020-04-28 03:54:32.549970: Epoch: 1, Batch: 774, Loss: 0.5922, Elapsed: 5m47s
2020-04-28 03:58:03.885818: Epoch: 1, Batch: 773, Loss: 0.5169, Elapsed: 4m43s
2020-04-28 04:00:14.697431: Epoch: 1, Batch: 788, Loss: 0.6291, Elapsed: 8m18s
2020-04-28 04:00:55.963352: Epoch: 1, Batch: 775, Loss: 0.6253, Elapsed: 6m23s
2020-04-28 04:03:15.629898: Epoch: 1, Batch: 789, Loss: 0.5263, Elapsed: 3m0s
2020-04-28 04:04:09.392518: Epoch: 1, Batch: 774, Loss: 0.5953, Elapsed: 6m5s
2020-04-28 04:05:37.100822: Epoch: 1, Batch: 776, Loss: 0.6487, Elapsed: 4m41s
2020-04-28 04:11:06.389018: Epoch: 1, Batch: 775, Loss: 0.5822, Elapsed: 6m56s
2020-04-28 04:13:56.394231: Epoch: 1, Batch: 776, Loss: 0.4742, Elapsed: 2m49s
2020-04-28 04:15:04.449911: Epoch: 1, Batch: 790, Loss: 0.6123, Elapsed: 11m48s
2020-04-28 04:15:34.465926: Epoch: 1, Batch: 777, Loss: 0.6824, Elapsed: 9m57s
2020-04-28 04:16:59.272868: Epoch: 1, Batch: 777, Loss: 0.4753, Elapsed: 3m2s
2020-04-28 04:22:15.141258: Epoch: 1, Batch: 791, Loss: 0.5596, Elapsed: 7m10s
2020-04-28 04:23:26.608735: Epoch: 1, Batch: 778, Loss: 0.6183, Elapsed: 7m52s
2020-04-28 04:24:35.108914: Epoch: 1, Batch: 778, Loss: 0.5903, Elapsed: 7m35s
2020-04-28 04:26:58.548013: Epoch: 1, Batch: 779, Loss: 0.5692, Elapsed: 3m31s
2020-04-28 04:30:03.786942: Epoch: 1, Batch: 779, Loss: 0.5333, Elapsed: 5m28s
2020-04-28 04:31:48.322613: Epoch: 1, Batch: 792, Loss: 0.6760, Elapsed: 9m33s
2020-04-28 04:33:59.487902: Epoch: 1, Batch: 780, Loss: 0.6624, Elapsed: 7m0s
2020-04-28 04:38:22.933646: Epoch: 1, Batch: 793, Loss: 0.5553, Elapsed: 6m34s
2020-04-28 04:38:38.060536: Epoch: 1, Batch: 780, Loss: 0.5825, Elapsed: 8m34s
2020-04-28 04:42:20.231300: Epoch: 1, Batch: 781, Loss: 0.4955, Elapsed: 3m42s
2020-04-28 04:44:22.536759: Epoch: 1, Batch: 781, Loss: 0.8422, Elapsed: 10m23s
2020-04-28 04:45:58.565195: Epoch: 1, Batch: 794, Loss: 0.5923, Elapsed: 7m35s
2020-04-28 04:48:48.292955: Epoch: 1, Batch: 782, Loss: 0.6232, Elapsed: 6m28s
2020-04-28 04:50:29.376323: Epoch: 1, Batch: 795, Loss: 0.5273, Elapsed: 4m30s
2020-04-28 04:52:10.326960: Epoch: 1, Batch: 782, Loss: 0.6394, Elapsed: 7m47s
2020-04-28 04:55:39.528948: Epoch: 1, Batch: 783, Loss: 0.5863, Elapsed: 6m51s
2020-04-28 04:59:04.330750: Epoch: 1, Batch: 783, Loss: 0.6580, Elapsed: 6m53s
2020-04-28 04:59:27.140769: Epoch: 1, Batch: 796, Loss: 0.5902, Elapsed: 8m57s
2020-04-28 05:03:00.065147: Epoch: 1, Batch: 784, Loss: 0.5823, Elapsed: 7m20s
2020-04-28 05:06:17.091281: Epoch: 1, Batch: 785, Loss: 0.4699, Elapsed: 3m17s
2020-04-28 05:07:43.349194: Epoch: 1, Batch: 784, Loss: 0.6526, Elapsed: 8m38s
2020-04-28 05:07:56.837490: Epoch: 1, Batch: 797, Loss: 0.5856, Elapsed: 8m29s
2020-04-28 05:12:33.083229: Epoch: 1, Batch: 786, Loss: 0.5965, Elapsed: 6m15s
2020-04-28 05:14:45.021686: Epoch: 1, Batch: 785, Loss: 0.6634, Elapsed: 7m1s
2020-04-28 05:14:45.384577: Epoch: 1, Batch: 798, Loss: 0.5740, Elapsed: 6m48s
2020-04-28 05:17:05.662006: Epoch: 1, Batch: 787, Loss: 0.5342, Elapsed: 4m32s
2020-04-28 05:18:32.136130: Epoch: 1, Batch: 786, Loss: 0.6213, Elapsed: 3m47s
2020-04-28 05:22:08.406389: Epoch: 1, Batch: 799, Loss: 0.5920, Elapsed: 7m22s
2020-04-28 05:23:03.751800: Epoch: 1, Batch: 787, Loss: 0.6174, Elapsed: 4m31s
2020-04-28 05:27:16.759226: Epoch: 1, Batch: 788, Loss: 0.6144, Elapsed: 10m11s
2020-04-28 05:28:11.589549: Epoch: 1, Batch: 788, Loss: 0.6262, Elapsed: 5m7s
2020-04-28 05:30:46.450134: Epoch: 1, Batch: 800, Loss: 0.6032, Elapsed: 8m38s
Starting testing the validation set with 200 subgraphs!
2020-04-28 05:34:05.289180: Epoch: 1, Batch: 789, Loss: 0.6582, Elapsed: 5m53s
2020-04-28 05:34:33.227278: Epoch: 1, Batch: 789, Loss: 0.5857, Elapsed: 7m16s
2020-04-28 05:41:21.520163: Epoch: 1, Batch: 790, Loss: 0.6362, Elapsed: 7m16s
2020-04-28 05:43:38.165710: Epoch: 1, Batch: 790, Loss: 0.6256, Elapsed: 9m4s
2020-04-28 05:50:21.683714: Epoch: 1, Batch: 791, Loss: 0.6647, Elapsed: 9m0s
2020-04-28 05:51:04.082764: Epoch: 1, Batch: 791, Loss: 0.5969, Elapsed: 7m25s
2020-04-28 05:58:59.135140: Epoch: 1, Batch: 792, Loss: 0.6490, Elapsed: 8m37s
2020-04-28 05:59:15.767937: Epoch: 1, Batch: 792, Loss: 0.5731, Elapsed: 8m11s
2020-04-28 06:05:39.220052: Epoch: 1, Batch: 793, Loss: 0.6423, Elapsed: 6m40s
2020-04-28 06:09:48.364206: Epoch: 1, Batch: 793, Loss: 0.5930, Elapsed: 10m32s
2020-04-28 06:12:13.546208: Validation Test:  Loss: 0.5875,  Acc: 69.2531, AUC: 0.7501, Precision: 0.8001 -- Elapsed: 41m27s
2020-04-28 06:13:32.196302: Epoch: 1, Batch: 794, Loss: 0.6460, Elapsed: 7m52s
2020-04-28 06:14:38.623062: Epoch: 1, Batch: 794, Loss: 0.5051, Elapsed: 4m50s
2020-04-28 06:19:02.802730: Epoch: 1, Batch: 801, Loss: 0.6333, Elapsed: 6m49s
2020-04-28 06:20:15.898174: Epoch: 1, Batch: 795, Loss: 0.6506, Elapsed: 6m43s
2020-04-28 06:23:00.465708: Epoch: 1, Batch: 795, Loss: 0.6485, Elapsed: 8m21s
2020-04-28 06:23:58.865889: Epoch: 1, Batch: 796, Loss: 0.6435, Elapsed: 3m42s
2020-04-28 06:24:58.959274: Epoch: 1, Batch: 802, Loss: 0.5741, Elapsed: 5m56s
2020-04-28 06:28:44.991162: Epoch: 1, Batch: 796, Loss: 0.5524, Elapsed: 5m44s
2020-04-28 06:30:57.759317: Epoch: 1, Batch: 797, Loss: 0.6178, Elapsed: 6m58s
2020-04-28 06:32:54.397555: Epoch: 1, Batch: 803, Loss: 0.6524, Elapsed: 7m55s
2020-04-28 06:34:26.001868: Epoch: 1, Batch: 797, Loss: 0.6056, Elapsed: 5m40s
2020-04-28 06:38:55.113229: Epoch: 1, Batch: 798, Loss: 0.6368, Elapsed: 7m57s
2020-04-28 06:39:08.569957: Epoch: 1, Batch: 804, Loss: 0.5881, Elapsed: 6m14s
2020-04-28 06:41:49.236576: Epoch: 1, Batch: 798, Loss: 0.5876, Elapsed: 7m23s
2020-04-28 06:45:07.341117: Epoch: 1, Batch: 799, Loss: 0.6355, Elapsed: 6m12s
2020-04-28 06:46:25.361386: Epoch: 1, Batch: 805, Loss: 0.6027, Elapsed: 7m16s
2020-04-28 06:49:28.189781: Epoch: 1, Batch: 800, Loss: 0.5979, Elapsed: 4m20s
Starting testing the validation set with 200 subgraphs!
2020-04-28 06:55:42.301129: Epoch: 1, Batch: 799, Loss: 0.6033, Elapsed: 13m53s
2020-04-28 06:57:06.159857: Epoch: 1, Batch: 806, Loss: 0.6258, Elapsed: 10m40s
2020-04-28 07:02:07.361272: Epoch: 1, Batch: 807, Loss: 0.5616, Elapsed: 5m1s
2020-04-28 07:05:27.324017: Epoch: 1, Batch: 800, Loss: 0.5630, Elapsed: 9m44s
Starting testing the validation set with 200 subgraphs!
2020-04-28 07:08:18.789509: Epoch: 1, Batch: 808, Loss: 0.5629, Elapsed: 6m11s
2020-04-28 07:18:48.726443: Epoch: 1, Batch: 809, Loss: 0.6330, Elapsed: 10m29s
2020-04-28 07:25:35.345430: Epoch: 1, Batch: 810, Loss: 0.5902, Elapsed: 6m46s
2020-04-28 07:29:07.654629: Validation Test:  Loss: 0.6420,  Acc: 62.6766, AUC: 0.6787, Precision: 0.7187 -- Elapsed: 39m39s
2020-04-28 07:32:33.438809: Epoch: 1, Batch: 811, Loss: 0.6280, Elapsed: 6m58s
2020-04-28 07:35:00.049929: Epoch: 1, Batch: 801, Loss: 0.6361, Elapsed: 5m52s
2020-04-28 07:38:21.457863: Epoch: 1, Batch: 812, Loss: 0.5601, Elapsed: 5m47s
2020-04-28 07:40:29.397083: Epoch: 1, Batch: 802, Loss: 0.5996, Elapsed: 5m29s
2020-04-28 07:43:10.158197: Epoch: 1, Batch: 813, Loss: 0.5311, Elapsed: 4m48s
2020-04-28 07:44:36.771958: Epoch: 1, Batch: 803, Loss: 0.6348, Elapsed: 4m7s
2020-04-28 07:45:26.048171: Validation Test:  Loss: 0.5804,  Acc: 70.4602, AUC: 0.7643, Precision: 0.8019 -- Elapsed: 39m58s
2020-04-28 07:50:28.994456: Epoch: 1, Batch: 804, Loss: 0.6735, Elapsed: 5m52s
2020-04-28 07:50:56.781217: Epoch: 1, Batch: 801, Loss: 0.5572, Elapsed: 5m30s
2020-04-28 07:52:02.039803: Epoch: 1, Batch: 814, Loss: 0.5940, Elapsed: 8m51s
2020-04-28 07:55:38.926187: Epoch: 1, Batch: 815, Loss: 0.4949, Elapsed: 3m36s
2020-04-28 07:56:07.584389: Epoch: 1, Batch: 802, Loss: 0.5475, Elapsed: 5m10s
2020-04-28 07:59:40.482150: Epoch: 1, Batch: 805, Loss: 0.6810, Elapsed: 9m11s
2020-04-28 08:01:07.622663: Epoch: 1, Batch: 816, Loss: 0.5754, Elapsed: 5m28s
2020-04-28 08:02:36.255069: Epoch: 1, Batch: 803, Loss: 0.5979, Elapsed: 6m28s
2020-04-28 08:05:08.096527: Epoch: 1, Batch: 806, Loss: 0.6101, Elapsed: 5m27s
2020-04-28 08:09:21.551882: Epoch: 1, Batch: 804, Loss: 0.5835, Elapsed: 6m45s
2020-04-28 08:09:58.471254: Epoch: 1, Batch: 807, Loss: 0.6432, Elapsed: 4m50s
2020-04-28 08:10:05.678118: Epoch: 1, Batch: 817, Loss: 0.6003, Elapsed: 8m58s
2020-04-28 08:15:03.739243: Epoch: 1, Batch: 805, Loss: 0.5645, Elapsed: 5m42s
2020-04-28 08:16:32.588594: Epoch: 1, Batch: 818, Loss: 0.5606, Elapsed: 6m26s
2020-04-28 08:16:54.472584: Epoch: 1, Batch: 808, Loss: 0.6817, Elapsed: 6m55s
2020-04-28 08:19:40.968397: Epoch: 1, Batch: 806, Loss: 0.5814, Elapsed: 4m37s
2020-04-28 08:22:21.803634: Epoch: 1, Batch: 809, Loss: 0.6146, Elapsed: 5m27s
2020-04-28 08:22:50.351416: Epoch: 1, Batch: 819, Loss: 0.5994, Elapsed: 6m17s
2020-04-28 08:24:31.299585: Epoch: 1, Batch: 807, Loss: 0.5944, Elapsed: 4m50s
2020-04-28 08:28:57.174911: Epoch: 1, Batch: 810, Loss: 0.6156, Elapsed: 6m35s
2020-04-28 08:30:41.257596: Epoch: 1, Batch: 820, Loss: 0.5768, Elapsed: 7m50s
2020-04-28 08:30:42.132208: Epoch: 1, Batch: 808, Loss: 0.5518, Elapsed: 6m10s
2020-04-28 08:34:40.456247: Epoch: 1, Batch: 821, Loss: 0.5403, Elapsed: 3m59s
2020-04-28 08:34:55.279758: Epoch: 1, Batch: 811, Loss: 0.6437, Elapsed: 5m58s
2020-04-28 08:37:56.629043: Epoch: 1, Batch: 809, Loss: 0.5546, Elapsed: 7m14s
2020-04-28 08:40:31.316084: Epoch: 1, Batch: 822, Loss: 0.5862, Elapsed: 5m50s
2020-04-28 08:40:50.209290: Epoch: 1, Batch: 812, Loss: 0.6514, Elapsed: 5m54s
2020-04-28 08:42:56.545299: Epoch: 1, Batch: 810, Loss: 0.5194, Elapsed: 4m59s
2020-04-28 08:46:47.394557: Epoch: 1, Batch: 813, Loss: 0.6203, Elapsed: 5m57s
2020-04-28 08:46:44.372575: Epoch: 1, Batch: 823, Loss: 0.5827, Elapsed: 6m13s
2020-04-28 08:50:17.906937: Epoch: 1, Batch: 811, Loss: 0.6151, Elapsed: 7m21s
2020-04-28 08:53:21.550132: Epoch: 1, Batch: 814, Loss: 0.6609, Elapsed: 6m34s
2020-04-28 08:53:27.779129: Epoch: 1, Batch: 824, Loss: 0.6318, Elapsed: 6m43s
2020-04-28 08:56:58.361977: Epoch: 1, Batch: 812, Loss: 0.5605, Elapsed: 6m40s
2020-04-28 08:57:19.265626: Epoch: 1, Batch: 825, Loss: 0.5514, Elapsed: 3m51s
2020-04-28 09:01:03.936757: Epoch: 1, Batch: 815, Loss: 0.6444, Elapsed: 7m42s
2020-04-28 09:01:50.533110: Epoch: 1, Batch: 813, Loss: 0.5996, Elapsed: 4m52s
2020-04-28 09:07:39.442375: Epoch: 1, Batch: 826, Loss: 0.5970, Elapsed: 10m20s
2020-04-28 09:08:43.237063: Epoch: 1, Batch: 816, Loss: 0.6237, Elapsed: 7m39s
2020-04-28 09:11:20.485236: Epoch: 1, Batch: 814, Loss: 0.6164, Elapsed: 9m29s
2020-04-28 09:17:01.163200: Epoch: 1, Batch: 817, Loss: 0.6359, Elapsed: 8m17s
2020-04-28 09:18:27.090768: Epoch: 1, Batch: 827, Loss: 0.6495, Elapsed: 10m47s
2020-04-28 09:20:46.659743: Epoch: 1, Batch: 815, Loss: 0.6361, Elapsed: 9m26s
2020-04-28 09:23:38.231294: Epoch: 1, Batch: 818, Loss: 0.6358, Elapsed: 6m37s
2020-04-28 09:28:50.755629: Epoch: 1, Batch: 828, Loss: 0.6091, Elapsed: 10m23s
2020-04-28 09:29:54.967594: Epoch: 1, Batch: 816, Loss: 0.5716, Elapsed: 9m8s
2020-04-28 09:31:40.989989: Epoch: 1, Batch: 819, Loss: 0.6196, Elapsed: 8m2s
2020-04-28 09:35:07.487193: Epoch: 1, Batch: 817, Loss: 0.5592, Elapsed: 5m12s
2020-04-28 09:35:28.006110: Epoch: 1, Batch: 829, Loss: 0.5812, Elapsed: 6m37s
2020-04-28 09:39:11.584631: Epoch: 1, Batch: 820, Loss: 0.6459, Elapsed: 7m30s
2020-04-28 09:42:46.452226: Epoch: 1, Batch: 818, Loss: 0.5747, Elapsed: 7m38s
2020-04-28 09:46:51.680644: Epoch: 1, Batch: 830, Loss: 0.6236, Elapsed: 11m23s
2020-04-28 09:47:17.671562: Epoch: 1, Batch: 821, Loss: 0.6789, Elapsed: 8m6s
2020-04-28 09:50:06.329372: Epoch: 1, Batch: 819, Loss: 0.6088, Elapsed: 7m19s
2020-04-28 09:52:31.558186: Epoch: 1, Batch: 831, Loss: 0.5743, Elapsed: 5m39s
2020-04-28 09:55:35.240865: Epoch: 1, Batch: 822, Loss: 0.6368, Elapsed: 8m17s
2020-04-28 09:58:05.555612: Epoch: 1, Batch: 820, Loss: 0.5987, Elapsed: 7m59s
2020-04-28 10:00:04.635893: Epoch: 1, Batch: 832, Loss: 0.5875, Elapsed: 7m33s
2020-04-28 10:00:48.258742: Epoch: 1, Batch: 823, Loss: 0.6160, Elapsed: 5m12s
2020-04-28 10:05:06.778193: Epoch: 1, Batch: 821, Loss: 0.5503, Elapsed: 7m1s
2020-04-28 10:06:05.135647: Epoch: 1, Batch: 824, Loss: 0.6025, Elapsed: 5m16s
2020-04-28 10:07:22.307960: Epoch: 1, Batch: 833, Loss: 0.6414, Elapsed: 7m17s
2020-04-28 10:10:43.965967: Epoch: 1, Batch: 825, Loss: 0.6174, Elapsed: 4m38s
2020-04-28 10:13:47.200003: Epoch: 1, Batch: 822, Loss: 0.6141, Elapsed: 8m40s
2020-04-28 10:15:39.636737: Epoch: 1, Batch: 834, Loss: 0.5715, Elapsed: 8m17s
2020-04-28 10:16:10.107667: Epoch: 1, Batch: 826, Loss: 0.6109, Elapsed: 5m26s
2020-04-28 10:18:34.020849: Epoch: 1, Batch: 823, Loss: 0.5875, Elapsed: 4m46s
2020-04-28 10:22:26.356107: Epoch: 1, Batch: 835, Loss: 0.5840, Elapsed: 6m46s
2020-04-28 10:25:51.050491: Epoch: 1, Batch: 827, Loss: 0.6431, Elapsed: 9m40s
2020-04-28 10:27:06.909537: Epoch: 1, Batch: 824, Loss: 0.5906, Elapsed: 8m32s
2020-04-28 10:31:29.134480: Epoch: 1, Batch: 836, Loss: 0.6098, Elapsed: 9m2s
2020-04-28 10:32:18.607002: Epoch: 1, Batch: 828, Loss: 0.6193, Elapsed: 6m27s
2020-04-28 10:38:19.948199: Epoch: 1, Batch: 837, Loss: 0.5583, Elapsed: 6m50s
2020-04-28 10:41:00.724581: Epoch: 1, Batch: 829, Loss: 0.6422, Elapsed: 8m42s
2020-04-28 10:41:41.689053: Epoch: 1, Batch: 838, Loss: 0.5666, Elapsed: 3m21s
2020-04-28 10:42:27.816557: Epoch: 1, Batch: 825, Loss: 0.6389, Elapsed: 15m20s
2020-04-28 10:47:28.330612: Epoch: 1, Batch: 826, Loss: 0.5819, Elapsed: 5m0s
2020-04-28 10:47:49.452184: Epoch: 1, Batch: 839, Loss: 0.6232, Elapsed: 6m7s
2020-04-28 10:48:02.769289: Epoch: 1, Batch: 830, Loss: 0.6481, Elapsed: 7m2s
2020-04-28 10:53:24.452507: Epoch: 1, Batch: 840, Loss: 0.5589, Elapsed: 5m34s
2020-04-28 10:54:42.949742: Epoch: 1, Batch: 831, Loss: 0.6758, Elapsed: 6m40s
2020-04-28 10:57:39.317626: Epoch: 1, Batch: 827, Loss: 0.6257, Elapsed: 10m10s
2020-04-28 11:00:15.645276: Epoch: 1, Batch: 841, Loss: 0.5680, Elapsed: 6m51s
2020-04-28 11:02:30.918504: Epoch: 1, Batch: 832, Loss: 0.6469, Elapsed: 7m47s
2020-04-28 11:03:17.739701: Epoch: 1, Batch: 828, Loss: 0.5710, Elapsed: 5m38s
2020-04-28 11:05:56.860607: Epoch: 1, Batch: 842, Loss: 0.5687, Elapsed: 5m41s
2020-04-28 11:09:27.634929: Epoch: 1, Batch: 833, Loss: 0.6777, Elapsed: 6m56s
2020-04-28 11:09:35.681556: Epoch: 1, Batch: 829, Loss: 0.5939, Elapsed: 6m17s
2020-04-28 11:12:05.540389: Epoch: 1, Batch: 843, Loss: 0.5939, Elapsed: 6m8s
2020-04-28 11:14:52.349264: Epoch: 1, Batch: 830, Loss: 0.5345, Elapsed: 5m16s
2020-04-28 11:17:19.569534: Epoch: 1, Batch: 834, Loss: 0.6142, Elapsed: 7m51s
2020-04-28 11:18:31.823068: Epoch: 1, Batch: 844, Loss: 0.5752, Elapsed: 6m26s
2020-04-28 11:22:58.231665: Epoch: 1, Batch: 831, Loss: 0.6082, Elapsed: 8m5s
2020-04-28 11:23:49.495085: Epoch: 1, Batch: 835, Loss: 0.6451, Elapsed: 6m29s
2020-04-28 11:24:46.582768: Epoch: 1, Batch: 845, Loss: 0.6462, Elapsed: 6m14s
2020-04-28 11:27:54.581533: Epoch: 1, Batch: 832, Loss: 0.5340, Elapsed: 4m56s
2020-04-28 11:30:48.373798: Epoch: 1, Batch: 836, Loss: 0.6334, Elapsed: 6m58s
2020-04-28 11:31:35.779280: Epoch: 1, Batch: 833, Loss: 0.5595, Elapsed: 3m41s
2020-04-28 11:32:26.330610: Epoch: 1, Batch: 846, Loss: 0.5954, Elapsed: 7m39s
2020-04-28 11:34:10.902952: Epoch: 1, Batch: 837, Loss: 0.5930, Elapsed: 3m22s
2020-04-28 11:36:59.161358: Epoch: 1, Batch: 834, Loss: 0.5595, Elapsed: 5m23s
2020-04-28 11:40:46.400166: Epoch: 1, Batch: 847, Loss: 0.5905, Elapsed: 8m20s
2020-04-28 11:41:52.953492: Epoch: 1, Batch: 838, Loss: 0.6299, Elapsed: 7m42s
2020-04-28 11:46:25.891776: Epoch: 1, Batch: 835, Loss: 0.5887, Elapsed: 9m26s
2020-04-28 11:48:49.005038: Epoch: 1, Batch: 848, Loss: 0.5794, Elapsed: 8m2s
2020-04-28 11:50:10.682172: Epoch: 1, Batch: 839, Loss: 0.6205, Elapsed: 8m17s
2020-04-28 11:52:39.151244: Epoch: 1, Batch: 836, Loss: 0.5710, Elapsed: 6m13s
2020-04-28 11:54:13.249588: Epoch: 1, Batch: 849, Loss: 0.5616, Elapsed: 5m24s
2020-04-28 11:55:19.832681: Epoch: 1, Batch: 840, Loss: 0.6280, Elapsed: 5m9s
2020-04-28 11:59:19.415129: Epoch: 1, Batch: 837, Loss: 0.5410, Elapsed: 6m40s
2020-04-28 12:00:12.272724: Epoch: 1, Batch: 850, Loss: 0.5870, Elapsed: 5m59s
Starting testing the validation set with 200 subgraphs!
2020-04-28 12:02:58.626933: Epoch: 1, Batch: 841, Loss: 0.6432, Elapsed: 7m38s
2020-04-28 12:06:47.458336: Epoch: 1, Batch: 838, Loss: 0.5984, Elapsed: 7m28s
2020-04-28 12:07:36.723860: Epoch: 1, Batch: 842, Loss: 0.6378, Elapsed: 4m38s
2020-04-28 12:13:21.064203: Epoch: 1, Batch: 843, Loss: 0.6209, Elapsed: 5m44s
2020-04-28 12:16:55.416775: Epoch: 1, Batch: 839, Loss: 0.6325, Elapsed: 10m7s
2020-04-28 12:18:04.069858: Epoch: 1, Batch: 844, Loss: 0.6281, Elapsed: 4m42s
2020-04-28 12:23:48.742753: Epoch: 1, Batch: 840, Loss: 0.5783, Elapsed: 6m53s
2020-04-28 12:25:15.254750: Epoch: 1, Batch: 845, Loss: 0.6294, Elapsed: 7m11s
2020-04-28 12:27:09.013396: Epoch: 1, Batch: 841, Loss: 0.4736, Elapsed: 3m20s
2020-04-28 12:29:46.927456: Epoch: 1, Batch: 846, Loss: 0.6231, Elapsed: 4m31s
2020-04-28 12:32:35.630462: Epoch: 1, Batch: 842, Loss: 0.5635, Elapsed: 5m26s
2020-04-28 12:35:08.692300: Epoch: 1, Batch: 847, Loss: 0.6242, Elapsed: 5m21s
2020-04-28 12:38:59.277844: Epoch: 1, Batch: 843, Loss: 0.5771, Elapsed: 6m23s
2020-04-28 12:39:24.984274: Epoch: 1, Batch: 848, Loss: 0.6397, Elapsed: 4m16s
2020-04-28 12:41:57.002351: Epoch: 1, Batch: 849, Loss: 0.6118, Elapsed: 2m31s
2020-04-28 12:42:06.227066: Validation Test:  Loss: 0.5876,  Acc: 69.5026, AUC: 0.7487, Precision: 0.7945 -- Elapsed: 41m53s
2020-04-28 12:45:12.951524: Epoch: 1, Batch: 844, Loss: 0.5825, Elapsed: 6m13s
2020-04-28 12:45:20.841799: Epoch: 1, Batch: 850, Loss: 0.5798, Elapsed: 3m23s
Starting testing the validation set with 200 subgraphs!
2020-04-28 12:47:59.574193: Epoch: 1, Batch: 851, Loss: 0.6108, Elapsed: 5m53s
2020-04-28 12:51:29.774541: Epoch: 1, Batch: 845, Loss: 0.6264, Elapsed: 6m16s
2020-04-28 12:53:49.998425: Epoch: 1, Batch: 852, Loss: 0.5621, Elapsed: 5m50s
2020-04-28 12:55:20.723386: Epoch: 1, Batch: 846, Loss: 0.5287, Elapsed: 3m50s
2020-04-28 13:00:39.312429: Epoch: 1, Batch: 853, Loss: 0.6361, Elapsed: 6m49s
2020-04-28 13:02:07.009501: Epoch: 1, Batch: 847, Loss: 0.5568, Elapsed: 6m46s
2020-04-28 13:07:06.392656: Epoch: 1, Batch: 848, Loss: 0.5274, Elapsed: 4m59s
2020-04-28 13:07:19.326708: Epoch: 1, Batch: 854, Loss: 0.5959, Elapsed: 6m39s
2020-04-28 13:13:03.678310: Epoch: 1, Batch: 849, Loss: 0.5507, Elapsed: 5m57s
2020-04-28 13:13:37.458283: Epoch: 1, Batch: 855, Loss: 0.5901, Elapsed: 6m18s
2020-04-28 13:18:13.050300: Epoch: 1, Batch: 856, Loss: 0.5876, Elapsed: 4m35s
2020-04-28 13:18:22.071729: Epoch: 1, Batch: 850, Loss: 0.5478, Elapsed: 5m18s
Starting testing the validation set with 200 subgraphs!
2020-04-28 13:21:37.031966: Epoch: 1, Batch: 857, Loss: 0.5215, Elapsed: 3m23s
2020-04-28 13:26:09.583267: Validation Test:  Loss: 0.6388,  Acc: 62.4636, AUC: 0.6804, Precision: 0.7233 -- Elapsed: 40m48s
2020-04-28 13:28:33.915162: Epoch: 1, Batch: 858, Loss: 0.5875, Elapsed: 6m56s
2020-04-28 13:32:43.871417: Epoch: 1, Batch: 851, Loss: 0.6579, Elapsed: 6m34s
2020-04-28 13:36:03.367330: Epoch: 1, Batch: 859, Loss: 0.5749, Elapsed: 7m29s
2020-04-28 13:38:49.163895: Epoch: 1, Batch: 852, Loss: 0.6455, Elapsed: 6m5s
2020-04-28 13:41:55.550280: Epoch: 1, Batch: 860, Loss: 0.5730, Elapsed: 5m52s
2020-04-28 13:44:37.967353: Epoch: 1, Batch: 853, Loss: 0.6158, Elapsed: 5m48s
2020-04-28 13:48:21.270019: Epoch: 1, Batch: 861, Loss: 0.6330, Elapsed: 6m25s
2020-04-28 13:51:11.823431: Epoch: 1, Batch: 854, Loss: 0.6257, Elapsed: 6m33s
2020-04-28 13:51:25.249481: Epoch: 1, Batch: 862, Loss: 0.4792, Elapsed: 3m3s
2020-04-28 13:57:01.412709: Epoch: 1, Batch: 855, Loss: 0.6425, Elapsed: 5m49s
2020-04-28 13:57:23.830723: Epoch: 1, Batch: 863, Loss: 0.5967, Elapsed: 5m58s
2020-04-28 14:00:02.754936: Validation Test:  Loss: 0.5788,  Acc: 70.1945, AUC: 0.7635, Precision: 0.8025 -- Elapsed: 41m40s
2020-04-28 14:00:27.115574: Epoch: 1, Batch: 864, Loss: 0.5439, Elapsed: 3m3s
2020-04-28 14:02:57.516918: Epoch: 1, Batch: 856, Loss: 0.6302, Elapsed: 5m56s
2020-04-28 14:03:22.592478: Epoch: 1, Batch: 865, Loss: 0.5056, Elapsed: 2m55s
2020-04-28 14:05:59.547812: Epoch: 1, Batch: 851, Loss: 0.5780, Elapsed: 5m56s
2020-04-28 14:08:17.047370: Epoch: 1, Batch: 857, Loss: 0.6164, Elapsed: 5m19s
2020-04-28 14:10:31.767496: Epoch: 1, Batch: 866, Loss: 0.5636, Elapsed: 7m9s
2020-04-28 14:14:01.707343: Epoch: 1, Batch: 852, Loss: 0.6162, Elapsed: 8m2s
2020-04-28 14:14:18.534242: Epoch: 1, Batch: 858, Loss: 0.6315, Elapsed: 6m1s
2020-04-28 14:16:57.425778: Epoch: 1, Batch: 867, Loss: 0.6035, Elapsed: 6m25s
2020-04-28 14:18:40.000933: Epoch: 1, Batch: 859, Loss: 0.6392, Elapsed: 4m21s
2020-04-28 14:19:49.897205: Epoch: 1, Batch: 853, Loss: 0.5636, Elapsed: 5m48s
2020-04-28 14:22:05.602716: Epoch: 1, Batch: 868, Loss: 0.5344, Elapsed: 5m8s
2020-04-28 14:24:32.847104: Epoch: 1, Batch: 854, Loss: 0.5145, Elapsed: 4m42s
2020-04-28 14:25:05.253702: Epoch: 1, Batch: 860, Loss: 0.6549, Elapsed: 6m25s
2020-04-28 14:27:51.284084: Epoch: 1, Batch: 869, Loss: 0.6161, Elapsed: 5m45s
2020-04-28 14:29:57.924378: Epoch: 1, Batch: 861, Loss: 0.6479, Elapsed: 4m52s
2020-04-28 14:31:11.331329: Epoch: 1, Batch: 855, Loss: 0.5635, Elapsed: 6m38s
2020-04-28 14:33:41.259734: Epoch: 1, Batch: 870, Loss: 0.5527, Elapsed: 5m49s
2020-04-28 14:37:12.746870: Epoch: 1, Batch: 862, Loss: 0.6789, Elapsed: 7m14s
2020-04-28 14:39:18.325708: Epoch: 1, Batch: 856, Loss: 0.5528, Elapsed: 8m6s
2020-04-28 14:39:36.948134: Epoch: 1, Batch: 871, Loss: 0.5726, Elapsed: 5m55s
2020-04-28 14:44:18.598234: Epoch: 1, Batch: 863, Loss: 0.6147, Elapsed: 7m5s
2020-04-28 14:44:42.719052: Epoch: 1, Batch: 872, Loss: 0.5709, Elapsed: 5m5s
2020-04-28 14:47:09.336753: Epoch: 1, Batch: 873, Loss: 0.5202, Elapsed: 2m26s
2020-04-28 14:49:10.648545: Epoch: 1, Batch: 857, Loss: 0.6042, Elapsed: 9m52s
2020-04-28 14:49:44.733149: Epoch: 1, Batch: 864, Loss: 0.6254, Elapsed: 5m26s
2020-04-28 14:51:45.715236: Epoch: 1, Batch: 874, Loss: 0.5886, Elapsed: 4m36s
2020-04-28 14:54:34.664097: Epoch: 1, Batch: 858, Loss: 0.5732, Elapsed: 5m23s
2020-04-28 14:55:37.983313: Epoch: 1, Batch: 865, Loss: 0.6350, Elapsed: 5m53s
2020-04-28 14:58:33.668635: Epoch: 1, Batch: 875, Loss: 0.5629, Elapsed: 6m47s
2020-04-28 14:59:43.020428: Epoch: 1, Batch: 859, Loss: 0.6023, Elapsed: 5m8s
2020-04-28 15:03:42.720675: Epoch: 1, Batch: 866, Loss: 0.6378, Elapsed: 8m4s
2020-04-28 15:06:11.119533: Epoch: 1, Batch: 867, Loss: 0.6467, Elapsed: 2m28s
2020-04-28 15:06:50.218945: Epoch: 1, Batch: 876, Loss: 0.6381, Elapsed: 8m16s
2020-04-28 15:09:33.342007: Epoch: 1, Batch: 860, Loss: 0.5734, Elapsed: 9m50s
2020-04-28 15:10:00.920672: Epoch: 1, Batch: 877, Loss: 0.4563, Elapsed: 3m10s
2020-04-28 15:10:14.080524: Epoch: 1, Batch: 868, Loss: 0.6036, Elapsed: 4m2s
2020-04-28 15:16:23.048581: Epoch: 1, Batch: 861, Loss: 0.6039, Elapsed: 6m49s
2020-04-28 15:17:16.977248: Epoch: 1, Batch: 878, Loss: 0.5823, Elapsed: 7m16s
2020-04-28 15:18:57.543538: Epoch: 1, Batch: 869, Loss: 0.6326, Elapsed: 8m43s
2020-04-28 15:20:20.513140: Epoch: 1, Batch: 879, Loss: 0.4662, Elapsed: 3m3s
2020-04-28 15:22:06.911560: Epoch: 1, Batch: 862, Loss: 0.5497, Elapsed: 5m43s
2020-04-28 15:22:20.166863: Epoch: 1, Batch: 870, Loss: 0.6057, Elapsed: 3m22s
2020-04-28 15:26:10.162169: Epoch: 1, Batch: 880, Loss: 0.5400, Elapsed: 5m49s
2020-04-28 15:28:48.937972: Epoch: 1, Batch: 863, Loss: 0.5653, Elapsed: 6m42s
2020-04-28 15:32:23.152871: Epoch: 1, Batch: 881, Loss: 0.5727, Elapsed: 6m12s
2020-04-28 15:33:19.795161: Epoch: 1, Batch: 871, Loss: 0.6700, Elapsed: 10m59s
2020-04-28 15:37:19.869781: Epoch: 1, Batch: 864, Loss: 0.6126, Elapsed: 8m30s
2020-04-28 15:38:44.374298: Epoch: 1, Batch: 872, Loss: 0.6336, Elapsed: 5m24s
2020-04-28 15:44:18.075678: Epoch: 1, Batch: 882, Loss: 0.6020, Elapsed: 11m54s
2020-04-28 15:46:44.907659: Epoch: 1, Batch: 865, Loss: 0.5917, Elapsed: 9m25s
2020-04-28 15:48:37.452452: Epoch: 1, Batch: 873, Loss: 0.6547, Elapsed: 9m53s
2020-04-28 15:49:49.075736: Epoch: 1, Batch: 866, Loss: 0.4487, Elapsed: 3m4s
2020-04-28 15:52:16.957536: Epoch: 1, Batch: 883, Loss: 0.5635, Elapsed: 7m58s
2020-04-28 15:55:30.399608: Epoch: 1, Batch: 874, Loss: 0.6339, Elapsed: 6m52s
2020-04-28 15:56:21.840600: Epoch: 1, Batch: 867, Loss: 0.5632, Elapsed: 6m32s
2020-04-28 15:58:40.909966: Epoch: 1, Batch: 884, Loss: 0.5791, Elapsed: 6m23s
2020-04-28 16:03:10.123552: Epoch: 1, Batch: 875, Loss: 0.6287, Elapsed: 7m39s
2020-04-28 16:05:02.088306: Epoch: 1, Batch: 868, Loss: 0.6099, Elapsed: 8m40s
2020-04-28 16:06:16.391550: Epoch: 1, Batch: 885, Loss: 0.5987, Elapsed: 7m35s
2020-04-28 16:08:46.288176: Epoch: 1, Batch: 876, Loss: 0.5979, Elapsed: 5m36s
2020-04-28 16:12:31.941435: Epoch: 1, Batch: 886, Loss: 0.5545, Elapsed: 6m15s
2020-04-28 16:13:41.744935: Epoch: 1, Batch: 869, Loss: 0.5769, Elapsed: 8m39s
2020-04-28 16:14:06.570736: Epoch: 1, Batch: 877, Loss: 0.6284, Elapsed: 5m20s
2020-04-28 16:19:40.400309: Epoch: 1, Batch: 870, Loss: 0.5862, Elapsed: 5m58s
2020-04-28 16:19:47.748353: Epoch: 1, Batch: 887, Loss: 0.5696, Elapsed: 7m15s
2020-04-28 16:22:02.489599: Epoch: 1, Batch: 878, Loss: 0.6222, Elapsed: 7m55s
2020-04-28 16:24:25.720077: Epoch: 1, Batch: 888, Loss: 0.5407, Elapsed: 4m37s
2020-04-28 16:29:38.863823: Epoch: 1, Batch: 879, Loss: 0.6321, Elapsed: 7m36s
2020-04-28 16:29:51.436911: Epoch: 1, Batch: 871, Loss: 0.5858, Elapsed: 10m11s
2020-04-28 16:32:25.455676: Epoch: 1, Batch: 889, Loss: 0.6050, Elapsed: 7m59s
2020-04-28 16:34:14.978527: Epoch: 1, Batch: 872, Loss: 0.5722, Elapsed: 4m23s
2020-04-28 16:38:04.108601: Epoch: 1, Batch: 880, Loss: 0.6323, Elapsed: 8m25s
2020-04-28 16:39:10.532916: Epoch: 1, Batch: 890, Loss: 0.5541, Elapsed: 6m45s
2020-04-28 16:39:48.676243: Epoch: 1, Batch: 873, Loss: 0.5746, Elapsed: 5m33s
2020-04-28 16:43:51.634120: Epoch: 1, Batch: 881, Loss: 0.6370, Elapsed: 5m47s
2020-04-28 16:46:33.402141: Epoch: 1, Batch: 882, Loss: 0.5909, Elapsed: 2m41s
2020-04-28 16:48:59.845520: Epoch: 1, Batch: 891, Loss: 0.5917, Elapsed: 9m49s
2020-04-28 16:48:56.661223: Epoch: 1, Batch: 874, Loss: 0.5702, Elapsed: 9m7s
2020-04-28 16:51:55.659122: Epoch: 1, Batch: 883, Loss: 0.6062, Elapsed: 5m22s
2020-04-28 16:53:36.837431: Epoch: 1, Batch: 892, Loss: 0.5742, Elapsed: 4m36s
2020-04-28 16:53:48.086139: Epoch: 1, Batch: 875, Loss: 0.5592, Elapsed: 4m51s
2020-04-28 16:56:50.988997: Epoch: 1, Batch: 876, Loss: 0.5175, Elapsed: 3m2s
2020-04-28 17:01:16.138798: Epoch: 1, Batch: 884, Loss: 0.6375, Elapsed: 9m20s
2020-04-28 17:03:13.601654: Epoch: 1, Batch: 877, Loss: 0.5612, Elapsed: 6m22s
2020-04-28 17:03:32.756048: Epoch: 1, Batch: 893, Loss: 0.5812, Elapsed: 9m55s
2020-04-28 17:07:34.681258: Epoch: 1, Batch: 885, Loss: 0.6463, Elapsed: 6m18s
2020-04-28 17:08:56.539122: Epoch: 1, Batch: 894, Loss: 0.5875, Elapsed: 5m23s
2020-04-28 17:08:57.590893: Epoch: 1, Batch: 878, Loss: 0.5314, Elapsed: 5m43s
2020-04-28 17:12:37.101995: Epoch: 1, Batch: 879, Loss: 0.5364, Elapsed: 3m39s
2020-04-28 17:15:05.166522: Epoch: 1, Batch: 895, Loss: 0.5736, Elapsed: 6m8s
2020-04-28 17:17:42.840509: Epoch: 1, Batch: 886, Loss: 0.6378, Elapsed: 10m8s
2020-04-28 17:18:19.994554: Epoch: 1, Batch: 880, Loss: 0.5744, Elapsed: 5m42s
2020-04-28 17:22:18.388829: Epoch: 1, Batch: 896, Loss: 0.6093, Elapsed: 7m13s
2020-04-28 17:23:32.705253: Epoch: 1, Batch: 887, Loss: 0.6800, Elapsed: 5m49s
2020-04-28 17:25:56.931003: Epoch: 1, Batch: 881, Loss: 0.5932, Elapsed: 7m36s
2020-04-28 17:26:54.130991: Epoch: 1, Batch: 897, Loss: 0.5632, Elapsed: 4m35s
2020-04-28 17:28:14.866523: Epoch: 1, Batch: 888, Loss: 0.6097, Elapsed: 4m42s
2020-04-28 17:30:13.147894: Epoch: 1, Batch: 882, Loss: 0.5417, Elapsed: 4m16s
2020-04-28 17:32:14.674958: Epoch: 1, Batch: 889, Loss: 0.5957, Elapsed: 3m59s
2020-04-28 17:36:49.662175: Epoch: 1, Batch: 890, Loss: 0.6229, Elapsed: 4m34s
2020-04-28 17:36:55.558337: Epoch: 1, Batch: 898, Loss: 0.5744, Elapsed: 10m1s
2020-04-28 17:38:11.913887: Epoch: 1, Batch: 883, Loss: 0.5935, Elapsed: 7m58s
2020-04-28 17:43:42.432842: Epoch: 1, Batch: 891, Loss: 0.6115, Elapsed: 6m52s
2020-04-28 17:43:51.187915: Epoch: 1, Batch: 884, Loss: 0.5829, Elapsed: 5m39s
2020-04-28 17:45:17.195794: Epoch: 1, Batch: 899, Loss: 0.6585, Elapsed: 8m21s
2020-04-28 17:49:29.463059: Epoch: 1, Batch: 885, Loss: 0.6024, Elapsed: 5m38s
2020-04-28 17:51:00.465028: Epoch: 1, Batch: 900, Loss: 0.5723, Elapsed: 5m43s
Starting testing the validation set with 200 subgraphs!
2020-04-28 17:52:58.062990: Epoch: 1, Batch: 892, Loss: 0.6342, Elapsed: 9m15s
2020-04-28 17:57:16.572630: Epoch: 1, Batch: 886, Loss: 0.6125, Elapsed: 7m47s
2020-04-28 17:58:58.194075: Epoch: 1, Batch: 893, Loss: 0.6156, Elapsed: 6m0s
2020-04-28 18:03:26.896283: Epoch: 1, Batch: 887, Loss: 0.5585, Elapsed: 6m10s
2020-04-28 18:07:24.898317: Epoch: 1, Batch: 894, Loss: 0.6297, Elapsed: 8m26s
2020-04-28 18:08:34.181970: Epoch: 1, Batch: 888, Loss: 0.5272, Elapsed: 5m7s
2020-04-28 18:14:52.660252: Epoch: 1, Batch: 895, Loss: 0.6409, Elapsed: 7m27s
2020-04-28 18:15:10.971971: Epoch: 1, Batch: 889, Loss: 0.6005, Elapsed: 6m36s
2020-04-28 18:19:22.449622: Epoch: 1, Batch: 896, Loss: 0.6229, Elapsed: 4m29s
2020-04-28 18:22:45.307593: Epoch: 1, Batch: 890, Loss: 0.5704, Elapsed: 7m34s
2020-04-28 18:23:35.406310: Epoch: 1, Batch: 897, Loss: 0.6071, Elapsed: 4m12s
2020-04-28 18:26:40.719290: Epoch: 1, Batch: 891, Loss: 0.5803, Elapsed: 3m55s
2020-04-28 18:31:51.981032: Epoch: 1, Batch: 898, Loss: 0.6562, Elapsed: 8m16s
2020-04-28 18:32:04.008207: Epoch: 1, Batch: 892, Loss: 0.5724, Elapsed: 5m23s
2020-04-28 18:32:30.026436: Validation Test:  Loss: 0.5857,  Acc: 70.0180, AUC: 0.7536, Precision: 0.7921 -- Elapsed: 41m29s
2020-04-28 18:38:37.785674: Epoch: 1, Batch: 899, Loss: 0.6300, Elapsed: 6m45s
2020-04-28 18:40:50.744691: Epoch: 1, Batch: 901, Loss: 0.5822, Elapsed: 8m20s
2020-04-28 18:40:49.357576: Epoch: 1, Batch: 893, Loss: 0.5865, Elapsed: 8m45s
2020-04-28 18:43:42.497300: Epoch: 1, Batch: 900, Loss: 0.6171, Elapsed: 5m4s
Starting testing the validation set with 200 subgraphs!
2020-04-28 18:48:25.152656: Epoch: 1, Batch: 894, Loss: 0.5742, Elapsed: 7m35s
2020-04-28 18:49:17.798830: Epoch: 1, Batch: 902, Loss: 0.6130, Elapsed: 8m26s
2020-04-28 18:54:34.399389: Epoch: 1, Batch: 903, Loss: 0.5682, Elapsed: 5m16s
2020-04-28 18:55:14.491712: Epoch: 1, Batch: 895, Loss: 0.5799, Elapsed: 6m49s
2020-04-28 18:59:58.345486: Epoch: 1, Batch: 904, Loss: 0.5825, Elapsed: 5m23s
2020-04-28 19:01:52.818929: Epoch: 1, Batch: 896, Loss: 0.5767, Elapsed: 6m38s
2020-04-28 19:05:35.218704: Epoch: 1, Batch: 905, Loss: 0.6039, Elapsed: 5m36s
2020-04-28 19:09:06.906833: Epoch: 1, Batch: 906, Loss: 0.5586, Elapsed: 3m31s
2020-04-28 19:09:59.038961: Epoch: 1, Batch: 897, Loss: 0.5488, Elapsed: 8m6s
2020-04-28 19:13:37.247899: Epoch: 1, Batch: 898, Loss: 0.5144, Elapsed: 3m38s
2020-04-28 19:13:38.645020: Epoch: 1, Batch: 907, Loss: 0.6028, Elapsed: 4m31s
2020-04-28 19:17:55.864451: Epoch: 1, Batch: 899, Loss: 0.5525, Elapsed: 4m18s
2020-04-28 19:18:02.573293: Epoch: 1, Batch: 908, Loss: 0.6495, Elapsed: 4m23s
2020-04-28 19:23:13.348876: Epoch: 1, Batch: 900, Loss: 0.5474, Elapsed: 5m17s
Starting testing the validation set with 200 subgraphs!
2020-04-28 19:25:04.615147: Validation Test:  Loss: 0.6336,  Acc: 63.6241, AUC: 0.6889, Precision: 0.7336 -- Elapsed: 41m22s
2020-04-28 19:25:26.745560: Epoch: 1, Batch: 909, Loss: 0.6089, Elapsed: 7m24s
2020-04-28 19:31:24.585584: Epoch: 1, Batch: 901, Loss: 0.6527, Elapsed: 6m19s
2020-04-28 19:33:02.290875: Epoch: 1, Batch: 910, Loss: 0.5792, Elapsed: 7m35s
2020-04-28 19:36:10.789071: Epoch: 1, Batch: 902, Loss: 0.6051, Elapsed: 4m46s
2020-04-28 19:38:55.190397: Epoch: 1, Batch: 911, Loss: 0.5891, Elapsed: 5m52s
2020-04-28 19:43:41.900164: Epoch: 1, Batch: 912, Loss: 0.5332, Elapsed: 4m46s
2020-04-28 19:45:00.766344: Epoch: 1, Batch: 903, Loss: 0.6829, Elapsed: 8m49s
2020-04-28 19:51:52.268912: Epoch: 1, Batch: 913, Loss: 0.6025, Elapsed: 8m10s
2020-04-28 19:52:52.199117: Epoch: 1, Batch: 904, Loss: 0.6563, Elapsed: 7m51s
2020-04-28 19:58:16.152673: Epoch: 1, Batch: 914, Loss: 0.5643, Elapsed: 6m23s
2020-04-28 19:59:46.140790: Epoch: 1, Batch: 905, Loss: 0.6060, Elapsed: 6m53s
2020-04-28 20:03:51.975002: Epoch: 1, Batch: 915, Loss: 0.5603, Elapsed: 5m35s
2020-04-28 20:05:30.963944: Validation Test:  Loss: 0.5772,  Acc: 70.7362, AUC: 0.7651, Precision: 0.7997 -- Elapsed: 42m17s
2020-04-28 20:06:00.749845: Epoch: 1, Batch: 906, Loss: 0.6201, Elapsed: 6m14s
2020-04-28 20:10:06.541636: Epoch: 1, Batch: 901, Loss: 0.5700, Elapsed: 4m35s
2020-04-28 20:11:01.500771: Epoch: 1, Batch: 907, Loss: 0.5907, Elapsed: 5m0s
2020-04-28 20:13:03.130320: Epoch: 1, Batch: 916, Loss: 0.5779, Elapsed: 9m11s
2020-04-28 20:15:47.425138: Epoch: 1, Batch: 908, Loss: 0.6043, Elapsed: 4m45s
2020-04-28 20:15:58.549060: Epoch: 1, Batch: 902, Loss: 0.5547, Elapsed: 5m51s
2020-04-28 20:17:42.946804: Epoch: 1, Batch: 917, Loss: 0.5690, Elapsed: 4m39s
2020-04-28 20:19:44.862308: Epoch: 1, Batch: 909, Loss: 0.6371, Elapsed: 3m57s
2020-04-28 20:22:19.319707: Epoch: 1, Batch: 918, Loss: 0.5542, Elapsed: 4m36s
2020-04-28 20:22:14.289828: Epoch: 1, Batch: 903, Loss: 0.5755, Elapsed: 6m15s
2020-04-28 20:24:18.831147: Epoch: 1, Batch: 910, Loss: 0.5918, Elapsed: 4m33s
2020-04-28 20:27:58.318266: Epoch: 1, Batch: 904, Loss: 0.5276, Elapsed: 5m44s
2020-04-28 20:30:55.505836: Epoch: 1, Batch: 919, Loss: 0.5939, Elapsed: 8m36s
2020-04-28 20:33:05.592518: Epoch: 1, Batch: 905, Loss: 0.5486, Elapsed: 5m7s
2020-04-28 20:34:01.342312: Epoch: 1, Batch: 911, Loss: 0.6047, Elapsed: 9m42s
2020-04-28 20:37:13.243080: Epoch: 1, Batch: 920, Loss: 0.6302, Elapsed: 6m17s
2020-04-28 20:37:56.348741: Epoch: 1, Batch: 906, Loss: 0.5765, Elapsed: 4m50s
2020-04-28 20:39:45.381920: Epoch: 1, Batch: 912, Loss: 0.6123, Elapsed: 5m44s
2020-04-28 20:42:05.060672: Epoch: 1, Batch: 907, Loss: 0.5560, Elapsed: 4m8s
2020-04-28 20:42:22.390017: Epoch: 1, Batch: 921, Loss: 0.5865, Elapsed: 5m9s
2020-04-28 20:45:22.302129: Epoch: 1, Batch: 913, Loss: 0.6492, Elapsed: 5m36s
2020-04-28 20:48:43.643983: Epoch: 1, Batch: 922, Loss: 0.5541, Elapsed: 6m21s
2020-04-28 20:49:36.326352: Epoch: 1, Batch: 908, Loss: 0.6068, Elapsed: 7m31s
2020-04-28 20:54:59.576488: Epoch: 1, Batch: 914, Loss: 0.6653, Elapsed: 9m37s
2020-04-28 20:55:00.360962: Epoch: 1, Batch: 909, Loss: 0.5503, Elapsed: 5m23s
2020-04-28 20:57:54.511585: Epoch: 1, Batch: 923, Loss: 0.6431, Elapsed: 9m10s
2020-04-28 21:01:14.080562: Epoch: 1, Batch: 915, Loss: 0.6205, Elapsed: 6m14s
2020-04-28 21:03:32.033199: Epoch: 1, Batch: 910, Loss: 0.6054, Elapsed: 8m31s
2020-04-28 21:03:47.772346: Epoch: 1, Batch: 924, Loss: 0.5746, Elapsed: 5m53s
2020-04-28 21:07:04.619772: Epoch: 1, Batch: 911, Loss: 0.5039, Elapsed: 3m32s
2020-04-28 21:08:10.139890: Epoch: 1, Batch: 916, Loss: 0.6188, Elapsed: 6m56s
2020-04-28 21:09:00.327560: Epoch: 1, Batch: 925, Loss: 0.5445, Elapsed: 5m12s
2020-04-28 21:15:15.039347: Epoch: 1, Batch: 917, Loss: 0.6526, Elapsed: 7m4s
2020-04-28 21:15:38.472890: Epoch: 1, Batch: 912, Loss: 0.6083, Elapsed: 8m33s
2020-04-28 21:16:59.367530: Epoch: 1, Batch: 926, Loss: 0.5632, Elapsed: 7m59s
2020-04-28 21:20:45.482013: Epoch: 1, Batch: 918, Loss: 0.6434, Elapsed: 5m30s
2020-04-28 21:20:55.218379: Epoch: 1, Batch: 927, Loss: 0.5112, Elapsed: 3m55s
2020-04-28 21:21:16.172145: Epoch: 1, Batch: 913, Loss: 0.5601, Elapsed: 5m37s
2020-04-28 21:24:52.694751: Epoch: 1, Batch: 928, Loss: 0.4955, Elapsed: 3m57s
2020-04-28 21:25:37.964643: Epoch: 1, Batch: 919, Loss: 0.5793, Elapsed: 4m52s
2020-04-28 21:26:13.743327: Epoch: 1, Batch: 914, Loss: 0.5342, Elapsed: 4m57s
2020-04-28 21:30:12.694501: Epoch: 1, Batch: 929, Loss: 0.5561, Elapsed: 5m19s
2020-04-28 21:31:33.575537: Epoch: 1, Batch: 920, Loss: 0.6288, Elapsed: 5m55s
2020-04-28 21:32:15.479872: Epoch: 1, Batch: 915, Loss: 0.5677, Elapsed: 6m1s
2020-04-28 21:37:53.781944: Epoch: 1, Batch: 916, Loss: 0.5431, Elapsed: 5m38s
2020-04-28 21:38:07.200768: Epoch: 1, Batch: 930, Loss: 0.5961, Elapsed: 7m54s
2020-04-28 21:40:31.896055: Epoch: 1, Batch: 921, Loss: 0.6387, Elapsed: 8m58s
2020-04-28 21:44:45.138278: Epoch: 1, Batch: 931, Loss: 0.5672, Elapsed: 6m37s
2020-04-28 21:50:52.166545: Epoch: 1, Batch: 922, Loss: 0.6361, Elapsed: 10m20s
2020-04-28 21:50:57.591342: Epoch: 1, Batch: 917, Loss: 0.6136, Elapsed: 13m3s
2020-04-28 21:54:01.903453: Epoch: 1, Batch: 932, Loss: 0.5424, Elapsed: 9m16s
2020-04-28 21:56:27.842012: Epoch: 1, Batch: 923, Loss: 0.6084, Elapsed: 5m35s
2020-04-28 21:57:27.795884: Epoch: 1, Batch: 918, Loss: 0.5460, Elapsed: 6m30s
2020-04-28 22:01:35.198495: Epoch: 1, Batch: 933, Loss: 0.6034, Elapsed: 7m33s
2020-04-28 22:02:41.991102: Epoch: 1, Batch: 924, Loss: 0.6288, Elapsed: 6m14s
2020-04-28 22:04:06.058730: Epoch: 1, Batch: 919, Loss: 0.5802, Elapsed: 6m38s
2020-04-28 22:07:54.669422: Epoch: 1, Batch: 934, Loss: 0.5637, Elapsed: 6m19s
2020-04-28 22:09:34.670584: Epoch: 1, Batch: 925, Loss: 0.5958, Elapsed: 6m52s
2020-04-28 22:11:29.528683: Epoch: 1, Batch: 920, Loss: 0.5593, Elapsed: 7m23s
2020-04-28 22:16:55.077103: Epoch: 1, Batch: 935, Loss: 0.5738, Elapsed: 9m0s
2020-04-28 22:17:06.435682: Epoch: 1, Batch: 926, Loss: 0.6674, Elapsed: 7m31s
2020-04-28 22:17:21.521231: Epoch: 1, Batch: 921, Loss: 0.5594, Elapsed: 5m51s
2020-04-28 22:23:22.543367: Epoch: 1, Batch: 927, Loss: 0.6589, Elapsed: 6m16s
2020-04-28 22:26:17.020603: Epoch: 1, Batch: 922, Loss: 0.5710, Elapsed: 8m55s
2020-04-28 22:27:21.412683: Epoch: 1, Batch: 936, Loss: 0.5935, Elapsed: 10m26s
2020-04-28 22:30:18.020902: Epoch: 1, Batch: 928, Loss: 0.6387, Elapsed: 6m55s
2020-04-28 22:31:05.838218: Epoch: 1, Batch: 937, Loss: 0.4643, Elapsed: 3m44s
2020-04-28 22:33:48.392342: Epoch: 1, Batch: 923, Loss: 0.6559, Elapsed: 7m31s
2020-04-28 22:34:11.368807: Epoch: 1, Batch: 938, Loss: 0.4589, Elapsed: 3m5s
2020-04-28 22:36:21.087830: Epoch: 1, Batch: 929, Loss: 0.6132, Elapsed: 6m3s
2020-04-28 22:39:40.972711: Epoch: 1, Batch: 939, Loss: 0.5757, Elapsed: 5m29s
2020-04-28 22:40:22.722298: Epoch: 1, Batch: 930, Loss: 0.5756, Elapsed: 4m1s
2020-04-28 22:44:05.886945: Epoch: 1, Batch: 924, Loss: 0.6926, Elapsed: 10m17s
2020-04-28 22:44:44.350216: Epoch: 1, Batch: 940, Loss: 0.5945, Elapsed: 5m3s
2020-04-28 22:48:45.027593: Epoch: 1, Batch: 931, Loss: 0.6416, Elapsed: 8m22s
2020-04-28 22:49:18.231256: Epoch: 1, Batch: 925, Loss: 0.5338, Elapsed: 5m12s
2020-04-28 22:52:31.656494: Epoch: 1, Batch: 941, Loss: 0.6312, Elapsed: 7m47s
2020-04-28 22:53:06.272925: Epoch: 1, Batch: 926, Loss: 0.5221, Elapsed: 3m48s
2020-04-28 22:55:17.376079: Epoch: 1, Batch: 932, Loss: 0.6242, Elapsed: 6m32s
2020-04-28 22:57:08.395169: Epoch: 1, Batch: 927, Loss: 0.5112, Elapsed: 4m2s
2020-04-28 23:01:31.867923: Epoch: 1, Batch: 933, Loss: 0.6254, Elapsed: 6m14s
2020-04-28 23:03:39.135540: Epoch: 1, Batch: 928, Loss: 0.6048, Elapsed: 6m30s
2020-04-28 23:06:20.606744: Epoch: 1, Batch: 934, Loss: 0.5970, Elapsed: 4m48s
2020-04-28 23:09:13.863340: Epoch: 1, Batch: 929, Loss: 0.5898, Elapsed: 5m34s
2020-04-28 23:16:18.978480: Epoch: 1, Batch: 935, Loss: 0.6265, Elapsed: 9m58s
2020-04-28 23:16:42.025371: Epoch: 1, Batch: 930, Loss: 0.5885, Elapsed: 7m28s
2020-04-28 23:20:09.497560: Epoch: 1, Batch: 942, Loss: 0.5949, Elapsed: 27m37s
2020-04-28 23:22:13.144299: Epoch: 1, Batch: 931, Loss: 0.5554, Elapsed: 5m31s
2020-04-28 23:23:08.031289: Epoch: 1, Batch: 936, Loss: 0.6399, Elapsed: 6m49s
2020-04-28 23:27:54.385964: Epoch: 1, Batch: 932, Loss: 0.5561, Elapsed: 5m41s
2020-04-28 23:28:54.879925: Epoch: 1, Batch: 943, Loss: 0.6463, Elapsed: 8m45s
2020-04-28 23:30:47.206572: Epoch: 1, Batch: 937, Loss: 0.6358, Elapsed: 7m39s
2020-04-28 23:35:40.154485: Epoch: 1, Batch: 933, Loss: 0.5565, Elapsed: 7m45s
2020-04-28 23:38:07.565409: Epoch: 1, Batch: 934, Loss: 0.5180, Elapsed: 2m27s
2020-04-28 23:39:56.177688: Epoch: 1, Batch: 944, Loss: 0.6148, Elapsed: 11m1s
2020-04-28 23:42:17.179936: Epoch: 1, Batch: 938, Loss: 0.6962, Elapsed: 11m29s
2020-04-28 23:44:47.707313: Epoch: 1, Batch: 945, Loss: 0.5419, Elapsed: 4m51s
2020-04-28 23:46:00.728037: Epoch: 1, Batch: 935, Loss: 0.5705, Elapsed: 7m53s
2020-04-28 23:48:22.927324: Epoch: 1, Batch: 939, Loss: 0.6893, Elapsed: 6m5s
2020-04-28 23:53:13.044536: Epoch: 1, Batch: 946, Loss: 0.5993, Elapsed: 8m25s
2020-04-28 23:54:04.699267: Epoch: 1, Batch: 936, Loss: 0.5731, Elapsed: 8m3s
2020-04-29 00:00:45.716097: Epoch: 1, Batch: 947, Loss: 0.6025, Elapsed: 7m32s
2020-04-29 00:00:46.834062: Epoch: 1, Batch: 937, Loss: 0.5551, Elapsed: 6m42s
2020-04-29 00:01:33.931845: Epoch: 1, Batch: 940, Loss: 0.6927, Elapsed: 13m10s
2020-04-29 00:06:57.320243: Epoch: 1, Batch: 938, Loss: 0.6506, Elapsed: 6m10s
2020-04-29 00:07:08.653652: Epoch: 1, Batch: 948, Loss: 0.5539, Elapsed: 6m22s
2020-04-29 00:11:16.680388: Epoch: 1, Batch: 941, Loss: 0.6545, Elapsed: 9m42s
2020-04-29 00:11:49.958180: Epoch: 1, Batch: 939, Loss: 0.5674, Elapsed: 4m52s
2020-04-29 00:14:38.536624: Epoch: 1, Batch: 949, Loss: 0.6292, Elapsed: 7m29s
2020-04-29 00:15:34.484440: Epoch: 1, Batch: 942, Loss: 0.6103, Elapsed: 4m17s
2020-04-29 00:19:18.576586: Epoch: 1, Batch: 940, Loss: 0.6320, Elapsed: 7m28s
2020-04-29 00:21:59.763480: Epoch: 1, Batch: 950, Loss: 0.6002, Elapsed: 7m21s
Starting testing the validation set with 200 subgraphs!
2020-04-29 00:22:31.845126: Epoch: 1, Batch: 943, Loss: 0.6311, Elapsed: 6m57s
2020-04-29 00:24:06.512014: Epoch: 1, Batch: 941, Loss: 0.6187, Elapsed: 4m47s
2020-04-29 00:27:54.279247: Epoch: 1, Batch: 944, Loss: 0.5960, Elapsed: 5m22s
2020-04-29 00:30:17.100154: Epoch: 1, Batch: 942, Loss: 0.5640, Elapsed: 6m10s
2020-04-29 00:33:49.593154: Epoch: 1, Batch: 945, Loss: 0.6567, Elapsed: 5m55s
2020-04-29 00:38:00.055329: Epoch: 1, Batch: 943, Loss: 0.5988, Elapsed: 7m42s
2020-04-29 00:41:57.808522: Epoch: 1, Batch: 946, Loss: 0.6409, Elapsed: 8m8s
2020-04-29 00:45:23.991065: Epoch: 1, Batch: 944, Loss: 0.5871, Elapsed: 7m23s
2020-04-29 00:47:47.372878: Epoch: 1, Batch: 947, Loss: 0.6132, Elapsed: 5m49s
2020-04-29 00:51:30.121585: Epoch: 1, Batch: 945, Loss: 0.5403, Elapsed: 6m6s
2020-04-29 00:53:38.820531: Epoch: 1, Batch: 948, Loss: 0.6149, Elapsed: 5m51s
2020-04-29 00:56:23.432572: Epoch: 1, Batch: 949, Loss: 0.6101, Elapsed: 2m44s
2020-04-29 01:00:54.188744: Epoch: 1, Batch: 950, Loss: 0.6025, Elapsed: 4m30s
Starting testing the validation set with 200 subgraphs!
2020-04-29 01:01:19.956003: Epoch: 1, Batch: 946, Loss: 0.5994, Elapsed: 9m49s
2020-04-29 01:03:12.923344: Validation Test:  Loss: 0.5929,  Acc: 68.6850, AUC: 0.7474, Precision: 0.8005 -- Elapsed: 41m13s
2020-04-29 01:05:34.771401: Epoch: 1, Batch: 947, Loss: 0.5655, Elapsed: 4m14s
2020-04-29 01:09:55.134315: Epoch: 1, Batch: 951, Loss: 0.5691, Elapsed: 6m42s
2020-04-29 01:12:30.943500: Epoch: 1, Batch: 948, Loss: 0.5718, Elapsed: 6m56s
2020-04-29 01:18:32.059954: Epoch: 1, Batch: 952, Loss: 0.6050, Elapsed: 8m36s
2020-04-29 01:18:29.388159: Epoch: 1, Batch: 949, Loss: 0.5386, Elapsed: 5m58s
2020-04-29 01:24:18.428271: Epoch: 1, Batch: 950, Loss: 0.5392, Elapsed: 5m49s
Starting testing the validation set with 200 subgraphs!
2020-04-29 01:25:57.346179: Epoch: 1, Batch: 953, Loss: 0.6210, Elapsed: 7m25s
2020-04-29 01:32:38.568148: Epoch: 1, Batch: 954, Loss: 0.6587, Elapsed: 6m41s
2020-04-29 01:37:54.324400: Epoch: 1, Batch: 955, Loss: 0.5783, Elapsed: 5m15s
2020-04-29 01:41:06.576058: Validation Test:  Loss: 0.6280,  Acc: 65.6397, AUC: 0.7031, Precision: 0.7468 -- Elapsed: 40m12s
2020-04-29 01:42:59.933341: Epoch: 1, Batch: 956, Loss: 0.5379, Elapsed: 5m5s
2020-04-29 01:47:54.041537: Epoch: 1, Batch: 951, Loss: 0.6184, Elapsed: 6m47s
2020-04-29 01:48:00.844927: Epoch: 1, Batch: 957, Loss: 0.5471, Elapsed: 5m0s
2020-04-29 01:54:16.144636: Epoch: 1, Batch: 958, Loss: 0.6036, Elapsed: 6m15s
2020-04-29 01:55:02.557380: Epoch: 1, Batch: 952, Loss: 0.6449, Elapsed: 7m8s
2020-04-29 01:59:51.662777: Epoch: 1, Batch: 953, Loss: 0.6038, Elapsed: 4m49s
2020-04-29 02:01:39.288867: Epoch: 1, Batch: 959, Loss: 0.5877, Elapsed: 7m23s
2020-04-29 02:05:07.834883: Validation Test:  Loss: 0.5848,  Acc: 69.5077, AUC: 0.7529, Precision: 0.7892 -- Elapsed: 40m49s
2020-04-29 02:06:57.289492: Epoch: 1, Batch: 960, Loss: 0.5899, Elapsed: 5m17s
2020-04-29 02:07:08.669982: Epoch: 1, Batch: 954, Loss: 0.6180, Elapsed: 7m16s
2020-04-29 02:13:03.912287: Epoch: 1, Batch: 955, Loss: 0.6224, Elapsed: 5m55s
2020-04-29 02:13:41.908812: Epoch: 1, Batch: 961, Loss: 0.5786, Elapsed: 6m44s
2020-04-29 02:14:52.963045: Epoch: 1, Batch: 951, Loss: 0.6475, Elapsed: 9m45s
2020-04-29 02:20:04.106618: Epoch: 1, Batch: 952, Loss: 0.5657, Elapsed: 5m11s
2020-04-29 02:20:11.196941: Epoch: 1, Batch: 956, Loss: 0.6111, Elapsed: 7m7s
2020-04-29 02:22:35.395130: Epoch: 1, Batch: 962, Loss: 0.6145, Elapsed: 8m53s
2020-04-29 02:26:08.160027: Epoch: 1, Batch: 957, Loss: 0.6315, Elapsed: 5m56s
2020-04-29 02:28:37.816341: Epoch: 1, Batch: 953, Loss: 0.5862, Elapsed: 8m33s
2020-04-29 02:30:30.208937: Epoch: 1, Batch: 963, Loss: 0.5889, Elapsed: 7m54s
2020-04-29 02:31:39.605116: Epoch: 1, Batch: 958, Loss: 0.6264, Elapsed: 5m31s
2020-04-29 02:35:49.981329: Epoch: 1, Batch: 964, Loss: 0.5662, Elapsed: 5m19s
2020-04-29 02:36:21.634387: Epoch: 1, Batch: 954, Loss: 0.5937, Elapsed: 7m43s
2020-04-29 02:38:54.774425: Epoch: 1, Batch: 959, Loss: 0.6406, Elapsed: 7m15s
2020-04-29 02:41:04.131725: Epoch: 1, Batch: 965, Loss: 0.6080, Elapsed: 5m14s
2020-04-29 02:41:23.167478: Epoch: 1, Batch: 955, Loss: 0.5527, Elapsed: 5m1s
2020-04-29 02:45:45.474568: Epoch: 1, Batch: 960, Loss: 0.6251, Elapsed: 6m50s
2020-04-29 02:48:31.467409: Epoch: 1, Batch: 956, Loss: 0.6173, Elapsed: 7m8s
2020-04-29 02:48:40.082371: Epoch: 1, Batch: 966, Loss: 0.6211, Elapsed: 7m35s
2020-04-29 02:51:57.526069: Epoch: 1, Batch: 961, Loss: 0.6309, Elapsed: 6m12s
2020-04-29 02:54:25.028784: Epoch: 1, Batch: 967, Loss: 0.5960, Elapsed: 5m44s
2020-04-29 02:55:38.719749: Epoch: 1, Batch: 957, Loss: 0.5738, Elapsed: 7m7s
2020-04-29 03:00:28.653418: Epoch: 1, Batch: 968, Loss: 0.6279, Elapsed: 6m3s
2020-04-29 03:02:11.311383: Epoch: 1, Batch: 962, Loss: 0.6638, Elapsed: 10m13s
2020-04-29 03:05:48.136651: Epoch: 1, Batch: 958, Loss: 0.6800, Elapsed: 10m9s
2020-04-29 03:07:06.818745: Epoch: 1, Batch: 969, Loss: 0.5967, Elapsed: 6m38s
2020-04-29 03:10:32.027902: Epoch: 1, Batch: 963, Loss: 0.6384, Elapsed: 8m20s
2020-04-29 03:10:58.633610: Epoch: 1, Batch: 959, Loss: 0.5281, Elapsed: 5m10s
2020-04-29 03:14:40.267443: Epoch: 1, Batch: 960, Loss: 0.5192, Elapsed: 3m41s
2020-04-29 03:15:50.810002: Epoch: 1, Batch: 970, Loss: 0.5655, Elapsed: 8m43s
2020-04-29 03:16:05.793976: Epoch: 1, Batch: 964, Loss: 0.6125, Elapsed: 5m33s
2020-04-29 03:21:54.218338: Epoch: 1, Batch: 961, Loss: 0.5673, Elapsed: 7m13s
2020-04-29 03:22:24.708753: Epoch: 1, Batch: 965, Loss: 0.6308, Elapsed: 6m18s
2020-04-29 03:22:22.905428: Epoch: 1, Batch: 971, Loss: 0.6010, Elapsed: 6m32s
2020-04-29 03:27:09.642027: Epoch: 1, Batch: 962, Loss: 0.5353, Elapsed: 5m15s
2020-04-29 03:28:07.944866: Epoch: 1, Batch: 966, Loss: 0.6169, Elapsed: 5m43s
2020-04-29 03:30:42.066304: Epoch: 1, Batch: 972, Loss: 0.5997, Elapsed: 8m19s
2020-04-29 03:33:42.152154: Epoch: 1, Batch: 967, Loss: 0.6132, Elapsed: 5m34s
2020-04-29 03:35:34.259887: Epoch: 1, Batch: 973, Loss: 0.5936, Elapsed: 4m52s
2020-04-29 03:36:12.379695: Epoch: 1, Batch: 963, Loss: 0.5567, Elapsed: 9m2s
2020-04-29 03:38:38.702062: Epoch: 1, Batch: 968, Loss: 0.5949, Elapsed: 4m56s
2020-04-29 03:42:29.447020: Epoch: 1, Batch: 974, Loss: 0.5850, Elapsed: 6m55s
2020-04-29 03:43:26.117553: Epoch: 1, Batch: 964, Loss: 0.5804, Elapsed: 7m13s
2020-04-29 03:46:18.305821: Epoch: 1, Batch: 969, Loss: 0.6291, Elapsed: 7m39s
2020-04-29 03:49:57.846697: Epoch: 1, Batch: 975, Loss: 0.6071, Elapsed: 7m28s
2020-04-29 03:51:29.232358: Epoch: 1, Batch: 965, Loss: 0.6688, Elapsed: 8m3s
2020-04-29 03:54:32.254297: Epoch: 1, Batch: 970, Loss: 0.6374, Elapsed: 8m13s
2020-04-29 03:55:19.906849: Epoch: 1, Batch: 966, Loss: 0.5213, Elapsed: 3m50s
2020-04-29 03:56:22.216796: Epoch: 1, Batch: 976, Loss: 0.5612, Elapsed: 6m24s
2020-04-29 04:00:14.030394: Epoch: 1, Batch: 967, Loss: 0.5511, Elapsed: 4m54s
2020-04-29 04:01:35.675742: Epoch: 1, Batch: 977, Loss: 0.5631, Elapsed: 5m13s
2020-04-29 04:01:53.299855: Epoch: 1, Batch: 971, Loss: 0.6262, Elapsed: 7m21s
2020-04-29 04:05:18.349289: Epoch: 1, Batch: 978, Loss: 0.4871, Elapsed: 3m42s
2020-04-29 04:07:42.773188: Epoch: 1, Batch: 968, Loss: 0.5874, Elapsed: 7m28s
2020-04-29 04:07:47.662650: Epoch: 1, Batch: 972, Loss: 0.5991, Elapsed: 5m54s
2020-04-29 04:13:27.073300: Epoch: 1, Batch: 969, Loss: 0.5732, Elapsed: 5m44s
2020-04-29 04:14:11.670906: Epoch: 1, Batch: 979, Loss: 0.6169, Elapsed: 8m53s
2020-04-29 04:14:35.686663: Epoch: 1, Batch: 973, Loss: 0.6334, Elapsed: 6m47s
2020-04-29 04:18:56.753533: Epoch: 1, Batch: 980, Loss: 0.5668, Elapsed: 4m45s
2020-04-29 04:19:21.686928: Epoch: 1, Batch: 974, Loss: 0.6197, Elapsed: 4m45s
2020-04-29 04:21:51.089819: Epoch: 1, Batch: 970, Loss: 0.6436, Elapsed: 8m23s
2020-04-29 04:24:15.455190: Epoch: 1, Batch: 981, Loss: 0.5291, Elapsed: 5m18s
2020-04-29 04:26:34.154986: Epoch: 1, Batch: 975, Loss: 0.6314, Elapsed: 7m12s
2020-04-29 04:29:53.918734: Epoch: 1, Batch: 971, Loss: 0.5845, Elapsed: 8m2s
2020-04-29 04:31:47.563732: Epoch: 1, Batch: 982, Loss: 0.5710, Elapsed: 7m32s
2020-04-29 04:32:44.042443: Epoch: 1, Batch: 976, Loss: 0.6504, Elapsed: 6m9s
2020-04-29 04:38:19.530740: Epoch: 1, Batch: 972, Loss: 0.5620, Elapsed: 8m25s
2020-04-29 04:40:16.914586: Epoch: 1, Batch: 983, Loss: 0.5947, Elapsed: 8m29s
2020-04-29 04:40:38.628301: Epoch: 1, Batch: 977, Loss: 0.6323, Elapsed: 7m54s
2020-04-29 04:42:48.225956: Epoch: 1, Batch: 973, Loss: 0.5341, Elapsed: 4m28s
2020-04-29 04:45:01.533780: Epoch: 1, Batch: 978, Loss: 0.6336, Elapsed: 4m22s
2020-04-29 04:45:14.083613: Epoch: 1, Batch: 984, Loss: 0.5638, Elapsed: 4m57s
2020-04-29 04:46:56.443499: Epoch: 1, Batch: 974, Loss: 0.5156, Elapsed: 4m8s
2020-04-29 04:51:03.274025: Epoch: 1, Batch: 985, Loss: 0.5518, Elapsed: 5m49s
2020-04-29 04:54:52.689630: Epoch: 1, Batch: 979, Loss: 0.6571, Elapsed: 9m51s
2020-04-29 04:54:57.275606: Epoch: 1, Batch: 975, Loss: 0.5917, Elapsed: 8m0s
2020-04-29 04:58:51.462751: Epoch: 1, Batch: 986, Loss: 0.6255, Elapsed: 7m48s
2020-04-29 04:59:41.680368: Epoch: 1, Batch: 976, Loss: 0.5867, Elapsed: 4m44s
2020-04-29 05:03:46.636106: Epoch: 1, Batch: 980, Loss: 0.6329, Elapsed: 8m53s
2020-04-29 05:07:04.200937: Epoch: 1, Batch: 987, Loss: 0.5921, Elapsed: 8m12s
2020-04-29 05:10:19.458690: Epoch: 1, Batch: 977, Loss: 0.6047, Elapsed: 10m37s
2020-04-29 05:10:58.604073: Epoch: 1, Batch: 981, Loss: 0.6257, Elapsed: 7m11s
2020-04-29 05:12:46.324201: Epoch: 1, Batch: 988, Loss: 0.5776, Elapsed: 5m42s
2020-04-29 05:16:17.009457: Epoch: 1, Batch: 978, Loss: 0.5818, Elapsed: 5m57s
2020-04-29 05:17:43.810015: Epoch: 1, Batch: 982, Loss: 0.5954, Elapsed: 6m45s
2020-04-29 05:17:46.887575: Epoch: 1, Batch: 989, Loss: 0.5606, Elapsed: 5m0s
2020-04-29 05:22:31.220441: Epoch: 1, Batch: 983, Loss: 0.5856, Elapsed: 4m47s
2020-04-29 05:22:38.676273: Epoch: 1, Batch: 979, Loss: 0.6041, Elapsed: 6m21s
2020-04-29 05:26:00.943336: Epoch: 1, Batch: 990, Loss: 0.5907, Elapsed: 8m14s
2020-04-29 05:27:51.560087: Epoch: 1, Batch: 984, Loss: 0.6047, Elapsed: 5m20s
2020-04-29 05:29:50.995838: Epoch: 1, Batch: 980, Loss: 0.5574, Elapsed: 7m12s
2020-04-29 05:31:05.262121: Epoch: 1, Batch: 991, Loss: 0.5372, Elapsed: 5m4s
2020-04-29 05:34:31.824399: Epoch: 1, Batch: 985, Loss: 0.6095, Elapsed: 6m40s
2020-04-29 05:35:46.848613: Epoch: 1, Batch: 981, Loss: 0.5574, Elapsed: 5m55s
2020-04-29 05:40:08.214689: Epoch: 1, Batch: 986, Loss: 0.5999, Elapsed: 5m36s
2020-04-29 05:40:38.243822: Epoch: 1, Batch: 992, Loss: 0.6128, Elapsed: 9m32s
2020-04-29 05:44:02.231289: Epoch: 1, Batch: 982, Loss: 0.5915, Elapsed: 8m15s
2020-04-29 05:45:23.549449: Epoch: 1, Batch: 993, Loss: 0.5723, Elapsed: 4m45s
2020-04-29 05:46:25.486825: Epoch: 1, Batch: 987, Loss: 0.5946, Elapsed: 6m17s
2020-04-29 05:48:49.111978: Epoch: 1, Batch: 983, Loss: 0.5124, Elapsed: 4m46s
2020-04-29 05:49:44.118359: Epoch: 1, Batch: 988, Loss: 0.6153, Elapsed: 3m18s
2020-04-29 05:52:28.405398: Epoch: 1, Batch: 994, Loss: 0.5756, Elapsed: 7m4s
2020-04-29 05:55:49.612724: Epoch: 1, Batch: 989, Loss: 0.5929, Elapsed: 6m5s
2020-04-29 05:56:36.016959: Epoch: 1, Batch: 984, Loss: 0.5649, Elapsed: 7m46s
2020-04-29 05:57:33.802505: Epoch: 1, Batch: 995, Loss: 0.5859, Elapsed: 5m5s
2020-04-29 06:00:48.579461: Epoch: 1, Batch: 996, Loss: 0.4727, Elapsed: 3m14s
2020-04-29 06:02:40.508607: Epoch: 1, Batch: 985, Loss: 0.5523, Elapsed: 6m4s
2020-04-29 06:03:55.317750: Epoch: 1, Batch: 990, Loss: 0.6545, Elapsed: 8m5s
2020-04-29 06:10:25.356769: Epoch: 1, Batch: 986, Loss: 0.5822, Elapsed: 7m44s
2020-04-29 06:11:21.208427: Epoch: 1, Batch: 991, Loss: 0.6408, Elapsed: 7m25s
2020-04-29 06:12:45.321559: Epoch: 1, Batch: 997, Loss: 0.6250, Elapsed: 11m56s
2020-04-29 06:17:21.797011: Epoch: 1, Batch: 992, Loss: 0.6402, Elapsed: 6m0s
2020-04-29 06:17:35.119192: Epoch: 1, Batch: 998, Loss: 0.5616, Elapsed: 4m49s
2020-04-29 06:18:59.721678: Epoch: 1, Batch: 987, Loss: 0.6023, Elapsed: 8m34s
2020-04-29 06:23:25.500929: Epoch: 1, Batch: 993, Loss: 0.6001, Elapsed: 6m3s
2020-04-29 06:24:11.390194: Epoch: 1, Batch: 999, Loss: 0.5774, Elapsed: 6m36s
2020-04-29 06:24:48.209141: Epoch: 1, Batch: 988, Loss: 0.5649, Elapsed: 5m48s
2020-04-29 06:30:01.105656: Epoch: 1, Batch: 989, Loss: 0.5995, Elapsed: 5m12s
2020-04-29 06:30:00.797548: Epoch: 1, Batch: 994, Loss: 0.6178, Elapsed: 6m35s
2020-04-29 06:31:16.792297: Epoch: 1, Batch: 1000, Loss: 0.6010, Elapsed: 7m5s
Starting testing the validation set with 200 subgraphs!
2020-04-29 06:35:23.576322: Epoch: 1, Batch: 995, Loss: 0.6137, Elapsed: 5m22s
2020-04-29 06:38:28.024656: Epoch: 1, Batch: 990, Loss: 0.6001, Elapsed: 8m26s
2020-04-29 06:40:18.939081: Epoch: 1, Batch: 996, Loss: 0.5866, Elapsed: 4m55s
2020-04-29 06:46:16.739604: Epoch: 1, Batch: 991, Loss: 0.6225, Elapsed: 7m48s
2020-04-29 06:47:50.273356: Epoch: 1, Batch: 997, Loss: 0.6228, Elapsed: 7m31s
2020-04-29 06:51:42.924553: Epoch: 1, Batch: 992, Loss: 0.5563, Elapsed: 5m26s
2020-04-29 06:53:45.201025: Epoch: 1, Batch: 998, Loss: 0.6169, Elapsed: 5m54s
2020-04-29 06:58:18.381478: Epoch: 1, Batch: 993, Loss: 0.6209, Elapsed: 6m35s
2020-04-29 06:59:33.021951: Epoch: 1, Batch: 999, Loss: 0.6069, Elapsed: 5m47s
2020-04-29 07:04:00.822309: Epoch: 1, Batch: 1000, Loss: 0.5838, Elapsed: 4m27s
Starting testing the validation set with 200 subgraphs!
2020-04-29 07:05:26.356122: Epoch: 1, Batch: 994, Loss: 0.5933, Elapsed: 7m7s
2020-04-29 07:10:24.585534: Epoch: 1, Batch: 995, Loss: 0.5338, Elapsed: 4m58s
2020-04-29 07:12:12.371735: Validation Test:  Loss: 0.5843,  Acc: 69.8090, AUC: 0.7535, Precision: 0.8024 -- Elapsed: 40m55s
2020-04-29 07:15:29.743684: Epoch: 1, Batch: 1001, Loss: 0.4563, Elapsed: 3m17s
2020-04-29 07:17:21.802319: Epoch: 1, Batch: 996, Loss: 0.5893, Elapsed: 6m57s
2020-04-29 07:19:46.245829: Epoch: 1, Batch: 1002, Loss: 0.5278, Elapsed: 4m16s
2020-04-29 07:21:06.791569: Epoch: 1, Batch: 997, Loss: 0.5612, Elapsed: 3m44s
2020-04-29 07:24:50.052905: Epoch: 1, Batch: 1003, Loss: 0.5375, Elapsed: 5m3s
2020-04-29 07:27:20.965204: Epoch: 1, Batch: 998, Loss: 0.5677, Elapsed: 6m14s
2020-04-29 07:31:03.640306: Epoch: 1, Batch: 999, Loss: 0.5699, Elapsed: 3m42s
2020-04-29 07:31:22.341166: Epoch: 1, Batch: 1004, Loss: 0.5710, Elapsed: 6m32s
2020-04-29 07:35:13.642860: Epoch: 1, Batch: 1000, Loss: 0.5402, Elapsed: 4m9s
Starting testing the validation set with 200 subgraphs!
2020-04-29 07:39:11.664944: Epoch: 1, Batch: 1005, Loss: 0.6063, Elapsed: 7m49s
2020-04-29 07:42:41.017966: Epoch: 1, Batch: 1006, Loss: 0.4972, Elapsed: 3m29s
2020-04-29 07:43:59.529668: Validation Test:  Loss: 0.6220,  Acc: 65.6986, AUC: 0.7055, Precision: 0.7443 -- Elapsed: 39m58s
2020-04-29 07:47:14.809962: Epoch: 1, Batch: 1007, Loss: 0.5492, Elapsed: 4m33s
2020-04-29 07:48:05.787631: Epoch: 1, Batch: 1001, Loss: 0.6529, Elapsed: 4m6s
2020-04-29 07:51:56.973015: Epoch: 1, Batch: 1008, Loss: 0.4727, Elapsed: 4m42s
2020-04-29 07:54:31.979966: Epoch: 1, Batch: 1002, Loss: 0.6281, Elapsed: 6m26s
2020-04-29 07:57:09.452655: Epoch: 1, Batch: 1009, Loss: 0.5387, Elapsed: 5m12s
2020-04-29 08:01:23.083149: Epoch: 1, Batch: 1003, Loss: 0.5995, Elapsed: 6m51s
2020-04-29 08:03:41.534135: Epoch: 1, Batch: 1010, Loss: 0.5375, Elapsed: 6m32s
2020-04-29 08:04:46.048672: Epoch: 1, Batch: 1004, Loss: 0.5804, Elapsed: 3m22s
2020-04-29 08:09:03.722978: Epoch: 1, Batch: 1011, Loss: 0.5795, Elapsed: 5m22s
2020-04-29 08:09:34.344746: Epoch: 1, Batch: 1005, Loss: 0.5640, Elapsed: 4m48s
2020-04-29 08:13:37.302260: Epoch: 1, Batch: 1012, Loss: 0.5366, Elapsed: 4m33s
2020-04-29 08:14:49.388461: Epoch: 1, Batch: 1006, Loss: 0.6075, Elapsed: 5m15s
2020-04-29 08:16:37.577597: Validation Test:  Loss: 0.5799,  Acc: 70.3071, AUC: 0.7612, Precision: 0.7985 -- Elapsed: 41m23s
2020-04-29 08:18:27.590042: Epoch: 1, Batch: 1007, Loss: 0.5455, Elapsed: 3m38s
2020-04-29 08:19:52.035490: Epoch: 1, Batch: 1013, Loss: 0.5973, Elapsed: 6m14s
2020-04-29 08:22:00.139813: Epoch: 1, Batch: 1001, Loss: 0.5936, Elapsed: 5m22s
2020-04-29 08:23:49.837510: Epoch: 1, Batch: 1008, Loss: 0.5990, Elapsed: 5m22s
2020-04-29 08:26:57.413898: Epoch: 1, Batch: 1002, Loss: 0.5549, Elapsed: 4m57s
2020-04-29 08:27:28.110257: Epoch: 1, Batch: 1014, Loss: 0.5604, Elapsed: 7m36s
2020-04-29 08:31:46.276100: Epoch: 1, Batch: 1009, Loss: 0.6894, Elapsed: 7m56s
2020-04-29 08:32:05.660003: Epoch: 1, Batch: 1003, Loss: 0.5399, Elapsed: 5m8s
2020-04-29 08:33:58.660647: Epoch: 1, Batch: 1015, Loss: 0.5914, Elapsed: 6m30s
2020-04-29 08:37:38.882666: Epoch: 1, Batch: 1004, Loss: 0.5389, Elapsed: 5m33s
2020-04-29 08:39:41.893783: Epoch: 1, Batch: 1010, Loss: 0.6183, Elapsed: 7m55s
2020-04-29 08:42:35.133098: Epoch: 1, Batch: 1016, Loss: 0.5962, Elapsed: 8m36s
2020-04-29 08:42:47.021785: Epoch: 1, Batch: 1005, Loss: 0.5605, Elapsed: 5m8s
2020-04-29 08:47:31.711702: Epoch: 1, Batch: 1006, Loss: 0.5534, Elapsed: 4m44s
2020-04-29 08:49:39.685057: Epoch: 1, Batch: 1017, Loss: 0.6060, Elapsed: 7m4s
2020-04-29 08:50:15.741191: Epoch: 1, Batch: 1011, Loss: 0.6369, Elapsed: 10m33s
2020-04-29 08:55:33.982108: Epoch: 1, Batch: 1007, Loss: 0.5602, Elapsed: 8m2s
2020-04-29 08:56:19.809220: Epoch: 1, Batch: 1018, Loss: 0.5436, Elapsed: 6m40s
2020-04-29 08:58:26.204927: Epoch: 1, Batch: 1012, Loss: 0.6195, Elapsed: 8m10s
2020-04-29 09:01:37.693749: Epoch: 1, Batch: 1008, Loss: 0.5546, Elapsed: 6m3s
2020-04-29 09:05:36.762011: Epoch: 1, Batch: 1019, Loss: 0.6228, Elapsed: 9m16s
2020-04-29 09:06:08.298613: Epoch: 1, Batch: 1009, Loss: 0.5102, Elapsed: 4m30s
2020-04-29 09:06:47.999912: Epoch: 1, Batch: 1013, Loss: 0.6545, Elapsed: 8m21s
2020-04-29 09:10:54.609085: Epoch: 1, Batch: 1010, Loss: 0.5343, Elapsed: 4m46s
2020-04-29 09:14:38.679701: Epoch: 1, Batch: 1014, Loss: 0.6397, Elapsed: 7m50s
2020-04-29 09:14:50.824832: Epoch: 1, Batch: 1020, Loss: 0.6264, Elapsed: 9m14s
2020-04-29 09:17:00.647232: Epoch: 1, Batch: 1011, Loss: 0.5385, Elapsed: 6m6s
2020-04-29 09:21:57.220654: Epoch: 1, Batch: 1015, Loss: 0.6231, Elapsed: 7m18s
2020-04-29 09:24:00.349283: Epoch: 1, Batch: 1021, Loss: 0.6241, Elapsed: 9m9s
2020-04-29 09:24:42.534998: Epoch: 1, Batch: 1012, Loss: 0.5807, Elapsed: 7m41s
2020-04-29 09:26:49.037702: Epoch: 1, Batch: 1016, Loss: 0.5968, Elapsed: 4m51s
2020-04-29 09:30:35.970394: Epoch: 1, Batch: 1017, Loss: 0.5266, Elapsed: 3m46s
2020-04-29 09:31:20.492856: Epoch: 1, Batch: 1022, Loss: 0.5748, Elapsed: 7m20s
2020-04-29 09:32:33.525182: Epoch: 1, Batch: 1013, Loss: 0.5593, Elapsed: 7m50s
2020-04-29 09:36:23.277072: Epoch: 1, Batch: 1018, Loss: 0.6113, Elapsed: 5m47s
2020-04-29 09:36:42.607106: Epoch: 1, Batch: 1023, Loss: 0.5965, Elapsed: 5m22s
2020-04-29 09:40:46.206533: Epoch: 1, Batch: 1014, Loss: 0.6095, Elapsed: 8m12s
2020-04-29 09:45:16.003896: Epoch: 1, Batch: 1024, Loss: 0.6083, Elapsed: 8m33s
2020-04-29 09:45:48.809639: Epoch: 1, Batch: 1019, Loss: 0.6258, Elapsed: 9m25s
2020-04-29 09:48:03.190680: Epoch: 1, Batch: 1015, Loss: 0.6225, Elapsed: 7m16s
2020-04-29 09:50:43.479836: Epoch: 1, Batch: 1020, Loss: 0.5952, Elapsed: 4m54s
2020-04-29 09:51:17.968410: Epoch: 1, Batch: 1025, Loss: 0.5747, Elapsed: 6m1s
2020-04-29 09:54:20.990944: Epoch: 1, Batch: 1016, Loss: 0.5791, Elapsed: 6m17s
2020-04-29 09:55:59.652689: Epoch: 1, Batch: 1021, Loss: 0.6116, Elapsed: 5m16s
2020-04-29 09:56:50.712459: Epoch: 1, Batch: 1026, Loss: 0.5734, Elapsed: 5m32s
2020-04-29 09:58:48.853045: Epoch: 1, Batch: 1017, Loss: 0.5341, Elapsed: 4m27s
2020-04-29 10:03:01.180090: Epoch: 1, Batch: 1018, Loss: 0.5233, Elapsed: 4m12s
2020-04-29 10:03:32.098391: Epoch: 1, Batch: 1027, Loss: 0.5556, Elapsed: 6m41s
2020-04-29 10:05:38.411636: Epoch: 1, Batch: 1022, Loss: 0.6401, Elapsed: 9m38s
2020-04-29 10:10:21.562160: Epoch: 1, Batch: 1023, Loss: 0.5911, Elapsed: 4m43s
2020-04-29 10:10:28.543096: Epoch: 1, Batch: 1019, Loss: 0.5962, Elapsed: 7m27s
2020-04-29 10:12:12.185990: Epoch: 1, Batch: 1028, Loss: 0.6344, Elapsed: 8m40s
2020-04-29 10:14:39.550534: Epoch: 1, Batch: 1024, Loss: 0.5838, Elapsed: 4m17s
2020-04-29 10:17:23.103469: Epoch: 1, Batch: 1029, Loss: 0.5324, Elapsed: 5m10s
2020-04-29 10:18:04.157235: Epoch: 1, Batch: 1020, Loss: 0.5935, Elapsed: 7m35s
2020-04-29 10:19:10.237923: Epoch: 1, Batch: 1025, Loss: 0.6046, Elapsed: 4m30s
2020-04-29 10:24:44.417912: Epoch: 1, Batch: 1030, Loss: 0.5614, Elapsed: 7m21s
2020-04-29 10:24:56.610324: Epoch: 1, Batch: 1026, Loss: 0.6161, Elapsed: 5m46s
2020-04-29 10:27:07.411985: Epoch: 1, Batch: 1021, Loss: 0.5553, Elapsed: 9m3s
2020-04-29 10:29:56.443439: Epoch: 1, Batch: 1031, Loss: 0.5171, Elapsed: 5m12s
2020-04-29 10:32:19.482361: Epoch: 1, Batch: 1027, Loss: 0.6227, Elapsed: 7m22s
2020-04-29 10:33:10.841584: Epoch: 1, Batch: 1022, Loss: 0.5915, Elapsed: 6m3s
2020-04-29 10:37:05.980619: Epoch: 1, Batch: 1028, Loss: 0.5950, Elapsed: 4m46s
2020-04-29 10:37:21.644606: Epoch: 1, Batch: 1023, Loss: 0.5411, Elapsed: 4m10s
2020-04-29 10:39:18.143368: Epoch: 1, Batch: 1032, Loss: 0.5838, Elapsed: 9m21s
2020-04-29 10:42:08.265539: Epoch: 1, Batch: 1029, Loss: 0.5976, Elapsed: 5m2s
2020-04-29 10:43:32.008206: Epoch: 1, Batch: 1033, Loss: 0.6004, Elapsed: 4m13s
2020-04-29 10:47:17.372450: Epoch: 1, Batch: 1030, Loss: 0.5949, Elapsed: 5m9s
2020-04-29 10:48:31.502554: Epoch: 1, Batch: 1024, Loss: 0.6117, Elapsed: 11m9s
2020-04-29 10:51:17.347869: Epoch: 1, Batch: 1034, Loss: 0.6206, Elapsed: 7m45s
2020-04-29 10:54:47.179900: Epoch: 1, Batch: 1025, Loss: 0.5726, Elapsed: 6m15s
2020-04-29 10:55:58.723638: Epoch: 1, Batch: 1031, Loss: 0.6180, Elapsed: 8m41s
2020-04-29 10:56:03.247149: Epoch: 1, Batch: 1035, Loss: 0.5481, Elapsed: 4m45s
2020-04-29 11:00:15.775399: Epoch: 1, Batch: 1026, Loss: 0.5357, Elapsed: 5m28s
2020-04-29 11:01:20.713415: Epoch: 1, Batch: 1032, Loss: 0.6119, Elapsed: 5m21s
2020-04-29 11:03:01.762665: Epoch: 1, Batch: 1036, Loss: 0.5734, Elapsed: 6m58s
2020-04-29 11:03:19.618002: Epoch: 1, Batch: 1027, Loss: 0.4769, Elapsed: 3m3s
2020-04-29 11:04:44.593502: Epoch: 1, Batch: 1033, Loss: 0.5350, Elapsed: 3m23s
2020-04-29 11:09:55.871836: Epoch: 1, Batch: 1037, Loss: 0.5815, Elapsed: 6m54s
2020-04-29 11:11:49.290932: Epoch: 1, Batch: 1034, Loss: 0.6149, Elapsed: 7m4s
2020-04-29 11:14:39.154491: Epoch: 1, Batch: 1028, Loss: 0.5967, Elapsed: 11m19s
2020-04-29 11:17:56.920530: Epoch: 1, Batch: 1038, Loss: 0.5861, Elapsed: 8m1s
2020-04-29 11:18:48.398332: Epoch: 1, Batch: 1035, Loss: 0.6174, Elapsed: 6m59s
2020-04-29 11:21:05.263841: Epoch: 1, Batch: 1029, Loss: 0.5626, Elapsed: 6m26s
2020-04-29 11:22:13.515400: Epoch: 1, Batch: 1039, Loss: 0.5213, Elapsed: 4m16s
2020-04-29 11:23:52.782235: Epoch: 1, Batch: 1036, Loss: 0.5969, Elapsed: 5m4s
2020-04-29 11:26:55.340940: Epoch: 1, Batch: 1037, Loss: 0.5424, Elapsed: 3m2s
2020-04-29 11:29:16.574806: Epoch: 1, Batch: 1030, Loss: 0.5802, Elapsed: 8m11s
2020-04-29 11:31:05.109755: Epoch: 1, Batch: 1040, Loss: 0.5741, Elapsed: 8m51s
2020-04-29 11:31:42.042128: Epoch: 1, Batch: 1038, Loss: 0.5740, Elapsed: 4m46s
2020-04-29 11:33:51.970555: Epoch: 1, Batch: 1031, Loss: 0.5266, Elapsed: 4m35s
2020-04-29 11:36:22.872087: Epoch: 1, Batch: 1041, Loss: 0.5644, Elapsed: 5m17s
2020-04-29 11:37:44.776585: Epoch: 1, Batch: 1039, Loss: 0.5957, Elapsed: 6m2s
2020-04-29 11:39:53.988358: Epoch: 1, Batch: 1042, Loss: 0.4955, Elapsed: 3m31s
2020-04-29 11:43:28.989985: Epoch: 1, Batch: 1040, Loss: 0.5934, Elapsed: 5m44s
2020-04-29 11:44:16.100725: Epoch: 1, Batch: 1032, Loss: 0.5862, Elapsed: 10m24s
2020-04-29 11:47:10.028206: Epoch: 1, Batch: 1043, Loss: 0.5960, Elapsed: 7m16s
2020-04-29 11:47:36.262126: Epoch: 1, Batch: 1033, Loss: 0.5123, Elapsed: 3m20s
2020-04-29 11:49:34.215537: Epoch: 1, Batch: 1041, Loss: 0.5826, Elapsed: 6m5s
2020-04-29 11:54:33.401880: Epoch: 1, Batch: 1034, Loss: 0.5392, Elapsed: 6m57s
2020-04-29 11:54:38.105919: Epoch: 1, Batch: 1044, Loss: 0.6392, Elapsed: 7m28s
2020-04-29 11:56:46.985285: Epoch: 1, Batch: 1042, Loss: 0.6366, Elapsed: 7m12s
2020-04-29 12:00:45.870475: Epoch: 1, Batch: 1043, Loss: 0.5204, Elapsed: 3m58s
2020-04-29 12:01:21.949944: Epoch: 1, Batch: 1035, Loss: 0.5865, Elapsed: 6m48s
2020-04-29 12:02:38.862565: Epoch: 1, Batch: 1045, Loss: 0.6227, Elapsed: 8m0s
2020-04-29 12:06:42.412893: Epoch: 1, Batch: 1044, Loss: 0.6070, Elapsed: 5m56s
2020-04-29 12:08:26.988423: Epoch: 1, Batch: 1036, Loss: 0.5657, Elapsed: 7m5s
2020-04-29 12:11:32.031177: Epoch: 1, Batch: 1046, Loss: 0.6147, Elapsed: 8m53s
2020-04-29 12:13:32.429731: Epoch: 1, Batch: 1045, Loss: 0.6152, Elapsed: 6m49s
2020-04-29 12:14:49.439847: Epoch: 1, Batch: 1037, Loss: 0.5896, Elapsed: 6m22s
2020-04-29 12:17:09.793307: Epoch: 1, Batch: 1047, Loss: 0.6030, Elapsed: 5m37s
2020-04-29 12:20:05.257709: Epoch: 1, Batch: 1046, Loss: 0.6249, Elapsed: 6m32s
2020-04-29 12:20:32.455299: Epoch: 1, Batch: 1038, Loss: 0.5687, Elapsed: 5m42s
2020-04-29 12:23:56.002033: Epoch: 1, Batch: 1039, Loss: 0.4998, Elapsed: 3m23s
2020-04-29 12:25:00.946033: Epoch: 1, Batch: 1048, Loss: 0.6151, Elapsed: 7m51s
2020-04-29 12:25:53.076625: Epoch: 1, Batch: 1047, Loss: 0.5884, Elapsed: 5m47s
2020-04-29 12:30:35.154558: Epoch: 1, Batch: 1040, Loss: 0.5677, Elapsed: 6m39s
2020-04-29 12:30:41.662290: Epoch: 1, Batch: 1049, Loss: 0.6218, Elapsed: 5m40s
2020-04-29 12:30:49.926539: Epoch: 1, Batch: 1048, Loss: 0.6241, Elapsed: 4m56s
2020-04-29 12:35:24.818344: Epoch: 1, Batch: 1050, Loss: 0.5554, Elapsed: 4m43s
Starting testing the validation set with 200 subgraphs!
2020-04-29 12:36:14.691966: Epoch: 1, Batch: 1041, Loss: 0.5340, Elapsed: 5m39s
2020-04-29 12:38:44.649706: Epoch: 1, Batch: 1049, Loss: 0.6273, Elapsed: 7m54s
2020-04-29 12:42:27.875841: Epoch: 1, Batch: 1042, Loss: 0.6008, Elapsed: 6m13s
2020-04-29 12:46:36.575327: Epoch: 1, Batch: 1050, Loss: 0.5960, Elapsed: 7m51s
Starting testing the validation set with 200 subgraphs!
2020-04-29 12:48:31.253579: Epoch: 1, Batch: 1043, Loss: 0.5915, Elapsed: 6m3s
2020-04-29 12:51:34.557586: Epoch: 1, Batch: 1044, Loss: 0.4912, Elapsed: 3m3s
2020-04-29 12:58:27.581036: Epoch: 1, Batch: 1045, Loss: 0.5796, Elapsed: 6m53s
2020-04-29 13:04:31.530713: Epoch: 1, Batch: 1046, Loss: 0.6112, Elapsed: 6m3s
2020-04-29 13:11:37.945678: Epoch: 1, Batch: 1047, Loss: 0.6079, Elapsed: 7m6s
2020-04-29 13:14:19.387386: Validation Test:  Loss: 0.5819,  Acc: 69.5767, AUC: 0.7568, Precision: 0.8054 -- Elapsed: 38m54s
2020-04-29 13:21:37.555927: Epoch: 1, Batch: 1051, Loss: 0.6056, Elapsed: 7m18s
2020-04-29 13:23:00.097846: Epoch: 1, Batch: 1048, Loss: 0.6584, Elapsed: 11m22s
2020-04-29 13:25:26.906406: Validation Test:  Loss: 0.6097,  Acc: 68.0622, AUC: 0.7257, Precision: 0.7582 -- Elapsed: 38m50s
2020-04-29 13:28:04.691357: Epoch: 1, Batch: 1049, Loss: 0.5303, Elapsed: 5m4s
2020-04-29 13:28:55.806720: Epoch: 1, Batch: 1052, Loss: 0.5835, Elapsed: 7m18s
2020-04-29 13:30:39.379562: Epoch: 1, Batch: 1051, Loss: 0.5945, Elapsed: 5m12s
2020-04-29 13:35:56.024716: Epoch: 1, Batch: 1050, Loss: 0.6008, Elapsed: 7m51s
Starting testing the validation set with 200 subgraphs!
2020-04-29 13:36:56.289654: Epoch: 1, Batch: 1053, Loss: 0.5743, Elapsed: 8m0s
2020-04-29 13:38:51.194857: Epoch: 1, Batch: 1052, Loss: 0.6292, Elapsed: 8m11s
2020-04-29 13:42:34.622208: Epoch: 1, Batch: 1054, Loss: 0.5471, Elapsed: 5m38s
2020-04-29 13:47:00.253553: Epoch: 1, Batch: 1055, Loss: 0.4936, Elapsed: 4m25s
2020-04-29 13:47:12.334905: Epoch: 1, Batch: 1053, Loss: 0.6115, Elapsed: 8m21s
2020-04-29 13:52:31.074803: Epoch: 1, Batch: 1056, Loss: 0.6277, Elapsed: 5m30s
2020-04-29 13:57:47.616164: Epoch: 1, Batch: 1057, Loss: 0.5633, Elapsed: 5m16s
2020-04-29 13:58:44.991524: Epoch: 1, Batch: 1054, Loss: 0.7887, Elapsed: 11m32s
2020-04-29 14:01:39.486135: Epoch: 1, Batch: 1058, Loss: 0.5536, Elapsed: 3m51s
2020-04-29 14:06:11.867126: Epoch: 1, Batch: 1055, Loss: 0.5914, Elapsed: 7m26s
2020-04-29 14:07:48.597226: Epoch: 1, Batch: 1059, Loss: 0.5524, Elapsed: 6m9s
2020-04-29 14:12:23.387505: Epoch: 1, Batch: 1060, Loss: 0.5423, Elapsed: 4m34s
2020-04-29 14:13:23.413446: Epoch: 1, Batch: 1056, Loss: 0.6050, Elapsed: 7m11s
2020-04-29 14:18:22.948808: Validation Test:  Loss: 0.5819,  Acc: 70.3852, AUC: 0.7640, Precision: 0.7986 -- Elapsed: 42m26s
2020-04-29 14:18:26.059683: Epoch: 1, Batch: 1061, Loss: 0.5716, Elapsed: 6m2s
2020-04-29 14:21:29.163419: Epoch: 1, Batch: 1057, Loss: 0.6312, Elapsed: 8m5s
2020-04-29 14:24:29.626372: Epoch: 1, Batch: 1051, Loss: 0.5593, Elapsed: 6m6s
2020-04-29 14:24:44.128803: Epoch: 1, Batch: 1062, Loss: 0.5754, Elapsed: 6m18s
2020-04-29 14:28:44.047067: Epoch: 1, Batch: 1058, Loss: 0.6094, Elapsed: 7m14s
2020-04-29 14:30:35.512678: Epoch: 1, Batch: 1052, Loss: 0.5481, Elapsed: 6m5s
2020-04-29 14:32:54.623853: Epoch: 1, Batch: 1063, Loss: 0.5847, Elapsed: 8m10s
2020-04-29 14:35:17.031800: Epoch: 1, Batch: 1059, Loss: 0.5867, Elapsed: 6m32s
2020-04-29 14:35:56.042035: Epoch: 1, Batch: 1053, Loss: 0.5656, Elapsed: 5m20s
2020-04-29 14:39:07.779911: Epoch: 1, Batch: 1064, Loss: 0.5726, Elapsed: 6m13s
2020-04-29 14:43:00.202459: Epoch: 1, Batch: 1060, Loss: 0.6044, Elapsed: 7m43s
2020-04-29 14:43:59.742732: Epoch: 1, Batch: 1054, Loss: 0.5960, Elapsed: 8m3s
2020-04-29 14:44:21.028008: Epoch: 1, Batch: 1065, Loss: 0.5489, Elapsed: 5m13s
2020-04-29 14:48:03.068011: Epoch: 1, Batch: 1066, Loss: 0.5383, Elapsed: 3m42s
2020-04-29 14:48:26.531418: Epoch: 1, Batch: 1055, Loss: 0.5630, Elapsed: 4m26s
2020-04-29 14:50:09.941789: Epoch: 1, Batch: 1061, Loss: 0.6167, Elapsed: 7m9s
2020-04-29 14:53:38.609117: Epoch: 1, Batch: 1056, Loss: 0.5812, Elapsed: 5m12s
2020-04-29 14:54:24.833290: Epoch: 1, Batch: 1067, Loss: 0.5542, Elapsed: 6m21s
2020-04-29 14:55:04.202967: Epoch: 1, Batch: 1062, Loss: 0.6074, Elapsed: 4m54s
2020-04-29 14:59:06.905285: Epoch: 1, Batch: 1063, Loss: 0.5586, Elapsed: 4m2s
2020-04-29 15:01:04.582505: Epoch: 1, Batch: 1068, Loss: 0.5611, Elapsed: 6m39s
2020-04-29 15:01:28.082559: Epoch: 1, Batch: 1057, Loss: 0.5969, Elapsed: 7m49s
2020-04-29 15:03:06.983826: Epoch: 1, Batch: 1064, Loss: 0.5410, Elapsed: 4m0s
2020-04-29 15:05:28.562642: Epoch: 1, Batch: 1058, Loss: 0.5529, Elapsed: 4m0s
2020-04-29 15:06:02.293110: Epoch: 1, Batch: 1069, Loss: 0.5222, Elapsed: 4m57s
2020-04-29 15:09:51.764372: Epoch: 1, Batch: 1065, Loss: 0.6167, Elapsed: 6m44s
2020-04-29 15:11:44.602104: Epoch: 1, Batch: 1070, Loss: 0.5626, Elapsed: 5m42s
2020-04-29 15:12:13.610083: Epoch: 1, Batch: 1059, Loss: 0.5800, Elapsed: 6m45s
2020-04-29 15:15:46.761999: Epoch: 1, Batch: 1066, Loss: 0.5991, Elapsed: 5m54s
2020-04-29 15:15:47.935297: Epoch: 1, Batch: 1060, Loss: 0.5125, Elapsed: 3m34s
2020-04-29 15:17:39.368414: Epoch: 1, Batch: 1071, Loss: 0.5527, Elapsed: 5m54s
2020-04-29 15:19:45.650471: Epoch: 1, Batch: 1061, Loss: 0.5147, Elapsed: 3m57s
2020-04-29 15:20:26.666510: Epoch: 1, Batch: 1067, Loss: 0.5856, Elapsed: 4m39s
2020-04-29 15:24:26.161014: Epoch: 1, Batch: 1072, Loss: 0.5414, Elapsed: 6m46s
2020-04-29 15:24:34.596309: Epoch: 1, Batch: 1062, Loss: 0.5825, Elapsed: 4m48s
2020-04-29 15:24:34.719722: Epoch: 1, Batch: 1068, Loss: 0.5570, Elapsed: 4m8s
2020-04-29 15:28:41.244628: Epoch: 1, Batch: 1069, Loss: 0.5750, Elapsed: 4m6s
2020-04-29 15:30:04.111801: Epoch: 1, Batch: 1073, Loss: 0.5446, Elapsed: 5m37s
2020-04-29 15:32:51.152606: Epoch: 1, Batch: 1063, Loss: 0.6081, Elapsed: 8m16s
2020-04-29 15:35:34.978331: Epoch: 1, Batch: 1074, Loss: 0.5347, Elapsed: 5m30s
2020-04-29 15:38:41.406630: Epoch: 1, Batch: 1064, Loss: 0.5634, Elapsed: 5m50s
2020-04-29 15:39:36.570506: Epoch: 1, Batch: 1070, Loss: 0.6344, Elapsed: 10m55s
2020-04-29 15:42:15.553891: Epoch: 1, Batch: 1065, Loss: 0.5102, Elapsed: 3m34s
2020-04-29 15:42:25.468253: Epoch: 1, Batch: 1075, Loss: 0.6282, Elapsed: 6m50s
2020-04-29 15:43:52.798608: Epoch: 1, Batch: 1071, Loss: 0.5641, Elapsed: 4m16s
2020-04-29 15:47:47.876921: Epoch: 1, Batch: 1066, Loss: 0.5603, Elapsed: 5m32s
2020-04-29 15:49:57.105314: Epoch: 1, Batch: 1076, Loss: 0.5820, Elapsed: 7m31s
2020-04-29 15:52:14.709771: Epoch: 1, Batch: 1072, Loss: 0.6420, Elapsed: 8m21s
2020-04-29 15:54:34.039776: Epoch: 1, Batch: 1077, Loss: 0.5348, Elapsed: 4m36s
2020-04-29 15:55:11.042920: Epoch: 1, Batch: 1067, Loss: 0.6336, Elapsed: 7m23s
2020-04-29 15:57:01.282835: Epoch: 1, Batch: 1073, Loss: 0.5969, Elapsed: 4m46s
2020-04-29 15:58:39.910460: Epoch: 1, Batch: 1078, Loss: 0.4990, Elapsed: 4m5s
2020-04-29 16:01:47.334205: Epoch: 1, Batch: 1068, Loss: 0.5564, Elapsed: 6m36s
2020-04-29 16:02:15.650701: Epoch: 1, Batch: 1079, Loss: 0.5028, Elapsed: 3m35s
2020-04-29 16:02:43.810406: Epoch: 1, Batch: 1074, Loss: 0.6211, Elapsed: 5m42s
2020-04-29 16:08:16.457940: Epoch: 1, Batch: 1080, Loss: 0.6201, Elapsed: 6m0s
2020-04-29 16:09:33.497108: Epoch: 1, Batch: 1075, Loss: 0.6110, Elapsed: 6m49s
2020-04-29 16:12:17.921550: Epoch: 1, Batch: 1069, Loss: 0.6388, Elapsed: 10m30s
2020-04-29 16:18:16.949868: Epoch: 1, Batch: 1081, Loss: 0.6284, Elapsed: 10m0s
2020-04-29 16:18:45.648612: Epoch: 1, Batch: 1070, Loss: 0.5841, Elapsed: 6m27s
2020-04-29 16:19:22.715545: Epoch: 1, Batch: 1076, Loss: 0.6282, Elapsed: 9m49s
2020-04-29 16:24:36.176857: Epoch: 1, Batch: 1082, Loss: 0.5553, Elapsed: 6m19s
2020-04-29 16:26:20.616943: Epoch: 1, Batch: 1077, Loss: 0.6171, Elapsed: 6m57s
2020-04-29 16:26:30.134567: Epoch: 1, Batch: 1071, Loss: 0.5704, Elapsed: 7m44s
2020-04-29 16:29:17.669830: Epoch: 1, Batch: 1078, Loss: 0.5101, Elapsed: 2m57s
2020-04-29 16:30:34.327270: Epoch: 1, Batch: 1083, Loss: 0.5740, Elapsed: 5m58s
2020-04-29 16:32:32.667528: Epoch: 1, Batch: 1072, Loss: 0.5493, Elapsed: 6m2s
2020-04-29 16:34:18.444967: Epoch: 1, Batch: 1079, Loss: 0.5493, Elapsed: 5m0s
2020-04-29 16:35:23.856682: Epoch: 1, Batch: 1084, Loss: 0.5258, Elapsed: 4m49s
2020-04-29 16:38:50.161738: Epoch: 1, Batch: 1073, Loss: 0.6153, Elapsed: 6m17s
2020-04-29 16:39:48.622755: Epoch: 1, Batch: 1080, Loss: 0.5840, Elapsed: 5m30s
2020-04-29 16:43:41.595132: Epoch: 1, Batch: 1085, Loss: 0.5873, Elapsed: 8m17s
2020-04-29 16:45:50.654827: Epoch: 1, Batch: 1074, Loss: 0.6106, Elapsed: 7m0s
2020-04-29 16:48:21.215617: Epoch: 1, Batch: 1086, Loss: 0.5801, Elapsed: 4m39s
2020-04-29 16:48:41.328152: Epoch: 1, Batch: 1081, Loss: 0.5921, Elapsed: 8m52s
2020-04-29 16:50:27.103414: Epoch: 1, Batch: 1075, Loss: 0.5386, Elapsed: 4m36s
2020-04-29 16:52:55.098746: Epoch: 1, Batch: 1082, Loss: 0.5473, Elapsed: 4m13s
2020-04-29 16:55:02.769437: Epoch: 1, Batch: 1087, Loss: 0.6018, Elapsed: 6m41s
2020-04-29 16:57:55.770561: Epoch: 1, Batch: 1076, Loss: 0.6021, Elapsed: 7m28s
2020-04-29 16:59:11.304736: Epoch: 1, Batch: 1083, Loss: 0.5781, Elapsed: 6m16s
2020-04-29 17:00:18.620778: Epoch: 1, Batch: 1088, Loss: 0.5834, Elapsed: 5m15s
2020-04-29 17:04:20.167548: Epoch: 1, Batch: 1084, Loss: 0.5905, Elapsed: 5m8s
2020-04-29 17:07:52.262171: Epoch: 1, Batch: 1089, Loss: 0.5838, Elapsed: 7m33s
2020-04-29 17:09:37.498651: Epoch: 1, Batch: 1077, Loss: 0.6064, Elapsed: 11m41s
2020-04-29 17:12:20.961302: Epoch: 1, Batch: 1085, Loss: 0.6081, Elapsed: 8m0s
2020-04-29 17:13:21.570317: Epoch: 1, Batch: 1078, Loss: 0.5491, Elapsed: 3m44s
2020-04-29 17:13:31.393779: Epoch: 1, Batch: 1090, Loss: 0.6087, Elapsed: 5m39s
2020-04-29 17:18:13.295138: Epoch: 1, Batch: 1079, Loss: 0.5882, Elapsed: 4m51s
2020-04-29 17:18:47.232786: Epoch: 1, Batch: 1091, Loss: 0.5572, Elapsed: 5m15s
2020-04-29 17:19:47.647312: Epoch: 1, Batch: 1086, Loss: 0.6203, Elapsed: 7m26s
2020-04-29 17:21:53.723385: Epoch: 1, Batch: 1092, Loss: 0.5105, Elapsed: 3m6s
2020-04-29 17:22:23.280013: Epoch: 1, Batch: 1080, Loss: 0.5396, Elapsed: 4m9s
2020-04-29 17:24:56.833419: Epoch: 1, Batch: 1087, Loss: 0.5653, Elapsed: 5m9s
2020-04-29 17:30:10.185408: Epoch: 1, Batch: 1081, Loss: 0.6271, Elapsed: 7m46s
2020-04-29 17:30:13.807549: Epoch: 1, Batch: 1093, Loss: 0.6006, Elapsed: 8m20s
2020-04-29 17:31:51.164606: Epoch: 1, Batch: 1088, Loss: 0.6119, Elapsed: 6m54s
2020-04-29 17:34:46.059328: Epoch: 1, Batch: 1094, Loss: 0.5398, Elapsed: 4m32s
2020-04-29 17:36:24.534128: Epoch: 1, Batch: 1082, Loss: 0.5542, Elapsed: 6m14s
2020-04-29 17:41:31.543126: Epoch: 1, Batch: 1089, Loss: 0.6545, Elapsed: 9m40s
2020-04-29 17:43:01.029652: Epoch: 1, Batch: 1083, Loss: 0.5686, Elapsed: 6m36s
2020-04-29 17:45:04.684438: Epoch: 1, Batch: 1095, Loss: 0.5606, Elapsed: 10m18s
2020-04-29 17:46:18.435387: Epoch: 1, Batch: 1090, Loss: 0.5574, Elapsed: 4m46s
2020-04-29 17:48:30.430402: Epoch: 1, Batch: 1084, Loss: 0.5822, Elapsed: 5m29s
2020-04-29 17:50:20.336376: Epoch: 1, Batch: 1096, Loss: 0.5548, Elapsed: 5m15s
2020-04-29 17:55:36.449713: Epoch: 1, Batch: 1085, Loss: 0.6016, Elapsed: 7m5s
2020-04-29 17:55:59.646573: Epoch: 1, Batch: 1091, Loss: 0.6002, Elapsed: 9m41s
2020-04-29 17:56:34.523446: Epoch: 1, Batch: 1097, Loss: 0.5528, Elapsed: 6m14s
2020-04-29 18:00:21.427460: Epoch: 1, Batch: 1086, Loss: 0.5679, Elapsed: 4m44s
2020-04-29 18:01:30.650093: Epoch: 1, Batch: 1092, Loss: 0.6132, Elapsed: 5m30s
2020-04-29 18:01:29.825303: Epoch: 1, Batch: 1098, Loss: 0.5799, Elapsed: 4m55s
2020-04-29 18:06:04.506770: Epoch: 1, Batch: 1099, Loss: 0.5283, Elapsed: 4m34s
2020-04-29 18:11:17.618673: Epoch: 1, Batch: 1087, Loss: 0.6041, Elapsed: 10m56s
2020-04-29 18:11:28.385160: Epoch: 1, Batch: 1100, Loss: 0.6001, Elapsed: 5m23s
Starting testing the validation set with 200 subgraphs!
2020-04-29 18:13:56.105726: Epoch: 1, Batch: 1093, Loss: 0.6470, Elapsed: 12m25s
2020-04-29 18:17:07.768471: Epoch: 1, Batch: 1088, Loss: 0.5426, Elapsed: 5m50s
2020-04-29 18:21:23.800540: Epoch: 1, Batch: 1089, Loss: 0.5496, Elapsed: 4m16s
2020-04-29 18:21:28.446552: Epoch: 1, Batch: 1094, Loss: 0.6297, Elapsed: 7m32s
2020-04-29 18:26:44.199216: Epoch: 1, Batch: 1095, Loss: 0.5684, Elapsed: 5m15s
2020-04-29 18:30:56.812575: Epoch: 1, Batch: 1090, Loss: 0.6113, Elapsed: 9m32s
2020-04-29 18:31:39.239387: Epoch: 1, Batch: 1096, Loss: 0.5824, Elapsed: 4m55s
2020-04-29 18:36:46.879477: Epoch: 1, Batch: 1091, Loss: 0.5685, Elapsed: 5m50s
2020-04-29 18:39:56.799601: Epoch: 1, Batch: 1097, Loss: 0.6141, Elapsed: 8m17s
2020-04-29 18:45:14.429558: Epoch: 1, Batch: 1092, Loss: 0.6035, Elapsed: 8m27s
2020-04-29 18:46:59.106355: Epoch: 1, Batch: 1098, Loss: 0.6359, Elapsed: 7m2s
2020-04-29 18:52:53.776654: Validation Test:  Loss: 0.5792,  Acc: 70.5231, AUC: 0.7595, Precision: 0.7990 -- Elapsed: 41m25s
2020-04-29 18:52:53.240850: Epoch: 1, Batch: 1093, Loss: 0.5812, Elapsed: 7m38s
2020-04-29 18:53:39.631411: Epoch: 1, Batch: 1099, Loss: 0.6077, Elapsed: 6m40s
2020-04-29 18:57:00.247010: Epoch: 1, Batch: 1100, Loss: 0.5877, Elapsed: 3m20s
Starting testing the validation set with 200 subgraphs!
2020-04-29 18:57:33.815266: Epoch: 1, Batch: 1101, Loss: 0.5686, Elapsed: 4m40s
2020-04-29 18:57:27.315291: Epoch: 1, Batch: 1094, Loss: 0.5722, Elapsed: 4m34s
2020-04-29 19:01:31.416586: Epoch: 1, Batch: 1095, Loss: 0.5648, Elapsed: 4m4s
2020-04-29 19:04:54.707882: Epoch: 1, Batch: 1096, Loss: 0.4745, Elapsed: 3m23s
2020-04-29 19:06:00.366574: Epoch: 1, Batch: 1102, Loss: 0.5897, Elapsed: 8m26s
2020-04-29 19:13:35.692220: Epoch: 1, Batch: 1097, Loss: 0.6010, Elapsed: 8m40s
2020-04-29 19:15:34.179555: Epoch: 1, Batch: 1103, Loss: 0.6130, Elapsed: 9m33s
2020-04-29 19:17:16.416935: Epoch: 1, Batch: 1098, Loss: 0.5053, Elapsed: 3m40s
2020-04-29 19:23:09.081473: Epoch: 1, Batch: 1104, Loss: 0.5722, Elapsed: 7m34s
2020-04-29 19:24:27.602585: Epoch: 1, Batch: 1099, Loss: 0.5819, Elapsed: 7m11s
2020-04-29 19:28:25.220233: Epoch: 1, Batch: 1100, Loss: 0.5403, Elapsed: 3m57s
Starting testing the validation set with 200 subgraphs!
2020-04-29 19:29:36.881693: Epoch: 1, Batch: 1105, Loss: 0.5882, Elapsed: 6m27s
2020-04-29 19:36:03.003374: Epoch: 1, Batch: 1106, Loss: 0.6236, Elapsed: 6m26s
2020-04-29 19:37:45.393616: Validation Test:  Loss: 0.6030,  Acc: 68.2934, AUC: 0.7357, Precision: 0.7655 -- Elapsed: 40m45s
2020-04-29 19:40:09.938908: Epoch: 1, Batch: 1107, Loss: 0.5075, Elapsed: 4m6s
2020-04-29 19:44:40.833256: Epoch: 1, Batch: 1101, Loss: 0.5862, Elapsed: 6m55s
2020-04-29 19:46:49.166641: Epoch: 1, Batch: 1108, Loss: 0.5803, Elapsed: 6m39s
2020-04-29 19:48:10.851828: Epoch: 1, Batch: 1102, Loss: 0.5209, Elapsed: 3m29s
2020-04-29 19:51:53.306641: Epoch: 1, Batch: 1103, Loss: 0.5456, Elapsed: 3m42s
2020-04-29 19:52:35.701829: Epoch: 1, Batch: 1109, Loss: 0.5656, Elapsed: 5m46s
2020-04-29 19:56:48.654637: Epoch: 1, Batch: 1104, Loss: 0.5771, Elapsed: 4m55s
2020-04-29 19:59:12.512130: Epoch: 1, Batch: 1110, Loss: 0.6073, Elapsed: 6m36s
2020-04-29 20:03:15.502567: Epoch: 1, Batch: 1105, Loss: 0.6110, Elapsed: 6m26s
2020-04-29 20:03:57.450325: Epoch: 1, Batch: 1111, Loss: 0.5829, Elapsed: 4m44s
2020-04-29 20:09:40.808620: Validation Test:  Loss: 0.5788,  Acc: 70.1417, AUC: 0.7598, Precision: 0.7977 -- Elapsed: 41m15s
2020-04-29 20:12:51.213503: Epoch: 1, Batch: 1112, Loss: 0.5633, Elapsed: 8m53s
2020-04-29 20:13:00.059560: Epoch: 1, Batch: 1106, Loss: 0.6539, Elapsed: 9m44s
2020-04-29 20:13:26.779787: Epoch: 1, Batch: 1101, Loss: 0.5758, Elapsed: 3m45s
2020-04-29 20:18:09.310337: Epoch: 1, Batch: 1102, Loss: 0.5237, Elapsed: 4m42s
2020-04-29 20:20:08.572181: Epoch: 1, Batch: 1107, Loss: 0.6096, Elapsed: 7m8s
2020-04-29 20:20:26.515745: Epoch: 1, Batch: 1113, Loss: 0.5840, Elapsed: 7m35s
2020-04-29 20:21:48.435674: Epoch: 1, Batch: 1103, Loss: 0.5589, Elapsed: 3m39s
2020-04-29 20:23:42.475881: Epoch: 1, Batch: 1108, Loss: 0.5272, Elapsed: 3m33s
2020-04-29 20:25:50.222597: Epoch: 1, Batch: 1104, Loss: 0.5410, Elapsed: 4m1s
2020-04-29 20:27:14.300268: Epoch: 1, Batch: 1114, Loss: 0.5599, Elapsed: 6m47s
2020-04-29 20:30:56.239998: Epoch: 1, Batch: 1105, Loss: 0.5813, Elapsed: 5m5s
2020-04-29 20:31:20.316913: Epoch: 1, Batch: 1109, Loss: 0.6165, Elapsed: 7m37s
2020-04-29 20:36:02.510018: Epoch: 1, Batch: 1115, Loss: 0.5759, Elapsed: 8m48s
2020-04-29 20:37:20.411657: Epoch: 1, Batch: 1110, Loss: 0.6222, Elapsed: 6m0s
2020-04-29 20:41:53.562492: Epoch: 1, Batch: 1106, Loss: 0.6115, Elapsed: 10m57s
2020-04-29 20:42:52.176937: Epoch: 1, Batch: 1111, Loss: 0.5900, Elapsed: 5m31s
2020-04-29 20:44:17.328917: Epoch: 1, Batch: 1116, Loss: 0.5840, Elapsed: 8m14s
2020-04-29 20:47:11.099166: Epoch: 1, Batch: 1107, Loss: 0.5569, Elapsed: 5m17s
2020-04-29 20:50:12.263712: Epoch: 1, Batch: 1117, Loss: 0.6124, Elapsed: 5m54s
2020-04-29 20:50:39.197126: Epoch: 1, Batch: 1112, Loss: 0.6154, Elapsed: 7m46s
2020-04-29 20:53:03.449166: Epoch: 1, Batch: 1108, Loss: 0.5883, Elapsed: 5m52s
2020-04-29 20:54:39.627147: Epoch: 1, Batch: 1113, Loss: 0.6060, Elapsed: 4m0s
2020-04-29 20:55:37.800828: Epoch: 1, Batch: 1118, Loss: 0.5645, Elapsed: 5m25s
2020-04-29 20:57:54.395302: Epoch: 1, Batch: 1109, Loss: 0.4721, Elapsed: 4m50s
2020-04-29 20:59:38.601520: Epoch: 1, Batch: 1119, Loss: 0.4871, Elapsed: 4m0s
2020-04-29 21:01:31.208218: Epoch: 1, Batch: 1110, Loss: 0.5516, Elapsed: 3m36s
2020-04-29 21:06:16.503037: Epoch: 1, Batch: 1120, Loss: 0.5847, Elapsed: 6m37s
2020-04-29 21:09:02.993350: Epoch: 1, Batch: 1111, Loss: 0.5919, Elapsed: 7m31s
2020-04-29 21:09:44.836093: Epoch: 1, Batch: 1114, Loss: 0.6049, Elapsed: 15m5s
2020-04-29 21:12:15.557840: Epoch: 1, Batch: 1121, Loss: 0.5301, Elapsed: 5m59s
2020-04-29 21:12:40.637308: Epoch: 1, Batch: 1112, Loss: 0.5069, Elapsed: 3m37s
2020-04-29 21:14:00.338092: Epoch: 1, Batch: 1115, Loss: 0.5773, Elapsed: 4m15s
2020-04-29 21:16:07.130605: Epoch: 1, Batch: 1113, Loss: 0.4595, Elapsed: 3m26s
2020-04-29 21:17:31.836796: Epoch: 1, Batch: 1122, Loss: 0.5521, Elapsed: 5m16s
2020-04-29 21:20:18.337056: Epoch: 1, Batch: 1116, Loss: 0.6377, Elapsed: 6m17s
2020-04-29 21:21:41.431300: Epoch: 1, Batch: 1123, Loss: 0.5433, Elapsed: 4m9s
2020-04-29 21:23:00.790878: Epoch: 1, Batch: 1114, Loss: 0.5752, Elapsed: 6m53s
2020-04-29 21:24:05.970109: Epoch: 1, Batch: 1117, Loss: 0.5794, Elapsed: 3m47s
2020-04-29 21:26:53.733046: Epoch: 1, Batch: 1124, Loss: 0.5474, Elapsed: 5m12s
2020-04-29 21:27:22.035166: Epoch: 1, Batch: 1115, Loss: 0.5815, Elapsed: 4m21s
2020-04-29 21:29:44.873324: Epoch: 1, Batch: 1118, Loss: 0.5810, Elapsed: 5m38s
2020-04-29 21:30:50.366981: Epoch: 1, Batch: 1125, Loss: 0.5145, Elapsed: 3m56s
2020-04-29 21:34:23.998739: Epoch: 1, Batch: 1116, Loss: 0.5894, Elapsed: 7m1s
2020-04-29 21:40:34.866919: Epoch: 1, Batch: 1126, Loss: 0.6330, Elapsed: 9m44s
2020-04-29 21:41:12.053241: Epoch: 1, Batch: 1119, Loss: 0.6268, Elapsed: 11m27s
2020-04-29 21:42:31.131038: Epoch: 1, Batch: 1117, Loss: 0.6112, Elapsed: 8m7s
2020-04-29 21:47:15.364058: Epoch: 1, Batch: 1127, Loss: 0.5862, Elapsed: 6m40s
2020-04-29 21:47:56.189846: Epoch: 1, Batch: 1120, Loss: 0.5941, Elapsed: 6m44s
2020-04-29 21:49:27.838723: Epoch: 1, Batch: 1118, Loss: 0.5970, Elapsed: 6m56s
2020-04-29 21:52:05.294755: Epoch: 1, Batch: 1128, Loss: 0.5464, Elapsed: 4m49s
2020-04-29 21:54:12.903939: Epoch: 1, Batch: 1121, Loss: 0.5847, Elapsed: 6m16s
2020-04-29 21:56:43.387990: Epoch: 1, Batch: 1119, Loss: 0.5771, Elapsed: 7m15s
2020-04-29 21:59:37.025329: Epoch: 1, Batch: 1122, Loss: 0.5706, Elapsed: 5m24s
2020-04-29 22:01:13.572827: Epoch: 1, Batch: 1129, Loss: 0.6106, Elapsed: 9m8s
2020-04-29 22:04:12.872474: Epoch: 1, Batch: 1123, Loss: 0.6171, Elapsed: 4m35s
2020-04-29 22:05:37.533995: Epoch: 1, Batch: 1120, Loss: 0.6310, Elapsed: 8m54s
2020-04-29 22:07:00.477298: Epoch: 1, Batch: 1130, Loss: 0.5728, Elapsed: 5m46s
2020-04-29 22:09:19.137283: Epoch: 1, Batch: 1124, Loss: 0.5762, Elapsed: 5m6s
2020-04-29 22:13:53.537883: Epoch: 1, Batch: 1121, Loss: 0.6200, Elapsed: 8m15s
2020-04-29 22:15:12.713631: Epoch: 1, Batch: 1131, Loss: 0.6106, Elapsed: 8m12s
2020-04-29 22:19:13.343579: Epoch: 1, Batch: 1125, Loss: 0.6041, Elapsed: 9m54s
2020-04-29 22:19:29.860271: Epoch: 1, Batch: 1122, Loss: 0.5689, Elapsed: 5m36s
2020-04-29 22:22:44.475029: Epoch: 1, Batch: 1132, Loss: 0.5861, Elapsed: 7m31s
2020-04-29 22:25:38.739087: Epoch: 1, Batch: 1123, Loss: 0.5994, Elapsed: 6m8s
2020-04-29 22:26:09.450926: Epoch: 1, Batch: 1126, Loss: 0.5923, Elapsed: 6m56s
2020-04-29 22:29:27.744877: Epoch: 1, Batch: 1133, Loss: 0.5773, Elapsed: 6m43s
2020-04-29 22:29:58.979189: Epoch: 1, Batch: 1127, Loss: 0.6035, Elapsed: 3m49s
2020-04-29 22:33:18.165348: Epoch: 1, Batch: 1124, Loss: 0.6210, Elapsed: 7m39s
2020-04-29 22:35:14.494437: Epoch: 1, Batch: 1128, Loss: 0.5769, Elapsed: 5m15s
2020-04-29 22:37:52.186810: Epoch: 1, Batch: 1134, Loss: 0.6179, Elapsed: 8m24s
2020-04-29 22:46:48.206101: Epoch: 1, Batch: 1135, Loss: 0.5837, Elapsed: 8m55s
2020-04-29 22:47:51.027344: Epoch: 1, Batch: 1125, Loss: 0.6090, Elapsed: 14m32s
2020-04-29 22:51:32.915519: Epoch: 1, Batch: 1129, Loss: 0.6439, Elapsed: 16m18s
2020-04-29 22:57:19.146511: Epoch: 1, Batch: 1126, Loss: 0.5635, Elapsed: 9m28s
2020-04-29 23:03:27.215671: Epoch: 1, Batch: 1130, Loss: 0.5899, Elapsed: 11m54s
2020-04-29 23:05:19.734083: Epoch: 1, Batch: 1136, Loss: 0.6194, Elapsed: 18m31s
2020-04-29 23:08:20.355972: Epoch: 1, Batch: 1131, Loss: 0.5258, Elapsed: 4m53s
2020-04-29 23:08:48.900456: Epoch: 1, Batch: 1127, Loss: 0.5867, Elapsed: 11m29s
2020-04-29 23:14:04.326681: Epoch: 1, Batch: 1137, Loss: 0.5709, Elapsed: 8m44s
2020-04-29 23:16:40.874001: Epoch: 1, Batch: 1128, Loss: 0.5022, Elapsed: 7m51s
2020-04-29 23:23:03.049682: Epoch: 1, Batch: 1132, Loss: 0.5833, Elapsed: 14m42s
2020-04-29 23:28:33.070649: Epoch: 1, Batch: 1138, Loss: 0.5572, Elapsed: 14m28s
2020-04-29 23:30:30.116258: Epoch: 1, Batch: 1129, Loss: 0.5759, Elapsed: 13m49s
2020-04-29 23:34:39.989912: Epoch: 1, Batch: 1133, Loss: 0.6032, Elapsed: 11m36s
2020-04-29 23:41:18.742553: Epoch: 1, Batch: 1130, Loss: 0.5537, Elapsed: 10m48s
2020-04-29 23:47:57.910000: Epoch: 1, Batch: 1139, Loss: 0.5778, Elapsed: 19m24s
2020-04-29 23:48:11.160554: Epoch: 1, Batch: 1134, Loss: 0.5816, Elapsed: 13m31s
2020-04-29 23:48:17.949894: Epoch: 1, Batch: 1131, Loss: 0.5136, Elapsed: 6m59s
2020-04-29 23:57:53.527802: Epoch: 1, Batch: 1132, Loss: 0.5255, Elapsed: 9m35s
2020-04-30 00:01:31.015352: Epoch: 1, Batch: 1140, Loss: 0.5821, Elapsed: 13m33s
2020-04-30 00:03:44.409087: Epoch: 1, Batch: 1135, Loss: 0.6005, Elapsed: 15m33s
2020-04-30 00:09:26.507637: Epoch: 1, Batch: 1141, Loss: 0.5418, Elapsed: 7m55s
2020-04-30 00:09:54.679959: Epoch: 1, Batch: 1133, Loss: 0.5642, Elapsed: 12m1s
2020-04-30 00:12:36.088923: Epoch: 1, Batch: 1136, Loss: 0.5768, Elapsed: 8m51s
2020-04-30 00:15:04.008473: Epoch: 1, Batch: 1142, Loss: 0.4912, Elapsed: 5m37s
2020-04-30 00:22:38.381746: Epoch: 1, Batch: 1137, Loss: 0.5725, Elapsed: 10m2s
2020-04-30 00:23:33.242199: Epoch: 1, Batch: 1143, Loss: 0.5401, Elapsed: 8m29s
2020-04-30 00:24:53.462531: Epoch: 1, Batch: 1134, Loss: 0.6116, Elapsed: 14m58s
2020-04-30 00:33:41.280904: Epoch: 1, Batch: 1138, Loss: 0.5893, Elapsed: 11m2s
2020-04-30 00:33:48.240435: Epoch: 1, Batch: 1144, Loss: 0.6026, Elapsed: 10m14s
2020-04-30 00:33:55.638292: Epoch: 1, Batch: 1135, Loss: 0.5586, Elapsed: 9m2s
2020-04-30 00:38:11.376911: Epoch: 1, Batch: 1136, Loss: 0.5217, Elapsed: 4m15s
2020-04-30 00:40:59.491905: Epoch: 1, Batch: 1139, Loss: 0.6541, Elapsed: 7m18s
2020-04-30 00:41:35.341475: Epoch: 1, Batch: 1145, Loss: 0.5818, Elapsed: 7m47s
2020-04-30 00:47:20.417721: Epoch: 1, Batch: 1146, Loss: 0.5722, Elapsed: 5m45s
2020-04-30 00:47:35.383702: Epoch: 1, Batch: 1137, Loss: 0.5699, Elapsed: 9m23s
2020-04-30 00:50:26.723354: Epoch: 1, Batch: 1140, Loss: 0.6343, Elapsed: 9m27s
2020-04-30 00:52:58.849136: Epoch: 1, Batch: 1147, Loss: 0.5579, Elapsed: 5m38s
2020-04-30 00:53:07.026509: Epoch: 1, Batch: 1138, Loss: 0.5351, Elapsed: 5m31s
2020-04-30 00:54:18.490816: Epoch: 1, Batch: 1141, Loss: 0.5634, Elapsed: 3m51s
2020-04-30 00:58:35.668856: Epoch: 1, Batch: 1148, Loss: 0.5803, Elapsed: 5m36s
2020-04-30 01:01:27.449553: Epoch: 1, Batch: 1139, Loss: 0.5914, Elapsed: 8m20s
2020-04-30 01:02:27.307869: Epoch: 1, Batch: 1142, Loss: 0.6172, Elapsed: 8m8s
2020-04-30 01:08:12.279348: Epoch: 1, Batch: 1140, Loss: 0.5934, Elapsed: 6m44s
2020-04-30 01:08:58.268391: Epoch: 1, Batch: 1143, Loss: 0.5955, Elapsed: 6m30s
2020-04-30 01:09:32.942430: Epoch: 1, Batch: 1149, Loss: 0.5956, Elapsed: 10m57s
2020-04-30 01:13:32.055904: Epoch: 1, Batch: 1141, Loss: 0.5179, Elapsed: 5m19s
2020-04-30 01:14:08.304021: Epoch: 1, Batch: 1144, Loss: 0.5844, Elapsed: 5m10s
2020-04-30 01:15:26.999509: Epoch: 1, Batch: 1150, Loss: 0.5487, Elapsed: 5m54s
Starting testing the validation set with 200 subgraphs!
2020-04-30 01:18:33.565899: Epoch: 1, Batch: 1142, Loss: 0.5271, Elapsed: 5m1s
2020-04-30 01:21:48.125187: Epoch: 1, Batch: 1145, Loss: 0.6050, Elapsed: 7m39s
2020-04-30 01:23:59.393720: Epoch: 1, Batch: 1143, Loss: 0.5079, Elapsed: 5m25s
2020-04-30 01:28:24.059531: Epoch: 1, Batch: 1146, Loss: 0.6064, Elapsed: 6m35s
2020-04-30 01:28:38.423124: Epoch: 1, Batch: 1144, Loss: 0.5468, Elapsed: 4m39s
2020-04-30 01:33:57.428231: Epoch: 1, Batch: 1147, Loss: 0.5593, Elapsed: 5m33s
2020-04-30 01:35:57.801405: Epoch: 1, Batch: 1145, Loss: 0.6004, Elapsed: 7m19s
2020-04-30 01:38:57.652509: Epoch: 1, Batch: 1148, Loss: 0.5629, Elapsed: 5m0s
2020-04-30 01:43:05.954785: Epoch: 1, Batch: 1146, Loss: 0.5820, Elapsed: 7m8s
2020-04-30 01:45:17.283901: Epoch: 1, Batch: 1149, Loss: 0.5916, Elapsed: 6m19s
2020-04-30 01:47:12.110729: Epoch: 1, Batch: 1147, Loss: 0.5012, Elapsed: 4m6s
2020-04-30 01:50:57.967722: Epoch: 1, Batch: 1150, Loss: 0.5566, Elapsed: 5m40s
Starting testing the validation set with 200 subgraphs!
2020-04-30 01:57:05.831332: Validation Test:  Loss: 0.5791,  Acc: 70.2726, AUC: 0.7577, Precision: 0.8033 -- Elapsed: 41m38s
2020-04-30 01:57:23.868314: Epoch: 1, Batch: 1148, Loss: 0.5720, Elapsed: 10m11s
2020-04-30 02:02:57.323461: Epoch: 1, Batch: 1151, Loss: 0.5986, Elapsed: 5m51s
2020-04-30 02:05:01.286822: Epoch: 1, Batch: 1149, Loss: 0.5697, Elapsed: 7m37s
2020-04-30 02:11:05.020675: Epoch: 1, Batch: 1150, Loss: 0.5732, Elapsed: 6m3s
Starting testing the validation set with 200 subgraphs!
2020-04-30 02:11:43.885115: Epoch: 1, Batch: 1152, Loss: 0.6008, Elapsed: 8m46s
2020-04-30 02:18:59.184803: Epoch: 1, Batch: 1153, Loss: 0.5820, Elapsed: 7m15s
2020-04-30 02:23:19.998837: Epoch: 1, Batch: 1154, Loss: 0.5320, Elapsed: 4m20s
2020-04-30 02:27:50.284711: Epoch: 1, Batch: 1155, Loss: 0.5285, Elapsed: 4m30s
2020-04-30 02:30:52.343399: Validation Test:  Loss: 0.5976,  Acc: 68.9325, AUC: 0.7418, Precision: 0.7711 -- Elapsed: 39m54s
2020-04-30 02:31:17.516425: Epoch: 1, Batch: 1156, Loss: 0.5232, Elapsed: 3m27s
2020-04-30 02:38:18.048907: Epoch: 1, Batch: 1151, Loss: 0.5910, Elapsed: 7m25s
2020-04-30 02:43:23.532945: Epoch: 1, Batch: 1157, Loss: 0.6100, Elapsed: 12m5s
2020-04-30 02:52:20.282381: Epoch: 1, Batch: 1152, Loss: 0.5611, Elapsed: 14m2s
2020-04-30 02:54:39.623754: Epoch: 1, Batch: 1158, Loss: 0.5533, Elapsed: 11m16s
2020-04-30 03:01:20.022363: Epoch: 1, Batch: 1153, Loss: 0.5779, Elapsed: 8m59s
2020-04-30 03:06:20.790817: Epoch: 1, Batch: 1159, Loss: 0.5716, Elapsed: 11m41s
2020-04-30 03:06:19.775470: Validation Test:  Loss: 0.5841,  Acc: 70.0514, AUC: 0.7599, Precision: 0.7983 -- Elapsed: 55m14s
2020-04-30 03:11:40.781930: Epoch: 1, Batch: 1154, Loss: 0.5975, Elapsed: 10m20s
2020-04-30 03:14:23.661542: Epoch: 1, Batch: 1160, Loss: 0.6016, Elapsed: 8m2s
2020-04-30 03:14:35.548957: Epoch: 1, Batch: 1151, Loss: 0.5907, Elapsed: 8m15s
2020-04-30 03:22:23.843995: Epoch: 1, Batch: 1161, Loss: 0.5484, Elapsed: 8m0s
2020-04-30 03:22:57.121124: Epoch: 1, Batch: 1155, Loss: 0.6241, Elapsed: 11m16s
2020-04-30 03:28:26.030901: Epoch: 1, Batch: 1152, Loss: 0.5681, Elapsed: 13m50s
2020-04-30 03:30:32.080376: Epoch: 1, Batch: 1162, Loss: 0.5779, Elapsed: 8m8s
2020-04-30 03:31:59.778567: Epoch: 1, Batch: 1156, Loss: 0.6076, Elapsed: 9m2s
2020-04-30 03:36:50.100657: Epoch: 1, Batch: 1153, Loss: 0.5248, Elapsed: 8m24s
2020-04-30 03:38:33.761609: Epoch: 1, Batch: 1157, Loss: 0.5338, Elapsed: 6m33s
2020-04-30 03:40:11.804493: Epoch: 1, Batch: 1163, Loss: 0.5848, Elapsed: 9m39s
2020-04-30 03:47:41.768626: Epoch: 1, Batch: 1158, Loss: 0.6132, Elapsed: 9m7s
2020-04-30 03:48:03.512202: Epoch: 1, Batch: 1154, Loss: 0.6061, Elapsed: 11m13s
2020-04-30 03:48:27.454329: Epoch: 1, Batch: 1164, Loss: 0.4895, Elapsed: 8m15s
2020-04-30 03:53:40.147474: Epoch: 1, Batch: 1159, Loss: 0.5906, Elapsed: 5m58s
2020-04-30 03:57:18.732631: Epoch: 1, Batch: 1165, Loss: 0.5361, Elapsed: 8m51s
2020-04-30 03:59:04.829946: Epoch: 1, Batch: 1155, Loss: 0.5992, Elapsed: 11m1s
2020-04-30 04:03:15.447258: Epoch: 1, Batch: 1160, Loss: 0.5620, Elapsed: 9m35s
2020-04-30 04:08:35.943867: Epoch: 1, Batch: 1161, Loss: 0.5661, Elapsed: 5m20s
2020-04-30 04:09:49.485098: Epoch: 1, Batch: 1166, Loss: 0.5934, Elapsed: 12m30s
2020-04-30 04:17:09.408198: Epoch: 1, Batch: 1167, Loss: 0.5362, Elapsed: 7m19s
2020-04-30 04:17:24.290756: Epoch: 1, Batch: 1156, Loss: 0.6134, Elapsed: 18m19s
2020-04-30 04:18:42.622228: Epoch: 1, Batch: 1162, Loss: 0.5834, Elapsed: 10m6s
2020-04-30 04:23:07.631435: Epoch: 1, Batch: 1168, Loss: 0.5260, Elapsed: 5m58s
2020-04-30 04:26:08.683201: Epoch: 1, Batch: 1163, Loss: 0.5837, Elapsed: 7m26s
2020-04-30 04:26:44.892824: Epoch: 1, Batch: 1157, Loss: 0.5433, Elapsed: 9m20s
2020-04-30 04:30:42.888704: Epoch: 1, Batch: 1169, Loss: 0.5387, Elapsed: 7m35s
2020-04-30 04:35:19.290502: Epoch: 1, Batch: 1158, Loss: 0.6193, Elapsed: 8m34s
2020-04-30 04:40:44.875813: Epoch: 1, Batch: 1164, Loss: 0.6410, Elapsed: 14m36s
2020-04-30 04:41:04.062530: Epoch: 1, Batch: 1170, Loss: 0.5942, Elapsed: 10m21s
2020-04-30 04:50:39.987661: Epoch: 1, Batch: 1159, Loss: 0.6104, Elapsed: 15m20s
2020-04-30 04:51:23.424955: Epoch: 1, Batch: 1165, Loss: 0.5601, Elapsed: 10m38s
2020-04-30 04:51:29.125752: Epoch: 1, Batch: 1171, Loss: 0.5893, Elapsed: 10m25s
2020-04-30 05:00:02.856384: Epoch: 1, Batch: 1166, Loss: 0.5526, Elapsed: 8m39s
2020-04-30 05:01:24.017691: Epoch: 1, Batch: 1160, Loss: 0.5840, Elapsed: 10m44s
2020-04-30 05:01:31.577103: Epoch: 1, Batch: 1172, Loss: 0.6048, Elapsed: 10m2s
2020-04-30 05:06:15.356724: Epoch: 1, Batch: 1173, Loss: 0.4904, Elapsed: 4m43s
2020-04-30 05:10:40.872464: Epoch: 1, Batch: 1167, Loss: 0.6219, Elapsed: 10m37s
2020-04-30 05:15:38.379114: Epoch: 1, Batch: 1174, Loss: 0.5824, Elapsed: 9m22s
2020-04-30 05:16:01.741723: Epoch: 1, Batch: 1161, Loss: 0.6068, Elapsed: 14m37s
2020-04-30 05:22:42.576750: Epoch: 1, Batch: 1162, Loss: 0.5473, Elapsed: 6m40s
2020-04-30 05:26:16.028503: Epoch: 1, Batch: 1175, Loss: 0.5507, Elapsed: 10m37s
2020-04-30 05:27:16.983880: Epoch: 1, Batch: 1168, Loss: 0.6195, Elapsed: 16m36s
2020-04-30 05:33:20.297913: Epoch: 1, Batch: 1163, Loss: 0.5553, Elapsed: 10m37s
2020-04-30 05:33:57.901277: Epoch: 1, Batch: 1176, Loss: 0.5190, Elapsed: 7m41s
2020-04-30 05:35:57.705680: Epoch: 1, Batch: 1169, Loss: 0.5833, Elapsed: 8m40s
2020-04-30 05:41:43.634044: Epoch: 1, Batch: 1177, Loss: 0.5262, Elapsed: 7m45s
2020-04-30 05:45:10.531581: Epoch: 1, Batch: 1164, Loss: 0.5779, Elapsed: 11m50s
2020-04-30 05:45:12.588111: Epoch: 1, Batch: 1170, Loss: 0.6257, Elapsed: 9m14s
2020-04-30 05:49:54.410942: Epoch: 1, Batch: 1178, Loss: 0.5884, Elapsed: 8m10s
2020-04-30 05:54:18.871417: Epoch: 1, Batch: 1171, Loss: 0.6321, Elapsed: 9m6s
2020-04-30 05:55:49.638168: Epoch: 1, Batch: 1165, Loss: 0.5665, Elapsed: 10m39s
2020-04-30 05:58:54.266933: Epoch: 1, Batch: 1179, Loss: 0.5511, Elapsed: 8m59s
2020-04-30 06:04:24.961459: Epoch: 1, Batch: 1172, Loss: 0.6355, Elapsed: 10m6s
2020-04-30 06:05:42.741838: Epoch: 1, Batch: 1166, Loss: 0.5963, Elapsed: 9m53s
2020-04-30 06:09:29.624722: Epoch: 1, Batch: 1180, Loss: 0.5831, Elapsed: 10m35s
2020-04-30 06:11:21.313451: Epoch: 1, Batch: 1167, Loss: 0.4804, Elapsed: 5m38s
2020-04-30 06:20:26.827268: Epoch: 1, Batch: 1173, Loss: 0.6153, Elapsed: 16m1s
2020-04-30 06:22:59.851312: Epoch: 1, Batch: 1181, Loss: 0.6245, Elapsed: 13m30s
2020-04-30 06:25:52.523912: Epoch: 1, Batch: 1168, Loss: 0.6145, Elapsed: 14m31s
2020-04-30 06:35:50.713625: Epoch: 1, Batch: 1174, Loss: 0.6221, Elapsed: 15m23s
2020-04-30 06:37:23.486612: Epoch: 1, Batch: 1169, Loss: 0.5951, Elapsed: 11m30s
2020-04-30 06:39:24.808424: Epoch: 1, Batch: 1182, Loss: 0.6152, Elapsed: 16m24s
2020-04-30 06:46:30.555059: Epoch: 1, Batch: 1175, Loss: 0.6259, Elapsed: 10m39s
2020-04-30 06:47:19.778644: Epoch: 1, Batch: 1170, Loss: 0.5764, Elapsed: 9m56s
2020-04-30 06:52:20.546617: Epoch: 1, Batch: 1183, Loss: 0.5930, Elapsed: 12m55s
2020-04-30 06:53:25.414159: Epoch: 1, Batch: 1176, Loss: 0.5283, Elapsed: 6m54s
2020-04-30 06:54:11.637611: Epoch: 1, Batch: 1171, Loss: 0.5865, Elapsed: 6m51s
2020-04-30 07:01:13.332601: Epoch: 1, Batch: 1184, Loss: 0.5662, Elapsed: 8m52s
2020-04-30 07:01:27.895838: Epoch: 1, Batch: 1177, Loss: 0.5912, Elapsed: 8m2s
2020-04-30 07:03:16.500628: Epoch: 1, Batch: 1172, Loss: 0.5866, Elapsed: 9m4s
2020-04-30 07:09:08.134386: Epoch: 1, Batch: 1185, Loss: 0.5339, Elapsed: 7m54s
2020-04-30 07:09:49.239710: Epoch: 1, Batch: 1173, Loss: 0.5100, Elapsed: 6m32s
2020-04-30 07:10:03.911982: Epoch: 1, Batch: 1178, Loss: 0.5795, Elapsed: 8m35s
2020-04-30 07:18:49.819159: Epoch: 1, Batch: 1186, Loss: 0.5936, Elapsed: 9m41s
2020-04-30 07:21:41.900984: Epoch: 1, Batch: 1174, Loss: 0.5598, Elapsed: 11m52s
2020-04-30 07:23:09.524921: Epoch: 1, Batch: 1179, Loss: 0.6190, Elapsed: 13m5s
2020-04-30 07:30:39.751872: Epoch: 1, Batch: 1187, Loss: 0.5782, Elapsed: 11m49s
2020-04-30 07:31:08.152592: Epoch: 1, Batch: 1175, Loss: 0.6013, Elapsed: 9m26s
2020-04-30 07:33:05.885889: Epoch: 1, Batch: 1180, Loss: 0.5957, Elapsed: 9m56s
2020-04-30 07:41:58.745759: Epoch: 1, Batch: 1188, Loss: 0.5643, Elapsed: 11m18s
2020-04-30 07:44:33.587920: Epoch: 1, Batch: 1181, Loss: 0.5936, Elapsed: 11m27s
2020-04-30 07:49:50.151365: Epoch: 1, Batch: 1176, Loss: 0.6549, Elapsed: 18m41s
2020-04-30 07:54:16.521660: Epoch: 1, Batch: 1182, Loss: 0.6135, Elapsed: 9m42s
2020-04-30 07:55:03.208628: Epoch: 1, Batch: 1189, Loss: 0.5989, Elapsed: 13m4s
2020-04-30 07:58:36.854154: Epoch: 1, Batch: 1177, Loss: 0.5560, Elapsed: 8m46s
2020-04-30 08:04:01.227216: Epoch: 1, Batch: 1190, Loss: 0.5117, Elapsed: 8m57s
2020-04-30 08:11:13.712063: Epoch: 1, Batch: 1178, Loss: 0.5862, Elapsed: 12m36s
2020-04-30 08:12:26.775771: Epoch: 1, Batch: 1183, Loss: 0.6120, Elapsed: 18m10s
2020-04-30 08:15:01.137264: Epoch: 1, Batch: 1191, Loss: 0.5973, Elapsed: 10m59s
2020-04-30 08:18:51.636305: Epoch: 1, Batch: 1179, Loss: 0.5323, Elapsed: 7m37s
2020-04-30 08:21:51.681653: Epoch: 1, Batch: 1184, Loss: 0.6006, Elapsed: 9m24s
2020-04-30 08:26:35.255946: Epoch: 1, Batch: 1180, Loss: 0.5354, Elapsed: 7m43s
2020-04-30 08:27:32.551567: Epoch: 1, Batch: 1192, Loss: 0.6058, Elapsed: 12m31s
2020-04-30 08:32:10.752441: Epoch: 1, Batch: 1193, Loss: 0.5091, Elapsed: 4m38s
2020-04-30 08:35:34.812948: Epoch: 1, Batch: 1185, Loss: 0.6131, Elapsed: 13m43s
2020-04-30 08:39:52.769949: Epoch: 1, Batch: 1181, Loss: 0.5903, Elapsed: 13m17s
2020-04-30 08:41:25.676112: Epoch: 1, Batch: 1194, Loss: 0.5631, Elapsed: 9m14s
2020-04-30 08:44:55.920287: Epoch: 1, Batch: 1186, Loss: 0.5871, Elapsed: 9m21s
2020-04-30 08:48:37.010854: Epoch: 1, Batch: 1182, Loss: 0.5420, Elapsed: 8m44s
2020-04-30 08:49:21.123554: Epoch: 1, Batch: 1195, Loss: 0.5853, Elapsed: 7m55s
2020-04-30 08:54:47.763722: Epoch: 1, Batch: 1187, Loss: 0.6186, Elapsed: 9m51s
2020-04-30 08:55:47.696039: Epoch: 1, Batch: 1196, Loss: 0.5547, Elapsed: 6m26s
2020-04-30 08:59:20.182892: Epoch: 1, Batch: 1183, Loss: 0.6470, Elapsed: 10m43s
2020-04-30 09:03:55.285503: Epoch: 1, Batch: 1188, Loss: 0.5859, Elapsed: 9m7s
2020-04-30 09:05:30.121646: Epoch: 1, Batch: 1197, Loss: 0.5670, Elapsed: 9m42s
2020-04-30 09:11:03.250754: Epoch: 1, Batch: 1184, Loss: 0.6113, Elapsed: 11m43s
2020-04-30 09:12:24.725969: Epoch: 1, Batch: 1189, Loss: 0.5593, Elapsed: 8m29s
2020-04-30 09:12:49.937060: Epoch: 1, Batch: 1198, Loss: 0.5368, Elapsed: 7m19s
2020-04-30 09:20:09.163954: Epoch: 1, Batch: 1199, Loss: 0.5483, Elapsed: 7m19s
2020-04-30 09:21:17.952240: Epoch: 1, Batch: 1185, Loss: 0.5783, Elapsed: 10m14s
2020-04-30 09:26:18.109946: Epoch: 1, Batch: 1190, Loss: 0.6578, Elapsed: 13m53s
2020-04-30 09:27:47.319541: Epoch: 1, Batch: 1186, Loss: 0.5267, Elapsed: 6m29s
2020-04-30 09:35:06.001207: Epoch: 1, Batch: 1200, Loss: 0.5927, Elapsed: 14m56s
Starting testing the validation set with 200 subgraphs!
2020-04-30 09:44:47.463818: Epoch: 1, Batch: 1191, Loss: 0.6087, Elapsed: 18m29s
2020-04-30 09:50:07.138866: Epoch: 1, Batch: 1187, Loss: 0.5951, Elapsed: 22m19s
2020-04-30 09:56:37.731481: Epoch: 1, Batch: 1192, Loss: 0.6203, Elapsed: 11m50s
2020-04-30 09:57:34.160856: Epoch: 1, Batch: 1188, Loss: 0.5701, Elapsed: 7m26s
2020-04-30 10:04:00.528449: Epoch: 1, Batch: 1193, Loss: 0.6080, Elapsed: 7m22s
2020-04-30 10:06:58.277717: Epoch: 1, Batch: 1189, Loss: 0.6067, Elapsed: 9m24s
2020-04-30 10:14:40.316928: Epoch: 1, Batch: 1194, Loss: 0.5981, Elapsed: 10m39s
2020-04-30 10:17:58.817915: Epoch: 1, Batch: 1190, Loss: 0.5665, Elapsed: 11m0s
2020-04-30 10:22:59.923844: Epoch: 1, Batch: 1195, Loss: 0.5845, Elapsed: 8m19s
2020-04-30 10:26:31.852203: Epoch: 1, Batch: 1191, Loss: 0.5685, Elapsed: 8m33s
2020-04-30 10:27:26.348457: Epoch: 1, Batch: 1196, Loss: 0.5355, Elapsed: 4m26s
2020-04-30 10:33:43.963097: Epoch: 1, Batch: 1192, Loss: 0.5621, Elapsed: 7m12s
2020-04-30 10:36:47.091623: Epoch: 1, Batch: 1197, Loss: 0.6066, Elapsed: 9m20s
2020-04-30 10:43:47.346778: Epoch: 1, Batch: 1198, Loss: 0.5341, Elapsed: 7m0s
2020-04-30 10:44:18.226959: Epoch: 1, Batch: 1193, Loss: 0.5798, Elapsed: 10m34s
2020-04-30 10:49:17.294634: Epoch: 1, Batch: 1199, Loss: 0.5471, Elapsed: 5m29s
2020-04-30 10:50:14.559712: Validation Test:  Loss: 0.5791,  Acc: 69.8577, AUC: 0.7587, Precision: 0.8067 -- Elapsed: 75m8s
2020-04-30 10:54:46.231645: Epoch: 1, Batch: 1194, Loss: 0.5713, Elapsed: 10m27s
2020-04-30 10:59:29.531176: Epoch: 1, Batch: 1200, Loss: 0.6043, Elapsed: 10m12s
Starting testing the validation set with 200 subgraphs!
2020-04-30 11:01:59.128438: Epoch: 1, Batch: 1201, Loss: 0.5804, Elapsed: 11m44s
2020-04-30 11:03:57.419576: Epoch: 1, Batch: 1195, Loss: 0.5360, Elapsed: 9m11s
2020-04-30 11:11:49.899205: Epoch: 1, Batch: 1196, Loss: 0.6222, Elapsed: 7m52s
2020-04-30 11:12:46.530477: Epoch: 1, Batch: 1202, Loss: 0.6035, Elapsed: 10m47s
2020-04-30 11:23:56.113178: Epoch: 1, Batch: 1203, Loss: 0.5602, Elapsed: 11m9s
2020-04-30 11:24:46.957828: Epoch: 1, Batch: 1197, Loss: 0.5936, Elapsed: 12m57s
2020-04-30 11:31:44.073725: Epoch: 1, Batch: 1204, Loss: 0.5414, Elapsed: 7m47s
2020-04-30 11:32:09.029774: Epoch: 1, Batch: 1198, Loss: 0.5500, Elapsed: 7m22s
2020-04-30 11:37:36.052191: Epoch: 1, Batch: 1199, Loss: 0.4931, Elapsed: 5m26s
2020-04-30 11:44:23.693362: Epoch: 1, Batch: 1205, Loss: 0.5976, Elapsed: 12m39s
2020-04-30 11:48:08.918851: Epoch: 1, Batch: 1200, Loss: 0.5824, Elapsed: 10m32s
Starting testing the validation set with 200 subgraphs!
2020-04-30 11:56:28.228789: Epoch: 1, Batch: 1206, Loss: 0.6040, Elapsed: 12m4s
2020-04-30 12:03:14.633533: Epoch: 1, Batch: 1207, Loss: 0.5677, Elapsed: 6m46s
2020-04-30 12:05:59.866642: Validation Test:  Loss: 0.5964,  Acc: 69.3079, AUC: 0.7484, Precision: 0.7878 -- Elapsed: 66m30s
2020-04-30 12:06:26.445797: Epoch: 1, Batch: 1208, Loss: 0.5225, Elapsed: 3m11s
2020-04-30 12:10:36.478349: Epoch: 1, Batch: 1209, Loss: 0.5305, Elapsed: 4m10s
2020-04-30 12:11:50.038909: Epoch: 1, Batch: 1201, Loss: 0.6150, Elapsed: 5m50s
2020-04-30 12:15:00.583683: Epoch: 1, Batch: 1210, Loss: 0.5736, Elapsed: 4m24s
2020-04-30 12:17:27.266101: Epoch: 1, Batch: 1202, Loss: 0.5849, Elapsed: 5m37s
2020-04-30 12:20:31.228089: Epoch: 1, Batch: 1211, Loss: 0.5610, Elapsed: 5m30s
2020-04-30 12:22:28.620542: Epoch: 1, Batch: 1203, Loss: 0.6337, Elapsed: 5m1s
2020-04-30 12:25:19.147600: Epoch: 1, Batch: 1212, Loss: 0.5342, Elapsed: 4m47s
2020-04-30 12:27:03.634838: Epoch: 1, Batch: 1204, Loss: 0.5457, Elapsed: 4m34s
2020-04-30 12:29:41.656929: Epoch: 1, Batch: 1213, Loss: 0.5699, Elapsed: 4m22s
2020-04-30 12:30:56.764734: Epoch: 1, Batch: 1205, Loss: 0.5493, Elapsed: 3m53s
2020-04-30 12:34:36.742133: Validation Test:  Loss: 0.5793,  Acc: 70.3852, AUC: 0.7626, Precision: 0.7998 -- Elapsed: 46m27s
2020-04-30 12:34:59.701011: Epoch: 1, Batch: 1214, Loss: 0.6062, Elapsed: 5m18s
2020-04-30 12:38:40.115080: Epoch: 1, Batch: 1206, Loss: 0.5828, Elapsed: 7m43s
2020-04-30 12:42:10.348287: Epoch: 1, Batch: 1215, Loss: 0.5972, Elapsed: 7m10s
2020-04-30 12:45:05.103514: Epoch: 1, Batch: 1201, Loss: 0.6406, Elapsed: 10m28s
2020-04-30 12:48:06.738092: Epoch: 1, Batch: 1216, Loss: 0.5679, Elapsed: 5m56s
2020-04-30 12:49:25.925178: Epoch: 1, Batch: 1202, Loss: 0.5661, Elapsed: 4m20s
2020-04-30 12:49:24.440832: Epoch: 1, Batch: 1207, Loss: 0.6768, Elapsed: 10m44s
2020-04-30 12:54:38.135149: Epoch: 1, Batch: 1217, Loss: 0.6255, Elapsed: 6m31s
2020-04-30 12:55:29.775271: Epoch: 1, Batch: 1203, Loss: 0.5246, Elapsed: 6m3s
2020-04-30 12:56:21.079257: Epoch: 1, Batch: 1208, Loss: 0.5994, Elapsed: 6m56s
2020-04-30 13:00:50.532597: Epoch: 1, Batch: 1218, Loss: 0.5251, Elapsed: 6m12s
2020-04-30 13:01:20.146883: Epoch: 1, Batch: 1204, Loss: 0.5555, Elapsed: 5m50s
2020-04-30 13:01:53.623031: Epoch: 1, Batch: 1209, Loss: 0.5840, Elapsed: 5m32s
2020-04-30 13:05:58.020431: Epoch: 1, Batch: 1219, Loss: 0.5613, Elapsed: 5m7s
2020-04-30 13:09:08.385062: Epoch: 1, Batch: 1210, Loss: 0.6053, Elapsed: 7m14s
2020-04-30 13:10:27.752177: Epoch: 1, Batch: 1205, Loss: 0.6646, Elapsed: 9m7s
2020-04-30 13:15:12.530749: Epoch: 1, Batch: 1211, Loss: 0.6049, Elapsed: 6m4s
2020-04-30 13:15:09.366562: Epoch: 1, Batch: 1206, Loss: 0.4981, Elapsed: 4m41s
2020-04-30 13:16:07.214162: Epoch: 1, Batch: 1220, Loss: 0.6070, Elapsed: 10m9s
2020-04-30 13:21:42.851114: Epoch: 1, Batch: 1212, Loss: 0.6065, Elapsed: 6m30s
2020-04-30 13:22:10.024304: Epoch: 1, Batch: 1221, Loss: 0.5744, Elapsed: 6m2s
2020-04-30 13:22:42.677527: Epoch: 1, Batch: 1207, Loss: 0.5730, Elapsed: 7m33s
2020-04-30 13:27:45.911067: Epoch: 1, Batch: 1208, Loss: 0.5340, Elapsed: 5m3s
2020-04-30 13:29:08.411257: Epoch: 1, Batch: 1213, Loss: 0.5967, Elapsed: 7m25s
2020-04-30 13:31:51.749598: Epoch: 1, Batch: 1222, Loss: 0.5785, Elapsed: 9m41s
2020-04-30 13:36:36.063716: Epoch: 1, Batch: 1209, Loss: 0.6142, Elapsed: 8m50s
2020-04-30 13:36:41.556085: Epoch: 1, Batch: 1223, Loss: 0.5229, Elapsed: 4m49s
2020-04-30 13:37:09.356328: Epoch: 1, Batch: 1214, Loss: 0.6061, Elapsed: 8m0s
2020-04-30 13:40:59.003979: Epoch: 1, Batch: 1210, Loss: 0.5838, Elapsed: 4m22s
2020-04-30 13:41:37.379545: Epoch: 1, Batch: 1215, Loss: 0.5582, Elapsed: 4m27s
2020-04-30 13:43:56.437761: Epoch: 1, Batch: 1224, Loss: 0.6200, Elapsed: 7m14s
2020-04-30 13:46:04.736781: Epoch: 1, Batch: 1216, Loss: 0.5598, Elapsed: 4m27s
2020-04-30 13:46:23.598842: Epoch: 1, Batch: 1211, Loss: 0.6002, Elapsed: 5m24s
2020-04-30 13:47:37.503260: Epoch: 1, Batch: 1225, Loss: 0.5450, Elapsed: 3m40s
2020-04-30 13:53:01.899379: Epoch: 1, Batch: 1212, Loss: 0.5821, Elapsed: 6m38s
2020-04-30 13:54:15.507264: Epoch: 1, Batch: 1217, Loss: 0.5832, Elapsed: 8m10s
2020-04-30 13:57:08.916321: Epoch: 1, Batch: 1226, Loss: 0.5999, Elapsed: 9m31s
2020-04-30 14:00:43.223414: Epoch: 1, Batch: 1213, Loss: 0.5836, Elapsed: 7m41s
2020-04-30 14:04:08.139994: Epoch: 1, Batch: 1218, Loss: 0.5889, Elapsed: 9m52s
2020-04-30 14:05:26.777829: Epoch: 1, Batch: 1227, Loss: 0.6282, Elapsed: 8m17s
2020-04-30 14:08:16.805358: Epoch: 1, Batch: 1219, Loss: 0.5733, Elapsed: 4m8s
2020-04-30 14:08:17.996019: Epoch: 1, Batch: 1214, Loss: 0.5765, Elapsed: 7m34s
2020-04-30 14:10:43.554676: Epoch: 1, Batch: 1228, Loss: 0.5854, Elapsed: 5m16s
2020-04-30 14:12:27.420614: Epoch: 1, Batch: 1220, Loss: 0.6018, Elapsed: 4m10s
2020-04-30 14:12:39.887457: Epoch: 1, Batch: 1215, Loss: 0.5383, Elapsed: 4m21s
2020-04-30 14:18:13.919539: Epoch: 1, Batch: 1229, Loss: 0.5629, Elapsed: 7m30s
2020-04-30 14:20:34.999194: Epoch: 1, Batch: 1221, Loss: 0.5858, Elapsed: 8m7s
2020-04-30 14:20:42.398422: Epoch: 1, Batch: 1230, Loss: 0.4956, Elapsed: 2m28s
2020-04-30 14:21:15.185677: Epoch: 1, Batch: 1216, Loss: 0.6264, Elapsed: 8m35s
2020-04-30 14:25:46.192052: Epoch: 1, Batch: 1231, Loss: 0.5383, Elapsed: 5m3s
2020-04-30 14:28:00.691161: Epoch: 1, Batch: 1217, Loss: 0.5627, Elapsed: 6m45s
2020-04-30 14:28:43.627376: Epoch: 1, Batch: 1222, Loss: 0.6055, Elapsed: 8m8s
2020-04-30 14:29:44.876121: Epoch: 1, Batch: 1232, Loss: 0.5379, Elapsed: 3m58s
2020-04-30 14:32:40.143049: Epoch: 1, Batch: 1218, Loss: 0.5727, Elapsed: 4m39s
2020-04-30 14:38:50.364856: Epoch: 1, Batch: 1223, Loss: 0.6586, Elapsed: 10m6s
2020-04-30 14:39:32.390008: Epoch: 1, Batch: 1219, Loss: 0.5814, Elapsed: 6m52s
2020-04-30 14:39:39.474866: Epoch: 1, Batch: 1233, Loss: 0.5358, Elapsed: 9m54s
2020-04-30 14:44:10.936889: Epoch: 1, Batch: 1220, Loss: 0.4976, Elapsed: 4m38s
2020-04-30 14:46:48.384678: Epoch: 1, Batch: 1234, Loss: 0.5927, Elapsed: 7m8s
2020-04-30 14:48:24.991540: Epoch: 1, Batch: 1224, Loss: 0.5994, Elapsed: 9m34s
2020-04-30 14:53:00.191695: Epoch: 1, Batch: 1235, Loss: 0.5898, Elapsed: 6m11s
2020-04-30 14:54:06.743978: Epoch: 1, Batch: 1221, Loss: 0.6253, Elapsed: 9m55s
2020-04-30 14:57:07.145753: Epoch: 1, Batch: 1225, Loss: 0.6132, Elapsed: 8m42s
2020-04-30 14:59:58.186597: Epoch: 1, Batch: 1236, Loss: 0.6033, Elapsed: 6m57s
2020-04-30 15:00:14.394371: Epoch: 1, Batch: 1222, Loss: 0.5666, Elapsed: 6m7s
2020-04-30 15:02:50.655017: Epoch: 1, Batch: 1226, Loss: 0.5709, Elapsed: 5m43s
2020-04-30 15:04:27.898885: Epoch: 1, Batch: 1223, Loss: 0.5031, Elapsed: 4m13s
2020-04-30 15:08:18.758267: Epoch: 1, Batch: 1237, Loss: 0.5981, Elapsed: 8m20s
2020-04-30 15:10:55.690047: Epoch: 1, Batch: 1227, Loss: 0.5968, Elapsed: 8m5s
2020-04-30 15:12:12.528777: Epoch: 1, Batch: 1224, Loss: 0.5682, Elapsed: 7m44s
2020-04-30 15:15:42.011733: Epoch: 1, Batch: 1238, Loss: 0.6270, Elapsed: 7m23s
2020-04-30 15:16:04.514891: Epoch: 1, Batch: 1225, Loss: 0.5256, Elapsed: 3m51s
2020-04-30 15:17:59.670901: Epoch: 1, Batch: 1228, Loss: 0.5735, Elapsed: 7m3s
2020-04-30 15:21:13.145336: Epoch: 1, Batch: 1229, Loss: 0.5634, Elapsed: 3m13s
2020-04-30 15:22:01.943320: Epoch: 1, Batch: 1239, Loss: 0.5544, Elapsed: 6m19s
2020-04-30 15:23:47.111974: Epoch: 1, Batch: 1226, Loss: 0.5914, Elapsed: 7m42s
2020-04-30 15:26:04.906665: Epoch: 1, Batch: 1240, Loss: 0.4664, Elapsed: 4m2s
2020-04-30 15:28:15.604298: Epoch: 1, Batch: 1230, Loss: 0.5961, Elapsed: 7m2s
2020-04-30 15:30:07.875709: Epoch: 1, Batch: 1241, Loss: 0.5433, Elapsed: 4m2s
2020-04-30 15:32:20.092986: Epoch: 1, Batch: 1227, Loss: 0.5894, Elapsed: 8m32s
2020-04-30 15:37:18.280893: Epoch: 1, Batch: 1231, Loss: 0.6150, Elapsed: 9m2s
2020-04-30 15:37:27.481653: Epoch: 1, Batch: 1242, Loss: 0.5864, Elapsed: 7m19s
2020-04-30 15:39:25.969919: Epoch: 1, Batch: 1228, Loss: 0.5665, Elapsed: 7m5s
2020-04-30 15:42:36.897782: Epoch: 1, Batch: 1232, Loss: 0.6192, Elapsed: 5m18s
2020-04-30 15:44:02.830066: Epoch: 1, Batch: 1243, Loss: 0.5439, Elapsed: 6m35s
2020-04-30 15:46:53.972575: Epoch: 1, Batch: 1229, Loss: 0.6049, Elapsed: 7m27s
2020-04-30 15:48:04.512159: Epoch: 1, Batch: 1233, Loss: 0.5716, Elapsed: 5m27s
2020-04-30 15:49:03.965997: Epoch: 1, Batch: 1244, Loss: 0.5483, Elapsed: 5m1s
2020-04-30 15:57:30.216004: Epoch: 1, Batch: 1230, Loss: 0.6021, Elapsed: 10m36s
2020-04-30 15:57:44.372837: Epoch: 1, Batch: 1245, Loss: 0.5961, Elapsed: 8m40s
2020-04-30 15:57:56.299275: Epoch: 1, Batch: 1234, Loss: 0.7041, Elapsed: 9m51s
2020-04-30 16:04:00.140873: Epoch: 1, Batch: 1235, Loss: 0.5782, Elapsed: 6m3s
2020-04-30 16:05:19.092715: Epoch: 1, Batch: 1231, Loss: 0.5395, Elapsed: 7m48s
2020-04-30 16:08:06.210161: Epoch: 1, Batch: 1246, Loss: 0.6289, Elapsed: 10m21s
2020-04-30 16:10:40.074385: Epoch: 1, Batch: 1236, Loss: 0.5843, Elapsed: 6m39s
2020-04-30 16:11:29.754143: Epoch: 1, Batch: 1232, Loss: 0.5909, Elapsed: 6m10s
2020-04-30 16:14:46.487115: Epoch: 1, Batch: 1237, Loss: 0.5337, Elapsed: 4m6s
2020-04-30 16:15:19.183429: Epoch: 1, Batch: 1247, Loss: 0.5801, Elapsed: 7m12s
2020-04-30 16:16:09.905412: Epoch: 1, Batch: 1233, Loss: 0.5151, Elapsed: 4m40s
2020-04-30 16:21:15.469727: Epoch: 1, Batch: 1238, Loss: 0.5764, Elapsed: 6m28s
2020-04-30 16:24:06.594121: Epoch: 1, Batch: 1248, Loss: 0.5709, Elapsed: 8m47s
2020-04-30 16:25:12.239783: Epoch: 1, Batch: 1234, Loss: 0.6174, Elapsed: 9m2s
2020-04-30 16:27:42.817870: Epoch: 1, Batch: 1249, Loss: 0.5057, Elapsed: 3m36s
2020-04-30 16:28:52.783730: Epoch: 1, Batch: 1239, Loss: 0.6364, Elapsed: 7m37s
2020-04-30 16:31:53.040880: Epoch: 1, Batch: 1235, Loss: 0.5483, Elapsed: 6m40s
2020-04-30 16:34:37.574841: Epoch: 1, Batch: 1240, Loss: 0.6395, Elapsed: 5m44s
2020-04-30 16:35:07.382511: Epoch: 1, Batch: 1250, Loss: 0.5975, Elapsed: 7m24s
Starting testing the validation set with 200 subgraphs!
2020-04-30 16:36:57.003883: Epoch: 1, Batch: 1236, Loss: 0.5442, Elapsed: 5m3s
2020-04-30 16:38:53.416512: Epoch: 1, Batch: 1241, Loss: 0.5513, Elapsed: 4m15s
2020-04-30 16:43:40.728862: Epoch: 1, Batch: 1242, Loss: 0.5541, Elapsed: 4m47s
2020-04-30 16:44:15.569059: Epoch: 1, Batch: 1237, Loss: 0.5673, Elapsed: 7m18s
2020-04-30 16:51:00.264789: Epoch: 1, Batch: 1238, Loss: 0.5666, Elapsed: 6m44s
2020-04-30 16:57:11.850952: Epoch: 1, Batch: 1239, Loss: 0.5679, Elapsed: 6m11s
2020-04-30 16:57:23.709321: Epoch: 1, Batch: 1243, Loss: 0.6043, Elapsed: 13m42s
2020-04-30 17:04:07.302947: Epoch: 1, Batch: 1244, Loss: 0.6342, Elapsed: 6m43s
2020-04-30 17:05:47.496551: Epoch: 1, Batch: 1240, Loss: 0.5992, Elapsed: 8m35s
2020-04-30 17:08:04.037581: Epoch: 1, Batch: 1245, Loss: 0.5820, Elapsed: 3m56s
2020-04-30 17:10:46.797000: Epoch: 1, Batch: 1241, Loss: 0.5345, Elapsed: 4m59s
2020-04-30 17:14:37.645389: Epoch: 1, Batch: 1246, Loss: 0.6159, Elapsed: 6m33s
2020-04-30 17:17:08.881189: Epoch: 1, Batch: 1242, Loss: 0.5724, Elapsed: 6m22s
2020-04-30 17:17:24.043012: Validation Test:  Loss: 0.5875,  Acc: 69.1527, AUC: 0.7465, Precision: 0.7991 -- Elapsed: 42m16s
2020-04-30 17:19:27.181521: Epoch: 1, Batch: 1247, Loss: 0.5821, Elapsed: 4m49s
2020-04-30 17:25:29.341113: Epoch: 1, Batch: 1251, Loss: 0.6134, Elapsed: 8m5s
2020-04-30 17:26:21.753184: Epoch: 1, Batch: 1243, Loss: 0.6239, Elapsed: 9m12s
2020-04-30 17:26:29.177411: Epoch: 1, Batch: 1248, Loss: 0.5628, Elapsed: 7m1s
2020-04-30 17:31:43.286759: Epoch: 1, Batch: 1244, Loss: 0.5792, Elapsed: 5m21s
2020-04-30 17:32:13.628964: Epoch: 1, Batch: 1252, Loss: 0.6736, Elapsed: 6m44s
2020-04-30 17:34:24.981907: Epoch: 1, Batch: 1249, Loss: 0.6136, Elapsed: 7m55s
2020-04-30 17:37:10.783729: Epoch: 1, Batch: 1245, Loss: 0.5628, Elapsed: 5m27s
2020-04-30 17:39:18.637684: Epoch: 1, Batch: 1253, Loss: 0.5906, Elapsed: 7m4s
2020-04-30 17:41:31.431388: Epoch: 1, Batch: 1250, Loss: 0.6160, Elapsed: 7m6s
Starting testing the validation set with 200 subgraphs!
2020-04-30 17:44:44.913875: Epoch: 1, Batch: 1246, Loss: 0.5926, Elapsed: 7m34s
2020-04-30 17:48:30.868036: Epoch: 1, Batch: 1254, Loss: 0.6188, Elapsed: 9m12s
2020-04-30 17:54:04.908594: Epoch: 1, Batch: 1247, Loss: 0.5907, Elapsed: 9m19s
2020-04-30 17:54:40.418167: Epoch: 1, Batch: 1255, Loss: 0.5590, Elapsed: 6m9s
2020-04-30 18:02:28.493867: Epoch: 1, Batch: 1248, Loss: 0.5883, Elapsed: 8m23s
2020-04-30 18:05:28.449923: Epoch: 1, Batch: 1256, Loss: 0.6224, Elapsed: 10m48s
2020-04-30 18:07:32.746826: Epoch: 1, Batch: 1249, Loss: 0.5462, Elapsed: 5m4s
2020-04-30 18:12:01.294974: Epoch: 1, Batch: 1250, Loss: 0.4919, Elapsed: 4m28s
Starting testing the validation set with 200 subgraphs!
2020-04-30 18:13:07.187052: Epoch: 1, Batch: 1257, Loss: 0.5720, Elapsed: 7m38s
2020-04-30 18:17:10.630950: Epoch: 1, Batch: 1258, Loss: 0.5326, Elapsed: 4m3s
2020-04-30 18:22:06.580773: Epoch: 1, Batch: 1259, Loss: 0.5912, Elapsed: 4m55s
2020-04-30 18:22:48.001064: Validation Test:  Loss: 0.5943,  Acc: 69.4610, AUC: 0.7487, Precision: 0.7849 -- Elapsed: 41m16s
2020-04-30 18:25:44.377292: Epoch: 1, Batch: 1260, Loss: 0.5725, Elapsed: 3m37s
2020-04-30 18:28:28.979460: Epoch: 1, Batch: 1251, Loss: 0.5824, Elapsed: 5m40s
2020-04-30 18:31:25.740816: Epoch: 1, Batch: 1261, Loss: 0.5968, Elapsed: 5m41s
2020-04-30 18:33:57.019289: Epoch: 1, Batch: 1252, Loss: 0.6135, Elapsed: 5m28s
2020-04-30 18:37:26.536029: Epoch: 1, Batch: 1253, Loss: 0.5275, Elapsed: 3m29s
2020-04-30 18:37:37.016491: Epoch: 1, Batch: 1262, Loss: 0.5473, Elapsed: 6m11s
2020-04-30 18:43:33.118140: Epoch: 1, Batch: 1263, Loss: 0.5831, Elapsed: 5m56s
2020-04-30 18:46:35.231876: Epoch: 1, Batch: 1254, Loss: 0.6304, Elapsed: 9m8s
2020-04-30 18:53:10.025918: Epoch: 1, Batch: 1255, Loss: 0.5766, Elapsed: 6m34s
2020-04-30 18:53:29.572783: Epoch: 1, Batch: 1264, Loss: 0.6434, Elapsed: 9m56s
2020-04-30 18:53:41.019184: Validation Test:  Loss: 0.5784,  Acc: 69.9135, AUC: 0.7597, Precision: 0.7989 -- Elapsed: 41m39s
2020-04-30 18:57:25.769363: Epoch: 1, Batch: 1256, Loss: 0.6077, Elapsed: 4m15s
2020-04-30 19:00:17.449738: Epoch: 1, Batch: 1251, Loss: 0.5444, Elapsed: 6m36s
2020-04-30 19:01:06.152345: Epoch: 1, Batch: 1265, Loss: 0.5903, Elapsed: 7m36s
2020-04-30 19:03:41.111804: Epoch: 1, Batch: 1257, Loss: 0.5826, Elapsed: 6m15s
2020-04-30 19:06:32.635847: Epoch: 1, Batch: 1252, Loss: 0.5887, Elapsed: 6m15s
2020-04-30 19:08:01.392322: Epoch: 1, Batch: 1266, Loss: 0.5537, Elapsed: 6m55s
2020-04-30 19:09:52.519762: Epoch: 1, Batch: 1253, Loss: 0.4733, Elapsed: 3m19s
2020-04-30 19:10:19.706627: Epoch: 1, Batch: 1258, Loss: 0.6008, Elapsed: 6m38s
2020-04-30 19:15:15.942187: Epoch: 1, Batch: 1259, Loss: 0.5539, Elapsed: 4m56s
2020-04-30 19:16:57.025376: Epoch: 1, Batch: 1254, Loss: 0.6404, Elapsed: 7m4s
2020-04-30 19:17:17.871643: Epoch: 1, Batch: 1267, Loss: 0.6208, Elapsed: 9m16s
2020-04-30 19:20:04.437409: Epoch: 1, Batch: 1260, Loss: 0.5858, Elapsed: 4m48s
2020-04-30 19:22:26.204554: Epoch: 1, Batch: 1255, Loss: 0.5408, Elapsed: 5m29s
2020-04-30 19:27:22.435592: Epoch: 1, Batch: 1261, Loss: 0.6079, Elapsed: 7m17s
2020-04-30 19:27:23.039998: Epoch: 1, Batch: 1268, Loss: 0.6172, Elapsed: 10m5s
2020-04-30 19:29:27.721137: Epoch: 1, Batch: 1256, Loss: 0.5458, Elapsed: 7m1s
2020-04-30 19:33:01.277263: Epoch: 1, Batch: 1262, Loss: 0.6157, Elapsed: 5m38s
2020-04-30 19:33:50.511517: Epoch: 1, Batch: 1269, Loss: 0.6346, Elapsed: 6m27s
2020-04-30 19:38:13.472692: Epoch: 1, Batch: 1257, Loss: 0.5992, Elapsed: 8m45s
2020-04-30 19:40:31.462443: Epoch: 1, Batch: 1270, Loss: 0.6101, Elapsed: 6m40s
2020-04-30 19:42:29.650392: Epoch: 1, Batch: 1263, Loss: 0.6136, Elapsed: 9m28s
2020-04-30 19:46:22.520709: Epoch: 1, Batch: 1264, Loss: 0.5383, Elapsed: 3m52s
2020-04-30 19:46:46.186052: Epoch: 1, Batch: 1258, Loss: 0.5845, Elapsed: 8m32s
2020-04-30 19:47:55.896519: Epoch: 1, Batch: 1271, Loss: 0.5855, Elapsed: 7m24s
2020-04-30 19:50:53.625421: Epoch: 1, Batch: 1265, Loss: 0.5748, Elapsed: 4m31s
2020-04-30 19:51:05.616093: Epoch: 1, Batch: 1259, Loss: 0.5731, Elapsed: 4m19s
2020-04-30 19:55:02.296939: Epoch: 1, Batch: 1260, Loss: 0.5536, Elapsed: 3m56s
2020-04-30 19:56:20.797691: Epoch: 1, Batch: 1266, Loss: 0.5817, Elapsed: 5m27s
2020-04-30 19:56:50.665583: Epoch: 1, Batch: 1272, Loss: 0.6076, Elapsed: 8m54s
2020-04-30 20:00:54.104799: Epoch: 1, Batch: 1267, Loss: 0.5219, Elapsed: 4m33s
2020-04-30 20:01:56.212985: Epoch: 1, Batch: 1261, Loss: 0.5819, Elapsed: 6m53s
2020-04-30 20:05:03.927805: Epoch: 1, Batch: 1273, Loss: 0.6019, Elapsed: 8m13s
2020-04-30 20:08:11.544308: Epoch: 1, Batch: 1268, Loss: 0.6034, Elapsed: 7m17s
2020-04-30 20:08:59.898365: Epoch: 1, Batch: 1262, Loss: 0.5673, Elapsed: 7m3s
2020-04-30 20:12:46.054804: Epoch: 1, Batch: 1269, Loss: 0.6047, Elapsed: 4m34s
2020-04-30 20:14:04.648002: Epoch: 1, Batch: 1274, Loss: 0.5949, Elapsed: 9m0s
2020-04-30 20:14:34.635236: Epoch: 1, Batch: 1263, Loss: 0.5449, Elapsed: 5m34s
2020-04-30 20:17:36.517704: Epoch: 1, Batch: 1270, Loss: 0.5576, Elapsed: 4m50s
2020-04-30 20:18:42.570914: Epoch: 1, Batch: 1275, Loss: 0.5678, Elapsed: 4m37s
2020-04-30 20:20:21.955283: Epoch: 1, Batch: 1264, Loss: 0.4904, Elapsed: 5m47s
2020-04-30 20:24:30.249975: Epoch: 1, Batch: 1271, Loss: 0.6184, Elapsed: 6m53s
2020-04-30 20:25:20.230835: Epoch: 1, Batch: 1276, Loss: 0.6061, Elapsed: 6m37s
2020-04-30 20:26:17.038082: Epoch: 1, Batch: 1265, Loss: 0.5576, Elapsed: 5m55s
2020-04-30 20:28:49.115390: Epoch: 1, Batch: 1272, Loss: 0.5489, Elapsed: 4m18s
2020-04-30 20:32:21.891201: Epoch: 1, Batch: 1277, Loss: 0.5789, Elapsed: 7m1s
2020-04-30 20:33:31.200941: Epoch: 1, Batch: 1273, Loss: 0.5600, Elapsed: 4m42s
2020-04-30 20:35:01.812444: Epoch: 1, Batch: 1266, Loss: 0.6015, Elapsed: 8m44s
2020-04-30 20:37:12.855201: Epoch: 1, Batch: 1278, Loss: 0.6287, Elapsed: 4m50s
2020-04-30 20:38:44.552860: Epoch: 1, Batch: 1267, Loss: 0.4558, Elapsed: 3m42s
2020-04-30 20:40:09.898086: Epoch: 1, Batch: 1274, Loss: 0.5826, Elapsed: 6m38s
2020-04-30 20:41:25.562115: Epoch: 1, Batch: 1279, Loss: 0.5847, Elapsed: 4m12s
2020-04-30 20:45:24.593282: Epoch: 1, Batch: 1280, Loss: 0.4669, Elapsed: 3m59s
2020-04-30 20:47:28.312229: Epoch: 1, Batch: 1268, Loss: 0.5934, Elapsed: 8m43s
2020-04-30 20:47:48.950028: Epoch: 1, Batch: 1275, Loss: 0.6085, Elapsed: 7m39s
2020-04-30 20:52:30.409066: Epoch: 1, Batch: 1281, Loss: 0.6029, Elapsed: 7m5s
2020-04-30 20:55:11.760182: Epoch: 1, Batch: 1269, Loss: 0.5631, Elapsed: 7m43s
2020-04-30 20:55:38.997893: Epoch: 1, Batch: 1276, Loss: 0.6238, Elapsed: 7m50s
2020-04-30 21:00:28.339236: Epoch: 1, Batch: 1282, Loss: 0.6282, Elapsed: 7m57s
2020-04-30 21:02:58.241198: Epoch: 1, Batch: 1270, Loss: 0.5849, Elapsed: 7m46s
2020-04-30 21:03:31.630925: Epoch: 1, Batch: 1277, Loss: 0.5951, Elapsed: 7m52s
2020-04-30 21:06:08.951178: Epoch: 1, Batch: 1283, Loss: 0.5807, Elapsed: 5m40s
2020-04-30 21:06:54.543444: Epoch: 1, Batch: 1278, Loss: 0.5629, Elapsed: 3m22s
2020-04-30 21:09:17.755028: Epoch: 1, Batch: 1271, Loss: 0.5528, Elapsed: 6m19s
2020-04-30 21:11:13.662472: Epoch: 1, Batch: 1279, Loss: 0.5899, Elapsed: 4m19s
2020-04-30 21:13:43.459767: Epoch: 1, Batch: 1284, Loss: 0.5657, Elapsed: 7m34s
2020-04-30 21:14:06.308230: Epoch: 1, Batch: 1272, Loss: 0.5268, Elapsed: 4m48s
2020-04-30 21:15:56.901046: Epoch: 1, Batch: 1280, Loss: 0.5338, Elapsed: 4m43s
2020-04-30 21:20:51.158141: Epoch: 1, Batch: 1285, Loss: 0.5613, Elapsed: 7m7s
2020-04-30 21:21:06.598305: Epoch: 1, Batch: 1273, Loss: 0.5418, Elapsed: 7m0s
2020-04-30 21:21:39.803568: Epoch: 1, Batch: 1281, Loss: 0.5951, Elapsed: 5m42s
2020-04-30 21:25:08.651575: Epoch: 1, Batch: 1282, Loss: 0.5188, Elapsed: 3m28s
2020-04-30 21:26:53.272498: Epoch: 1, Batch: 1274, Loss: 0.5954, Elapsed: 5m46s
2020-04-30 21:29:06.467904: Epoch: 1, Batch: 1286, Loss: 0.5858, Elapsed: 8m15s
2020-04-30 21:31:51.112766: Epoch: 1, Batch: 1283, Loss: 0.6105, Elapsed: 6m42s
2020-04-30 21:34:11.558447: Epoch: 1, Batch: 1275, Loss: 0.5565, Elapsed: 7m18s
2020-04-30 21:34:14.928686: Epoch: 1, Batch: 1287, Loss: 0.5852, Elapsed: 5m8s
2020-04-30 21:37:07.122302: Epoch: 1, Batch: 1284, Loss: 0.5627, Elapsed: 5m15s
2020-04-30 21:40:12.345187: Epoch: 1, Batch: 1288, Loss: 0.5961, Elapsed: 5m57s
2020-04-30 21:42:02.732217: Epoch: 1, Batch: 1276, Loss: 0.5909, Elapsed: 7m51s
2020-04-30 21:45:50.270583: Epoch: 1, Batch: 1285, Loss: 0.6353, Elapsed: 8m43s
2020-04-30 21:48:11.712023: Epoch: 1, Batch: 1277, Loss: 0.5653, Elapsed: 6m8s
2020-04-30 21:49:58.298687: Epoch: 1, Batch: 1289, Loss: 0.6125, Elapsed: 9m45s
2020-04-30 21:53:02.483509: Epoch: 1, Batch: 1286, Loss: 0.5967, Elapsed: 7m12s
2020-04-30 21:54:07.250517: Epoch: 1, Batch: 1278, Loss: 0.5248, Elapsed: 5m55s
2020-04-30 21:56:39.555568: Epoch: 1, Batch: 1279, Loss: 0.4640, Elapsed: 2m32s
2020-04-30 21:57:01.569722: Epoch: 1, Batch: 1290, Loss: 0.5658, Elapsed: 7m3s
2020-04-30 21:57:30.147239: Epoch: 1, Batch: 1287, Loss: 0.5617, Elapsed: 4m27s
2020-04-30 22:01:45.663761: Epoch: 1, Batch: 1291, Loss: 0.5577, Elapsed: 4m44s
2020-04-30 22:03:51.818126: Epoch: 1, Batch: 1288, Loss: 0.5993, Elapsed: 6m21s
2020-04-30 22:06:00.298179: Epoch: 1, Batch: 1280, Loss: 0.5929, Elapsed: 9m20s
2020-04-30 22:06:20.432962: Epoch: 1, Batch: 1292, Loss: 0.5440, Elapsed: 4m34s
2020-04-30 22:08:29.348310: Epoch: 1, Batch: 1289, Loss: 0.5587, Elapsed: 4m37s
2020-04-30 22:12:00.477855: Epoch: 1, Batch: 1281, Loss: 0.5580, Elapsed: 6m0s
2020-04-30 22:12:12.011715: Epoch: 1, Batch: 1290, Loss: 0.5471, Elapsed: 3m42s
2020-04-30 22:14:51.809974: Epoch: 1, Batch: 1293, Loss: 0.6123, Elapsed: 8m31s
2020-04-30 22:17:38.994735: Epoch: 1, Batch: 1282, Loss: 0.5535, Elapsed: 5m38s
2020-04-30 22:18:26.780919: Epoch: 1, Batch: 1291, Loss: 0.5808, Elapsed: 6m14s
2020-04-30 22:19:52.382599: Epoch: 1, Batch: 1294, Loss: 0.5641, Elapsed: 5m0s
2020-04-30 22:21:55.563607: Epoch: 1, Batch: 1292, Loss: 0.5345, Elapsed: 3m28s
2020-04-30 22:24:37.732380: Epoch: 1, Batch: 1283, Loss: 0.5666, Elapsed: 6m58s
2020-04-30 22:25:35.230102: Epoch: 1, Batch: 1295, Loss: 0.5600, Elapsed: 5m42s
2020-04-30 22:29:39.676259: Epoch: 1, Batch: 1293, Loss: 0.5946, Elapsed: 7m44s
2020-04-30 22:30:14.747904: Epoch: 1, Batch: 1284, Loss: 0.5058, Elapsed: 5m36s
2020-04-30 22:32:30.091273: Epoch: 1, Batch: 1296, Loss: 0.5912, Elapsed: 6m54s
2020-04-30 22:38:25.193168: Epoch: 1, Batch: 1294, Loss: 0.6023, Elapsed: 8m45s
2020-04-30 22:40:31.591984: Epoch: 1, Batch: 1285, Loss: 0.6079, Elapsed: 10m16s
2020-04-30 22:41:55.047651: Epoch: 1, Batch: 1295, Loss: 0.5071, Elapsed: 3m29s
2020-04-30 22:42:06.243351: Epoch: 1, Batch: 1297, Loss: 0.6221, Elapsed: 9m36s
2020-04-30 22:47:35.869962: Epoch: 1, Batch: 1296, Loss: 0.5901, Elapsed: 5m40s
2020-04-30 22:49:34.107227: Epoch: 1, Batch: 1286, Loss: 0.6048, Elapsed: 9m2s
2020-04-30 22:54:01.534927: Epoch: 1, Batch: 1298, Loss: 0.6196, Elapsed: 11m55s
2020-04-30 22:57:41.463594: Epoch: 1, Batch: 1287, Loss: 0.5997, Elapsed: 8m7s
2020-04-30 22:59:17.631026: Epoch: 1, Batch: 1299, Loss: 0.5606, Elapsed: 5m16s
2020-04-30 23:02:18.117522: Epoch: 1, Batch: 1297, Loss: 0.6737, Elapsed: 14m42s
2020-04-30 23:02:23.440769: Epoch: 1, Batch: 1288, Loss: 0.5324, Elapsed: 4m41s
2020-04-30 23:04:52.808107: Epoch: 1, Batch: 1300, Loss: 0.5412, Elapsed: 5m35s
Starting testing the validation set with 200 subgraphs!
2020-04-30 23:07:24.510779: Epoch: 1, Batch: 1289, Loss: 0.5589, Elapsed: 5m1s
2020-04-30 23:10:15.868825: Epoch: 1, Batch: 1298, Loss: 0.6162, Elapsed: 7m57s
2020-04-30 23:13:49.844785: Epoch: 1, Batch: 1290, Loss: 0.6001, Elapsed: 6m25s
2020-04-30 23:17:17.520169: Epoch: 1, Batch: 1299, Loss: 0.5875, Elapsed: 7m1s
2020-04-30 23:20:44.557730: Epoch: 1, Batch: 1291, Loss: 0.6172, Elapsed: 6m54s
2020-04-30 23:25:39.307969: Epoch: 1, Batch: 1300, Loss: 0.5934, Elapsed: 8m21s
Starting testing the validation set with 200 subgraphs!
2020-04-30 23:28:33.442111: Epoch: 1, Batch: 1292, Loss: 0.6232, Elapsed: 7m48s
2020-04-30 23:32:08.027422: Epoch: 1, Batch: 1293, Loss: 0.5106, Elapsed: 3m34s
2020-04-30 23:37:24.357092: Epoch: 1, Batch: 1294, Loss: 0.5856, Elapsed: 5m16s
2020-04-30 23:42:24.812742: Epoch: 1, Batch: 1295, Loss: 0.5823, Elapsed: 5m0s
2020-04-30 23:44:46.481432: Validation Test:  Loss: 0.5851,  Acc: 69.3758, AUC: 0.7509, Precision: 0.8017 -- Elapsed: 39m53s
2020-04-30 23:49:07.872405: Epoch: 1, Batch: 1296, Loss: 0.5721, Elapsed: 6m43s
2020-04-30 23:53:53.116142: Epoch: 1, Batch: 1301, Loss: 0.5861, Elapsed: 9m6s
2020-04-30 23:54:38.594107: Epoch: 1, Batch: 1297, Loss: 0.6334, Elapsed: 5m30s
2020-04-30 23:59:47.520888: Epoch: 1, Batch: 1302, Loss: 0.5630, Elapsed: 5m54s
2020-05-01 00:03:48.438144: Epoch: 1, Batch: 1298, Loss: 0.5816, Elapsed: 9m9s
2020-05-01 00:04:43.787509: Epoch: 1, Batch: 1303, Loss: 0.5617, Elapsed: 4m56s
2020-05-01 00:05:32.751730: Validation Test:  Loss: 0.5942,  Acc: 69.4854, AUC: 0.7456, Precision: 0.7832 -- Elapsed: 39m53s
2020-05-01 00:09:39.557032: Epoch: 1, Batch: 1304, Loss: 0.5662, Elapsed: 4m55s
2020-05-01 00:10:54.271460: Epoch: 1, Batch: 1299, Loss: 0.5773, Elapsed: 7m5s
2020-05-01 00:11:14.619503: Epoch: 1, Batch: 1301, Loss: 0.6069, Elapsed: 5m41s
2020-05-01 00:14:49.931357: Epoch: 1, Batch: 1305, Loss: 0.6125, Elapsed: 5m10s
2020-05-01 00:15:25.838341: Epoch: 1, Batch: 1300, Loss: 0.5371, Elapsed: 4m31s
Starting testing the validation set with 200 subgraphs!
2020-05-01 00:15:35.032475: Epoch: 1, Batch: 1302, Loss: 0.5635, Elapsed: 4m20s
2020-05-01 00:18:20.803740: Epoch: 1, Batch: 1306, Loss: 0.5701, Elapsed: 3m30s
2020-05-01 00:23:44.132331: Epoch: 1, Batch: 1307, Loss: 0.5210, Elapsed: 5m23s
2020-05-01 00:25:20.151089: Epoch: 1, Batch: 1303, Loss: 0.6447, Elapsed: 9m45s
2020-05-01 00:27:20.734291: Epoch: 1, Batch: 1308, Loss: 0.5487, Elapsed: 3m36s
2020-05-01 00:31:18.869468: Epoch: 1, Batch: 1304, Loss: 0.5692, Elapsed: 5m58s
2020-05-01 00:33:51.620698: Epoch: 1, Batch: 1309, Loss: 0.5785, Elapsed: 6m30s
2020-05-01 00:36:48.470332: Epoch: 1, Batch: 1305, Loss: 0.5813, Elapsed: 5m29s
2020-05-01 00:41:06.119260: Epoch: 1, Batch: 1310, Loss: 0.6048, Elapsed: 7m14s
2020-05-01 00:44:38.054118: Epoch: 1, Batch: 1306, Loss: 0.6098, Elapsed: 7m49s
2020-05-01 00:48:14.524052: Epoch: 1, Batch: 1311, Loss: 0.5774, Elapsed: 7m8s
2020-05-01 00:49:12.549852: Epoch: 1, Batch: 1307, Loss: 0.6036, Elapsed: 4m34s
2020-05-01 00:54:04.242174: Epoch: 1, Batch: 1308, Loss: 0.5359, Elapsed: 4m51s
2020-05-01 00:54:38.183133: Epoch: 1, Batch: 1312, Loss: 0.6013, Elapsed: 6m23s
2020-05-01 00:57:55.319143: Validation Test:  Loss: 0.5766,  Acc: 70.3101, AUC: 0.7639, Precision: 0.8048 -- Elapsed: 42m29s
2020-05-01 01:02:59.234478: Epoch: 1, Batch: 1309, Loss: 0.5876, Elapsed: 8m54s
2020-05-01 01:04:40.186959: Epoch: 1, Batch: 1301, Loss: 0.5857, Elapsed: 6m44s
2020-05-01 01:06:11.073754: Epoch: 1, Batch: 1310, Loss: 0.5108, Elapsed: 3m11s
2020-05-01 01:06:37.794997: Epoch: 1, Batch: 1313, Loss: 0.5782, Elapsed: 11m59s
2020-05-01 01:11:50.449872: Epoch: 1, Batch: 1311, Loss: 0.5783, Elapsed: 5m39s
2020-05-01 01:12:19.070129: Epoch: 1, Batch: 1314, Loss: 0.5605, Elapsed: 5m41s
2020-05-01 01:12:42.684623: Epoch: 1, Batch: 1302, Loss: 0.6048, Elapsed: 8m2s
2020-05-01 01:19:08.886194: Epoch: 1, Batch: 1303, Loss: 0.5480, Elapsed: 6m26s
2020-05-01 01:19:26.245871: Epoch: 1, Batch: 1312, Loss: 0.5761, Elapsed: 7m35s
2020-05-01 01:24:13.529701: Epoch: 1, Batch: 1315, Loss: 0.6060, Elapsed: 11m54s
2020-05-01 01:25:55.896730: Epoch: 1, Batch: 1304, Loss: 0.5705, Elapsed: 6m46s
2020-05-01 01:27:32.697089: Epoch: 1, Batch: 1313, Loss: 0.5870, Elapsed: 8m6s
2020-05-01 01:31:21.493279: Epoch: 1, Batch: 1316, Loss: 0.5676, Elapsed: 7m7s
2020-05-01 01:34:16.450276: Epoch: 1, Batch: 1305, Loss: 0.5720, Elapsed: 8m20s
2020-05-01 01:37:22.194077: Epoch: 1, Batch: 1314, Loss: 0.5872, Elapsed: 9m49s
2020-05-01 01:37:38.774528: Epoch: 1, Batch: 1317, Loss: 0.5670, Elapsed: 6m17s
2020-05-01 01:40:52.685084: Epoch: 1, Batch: 1306, Loss: 0.5794, Elapsed: 6m36s
2020-05-01 01:42:41.329885: Epoch: 1, Batch: 1315, Loss: 0.5921, Elapsed: 5m19s
2020-05-01 01:45:08.835014: Epoch: 1, Batch: 1318, Loss: 0.6048, Elapsed: 7m30s
2020-05-01 01:51:45.684145: Epoch: 1, Batch: 1316, Loss: 0.6166, Elapsed: 9m4s
2020-05-01 01:51:56.708589: Epoch: 1, Batch: 1307, Loss: 0.5902, Elapsed: 11m4s
2020-05-01 01:52:53.517691: Epoch: 1, Batch: 1319, Loss: 0.5935, Elapsed: 7m44s
2020-05-01 01:57:34.833762: Epoch: 1, Batch: 1317, Loss: 0.5770, Elapsed: 5m49s
2020-05-01 02:01:15.353605: Epoch: 1, Batch: 1308, Loss: 0.6031, Elapsed: 9m18s
2020-05-01 02:03:27.090494: Epoch: 1, Batch: 1320, Loss: 0.5854, Elapsed: 10m33s
2020-05-01 02:05:20.533297: Epoch: 1, Batch: 1309, Loss: 0.5160, Elapsed: 4m5s
2020-05-01 02:06:31.101085: Epoch: 1, Batch: 1318, Loss: 0.6244, Elapsed: 8m56s
2020-05-01 02:07:36.586890: Epoch: 1, Batch: 1321, Loss: 0.5320, Elapsed: 4m9s
2020-05-01 02:10:31.280999: Epoch: 1, Batch: 1310, Loss: 0.5701, Elapsed: 5m10s
2020-05-01 02:14:11.815566: Epoch: 1, Batch: 1322, Loss: 0.5920, Elapsed: 6m35s
2020-05-01 02:14:18.390485: Epoch: 1, Batch: 1319, Loss: 0.6093, Elapsed: 7m47s
2020-05-01 02:17:46.525248: Epoch: 1, Batch: 1311, Loss: 0.5984, Elapsed: 7m15s
2020-05-01 02:20:43.560585: Epoch: 1, Batch: 1323, Loss: 0.5644, Elapsed: 6m31s
2020-05-01 02:23:37.241774: Epoch: 1, Batch: 1320, Loss: 0.6383, Elapsed: 9m18s
2020-05-01 02:23:53.485535: Epoch: 1, Batch: 1312, Loss: 0.5724, Elapsed: 6m6s
2020-05-01 02:24:10.220664: Epoch: 1, Batch: 1324, Loss: 0.5165, Elapsed: 3m26s
2020-05-01 02:27:54.734650: Epoch: 1, Batch: 1313, Loss: 0.5247, Elapsed: 4m1s
2020-05-01 02:31:32.952084: Epoch: 1, Batch: 1321, Loss: 0.6141, Elapsed: 7m55s
2020-05-01 02:32:05.095982: Epoch: 1, Batch: 1314, Loss: 0.5049, Elapsed: 4m10s
2020-05-01 02:32:54.869993: Epoch: 1, Batch: 1325, Loss: 0.5909, Elapsed: 8m44s
2020-05-01 02:37:50.779785: Epoch: 1, Batch: 1322, Loss: 0.5786, Elapsed: 6m17s
2020-05-01 02:38:02.258536: Epoch: 1, Batch: 1315, Loss: 0.6388, Elapsed: 5m57s
2020-05-01 02:40:16.927902: Epoch: 1, Batch: 1326, Loss: 0.6037, Elapsed: 7m22s
2020-05-01 02:41:38.268775: Epoch: 1, Batch: 1323, Loss: 0.5369, Elapsed: 3m47s
2020-05-01 02:47:24.415598: Epoch: 1, Batch: 1324, Loss: 0.5958, Elapsed: 5m46s
2020-05-01 02:48:44.432759: Epoch: 1, Batch: 1316, Loss: 0.6626, Elapsed: 10m42s
2020-05-01 02:49:56.090791: Epoch: 1, Batch: 1325, Loss: 0.5012, Elapsed: 2m31s
2020-05-01 02:49:54.700949: Epoch: 1, Batch: 1327, Loss: 0.5865, Elapsed: 9m37s
2020-05-01 02:54:35.989082: Epoch: 1, Batch: 1326, Loss: 0.5460, Elapsed: 4m39s
2020-05-01 02:57:59.391537: Epoch: 1, Batch: 1328, Loss: 0.5644, Elapsed: 8m4s
2020-05-01 02:58:48.161577: Epoch: 1, Batch: 1317, Loss: 0.6252, Elapsed: 10m3s
2020-05-01 03:00:53.887125: Epoch: 1, Batch: 1327, Loss: 0.5675, Elapsed: 6m17s
2020-05-01 03:01:39.146355: Epoch: 1, Batch: 1329, Loss: 0.5767, Elapsed: 3m39s
2020-05-01 03:07:19.272452: Epoch: 1, Batch: 1318, Loss: 0.5639, Elapsed: 8m31s
2020-05-01 03:07:56.063353: Epoch: 1, Batch: 1330, Loss: 0.6380, Elapsed: 6m16s
2020-05-01 03:08:11.244960: Epoch: 1, Batch: 1328, Loss: 0.6149, Elapsed: 7m17s
2020-05-01 03:11:38.510943: Epoch: 1, Batch: 1331, Loss: 0.5667, Elapsed: 3m42s
2020-05-01 03:15:05.256741: Epoch: 1, Batch: 1329, Loss: 0.5956, Elapsed: 6m53s
2020-05-01 03:15:04.544031: Epoch: 1, Batch: 1319, Loss: 0.5843, Elapsed: 7m45s
2020-05-01 03:17:31.453844: Epoch: 1, Batch: 1332, Loss: 0.5761, Elapsed: 5m52s
2020-05-01 03:21:53.114757: Epoch: 1, Batch: 1320, Loss: 0.5611, Elapsed: 6m48s
2020-05-01 03:24:40.606104: Epoch: 1, Batch: 1330, Loss: 0.6102, Elapsed: 9m35s
2020-05-01 03:24:55.139344: Epoch: 1, Batch: 1333, Loss: 0.5782, Elapsed: 7m23s
2020-05-01 03:27:45.810785: Epoch: 1, Batch: 1321, Loss: 0.5613, Elapsed: 5m52s
2020-05-01 03:30:25.549532: Epoch: 1, Batch: 1334, Loss: 0.5761, Elapsed: 5m30s
2020-05-01 03:30:40.481472: Epoch: 1, Batch: 1331, Loss: 0.5886, Elapsed: 5m59s
2020-05-01 03:33:27.478632: Epoch: 1, Batch: 1322, Loss: 0.5666, Elapsed: 5m41s
2020-05-01 03:37:10.180229: Epoch: 1, Batch: 1332, Loss: 0.6134, Elapsed: 6m29s
2020-05-01 03:39:38.367203: Epoch: 1, Batch: 1335, Loss: 0.5942, Elapsed: 9m12s
2020-05-01 03:40:12.507548: Epoch: 1, Batch: 1323, Loss: 0.6189, Elapsed: 6m45s
2020-05-01 03:43:27.405527: Epoch: 1, Batch: 1336, Loss: 0.5363, Elapsed: 3m49s
2020-05-01 03:45:19.994629: Epoch: 1, Batch: 1333, Loss: 0.6240, Elapsed: 8m9s
2020-05-01 03:47:16.182509: Epoch: 1, Batch: 1324, Loss: 0.6000, Elapsed: 7m3s
2020-05-01 03:48:29.381256: Epoch: 1, Batch: 1337, Loss: 0.5566, Elapsed: 5m1s
2020-05-01 03:52:26.953057: Epoch: 1, Batch: 1334, Loss: 0.5804, Elapsed: 7m6s
2020-05-01 03:53:31.047993: Epoch: 1, Batch: 1325, Loss: 0.5562, Elapsed: 6m14s
2020-05-01 03:53:47.311774: Epoch: 1, Batch: 1338, Loss: 0.5583, Elapsed: 5m17s
2020-05-01 03:59:31.149580: Epoch: 1, Batch: 1326, Loss: 0.5454, Elapsed: 6m0s
2020-05-01 04:00:18.341676: Epoch: 1, Batch: 1339, Loss: 0.5567, Elapsed: 6m31s
2020-05-01 04:00:29.691571: Epoch: 1, Batch: 1335, Loss: 0.5857, Elapsed: 8m2s
2020-05-01 04:05:12.950977: Epoch: 1, Batch: 1340, Loss: 0.5710, Elapsed: 4m54s
2020-05-01 04:05:26.045627: Epoch: 1, Batch: 1336, Loss: 0.5659, Elapsed: 4m56s
2020-05-01 04:05:32.084072: Epoch: 1, Batch: 1327, Loss: 0.5874, Elapsed: 6m0s
2020-05-01 04:12:44.838976: Epoch: 1, Batch: 1341, Loss: 0.5788, Elapsed: 7m31s
2020-05-01 04:13:16.106018: Epoch: 1, Batch: 1328, Loss: 0.5785, Elapsed: 7m43s
2020-05-01 04:15:32.958045: Epoch: 1, Batch: 1337, Loss: 0.6117, Elapsed: 10m6s
2020-05-01 04:18:12.487479: Epoch: 1, Batch: 1342, Loss: 0.5300, Elapsed: 5m27s
2020-05-01 04:20:45.120699: Epoch: 1, Batch: 1338, Loss: 0.5526, Elapsed: 5m12s
2020-05-01 04:24:36.536706: Epoch: 1, Batch: 1343, Loss: 0.5764, Elapsed: 6m24s
2020-05-01 04:25:41.710569: Epoch: 1, Batch: 1329, Loss: 0.6012, Elapsed: 12m25s
2020-05-01 04:30:32.379957: Epoch: 1, Batch: 1344, Loss: 0.5300, Elapsed: 5m55s
2020-05-01 04:30:57.854197: Epoch: 1, Batch: 1339, Loss: 0.6161, Elapsed: 10m12s
2020-05-01 04:32:52.466442: Epoch: 1, Batch: 1330, Loss: 0.5703, Elapsed: 7m10s
2020-05-01 04:34:27.364520: Epoch: 1, Batch: 1340, Loss: 0.5336, Elapsed: 3m29s
2020-05-01 04:35:20.949742: Epoch: 1, Batch: 1345, Loss: 0.5061, Elapsed: 4m48s
2020-05-01 04:37:58.395036: Epoch: 1, Batch: 1331, Loss: 0.5479, Elapsed: 5m5s
2020-05-01 04:41:25.706058: Epoch: 1, Batch: 1346, Loss: 0.6108, Elapsed: 6m4s
2020-05-01 04:42:49.732674: Epoch: 1, Batch: 1341, Loss: 0.5915, Elapsed: 8m22s
2020-05-01 04:44:47.696417: Epoch: 1, Batch: 1332, Loss: 0.5752, Elapsed: 6m49s
2020-05-01 04:47:10.370640: Epoch: 1, Batch: 1347, Loss: 0.5344, Elapsed: 5m44s
2020-05-01 04:47:47.000170: Epoch: 1, Batch: 1342, Loss: 0.5028, Elapsed: 4m57s
2020-05-01 04:52:21.547275: Epoch: 1, Batch: 1333, Loss: 0.5907, Elapsed: 7m33s
2020-05-01 04:53:16.959567: Epoch: 1, Batch: 1348, Loss: 0.5993, Elapsed: 6m6s
2020-05-01 04:54:06.256790: Epoch: 1, Batch: 1343, Loss: 0.6175, Elapsed: 6m19s
2020-05-01 04:58:42.256835: Epoch: 1, Batch: 1344, Loss: 0.5754, Elapsed: 4m35s
2020-05-01 04:59:43.865876: Epoch: 1, Batch: 1334, Loss: 0.5579, Elapsed: 7m22s
2020-05-01 05:01:07.689087: Epoch: 1, Batch: 1349, Loss: 0.5851, Elapsed: 7m50s
2020-05-01 05:06:53.087661: Epoch: 1, Batch: 1345, Loss: 0.5823, Elapsed: 8m10s
2020-05-01 05:11:08.004083: Epoch: 1, Batch: 1335, Loss: 0.6061, Elapsed: 11m24s
2020-05-01 05:12:44.823804: Epoch: 1, Batch: 1350, Loss: 0.6116, Elapsed: 11m37s
Starting testing the validation set with 200 subgraphs!
2020-05-01 05:14:37.060987: Epoch: 1, Batch: 1346, Loss: 0.5883, Elapsed: 7m43s
2020-05-01 05:19:55.647171: Epoch: 1, Batch: 1336, Loss: 0.5952, Elapsed: 8m47s
2020-05-01 05:23:52.257766: Epoch: 1, Batch: 1347, Loss: 0.6489, Elapsed: 9m15s
2020-05-01 05:24:35.843156: Epoch: 1, Batch: 1337, Loss: 0.5706, Elapsed: 4m40s
2020-05-01 05:28:17.046713: Epoch: 1, Batch: 1338, Loss: 0.5479, Elapsed: 3m41s
2020-05-01 05:28:58.755178: Epoch: 1, Batch: 1348, Loss: 0.5918, Elapsed: 5m6s
2020-05-01 05:32:50.222156: Epoch: 1, Batch: 1349, Loss: 0.5496, Elapsed: 3m51s
2020-05-01 05:33:25.495661: Epoch: 1, Batch: 1339, Loss: 0.5677, Elapsed: 5m8s
2020-05-01 05:40:29.096838: Epoch: 1, Batch: 1350, Loss: 0.5961, Elapsed: 7m38s
Starting testing the validation set with 200 subgraphs!
2020-05-01 05:40:38.924672: Epoch: 1, Batch: 1340, Loss: 0.5728, Elapsed: 7m13s
2020-05-01 05:43:43.606556: Epoch: 1, Batch: 1341, Loss: 0.4928, Elapsed: 3m4s
2020-05-01 05:50:41.514342: Epoch: 1, Batch: 1342, Loss: 0.6096, Elapsed: 6m57s
2020-05-01 05:53:24.556690: Validation Test:  Loss: 0.5816,  Acc: 69.9825, AUC: 0.7564, Precision: 0.8032 -- Elapsed: 40m39s
2020-05-01 05:56:09.231260: Epoch: 1, Batch: 1343, Loss: 0.5578, Elapsed: 5m27s
2020-05-01 06:01:05.346818: Epoch: 1, Batch: 1351, Loss: 0.6181, Elapsed: 7m40s
2020-05-01 06:03:47.421788: Epoch: 1, Batch: 1344, Loss: 0.6300, Elapsed: 7m38s
2020-05-01 06:06:55.334303: Epoch: 1, Batch: 1352, Loss: 0.5931, Elapsed: 5m49s
2020-05-01 06:12:00.882807: Epoch: 1, Batch: 1345, Loss: 0.6131, Elapsed: 8m13s
2020-05-01 06:12:49.929163: Epoch: 1, Batch: 1353, Loss: 0.5840, Elapsed: 5m54s
2020-05-01 06:21:09.985417: Validation Test:  Loss: 0.5910,  Acc: 69.6254, AUC: 0.7511, Precision: 0.7879 -- Elapsed: 40m40s
2020-05-01 06:21:44.524035: Epoch: 1, Batch: 1354, Loss: 0.5723, Elapsed: 8m54s
2020-05-01 06:22:25.125834: Epoch: 1, Batch: 1346, Loss: 0.6211, Elapsed: 10m24s
2020-05-01 06:26:03.981777: Epoch: 1, Batch: 1347, Loss: 0.5423, Elapsed: 3m38s
2020-05-01 06:26:15.482725: Epoch: 1, Batch: 1355, Loss: 0.5258, Elapsed: 4m30s
2020-05-01 06:29:23.815018: Epoch: 1, Batch: 1351, Loss: 0.5675, Elapsed: 8m13s
2020-05-01 06:30:33.920459: Epoch: 1, Batch: 1356, Loss: 0.5262, Elapsed: 4m18s
2020-05-01 06:32:24.485751: Epoch: 1, Batch: 1348, Loss: 0.5505, Elapsed: 6m20s
2020-05-01 06:33:42.539580: Epoch: 1, Batch: 1352, Loss: 0.5684, Elapsed: 4m18s
2020-05-01 06:36:24.569201: Epoch: 1, Batch: 1357, Loss: 0.5521, Elapsed: 5m50s
2020-05-01 06:38:57.393560: Epoch: 1, Batch: 1353, Loss: 0.5661, Elapsed: 5m14s
2020-05-01 06:39:33.631304: Epoch: 1, Batch: 1349, Loss: 0.5646, Elapsed: 7m9s
2020-05-01 06:42:00.517974: Epoch: 1, Batch: 1358, Loss: 0.5760, Elapsed: 5m35s
2020-05-01 06:44:49.801073: Epoch: 1, Batch: 1350, Loss: 0.5663, Elapsed: 5m16s
Starting testing the validation set with 200 subgraphs!
2020-05-01 06:46:10.317859: Epoch: 1, Batch: 1354, Loss: 0.5687, Elapsed: 7m12s
2020-05-01 06:46:12.085269: Epoch: 1, Batch: 1359, Loss: 0.5694, Elapsed: 4m11s
2020-05-01 06:52:28.632031: Epoch: 1, Batch: 1360, Loss: 0.5871, Elapsed: 6m16s
2020-05-01 06:56:55.942440: Epoch: 1, Batch: 1355, Loss: 0.6118, Elapsed: 10m45s
2020-05-01 07:00:09.358884: Epoch: 1, Batch: 1361, Loss: 0.5998, Elapsed: 7m40s
2020-05-01 07:01:22.398723: Epoch: 1, Batch: 1356, Loss: 0.5866, Elapsed: 4m26s
2020-05-01 07:06:10.226995: Epoch: 1, Batch: 1362, Loss: 0.5549, Elapsed: 6m0s
2020-05-01 07:07:05.623775: Epoch: 1, Batch: 1357, Loss: 0.6146, Elapsed: 5m43s
2020-05-01 07:10:41.894780: Epoch: 1, Batch: 1363, Loss: 0.4908, Elapsed: 4m31s
2020-05-01 07:15:13.142249: Epoch: 1, Batch: 1358, Loss: 0.5785, Elapsed: 8m7s
2020-05-01 07:19:03.399509: Epoch: 1, Batch: 1364, Loss: 0.5924, Elapsed: 8m21s
2020-05-01 07:23:44.443591: Epoch: 1, Batch: 1365, Loss: 0.5936, Elapsed: 4m41s
2020-05-01 07:23:59.448749: Epoch: 1, Batch: 1359, Loss: 0.6119, Elapsed: 8m46s
2020-05-01 07:27:12.744281: Validation Test:  Loss: 0.5846,  Acc: 70.3537, AUC: 0.7625, Precision: 0.7970 -- Elapsed: 42m22s
2020-05-01 07:30:04.984064: Epoch: 1, Batch: 1366, Loss: 0.5846, Elapsed: 6m20s
2020-05-01 07:31:04.211888: Epoch: 1, Batch: 1360, Loss: 0.5646, Elapsed: 7m4s
2020-05-01 07:34:27.353446: Epoch: 1, Batch: 1351, Loss: 0.5892, Elapsed: 7m14s
2020-05-01 07:36:35.090287: Epoch: 1, Batch: 1367, Loss: 0.5379, Elapsed: 6m30s
2020-05-01 07:39:07.596003: Epoch: 1, Batch: 1361, Loss: 0.6070, Elapsed: 8m3s
2020-05-01 07:43:17.886530: Epoch: 1, Batch: 1368, Loss: 0.5900, Elapsed: 6m42s
2020-05-01 07:44:14.994239: Epoch: 1, Batch: 1352, Loss: 0.6049, Elapsed: 9m47s
2020-05-01 07:45:13.252697: Epoch: 1, Batch: 1362, Loss: 0.5631, Elapsed: 6m5s
2020-05-01 07:48:50.840320: Epoch: 1, Batch: 1353, Loss: 0.5472, Elapsed: 4m35s
2020-05-01 07:50:30.653341: Epoch: 1, Batch: 1369, Loss: 0.5844, Elapsed: 7m12s
2020-05-01 07:56:57.752412: Epoch: 1, Batch: 1363, Loss: 0.5931, Elapsed: 11m44s
2020-05-01 07:57:46.815404: Epoch: 1, Batch: 1354, Loss: 0.5853, Elapsed: 8m55s
2020-05-01 08:00:30.387622: Epoch: 1, Batch: 1370, Loss: 0.6030, Elapsed: 9m59s
2020-05-01 08:02:33.888262: Epoch: 1, Batch: 1355, Loss: 0.5301, Elapsed: 4m47s
2020-05-01 08:05:54.175147: Epoch: 1, Batch: 1364, Loss: 0.5655, Elapsed: 8m56s
2020-05-01 08:07:49.192178: Epoch: 1, Batch: 1356, Loss: 0.5421, Elapsed: 5m15s
2020-05-01 08:11:23.630876: Epoch: 1, Batch: 1371, Loss: 0.6192, Elapsed: 10m53s
2020-05-01 08:11:48.463405: Epoch: 1, Batch: 1365, Loss: 0.5725, Elapsed: 5m54s
2020-05-01 08:14:15.110699: Epoch: 1, Batch: 1357, Loss: 0.6216, Elapsed: 6m25s
2020-05-01 08:17:07.025271: Epoch: 1, Batch: 1366, Loss: 0.5636, Elapsed: 5m18s
2020-05-01 08:22:59.060309: Epoch: 1, Batch: 1358, Loss: 0.5823, Elapsed: 8m43s
2020-05-01 08:23:17.306669: Epoch: 1, Batch: 1367, Loss: 0.5895, Elapsed: 6m10s
2020-05-01 08:23:43.539981: Epoch: 1, Batch: 1372, Loss: 0.6179, Elapsed: 12m19s
2020-05-01 08:29:38.671096: Epoch: 1, Batch: 1359, Loss: 0.5857, Elapsed: 6m39s
2020-05-01 08:29:49.886908: Epoch: 1, Batch: 1373, Loss: 0.5762, Elapsed: 6m6s
2020-05-01 08:30:29.368660: Epoch: 1, Batch: 1368, Loss: 0.6059, Elapsed: 7m12s
2020-05-01 08:37:10.397178: Epoch: 1, Batch: 1374, Loss: 0.5652, Elapsed: 7m20s
2020-05-01 08:37:26.481584: Epoch: 1, Batch: 1369, Loss: 0.5599, Elapsed: 6m57s
2020-05-01 08:39:31.833812: Epoch: 1, Batch: 1360, Loss: 0.6113, Elapsed: 9m53s
2020-05-01 08:45:14.082621: Epoch: 1, Batch: 1375, Loss: 0.6009, Elapsed: 8m3s
2020-05-01 08:46:15.271000: Epoch: 1, Batch: 1370, Loss: 0.6093, Elapsed: 8m48s
2020-05-01 08:46:14.481240: Epoch: 1, Batch: 1361, Loss: 0.5221, Elapsed: 6m42s
2020-05-01 08:51:30.680429: Epoch: 1, Batch: 1376, Loss: 0.5860, Elapsed: 6m16s
2020-05-01 08:54:31.454313: Epoch: 1, Batch: 1371, Loss: 0.6191, Elapsed: 8m16s
2020-05-01 08:57:33.681198: Epoch: 1, Batch: 1377, Loss: 0.5654, Elapsed: 6m2s
2020-05-01 08:58:01.486752: Epoch: 1, Batch: 1362, Loss: 0.6189, Elapsed: 11m46s
2020-05-01 09:02:25.111654: Epoch: 1, Batch: 1372, Loss: 0.5744, Elapsed: 7m53s
2020-05-01 09:06:44.020311: Epoch: 1, Batch: 1373, Loss: 0.5805, Elapsed: 4m18s
2020-05-01 09:07:53.603049: Epoch: 1, Batch: 1363, Loss: 0.5366, Elapsed: 9m52s
2020-05-01 09:12:42.485291: Epoch: 1, Batch: 1374, Loss: 0.5870, Elapsed: 5m58s
2020-05-01 09:12:49.083303: Epoch: 1, Batch: 1378, Loss: 0.6375, Elapsed: 15m15s
2020-05-01 09:14:01.514067: Epoch: 1, Batch: 1364, Loss: 0.5530, Elapsed: 6m7s
2020-05-01 09:19:12.213065: Epoch: 1, Batch: 1379, Loss: 0.5693, Elapsed: 6m23s
2020-05-01 09:21:47.275145: Epoch: 1, Batch: 1375, Loss: 0.6714, Elapsed: 9m4s
2020-05-01 09:22:01.597801: Epoch: 1, Batch: 1365, Loss: 0.6151, Elapsed: 8m0s
2020-05-01 09:26:41.159992: Epoch: 1, Batch: 1380, Loss: 0.6267, Elapsed: 7m28s
2020-05-01 09:28:45.416624: Epoch: 1, Batch: 1366, Loss: 0.5737, Elapsed: 6m43s
2020-05-01 09:31:17.334636: Epoch: 1, Batch: 1376, Loss: 0.6073, Elapsed: 9m30s
2020-05-01 09:35:16.516165: Epoch: 1, Batch: 1381, Loss: 0.5823, Elapsed: 8m35s
2020-05-01 09:35:26.967878: Epoch: 1, Batch: 1367, Loss: 0.6208, Elapsed: 6m41s
2020-05-01 09:40:37.559066: Epoch: 1, Batch: 1382, Loss: 0.5583, Elapsed: 5m21s
2020-05-01 09:42:45.379820: Epoch: 1, Batch: 1368, Loss: 0.5882, Elapsed: 7m18s
2020-05-01 09:43:32.830736: Epoch: 1, Batch: 1377, Loss: 0.5972, Elapsed: 12m15s
2020-05-01 09:46:05.375674: Epoch: 1, Batch: 1383, Loss: 0.5913, Elapsed: 5m27s
2020-05-01 09:49:39.165181: Epoch: 1, Batch: 1369, Loss: 0.6174, Elapsed: 6m53s
2020-05-01 09:50:44.591508: Epoch: 1, Batch: 1384, Loss: 0.5251, Elapsed: 4m39s
2020-05-01 09:53:09.614690: Epoch: 1, Batch: 1378, Loss: 0.5803, Elapsed: 9m36s
2020-05-01 09:54:30.625125: Epoch: 1, Batch: 1370, Loss: 0.5795, Elapsed: 4m51s
2020-05-01 09:59:49.873897: Epoch: 1, Batch: 1371, Loss: 0.5761, Elapsed: 5m19s
2020-05-01 10:00:31.627683: Epoch: 1, Batch: 1385, Loss: 0.6048, Elapsed: 9m47s
2020-05-01 10:01:19.952390: Epoch: 1, Batch: 1379, Loss: 0.6105, Elapsed: 8m10s
2020-05-01 10:05:13.100840: Epoch: 1, Batch: 1372, Loss: 0.5853, Elapsed: 5m23s
2020-05-01 10:06:00.691195: Epoch: 1, Batch: 1386, Loss: 0.5537, Elapsed: 5m29s
2020-05-01 10:07:58.444895: Epoch: 1, Batch: 1380, Loss: 0.5915, Elapsed: 6m38s
2020-05-01 10:08:38.964175: Epoch: 1, Batch: 1387, Loss: 0.5676, Elapsed: 2m38s
2020-05-01 10:11:37.677008: Epoch: 1, Batch: 1373, Loss: 0.5932, Elapsed: 6m24s
2020-05-01 10:14:37.421723: Epoch: 1, Batch: 1388, Loss: 0.5871, Elapsed: 5m58s
2020-05-01 10:15:03.191385: Epoch: 1, Batch: 1381, Loss: 0.6046, Elapsed: 7m4s
2020-05-01 10:17:22.245571: Epoch: 1, Batch: 1374, Loss: 0.5986, Elapsed: 5m44s
2020-05-01 10:22:32.483644: Epoch: 1, Batch: 1382, Loss: 0.6097, Elapsed: 7m29s
2020-05-01 10:24:50.331672: Epoch: 1, Batch: 1375, Loss: 0.6103, Elapsed: 7m28s
2020-05-01 10:26:05.816477: Epoch: 1, Batch: 1389, Loss: 0.6095, Elapsed: 11m28s
2020-05-01 10:29:00.066344: Epoch: 1, Batch: 1383, Loss: 0.5569, Elapsed: 6m27s
2020-05-01 10:32:27.582954: Epoch: 1, Batch: 1376, Loss: 0.6077, Elapsed: 7m37s
2020-05-01 10:35:13.117851: Epoch: 1, Batch: 1384, Loss: 0.6174, Elapsed: 6m13s
2020-05-01 10:36:27.339103: Epoch: 1, Batch: 1390, Loss: 0.6085, Elapsed: 10m21s
2020-05-01 10:38:19.441694: Epoch: 1, Batch: 1377, Loss: 0.6030, Elapsed: 5m51s
2020-05-01 10:38:54.970815: Epoch: 1, Batch: 1391, Loss: 0.4904, Elapsed: 2m27s
2020-05-01 10:43:10.956167: Epoch: 1, Batch: 1385, Loss: 0.5748, Elapsed: 7m57s
2020-05-01 10:47:41.557731: Epoch: 1, Batch: 1392, Loss: 0.6264, Elapsed: 8m46s
2020-05-01 10:48:29.232194: Epoch: 1, Batch: 1386, Loss: 0.5108, Elapsed: 5m18s
2020-05-01 10:55:18.451728: Epoch: 1, Batch: 1387, Loss: 0.6064, Elapsed: 6m49s
2020-05-01 10:58:00.312533: Epoch: 1, Batch: 1388, Loss: 0.5212, Elapsed: 2m41s
2020-05-01 10:59:07.339354: Epoch: 1, Batch: 1393, Loss: 0.6778, Elapsed: 11m25s
2020-05-01 11:02:28.981421: Epoch: 1, Batch: 1389, Loss: 0.5556, Elapsed: 4m28s
2020-05-01 11:04:44.452646: Epoch: 1, Batch: 1394, Loss: 0.5773, Elapsed: 5m37s
2020-05-01 11:06:40.061355: Epoch: 1, Batch: 1390, Loss: 0.5101, Elapsed: 4m11s
2020-05-01 11:06:57.883842: Epoch: 1, Batch: 1378, Loss: 0.7027, Elapsed: 28m38s
2020-05-01 11:08:30.107858: Epoch: 1, Batch: 1395, Loss: 0.5370, Elapsed: 3m45s
2020-05-01 11:09:38.997391: Epoch: 1, Batch: 1379, Loss: 0.5264, Elapsed: 2m41s
2020-05-01 11:12:02.167560: Epoch: 1, Batch: 1391, Loss: 0.5882, Elapsed: 5m22s
2020-05-01 11:17:28.213358: Epoch: 1, Batch: 1392, Loss: 0.5616, Elapsed: 5m26s
2020-05-01 11:17:56.552242: Epoch: 1, Batch: 1380, Loss: 0.6201, Elapsed: 8m17s
2020-05-01 11:19:50.097903: Epoch: 1, Batch: 1396, Loss: 0.6260, Elapsed: 11m19s
2020-05-01 11:22:53.852446: Epoch: 1, Batch: 1397, Loss: 0.5058, Elapsed: 3m3s
2020-05-01 11:23:39.176357: Epoch: 1, Batch: 1381, Loss: 0.5572, Elapsed: 5m42s
2020-05-01 11:25:59.031780: Epoch: 1, Batch: 1393, Loss: 0.6127, Elapsed: 8m30s
2020-05-01 11:27:30.473564: Epoch: 1, Batch: 1398, Loss: 0.5662, Elapsed: 4m36s
2020-05-01 11:28:39.380895: Epoch: 1, Batch: 1382, Loss: 0.5276, Elapsed: 5m0s
2020-05-01 11:30:08.073313: Epoch: 1, Batch: 1394, Loss: 0.5238, Elapsed: 4m9s
2020-05-01 11:32:48.707152: Epoch: 1, Batch: 1383, Loss: 0.5632, Elapsed: 4m9s
2020-05-01 11:35:06.120982: Epoch: 1, Batch: 1399, Loss: 0.6113, Elapsed: 7m35s
2020-05-01 11:36:49.704683: Epoch: 1, Batch: 1395, Loss: 0.5764, Elapsed: 6m41s
2020-05-01 11:38:49.652387: Epoch: 1, Batch: 1384, Loss: 0.5800, Elapsed: 6m0s
2020-05-01 11:42:36.438976: Epoch: 1, Batch: 1400, Loss: 0.5809, Elapsed: 7m30s
Starting testing the validation set with 200 subgraphs!
2020-05-01 11:45:33.838966: Epoch: 1, Batch: 1396, Loss: 0.5715, Elapsed: 8m44s
2020-05-01 11:47:35.789062: Epoch: 1, Batch: 1385, Loss: 0.6150, Elapsed: 8m46s
2020-05-01 11:49:45.715993: Epoch: 1, Batch: 1397, Loss: 0.5417, Elapsed: 4m11s
2020-05-01 11:53:31.410700: Epoch: 1, Batch: 1386, Loss: 0.5505, Elapsed: 5m55s
2020-05-01 11:54:07.336307: Epoch: 1, Batch: 1398, Loss: 0.5141, Elapsed: 4m21s
2020-05-01 11:58:57.869871: Epoch: 1, Batch: 1387, Loss: 0.6346, Elapsed: 5m26s
2020-05-01 11:59:38.118655: Epoch: 1, Batch: 1399, Loss: 0.5427, Elapsed: 5m30s
2020-05-01 12:03:17.812935: Epoch: 1, Batch: 1388, Loss: 0.5164, Elapsed: 4m19s
2020-05-01 12:04:22.367081: Epoch: 1, Batch: 1400, Loss: 0.5745, Elapsed: 4m44s
Starting testing the validation set with 200 subgraphs!
2020-05-01 12:10:04.224252: Epoch: 1, Batch: 1389, Loss: 0.6007, Elapsed: 6m46s
2020-05-01 12:15:02.794822: Epoch: 1, Batch: 1390, Loss: 0.5870, Elapsed: 4m58s
2020-05-01 12:18:14.046001: Epoch: 1, Batch: 1391, Loss: 0.5362, Elapsed: 3m11s
2020-05-01 12:22:43.607655: Validation Test:  Loss: 0.5880,  Acc: 69.1385, AUC: 0.7502, Precision: 0.7928 -- Elapsed: 40m7s
Starting testing the validation set with 200 subgraphs!
2020-05-01 12:24:01.439956: Epoch: 1, Batch: 1392, Loss: 0.5659, Elapsed: 5m47s
2020-05-01 12:31:19.459308: Epoch: 1, Batch: 1393, Loss: 0.6091, Elapsed: 7m17s
2020-05-01 12:34:57.303437: Epoch: 1, Batch: 1394, Loss: 0.5255, Elapsed: 3m37s
2020-05-01 12:40:44.675071: Epoch: 1, Batch: 1395, Loss: 0.6194, Elapsed: 5m47s
2020-05-01 12:42:31.716404: Validation Test:  Loss: 0.5919,  Acc: 69.2460, AUC: 0.7483, Precision: 0.7837 -- Elapsed: 38m9s
Starting testing the validation set with 200 subgraphs!
2020-05-01 12:47:17.630223: Epoch: 1, Batch: 1396, Loss: 0.5553, Elapsed: 6m32s
2020-05-01 12:51:50.719328: Epoch: 1, Batch: 1397, Loss: 0.5607, Elapsed: 4m33s
2020-05-01 13:00:50.903588: Validation Test:  Loss: 0.5880,  Acc: 69.1405, AUC: 0.7502, Precision: 0.7928 -- Elapsed: 38m7s
2020-05-01 13:00:50.903691: Training completed!
Singularity> 2020-05-01 13:04:15.869173: Epoch: 1, Batch: 1398, Loss: 0.5851, Elapsed: 12m25s
2020-05-01 13:10:16.967861: Epoch: 1, Batch: 1399, Loss: 0.6255, Elapsed: 6m1s
2020-05-01 13:13:45.009333: Epoch: 1, Batch: 1400, Loss: 0.5560, Elapsed: 3m28s
Starting testing the validation set with 200 subgraphs!
2020-05-01 13:19:26.943844: Validation Test:  Loss: 0.5919,  Acc: 69.2450, AUC: 0.7483, Precision: 0.7837 -- Elapsed: 36m55s
2020-05-01 13:19:26.943968: Training completed!
Singularity> 2020-05-01 13:45:36.137789: Validation Test:  Loss: 0.5809,  Acc: 69.8678, AUC: 0.7592, Precision: 0.7993 -- Elapsed: 31m51s
Starting testing the validation set with 200 subgraphs!
2020-05-01 14:16:57.487521: Validation Test:  Loss: 0.5809,  Acc: 69.8678, AUC: 0.7592, Precision: 0.7993 -- Elapsed: 31m21s
2020-05-01 14:16:57.487613: Training completed!
Singularity> [KSingularity> clear
[H[JSingularity> python3 tools/     rain.py tt est.py configs/rte  te   test.yaml 
usage: train.py [-h] config
train.py: error: unrecognized arguments: configs/test.yaml
Singularity> python3 train.py test.py configs/test.yaml [1P[1P[1P[1P[1P[1P[1P[1P
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:100: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/test/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/test/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
2020-05-04 00:31:16.976508 Deleted old log: logs/test/log_validation.csv
Starting testing the validation set with 200 subgraphs!
2020-05-04 01:02:39.861681: Validation Test:  Loss: 0.7334,  Acc: 49.6262, AUC: 0.4900, Precision: 0.5719 -- Elapsed: 31m20s
2020-05-04 01:02:39.861776: Training is starting!
2020-05-04 01:06:19.555552: Epoch: 1, Batch: 1, Loss: 0.7709, Elapsed: 3m39s
2020-05-04 01:10:25.225660: Epoch: 1, Batch: 2, Loss: 0.6994, Elapsed: 4m5s
2020-05-04 01:16:13.834301: Epoch: 1, Batch: 3, Loss: 0.7087, Elapsed: 5m48s
2020-05-04 01:20:50.071090: Epoch: 1, Batch: 4, Loss: 0.7160, Elapsed: 4m36s
2020-05-04 01:25:03.275963: Epoch: 1, Batch: 5, Loss: 0.7139, Elapsed: 4m13s
2020-05-04 01:28:27.619342: Epoch: 1, Batch: 6, Loss: 0.7106, Elapsed: 3m24s
2020-05-04 01:31:33.185669: Epoch: 1, Batch: 7, Loss: 0.7194, Elapsed: 3m5s
2020-05-04 01:36:42.175930: Epoch: 1, Batch: 8, Loss: 0.6824, Elapsed: 5m8s
2020-05-04 01:41:00.891261: Epoch: 1, Batch: 9, Loss: 0.7161, Elapsed: 4m18s
2020-05-04 01:44:53.528177: Epoch: 1, Batch: 10, Loss: 0.6880, Elapsed: 3m52s
2020-05-04 01:48:07.519110: Epoch: 1, Batch: 11, Loss: 0.7004, Elapsed: 3m13s
2020-05-04 01:51:47.004617: Epoch: 1, Batch: 12, Loss: 0.6890, Elapsed: 3m39s
2020-05-04 01:56:22.401345: Epoch: 1, Batch: 13, Loss: 0.6791, Elapsed: 4m35s
2020-05-04 01:59:15.657001: Epoch: 1, Batch: 14, Loss: 0.7155, Elapsed: 2m53s
2020-05-04 02:05:21.594623: Epoch: 1, Batch: 15, Loss: 0.6863, Elapsed: 6m5s
2020-05-04 02:10:47.715479: Epoch: 1, Batch: 16, Loss: 0.7037, Elapsed: 5m26s
2020-05-04 02:15:29.477051: Epoch: 1, Batch: 17, Loss: 0.6764, Elapsed: 4m41s
2020-05-04 02:19:51.914739: Epoch: 1, Batch: 18, Loss: 0.6857, Elapsed: 4m22s
2020-05-04 02:24:23.695866: Epoch: 1, Batch: 19, Loss: 0.6877, Elapsed: 4m31s
2020-05-04 02:29:02.099547: Epoch: 1, Batch: 20, Loss: 0.6769, Elapsed: 4m38s
2020-05-04 02:32:34.939489: Epoch: 1, Batch: 21, Loss: 0.6861, Elapsed: 3m32s
2020-05-04 02:37:55.854481: Epoch: 1, Batch: 22, Loss: 0.6935, Elapsed: 5m20s
2020-05-04 02:42:52.959242: Epoch: 1, Batch: 23, Loss: 0.6871, Elapsed: 4m57s
2020-05-04 02:47:18.930390: Epoch: 1, Batch: 24, Loss: 0.6807, Elapsed: 4m25s
2020-05-04 02:52:54.153863: Epoch: 1, Batch: 25, Loss: 0.6616, Elapsed: 5m35s
2020-05-04 02:57:23.927013: Epoch: 1, Batch: 26, Loss: 0.6911, Elapsed: 4m29s
2020-05-04 03:04:04.961957: Epoch: 1, Batch: 27, Loss: 0.6724, Elapsed: 6m41s
2020-05-04 03:07:23.478401: Epoch: 1, Batch: 28, Loss: 0.6750, Elapsed: 3m18s
2020-05-04 03:11:46.652555: Epoch: 1, Batch: 29, Loss: 0.6624, Elapsed: 4m23s
2020-05-04 03:15:55.085511: Epoch: 1, Batch: 30, Loss: 0.6731, Elapsed: 4m8s
2020-05-04 03:20:32.715400: Epoch: 1, Batch: 31, Loss: 0.6653, Elapsed: 4m37s
2020-05-04 03:24:50.152916: Epoch: 1, Batch: 32, Loss: 0.6504, Elapsed: 4m17s
2020-05-04 03:29:15.868951: Epoch: 1, Batch: 33, Loss: 0.6816, Elapsed: 4m25s
2020-05-04 03:34:26.618674: Epoch: 1, Batch: 34, Loss: 0.6731, Elapsed: 5m10s
2020-05-04 03:37:44.607788: Epoch: 1, Batch: 35, Loss: 0.7133, Elapsed: 3m17s
2020-05-04 03:41:57.225356: Epoch: 1, Batch: 36, Loss: 0.7001, Elapsed: 4m12s
2020-05-04 03:48:28.319528: Epoch: 1, Batch: 37, Loss: 0.6708, Elapsed: 6m31s
2020-05-04 03:52:35.281289: Epoch: 1, Batch: 38, Loss: 0.6665, Elapsed: 4m6s
2020-05-04 03:56:41.250730: Epoch: 1, Batch: 39, Loss: 0.6625, Elapsed: 4m5s
2020-05-04 04:00:23.928494: Epoch: 1, Batch: 40, Loss: 0.6934, Elapsed: 3m42s
2020-05-04 04:04:37.532626: Epoch: 1, Batch: 41, Loss: 0.6685, Elapsed: 4m13s
2020-05-04 04:08:03.736717: Epoch: 1, Batch: 42, Loss: 0.6602, Elapsed: 3m26s
2020-05-04 04:11:07.881365: Epoch: 1, Batch: 43, Loss: 0.6759, Elapsed: 3m4s
2020-05-04 04:12:57.191813: Epoch: 1, Batch: 44, Loss: 0.6494, Elapsed: 1m49s
2020-05-04 04:17:21.335480: Epoch: 1, Batch: 45, Loss: 0.6729, Elapsed: 4m24s
2020-05-04 04:20:29.556868: Epoch: 1, Batch: 46, Loss: 0.6483, Elapsed: 3m8s
2020-05-04 04:25:08.448617: Epoch: 1, Batch: 47, Loss: 0.7080, Elapsed: 4m38s
2020-05-04 04:29:05.327144: Epoch: 1, Batch: 48, Loss: 0.6668, Elapsed: 3m56s
[KSingularity> python3 train.py configs/gnn1_general/dim1_gen.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/gnn1_general/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/gnn1_general/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
[KSingularity> python3 train.py configs/di  comparisons/dnn/     q /qgnn/dimension_comparison/                                 2020-05-04 04:32:44.137566: Epoch: 1, Batch: 49, Loss: 0.6594, Elapsed: 3m38s
risons/qgnn/learning_rate_comparison/lr_3e-2.yaml Singularity> python3 train.py configs/compa[Krisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
2020-05-04 04:33:57.802985 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_params_NN.csv
2020-05-04 04:33:57.803293 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/summary.csv
2020-05-04 04:33:57.803557 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_params_IN.csv
2020-05-04 04:33:57.803812 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_grads_EN.csv
2020-05-04 04:33:57.804060 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_grads_NN.csv
2020-05-04 04:33:57.804312 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_params_EN.csv
2020-05-04 04:33:57.804559 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_grads_IN.csv
2020-05-04 04:33:57.804794 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_validation.csv
2020-05-04 04:33:57.805136 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_loss.csv
Starting testing the validation set with 200 subgraphs!
Traceback (most recent call last):
  File "train.py", line 68, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 195, in call
    H = self.InputNet(X) 
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 182, in call
    return tf.math.sigmoid(tf.matmul(arr, self.params))*2*np.pi
AttributeError: 'InputNet' object has no attribute 'params'
Singularity> 2020-05-04 04:36:02.645057: Epoch: 1, Batch: 50, Loss: 0.6535, Elapsed: 3m18s
Starting testing the validation set with 200 subgraphs!
python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
Traceback (most recent call last):
  File "train.py", line 68, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 198, in call
    e = self.EdgeNet(H, Ri, Ro)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 152, in call
    return edge_forward(B,self.theta_learn)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 125, in edge_forward
    out = tf.constant((1-TTN_edge_forward(edge_array[i,:],theta_learn[0,:]))/2.,dtype=tf.float64)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1138, in _SliceHelperVar
    return _slice_helper(var.value(), slice_spec, var)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 898, in _slice_helper
    name=name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1064, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py", line 9513, in strided_slice
    _ops.raise_from_not_ok_status(e, name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: GNN/EdgeNet/strided_slice/
Singularity> ^CTraceback (most recent call last):
  File "train.py", line 131, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN1.py", line 182, in call
    e = self.EdgeNet(H, Ri, Ro)             # execute EdgeNet one more time to obtain edge predictions
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN1.py", line 131, in call
    return edge_forward(B,self.theta_learn)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN1.py", line 104, in edge_forward
    out = tf.constant((1-TTN_edge_forward(edge_array[i,:],theta_learn))/2.,dtype=tf.float64)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 256, in __call__
    return self._d(self._f, a, k)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 210, in decorated
    return _eager_mode_decorator(wrapped, args, kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 406, in _eager_mode_decorator
    result, grad_fn = f(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/interfaces/tf.py", line 77, in _TFQNode
    res = qnode(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 606, in __call__
    return self.evaluate(args, **kwargs)  # args as one tuple
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/autograd/tracer.py", line 48, in f_wrapped
    return f_raw(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 637, in evaluate
    self.construct(shaped_args, kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 367, in construct
    self.circuit = CircuitGraph(self.ops, self.variable_deps)
KeyboardInterrupt
Singularity> python3 train.py configs/test.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/test/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/test/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
2020-05-04 04:37:20.694627 Deleted old log: logs/test/log_params_NN.csv
2020-05-04 04:37:20.694877 Deleted old log: logs/test/summary.csv
2020-05-04 04:37:20.695112 Deleted old log: logs/test/log_params_IN.csv
2020-05-04 04:37:20.695344 Deleted old log: logs/test/log_grads_EN.csv
2020-05-04 04:37:20.695579 Deleted old log: logs/test/log_grads_NN.csv
2020-05-04 04:37:20.695810 Deleted old log: logs/test/log_params_EN.csv
2020-05-04 04:37:20.696032 Deleted old log: logs/test/log_grads_IN.csv
2020-05-04 04:37:20.696275 Deleted old log: logs/test/log_validation.csv
2020-05-04 04:37:20.696505 Deleted old log: logs/test/log_loss.csv
Starting testing the validation set with 200 subgraphs!
^[[A^CTraceback (most recent call last):
  File "train.py", line 68, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN1.py", line 180, in call
    H = self.NodeNet(H, e, Ri, Ro)      # execute NodeNet using the output of EdgeNet
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN1.py", line 151, in call
    return node_forward(M,self.theta_learn)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN1.py", line 113, in node_forward
    out = tf.constant(4*np.pi*(1-TTN_node_forward(node_array[i,:],theta_learn))/2.,dtype=tf.float64)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 256, in __call__
    return self._d(self._f, a, k)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 210, in decorated
    return _eager_mode_decorator(wrapped, args, kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 406, in _eager_mode_decorator
    result, grad_fn = f(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/interfaces/tf.py", line 77, in _TFQNode
    res = qnode(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 606, in __call__
    return self.evaluate(args, **kwargs)  # args as one tuple
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/autograd/tracer.py", line 48, in f_wrapped
    return f_raw(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 637, in evaluate
    self.construct(shaped_args, kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 292, in construct
    res = self.func(*variables, **keyword_values)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN1.py", line 56, in TTN_node_forward
    qml.RY(node_array[i],wires=i)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/operation.py", line 552, in __init__
    super().__init__(*params, wires=wires, do_queue=do_queue)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/operation.py", line 255, in __init__
    self._name = self.__class__.__name__   #: str: name of the operator
KeyboardInterrupt
Singularity> python3 train.py configs/gnn1_general/dim1_gen.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/gnn1_general/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/gnn1_general/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
vim qnetworks/GNN2.py 
[?1049h[?1h=[1;71r[34l[34h[?25h[23m[24m[0m[H[J[?25l[71;1H"qnetworks/GNN2.py" 203L, 7262C[1;1H[33m134 [0m[16Cout = tf.constant([31m4[0m*np.pi*([31m1[0m-TTN_node_forward(node_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)
[33m135 [0m[16Coutputs.append(out)
[33m136 [8Creturn[0m tf.stack(outputs) [34m# output is between [0,4*pi][0m
[33m137 [0m[34m#################################################[0m
[33m138 class[0m [36mEdgeNet[0m(tf.keras.layers.Layer):
[33m139 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'EdgeNet'[0m):
[33m140 [0m[16C[36msuper[0m(EdgeNet, self).__init__(name=name)
[33m141 [0m[16C[34m# can only work with hid_dim = 2[0m
[33m142 [0m[16C[34m# read parameters of the network from file[0m
[33m143 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m144 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('EN',config)[0])[0m
[33m145 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m19[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m146 [8Cdef[0m [36mcall[0m(self,X, Ri, Ro):
[33m147 [0m[16Cbo = tf.matmul(Ro,X,transpose_a=[36mTrue[0m)
[33m148 [0m[16Cbi = tf.matmul(Ri,X,transpose_a=[36mTrue[0m)
[33m149 [0m[16C[34m# Shape of B = N_edges x 6 (2x (3 coordinates))[0m
[33m150 [0m[16C[34m# each row consists of two node that are possibly connected.[0m
[33m151 [0m[16CB  = tf.concat([bo, bi], axis=[31m1[0m)
[33m152 [16Creturn[0m edge_forward(B,self.theta_learn)
[33m153 [0m[34m#################################################[0m
[33m154 class[0m [36mNodeNet[0m(tf.keras.layers.Layer):
[33m155 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'NodeNet'[0m):
[33m156 [0m[16C[36msuper[0m(NodeNet, self).__init__(name=name)
[33m157 [0m[16C[34m# can only work with hid_dim = 1[0m
[33m158 [0m[16C[34m# read parameters of the network from file[0m
[33m159 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m160 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('NN',config)[0])[0m
[33m161 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m31[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m162 [8Cdef[0m [36mcall[0m(self, X, e, Ri, Ro):
[33m163 [0m[16Cbo  = tf.matmul(Ro, X, transpose_a=[36mTrue[0m)
[33m164 [0m[16Cbi  = tf.matmul(Ri, X, transpose_a=[36mTrue[0m)
[33m165 [0m[16CRwo = tf.math.multiply(Ro,e)
[33m166 [0m[16CRwi = tf.math.multiply(Ri,e)
[33m167 [0m[16Cmi = tf.matmul(Rwi, bo)
[33m168 [0m[16Cmo = tf.matmul(Rwo, bi)
[33m169 [0m[16CM = tf.concat([mi, mo, X], axis=[31m1[0m)
[33m170 [16Creturn[0m node_forward(M,self.theta_learn)
[33m171 [0m[34m#################################################[0m
[33m172 class[0m [36mInputNet[0m(tf.keras.layers.Layer):
[33m173 [8Cdef[0m [36m__init__[0m(self, config, name):
[33m174 [0m[16C[36msuper[0m(InputNet, self).__init__(name=name)
[33m175 [0m[16Cself.num_outputs = config[[31m'hid_dim'[0m] [34m# num_outputs = number of hidden dimensions[0m
[33m176 [0m[16C[34m# read parameters of the network from file[0m
[33m177 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m178 [0m[16C[34m#init = tf.constant_initializer(get_params('IN',config)[0])[0m
[33m179 [0m[16C[34m# setup a Dense layer with the given config[0m
[33m180 [0m[16Cself.layer = tf.keras.layers.Dense(self.num_outputs,input_shape=([31m3[0m,),activation=[31m'sigmoid'[0m)
[33m181 [8Cdef[0m [36mcall[0m(self, arr):
[33m182 [16Creturn[0m self.layer(arr)*[31m4[0m*np.pi [34m# to map to output to [0,2*pi][0m
[33m183 [0m[34m#################################################[0m
[33m184 class[0m [36mGNN[0m(tf.keras.Model):
[33m185 [8Cdef[0m [36m__init__[0m(self, config):
[33m186 [0m[16C[36msuper[0m(GNN, self).__init__(name=[31m'GNN'[0m)
[33m187 
188 [0m[16Cself.InputNet = InputNet(config,name=[31m'InputNet'[0m)
[33m189 [0m[16Cself.EdgeNet  = EdgeNet(config,name=[31m'EdgeNet'[0m)
[33m190 [0m[16Cself.NodeNet  = NodeNet(config,name=[31m'NodeNet'[0m)
[33m191 [0m[16Cself.n_iters = config[[31m'n_iters'[0m]
[33m192 
193 [8Cdef[0m [36mcall[0m(self, edge_array):
[33m194 [0m[16CX,Ri,Ro = edge_array
[33m195 [0m[16CH = self.InputNet(X)
[33m196 [0m[16CH = tf.concat([H,X],axis=[31m1[0m)
[33m197 [16Cfor[0m i [33min[0m [36mrange[0m(self.n_iters):
[33m198 [0m[24Ce = self.EdgeNet(H, Ri, Ro)
[33m199 [0m[24CH = self.NodeNet(H, e, Ri, Ro)
[33m200 [0m[24CH = tf.concat([H[:,[36mNone[0m],X], axis=[31m1[0m)
[33m201 [0m[16Ce = self.EdgeNet(H, Ri, Ro)
[33m202 [16Creturn[0m e
[33m203 [0m[34m#################################################[0m[71;118H182,3-17      Bot[49;21H[34h[?25h[?25l[48;21H[46m([9C)[0m[71;120H1,10-17[48;21H[34h[?25h[?25l([9C)[71;120H0,3-17 [47;21H[34h[?25h[?25l[71;119H79[46;21H[34h[?25h[?25l[71;120H8[45;21H[34h[?25h[?25l[71;120H7[44;21H[34h[?25h[?25l[71;120H6[43;21H[34h[?25h[?25l[71;120H5[42;21H[34h[?25h[?25l[71;120H4[41;21H[34h[?25h[?25l[71;120H3,10-17[40;21H[34h[?25h[?25l[71;120H2,17   [39;21H[34h[?25h[?25l[71;120H1[38;21H[34h[?25h[?25l[71;120H0,3-17[37;21H[34h[?25h[?25l[71;119H69[36;21H[34h[?25h[?25l[71;120H8[35;21H[34h[?25h[?25l[71;120H7[34;21H[34h[?25h[?25l[71;120H6[33;21H[34h[?25h[?25l[71;120H5[32;21H[34h[?25h[?25l[71;120H4[31;21H[34h[?25h[?25l[71;120H3[30;21H[34h[?25h[?25l[29;21H[46m([18C)[0m[71;120H2,10-17[29;21H[34h[?25h[?25l([18C)[71;120H1,3-17 [28;21H[34h[?25h[?25l[71;120H0[27;21H[34h[?25h[?25l[71;119H59[26;21H[34h[?25h[?25l[71;120H8[25;21H[34h[?25h[?25l[71;120H7[24;21H[34h[?25h[?25l[71;120H6[23;21H[34h[?25h[?25l[71;120H5,10-17[22;21H[34h[?25h[?25l[71;120H4,17   [21;21H[34h[?25h[?25l[71;120H3[20;21H[34h[?25h[?25l[71;120H2,3-17[19;21H[34h[?25h[?25l[71;120H1[18;21H[34h[?25h[?25l[71;120H0[17;21H[34h[?25h[?25l[71;119H49[16;21H[34h[?25h[?25l[71;120H8[15;21H[34h[?25h[?25l[71;120H7[14;21H[34h[?25h[?25l[13;21H[46m([14C)[0m[71;120H6,10-17[13;21H[34h[?25h[?25l([14C)[71;120H5,3-17 [12;21H[34h[?25h[?25l[71;120H4[11;21H[34h[?25h[?25l[71;120H3[10;21H[34h[?25h[?25l[71;120H2[9;21H[34h[?25h[?25l[71;120H1[8;21H[34h[?25h[?25l[71;120H0[7;21H[34h[?25h[?25l[71;119H39,10-17[6;21H[34h[?25h[?25l[71;120H8,17   [5;21H[34h[?25h[?25l[71;120H7[4;21H[34h[?25h[?25l[71;120H6,10-17[3;21H[34h[?25h[?25l[71;120H5,3-17 [2;21H[34h[?25h[?25l[71;120H4[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m133 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(node_array[:,[31m0[0m])):[71;1H[K[71;118H133,10-17     99%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m132 [0m[8Coutputs = [][71;118H[K[71;118H132,10-17     98%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m131 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H131,10-17     97%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m130 [0m[8C[34m# executes TTN_node circuit for each node in node_array[0m[71;118H[K[71;118H130,10-17     96%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m129 def[0m [36mnode_forward[0m[46m([0mnode_array,theta_learn[46m)[0m:[71;118H[K[71;118H129,17[8C96%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m128 [0m[34m#################################################[0m[2;21H([22C)[71;118H[K[71;118H128,17[8C95%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m127 [8Creturn[0m tf.stack(outputs)[71;118H[K[71;118H127,10-17     94%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m126 [0m[16Coutputs.append(out)[71;118H[K[71;118H126,3-17      93%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m125 [0m[16Cout = tf.constant(([31m1[0m-TTN_edge_forward(edge_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)[71;118H[K[71;118H125,3-17      93%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m124 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(edge_array[:,[31m0[0m])):[71;118H[K[71;118H124,10-17     92%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m123 [0m[8Coutputs = [][71;118H[K[71;118H123,10-17     91%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m122 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H122,10-17     90%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m121 [0m[8C[34m# executes TTN_edge circuit for each edge in edge_array[0m[71;118H[K[71;118H121,10-17     90%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m120 def[0m [36medge_forward[0m[46m([0medge_array,theta_learn[46m)[0m:[71;118H[K[71;118H120,17[8C89%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m119 [0m[34m#################################################[0m[2;21H([22C)[71;118H[K[71;118H119,17[8C88%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m118 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m5[0m)), qml.expval(qml.PauliZ(wires=[31m10[0m))[71;118H[K[71;118H118,10-17     87%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m117 [0m[71;118H[K[71;118H117,0-1[7C87%[1;5H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m116 [0m[8Cqml.RY(theta_learn[[31m30[0m],wires=[31m10[0m)[71;118H[K[71;118H116,10-17     86%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m115 [0m[8Cqml.RY(theta_learn[[31m29[0m],wires=[31m5[0m)[71;118H[K[71;118H115,10-17     85%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m114 [0m[8C[34m# Last Layer[0m[71;118H[K[71;118H114,10-17     84%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m113 [0m[8Cqml.CNOT[46m([0mwires=[[31m14[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H113,10-17     84%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m112 [0m[8Cqml.RY(theta_learn[[31m28[0m],wires=[31m14[0m)[2;21H([13C)[71;118H[K[71;118H112,10-17     83%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m111 [0m[8Cqml.RY(theta_learn[[31m27[0m],wires=[31m10[0m)[71;118H[K[71;118H111,10-17     82%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m110 [0m[8Cqml.CNOT[46m([0mwires=[[31m0[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H110,10-17     81%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m109 [0m[8Cqml.RY(theta_learn[[31m26[0m],wires=[31m5[0m)[2;21H([11C)[71;118H[K[71;118H109,10-17     81%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m108 [0m[8Cqml.RY(theta_learn[[31m25[0m],wires=[31m0[0m)[71;118H[K[71;118H108,10-17     80%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m107 [0m[8C[34m# Fifth Layer[0m[71;118H[K[71;118H107,10-17     79%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m106 [0m[8Cqml.RY(theta_learn[[31m24[0m],wires=[31m10[0m)[71;118H[K[71;118H106,10-17     78%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m105 [0m[8Cqml.RY(theta_learn[[31m23[0m],wires=[31m5[0m)[71;118H[K[71;118H105,10-17     78%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m104 [0m[8C[34m# Forth Layer[0m[71;118H[K[71;118H104,10-17     77%[1;21H[34h[?25h[?25l[71;120H5[2;21H[34h[?25h[?25l[71;120H6[3;21H[34h[?25h[?25l[71;120H7[4;21H[34h[?25h[?25l[71;120H8[5;21H[34h[?25h[?25l[71;120H9[6;21H[34h[?25h[?25l[7;21H[46m([11C)[0m[71;119H10[7;21H[34h[?25h[?25l([11C)[71;120H1[8;21H[34h[?25h[?25l[71;120H2[9;21H[34h[?25h[?25l[10;21H[46m([13C)[0m[71;120H3[10;21H[34h[?25h[?25l([13C)[71;120H4[11;21H[34h[?25h[?25l[71;120H5[12;21H[34h[?25h[?25l[71;120H6[13;21H[34h[?25h[?25l[71;120H7,0-1  [14;5H[34h[?25h[?25l[71;120H8,10-17[15;21H[34h[?25h[?25l[71;120H9,17   [16;21H[34h[?25h[?25l[17;21H[46m([22C)[0m[71;119H20[17;21H[34h[?25h[?25l([22C)[71;120H1,10-17[18;21H[34h[?25h[?25l[71;120H2[19;21H[34h[?25h[?25l[71;120H3[20;21H[34h[?25h[?25l[71;120H4[21;21H[34h[?25h[?25l[71;120H5,3-17 [22;21H[34h[?25h[?25l[71;120H6[23;21H[34h[?25h[?25l[71;120H7,10-17[24;21H[34h[?25h[?25l[71;120H8,17   [25;21H[34h[?25h[?25l[26;21H[46m([22C)[0m[71;120H9[26;21H[34h[?25h[?25l([22C)[71;119H30,10-17[27;21H[34h[?25h[?25l[71;120H1[28;21H[34h[?25h[?25l[71;120H2[29;21H[34h[?25h[?25l[71;120H3[30;21H[34h[?25h[?25l[71;120H4,3-17 [31;21H[34h[?25h[?25l[71;120H5[32;21H[34h[?25h[?25l[71;120H6,10-17[33;21H[34h[?25h[?25l[71;120H7,17   [34;21H[34h[?25h[?25l[71;120H8[35;21H[34h[?25h[?25l[71;120H9,10-17[36;21H[34h[?25h[?25l[71;119H40,3-17 [37;21H[34h[?25h[?25l[71;120H1[38;21H[34h[?25h[?25l[71;120H2[39;21H[34h[?25h[?25l[71;120H3[40;21H[34h[?25h[?25l[71;120H4[41;21H[34h[?25h[?25l[71;120H5[42;21H[34h[?25h[?25l[43;21H[46m([14C)[0m[71;120H6,10-17[43;21H[34h[?25h[?25l([14C)[71;120H7,3-17 [44;21H[34h[?25h[?25l[71;120H8[45;21H[34h[?25h[?25l[71;120H9[46;21H[34h[?25h[?25l[71;119H50[47;21H[34h[?25h[?25l[71;120H1[48;21H[34h[?25h[?25l[71;120H2[49;21H[34h[?25h[?25l[71;120H3,17  [50;21H[34h[?25h[?25l[71;120H4[51;21H[34h[?25h[?25l[71;120H5,10-17[52;21H[34h[?25h[?25l[71;120H6,3-17 [53;21H[34h[?25h[?25l[71;120H7[54;21H[34h[?25h[?25l[71;120H8[55;21H[34h[?25h[?25l[71;120H9[56;21H[34h[?25h[?25l[71;119H60[57;21H[34h[?25h[?25l[71;120H1[58;21H[34h[?25h[?25l[59;21H[46m([18C)[0m[71;120H2,10-17[59;21H[34h[?25h[?25l([18C)[71;120H3,3-17 [60;21H[34h[?25h[?25l[71;120H4[61;21H[34h[?25h[?25l[71;120H5[62;21H[34h[?25h[?25l[71;120H6[63;21H[34h[?25h[?25l[71;120H7[64;21H[34h[?25h[?25l[71;120H8[65;21H[34h[?25h[?25l[71;120H9[66;21H[34h[?25h[?25l[71;119H70[67;21H[34h[?25h[?25l[71;120H1,17  [68;21H[34h[?25h[?25l[71;120H2[69;21H[34h[?25h[?25l[71;120H3,10-17[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m174 [0m[16C[36msuper[0m(InputNet, self).__init__(name=name)[71;118H[K[71;118H174,3-17      78%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m175 [0m[16Cself.num_outputs = config[[31m'hid_dim'[0m] [34m# num_outputs = number of hidden dimensions[0m[71;118H[K[71;118H175,3-17      78%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m176 [0m[16C[34m# read parameters of the network from file[0m[71;118H[K[71;118H176,3-17      79%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m177 [0m[16C[34m# params are created using tools/init_params.py[0m[71;118H[K[71;118H177,3-17      80%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m178 [0m[16C[34m#init = tf.constant_initializer(get_params('IN',config)[0])[0m[71;118H[K[71;118H178,3-17      81%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m179 [0m[16C[34m# setup a Dense layer with the given config[0m[71;118H[K[71;118H179,3-17      81%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m180 [0m[16Cself.layer = tf.keras.layers.Dense(self.num_outputs,input_shape=([31m3[0m,),activation=[31m'sigmoid'[0m)[71;118H[K[71;118H180,3-17      82%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m181 [8Cdef[0m [36mcall[0m[46m([0mself, arr[46m)[0m:[71;118H[K[71;118H181,10-17     83%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([9C)
[33m182 [16Creturn[0m self.layer(arr)*[31m4[0m*np.pi [34m# to map to output to [0,2*pi][0m[71;118H[K[71;118H182,3-17      84%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m183 [0m[34m#################################################[0m[71;118H[K[71;118H183,17[8C84%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m184 class[0m [36mGNN[0m(tf.keras.Model):[71;118H[K[71;118H184,17[8C85%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m185 [8Cdef[0m [36m__init__[0m(self, config):[71;118H[K[71;118H185,10-17     86%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m186 [0m[16C[36msuper[0m(GNN, self).__init__(name=[31m'GNN'[0m)[71;118H[K[71;118H186,3-17      87%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m187 [0m[71;118H[K[71;118H187,0-1[7C87%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m188 [0m[16Cself.InputNet = InputNet(config,name=[31m'InputNet'[0m)[71;118H[K[71;118H188,3-17      88%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m189 [0m[16Cself.EdgeNet  = EdgeNet(config,name=[31m'EdgeNet'[0m)[71;118H[K[71;118H189,3-17      89%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m190 [0m[16Cself.NodeNet  = NodeNet(config,name=[31m'NodeNet'[0m)[71;118H[K[71;118H190,3-17      90%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m191 [0m[16Cself.n_iters = config[[31m'n_iters'[0m][71;118H[K[71;118H191,3-17      90%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m192 [0m[71;118H[K[71;118H192,0-1[7C91%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m193 [8Cdef[0m [36mcall[0m[46m([0mself, edge_array[46m)[0m:[71;118H[K[71;118H193,10-17     92%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([16C)
[33m194 [0m[16CX,Ri,Ro = edge_array[71;118H[K[71;118H194,3-17      93%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m195 [0m[16CH = self.InputNet(X)[71;118H[K[71;118H195,3-17      93%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m196 [0m[16CH = tf.concat([H,X],axis=[31m1[0m)[71;118H[K[71;118H196,3-17      94%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m197 [16Cfor[0m i [33min[0m [36mrange[0m(self.n_iters):[71;118H[K[71;118H197,3-17      95%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m198 [0m[24Ce = self.EdgeNet(H, Ri, Ro)[71;118H[K[71;118H198,3-24      96%[70;28H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m199 [0m[24CH = self.NodeNet(H, e, Ri, Ro)[71;118H[K[71;118H199,3-24      96%[70;28H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m200 [0m[24CH = tf.concat([H[:,[36mNone[0m],X], axis=[31m1[0m)[71;118H[K[71;118H200,3-24      97%[70;28H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m201 [0m[16Ce = self.EdgeNet(H, Ri, Ro)[71;118H[K[71;118H201,3-17      98%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m202 [16Creturn[0m e[71;118H[K[71;118H202,3-17      99%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m203 [0m[34m#################################################[0m[71;118H[K[71;118H203,17[8CBot[70;21H[34h[?25h[?25l[71;120H2,3-17[69;21H[34h[?25h[?25l[71;120H1[68;21H[34h[?25h[?25l[71;122H4-18[68;22H[34h[?25h[?25l[71;122H5-19[68;23H[34h[?25h[?25l[71;122H6-20[68;24H[34h[?25h[?25l[71;122H7-21[68;25H[34h[?25h[?25l[71;122H8-22[68;26H[34h[?25h[?25l[71;122H9-23[68;27H[34h[?25h[?25l[71;122H10-24[68;28H[34h[?25h[?25l[71;123H1-25[68;29H[34h[?25h[?25l[71;123H2-26[68;30H[34h[?25h[?25l[71;123H3-27[68;31H[34h[?25h[?25l[71;123H4-28[68;32H[34h[?25h[?25l[71;123H5-29[68;33H[34h[?25h[?25l[71;123H6-30[68;34H[34h[?25h[?25l[71;123H7-31[68;35H[34h[?25h[?25l[71;123H8-32[68;36H[34h[?25h[?25lt[46m([9C)[0m[71;123H9-33[68;37H[34h[?25h[?25l([9C)[71;122H20-34[68;38H[34h[?25h[?25l[71;120H0,13[67;38H[34h[?25h[?25l[71;118H199[66;38H[34h[?25h[?25l[71;120H8[65;38H[34h[?25h[?25l[71;120H7,20[64;38H[34h[?25h[?25l[71;120H6[63;38H[34h[?25h[?25l[71;122H19-33[63;37H[34h[?25h[?25l[71;1H[1m-- INSERT --[0m[71;118H[K[71;118H196,19-33     Bot[63;37H[34h[?25h[?25l:,X],axis=[31m1[0m)[71;122H20-34[63;38H[34h[?25h[?25l,,X],axis=[31m1[0m)[71;123H1-35[63;39H[34h[?25h[?25lN,X],axis=[31m1[0m)[71;123H2-36[63;40H[34h[?25h[?25lo,X],axis=[31m1[0m)[71;123H3-37[63;41H[34h[?25h[?25ln,X],axis=[31m1[0m)[71;123H4-38[63;42H[34h[?25h[?25l[36mNone[0m,X],axis=[31m1[0m)[71;123H5-39[63;43H[34h[?25h[?25lNon,X],axis=[31m1[0m)[63;53H[K[71;123H4-38[63;42H[34h[?25h[?25l,X],axis=[31m1[0m)[63;52H[K[71;123H3-37[63;41H[34h[?25h[?25l,X],axis=[31m1[0m)[63;51H[K[71;123H2-36[63;40H[34h[?25h[?25l,X],axis=[31m1[0m)[63;50H[K[71;123H1-35[63;39H[34h[?25h[?25lX],axis=[31m1[0m)[63;49H[K[71;123H0-34[63;38H[34h[?25h[?25l,X],axis=[31m1[0m)[63;48H[K[71;122H19-33[63;37H[34h[?25h[?25l[71;120H7[64;37H[34h[?25h[?25l[71;120H8,12[65;37H[34h[?25h[?25l[71;120H9[66;37H[34h[?25h[?25l[71;118H200[67;37H[34h[?25h[?25l[71;123H3-34[67;38H[34h[?25h[?25l[71;123H4-35[67;39H[34h[?25h[?25l[71;123H5-36[67;40H[34h[?25h[?25l[71;123H6-37[67;41H[34h[?25h[?25lt[46m([21C)[0m[71;123H7-38[67;42H[34h[?25h[?25l([46m[[11C][0m[8C)[71;123H8-39[67;43H[34h[?25h[?25l[71;123H9-40[67;44H[34h[?25h[?25l[H[46m[[6C][0m,X][71;122H20-41[67;45H[34h[?25h[?25l[71;123H1-42[67;46H[34h[?25h[?25l[[6C][71;123H2-43[67;47H[34h[?25h[?25l[71;123H3-44[67;48H[34h[?25h[?25l[71;123H4-45[67;49H[34h[?25h[?25l[71;123H5-46[67;50H[34h[?25h[?25l[71;123H6-47[67;51H[34h[?25h[?25l[46m[[6C][0m[71;123H7-48[67;52H[34h[?25h[?25lNon][46m,[0mX], axis=[31m1[0m)[67;64H[K[67;51H[46m][0m,[71;123H6-47[67;51H[34h[?25h[?25l][46m,[0mX], axis=[31m1[0m)[67;63H[K[67;50H[46m][0m,[71;123H5-46[67;50H[34h[?25h[?25l][46m,[0mX], axis=[31m1[0m)[67;62H[K[67;49H[46m][0m,[71;123H4-45[67;49H[34h[?25h[?25l][46m,[0mX], axis=[31m1[0m)[67;61H[K[67;48H[46m][0m,[71;123H3-44[67;48H[34h[?25h[?25l][46m,[0mX], axis=[31m1[0m)[67;60H[K[67;47H[46m][0m,[71;123H2-43[67;47H[34h[?25h[?25l][46m,[0mX], axis=[31m1[0m)[67;59H[K[67;46H[46m][0m,[71;123H1-42[67;46H[34h[?25h[?25l[71;123H2-43[67;47H[34h[?25h[?25l[46m,[0mX], axis=[31m1[0m)[67;58H[K[67;46H,X[46m][0m[71;123H1-42[67;46H[34h[?25h[?25l[46m,[0mX][46m,[0m axis=[31m1[0m)[67;57H[K[67;45H,X],[71;123H0-41[67;45H[34h[?25h[?25l[66;45H[46m([12C)[0m[71;118H199[66;45H[34h[?25h[?25l[65;45H[46m([9C)[0m[66;45H([12C)[71;120H8[65;45H[34h[?25h[71;1H[K[?25l[65;45H([9C)[71;118H198,19-40     Bot[65;44H[34h[?25h[?25l[71;118H[K[71;1H:[34h[?25hqw[?25l[71;3H[K[71;3H[34h[?25h[?25l[71;2H[K[71;2H[34h[?25hwq[?25l"qnetworks/GNN2.py" 203L, 7254C written
[?1l>[34h[?25h[?1049lSingularity> vim qnetworks/GNN2.py Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
Traceback (most recent call last):
  File "train.py", line 68, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 198, in call
    e = self.EdgeNet(H, Ri, Ro)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 152, in call
    return edge_forward(B,self.theta_learn)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 125, in edge_forward
    out = tf.constant((1-TTN_edge_forward(edge_array[i,:],theta_learn[0,:]))/2.,dtype=tf.float64)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1138, in _SliceHelperVar
    return _slice_helper(var.value(), slice_spec, var)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 898, in _slice_helper
    name=name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1064, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py", line 9513, in strided_slice
    _ops.raise_from_not_ok_status(e, name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: GNN/EdgeNet/strided_slice/
Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml Singularity> vim qnetworks/GNN2.py [K
[?1049h[?1h=[1;71r[34l[34h[?25h[23m[24m[0m[H[J[?25l[71;1H"qnetworks/GNN2.py" 203L, 7254C[1;1H[33m134 [0m[16Cout = tf.constant([31m4[0m*np.pi*([31m1[0m-TTN_node_forward(node_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)
[33m135 [0m[16Coutputs.append(out)
[33m136 [8Creturn[0m tf.stack(outputs) [34m# output is between [0,4*pi][0m
[33m137 [0m[34m#################################################[0m
[33m138 class[0m [36mEdgeNet[0m(tf.keras.layers.Layer):
[33m139 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'EdgeNet'[0m):
[33m140 [0m[16C[36msuper[0m(EdgeNet, self).__init__(name=name)
[33m141 [0m[16C[34m# can only work with hid_dim = 2[0m
[33m142 [0m[16C[34m# read parameters of the network from file[0m
[33m143 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m144 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('EN',config)[0])[0m
[33m145 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m19[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m146 [8Cdef[0m [36mcall[0m(self,X, Ri, Ro):
[33m147 [0m[16Cbo = tf.matmul(Ro,X,transpose_a=[36mTrue[0m)
[33m148 [0m[16Cbi = tf.matmul(Ri,X,transpose_a=[36mTrue[0m)
[33m149 [0m[16C[34m# Shape of B = N_edges x 6 (2x (3 coordinates))[0m
[33m150 [0m[16C[34m# each row consists of two node that are possibly connected.[0m
[33m151 [0m[16CB  = tf.concat([bo, bi], axis=[31m1[0m)
[33m152 [16Creturn[0m edge_forward(B,self.theta_learn)
[33m153 [0m[34m#################################################[0m
[33m154 class[0m [36mNodeNet[0m(tf.keras.layers.Layer):
[33m155 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'NodeNet'[0m):
[33m156 [0m[16C[36msuper[0m(NodeNet, self).__init__(name=name)
[33m157 [0m[16C[34m# can only work with hid_dim = 1[0m
[33m158 [0m[16C[34m# read parameters of the network from file[0m
[33m159 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m160 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('NN',config)[0])[0m
[33m161 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m31[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m162 [8Cdef[0m [36mcall[0m(self, X, e, Ri, Ro):
[33m163 [0m[16Cbo  = tf.matmul(Ro, X, transpose_a=[36mTrue[0m)
[33m164 [0m[16Cbi  = tf.matmul(Ri, X, transpose_a=[36mTrue[0m)
[33m165 [0m[16CRwo = tf.math.multiply(Ro,e)
[33m166 [0m[16CRwi = tf.math.multiply(Ri,e)
[33m167 [0m[16Cmi = tf.matmul(Rwi, bo)
[33m168 [0m[16Cmo = tf.matmul(Rwo, bi)
[33m169 [0m[16CM = tf.concat([mi, mo, X], axis=[31m1[0m)
[33m170 [16Creturn[0m node_forward(M,self.theta_learn)
[33m171 [0m[34m#################################################[0m
[33m172 class[0m [36mInputNet[0m(tf.keras.layers.Layer):
[33m173 [8Cdef[0m [36m__init__[0m(self, config, name):
[33m174 [0m[16C[36msuper[0m(InputNet, self).__init__(name=name)
[33m175 [0m[16Cself.num_outputs = config[[31m'hid_dim'[0m] [34m# num_outputs = number of hidden dimensions[0m
[33m176 [0m[16C[34m# read parameters of the network from file[0m
[33m177 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m178 [0m[16C[34m#init = tf.constant_initializer(get_params('IN',config)[0])[0m
[33m179 [0m[16C[34m# setup a Dense layer with the given config[0m
[33m180 [0m[16Cself.layer = tf.keras.layers.Dense(self.num_outputs,input_shape=([31m3[0m,),activation=[31m'sigmoid'[0m)
[33m181 [8Cdef[0m [36mcall[0m(self, arr):
[33m182 [16Creturn[0m self.layer(arr)*[31m4[0m*np.pi [34m# to map to output to [0,2*pi][0m
[33m183 [0m[34m#################################################[0m
[33m184 class[0m [36mGNN[0m(tf.keras.Model):
[33m185 [8Cdef[0m [36m__init__[0m(self, config):
[33m186 [0m[16C[36msuper[0m(GNN, self).__init__(name=[31m'GNN'[0m)
[33m187 
188 [0m[16Cself.InputNet = InputNet(config,name=[31m'InputNet'[0m)
[33m189 [0m[16Cself.EdgeNet  = EdgeNet(config,name=[31m'EdgeNet'[0m)
[33m190 [0m[16Cself.NodeNet  = NodeNet(config,name=[31m'NodeNet'[0m)
[33m191 [0m[16Cself.n_iters = config[[31m'n_iters'[0m]
[33m192 
193 [8Cdef[0m [36mcall[0m(self, edge_array):
[33m194 [0m[16CX,Ri,Ro = edge_array
[33m195 [0m[16CH = self.InputNet(X)
[33m196 [0m[16CH = tf.concat([H,X],axis=[31m1[0m)
[33m197 [16Cfor[0m i [33min[0m [36mrange[0m(self.n_iters):
[33m198 [0m[24Ce = self.EdgeNet(H, Ri, Ro)
[33m199 [0m[24CH = self.NodeNet(H, e, Ri, Ro)
[33m200 [0m[24CH = tf.concat([H,X], axis=[31m1[0m)
[33m201 [0m[16Ce = self.EdgeNet(H, Ri, Ro)
[33m202 [16Creturn[0m e
[33m203 [0m[34m#################################################[0m[71;118H198,4-25      Bot[65;29H[34h[?25h[?25l[71;120H9[66;29H[34h[?25h[?25l[71;120H8[65;29H[34h[?25h[?25l[71;120H7,11-25[64;29H[34h[?25h[?25l[71;120H6[63;29H[34h[?25h[?25l[71;120H5[62;29H[34h[?25h[?25l[71;120H4[61;29H[34h[?25h[?25l[71;120H3,18[60;29H[34h[?25h[?25l[71;120H2,0-1  [59;5H[34h[?25h[?25l[71;120H1,11-25[58;29H[34h[?25h[?25l[71;120H0[57;29H[34h[?25h[?25l[71;119H89[56;29H[34h[?25h[?25l[71;120H8[55;29H[34h[?25h[?25l[71;120H7,0-1  [54;5H[34h[?25h[?25l[71;120H6,11-25[53;29H[34h[?25h[?25l[71;120H5,18[52;29H[34h[?25h[?25l[51;14H[46m([14C)[0m[71;120H4,25   [51;29H[34h[?25h[?25l[51;14H([14C)[71;120H3[50;29H[34h[?25h[?25l[71;120H2,11-25[49;29H[34h[?25h[?25l[71;120H1,18[48;29H[34h[?25h[?25l[71;120H0,11[47;29H[34h[?25h[?25l[71;119H79[46;29H[34h[?25h[?25l[71;120H8[45;29H[34h[?25h[?25l[71;120H7[44;29H[34h[?25h[?25l[71;120H6[43;29H[34h[?25h[?25l[71;120H5[42;29H[34h[?25h[?25l[71;120H4[41;29H[34h[?25h[?25l[71;120H3,18[40;29H[34h[?25h[?25l[71;120H2,25   [39;29H[34h[?25h[?25l[71;120H1[38;29H[34h[?25h[?25l[71;120H0,11-25[37;29H[34h[?25h[?25l[71;119H69[36;29H[34h[?25h[?25l[71;120H8[35;29H[34h[?25h[?25l[71;120H7[34;29H[34h[?25h[?25l[71;120H6[33;29H[34h[?25h[?25l[71;120H5[32;29H[34h[?25h[?25l[71;120H4[31;29H[34h[?25h[?25l[71;120H3[30;29H[34h[?25h[?25l[71;120H2,18[29;29H[34h[?25h[?25l[71;120H1,11[28;29H[34h[?25h[?25l[71;120H0[27;29H[34h[?25h[?25l[71;119H59[26;29H[34h[?25h[?25l[71;120H8[25;29H[34h[?25h[?25l[71;120H7[24;29H[34h[?25h[?25l[71;120H6[23;29H[34h[?25h[?25l[71;120H5,18[22;29H[34h[?25h[?25l[71;120H4,25   [21;29H[34h[?25h[?25l[71;120H3[20;29H[34h[?25h[?25l[71;120H2,11-25[19;29H[34h[?25h[?25l[71;120H1[18;29H[34h[?25h[?25l[71;120H0[17;29H[34h[?25h[?25l[71;119H49[16;29H[34h[?25h[?25l[71;120H8[15;29H[34h[?25h[?25l[71;120H7[14;29H[34h[?25h[?25l[71;120H6,18[13;29H[34h[?25h[?25l[71;120H5,11[12;29H[34h[?25h[?25l[71;120H4[11;29H[34h[?25h[?25l[71;120H3[10;29H[34h[?25h[?25l[71;120H2[9;29H[34h[?25h[?25l[71;120H1[8;29H[34h[?25h[?25l[71;120H0[7;29H[34h[?25h[?25l[71;119H39,18[6;29H[34h[?25h[?25l[71;120H8,25   [5;29H[34h[?25h[?25l[71;120H7[4;29H[34h[?25h[?25l[71;120H6,18-25[3;29H[34h[?25h[?25l[71;120H5,11[2;29H[34h[?25h[?25l[71;120H4[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m133 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(node_array[:,[31m0[0m])):[71;1H[K[71;118H133,18-25     99%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m132 [0m[8Coutputs = [46m[][0m[71;118H[K[71;118H132,13-20     98%[1;24H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m131 [0m[8C[34m# To Do: can parallize the for loop[0m[2;23H[][71;118H[K[71;118H131,18-25     97%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m130 [0m[8C[34m# executes TTN_node circuit for each node in node_array[0m[71;118H[K[71;118H130,18-25     96%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m129 def[0m [36mnode_forward[0m(node_array,theta_learn):[71;118H[K[71;118H129,25[8C96%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m128 [0m[34m#################################################[0m[71;118H[K[71;118H128,25[8C95%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m127 [8Creturn[0m tf.stack(outputs)[71;118H[K[71;118H127,18-25     94%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m126 [0m[16Coutputs.append(out)[71;118H[K[71;118H126,11-25     93%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m125 [0m[16Cout = tf.constant(([31m1[0m-TTN_edge_forward(edge_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)[71;118H[K[71;118H125,11-25     93%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m124 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(edge_array[:,[31m0[0m])):[71;118H[K[71;118H124,18-25     92%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m123 [0m[8Coutputs = [46m[][0m[71;118H[K[71;118H123,13-20     91%[1;24H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m122 [0m[8C[34m# To Do: can parallize the for loop[0m[2;23H[][71;118H[K[71;118H122,18-25     90%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m121 [0m[8C[34m# executes TTN_edge circuit for each edge in edge_array[0m[71;118H[K[71;118H121,18-25     90%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m120 def[0m [36medge_forward[0m(edge_array,theta_learn):[71;118H[K[71;118H120,25[8C89%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m119 [0m[34m#################################################[0m[71;118H[K[71;118H119,25[8C88%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m118 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m5[0m)), qml.expval(qml.PauliZ(wires=[31m10[0m))[71;118H[K[71;118H118,18-25     87%[1;29H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m117 [0m[71;118H[K[71;118H117,0-1[7C87%[1;5H[34h[?25h[?25l[71;118H[K[71;1H:[34h[?25hwq[?25l"qnetworks/GNN2.py" 203L, 7254C written
[?1l>[34h[?25h[?1049lSingularity> vim qnetworks/GNN2.py 
[?1049h[?1h=[1;71r[34l[34h[?25h[23m[24m[0m[H[J[?25l[71;1H"qnetworks/GNN2.py" 203L, 7254C[1;1H[33m 83 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m14[0m)
[33m 84 [0m[8C[34m# Second Layer[0m
[33m 85 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m1[0m)
[33m 86 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m2[0m)
[33m 87 [0m[8Cqml.CNOT(wires=[[31m1[0m,[31m2[0m])
[33m 88 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m5[0m)
[33m 89 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m6[0m)
[33m 90 [0m[8Cqml.CNOT(wires=[[31m6[0m,[31m5[0m])
[33m 91 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m9[0m)
[33m 92 [0m[8Cqml.RY(theta_learn[[31m17[0m],wires=[31m10[0m)
[33m 93 [0m[8Cqml.CNOT(wires=[[31m9[0m,[31m10[0m])
[33m 94 [0m[8Cqml.RY(theta_learn[[31m18[0m],wires=[31m13[0m)
[33m 95 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m14[0m)
[33m 96 [0m[8Cqml.CNOT(wires=[[31m9[0m,[31m10[0m])
[33m 97 [0m[8C[34m# Third Layer[0m
[33m 98 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m2[0m)
[33m 99 [0m[8Cqml.RY(theta_learn[[31m20[0m],wires=[31m5[0m)
[33m100 [0m[8Cqml.CNOT(wires=[[31m2[0m,[31m5[0m])
[33m101 [0m[8Cqml.RY(theta_learn[[31m21[0m],wires=[31m10[0m)
[33m102 [0m[8Cqml.RY(theta_learn[[31m22[0m],wires=[31m13[0m)
[33m103 [0m[8Cqml.CNOT(wires=[[31m13[0m,[31m10[0m])
[33m104 [0m[8C[34m# Forth Layer[0m
[33m105 [0m[8Cqml.RY(theta_learn[[31m23[0m],wires=[31m5[0m)
[33m106 [0m[8Cqml.RY(theta_learn[[31m24[0m],wires=[31m10[0m)
[33m107 [0m[8C[34m# Fifth Layer[0m
[33m108 [0m[8Cqml.RY(theta_learn[[31m25[0m],wires=[31m0[0m)
[33m109 [0m[8Cqml.RY(theta_learn[[31m26[0m],wires=[31m5[0m)
[33m110 [0m[8Cqml.CNOT(wires=[[31m0[0m,[31m5[0m])
[33m111 [0m[8Cqml.RY(theta_learn[[31m27[0m],wires=[31m10[0m)
[33m112 [0m[8Cqml.RY(theta_learn[[31m28[0m],wires=[31m14[0m)
[33m113 [0m[8Cqml.CNOT(wires=[[31m14[0m,[31m10[0m])
[33m114 [0m[8C[34m# Last Layer[0m
[33m115 [0m[8Cqml.RY(theta_learn[[31m29[0m],wires=[31m5[0m)
[33m116 [0m[8Cqml.RY(theta_learn[[31m30[0m],wires=[31m10[0m)
[33m117 
118 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m5[0m)), qml.expval(qml.PauliZ(wires=[31m10[0m))
[33m119 [0m[34m#################################################[0m
[33m120 def[0m [36medge_forward[0m(edge_array,theta_learn):
[33m121 [0m[8C[34m# executes TTN_edge circuit for each edge in edge_array[0m
[33m122 [0m[8C[34m# To Do: can parallize the for loop[0m
[33m123 [0m[8Coutputs = []
[33m124 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(edge_array[:,[31m0[0m])):
[33m125 [0m[16Cout = tf.constant(([31m1[0m-TTN_edge_forward(edge_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)
[33m126 [0m[16Coutputs.append(out)
[33m127 [8Creturn[0m tf.stack(outputs)
[33m128 [0m[34m#################################################[0m
[33m129 def[0m [36mnode_forward[0m(node_array,theta_learn):
[33m130 [0m[8C[34m# executes TTN_node circuit for each node in node_array[0m
[33m131 [0m[8C[34m# To Do: can parallize the for loop[0m
[33m132 [0m[8Coutputs = []
[33m133 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(node_array[:,[31m0[0m])):
[33m134 [0m[16Cout = tf.constant([31m4[0m*np.pi*([31m1[0m-TTN_node_forward(node_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)
[33m135 [0m[16Coutputs.append(out)
[33m136 [8Creturn[0m tf.stack(outputs) [34m# output is between [0,4*pi][0m
[33m137 [0m[34m#################################################[0m
[33m138 class[0m [36mEdgeNet[0m(tf.keras.layers.Layer):
[33m139 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'EdgeNet'[0m):
[33m140 [0m[16C[36msuper[0m(EdgeNet, self).__init__(name=name)
[33m141 [0m[16C[34m# can only work with hid_dim = 2[0m
[33m142 [0m[16C[34m# read parameters of the network from file[0m
[33m143 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m144 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('EN',config)[0])[0m
[33m145 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m19[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m146 [8Cdef[0m [36mcall[0m(self,X, Ri, Ro):
[33m147 [0m[16Cbo = tf.matmul(Ro,X,transpose_a=[36mTrue[0m)
[33m148 [0m[16Cbi = tf.matmul(Ri,X,transpose_a=[36mTrue[0m)
[33m149 [0m[16C[34m# Shape of B = N_edges x 6 (2x (3 coordinates))[0m
[33m150 [0m[16C[34m# each row consists of two node that are possibly connected.[0m
[33m151 [0m[16CB  = tf.concat([bo, bi], axis=[31m1[0m)
[33m152 [16Creturn[0m edge_forward(B,self.theta_learn)[71;118H117,0-1[7C61%[35;5H[34h[?25h[?25l[71;120H6,1-8[34;12H[34h[?25h[?25l[71;1H[K[71;1H:[34h[?25hwq[?25l"qnetworks/GNN2.py" 203L, 7254C written
[?1l>[34h[?25h[?1049lSingularity> vim qnetworks/GNN2.py Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml Singularity> vim qnetworks/GNN2.py [KSingularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
Traceback (most recent call last):
  File "train.py", line 68, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 198, in call
    e = self.EdgeNet(H, Ri, Ro)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 152, in call
    return edge_forward(B,self.theta_learn)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 125, in edge_forward
    out = tf.constant((1-TTN_edge_forward(edge_array[i,:],theta_learn[0,:]))/2.,dtype=tf.float64)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1138, in _SliceHelperVar
    return _slice_helper(var.value(), slice_spec, var)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 898, in _slice_helper
    name=name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1064, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py", line 9513, in strided_slice
    _ops.raise_from_not_ok_status(e, name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: GNN/EdgeNet/strided_slice/
Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml Singularity> vim qnetworks/GNN2.py [K
[?1049h[?1h=[1;71r[34l[34h[?25h[23m[24m[0m[H[J[?25l[71;1H"qnetworks/GNN2.py" 203L, 7254C[1;1H[33m 82 [0m[8Cqml.CNOT(wires=[[31m8[0m,[31m9[0m])
[33m 83 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m14[0m)
[33m 84 [0m[8C[34m# Second Layer[0m
[33m 85 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m1[0m)
[33m 86 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m2[0m)
[33m 87 [0m[8Cqml.CNOT(wires=[[31m1[0m,[31m2[0m])
[33m 88 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m5[0m)
[33m 89 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m6[0m)
[33m 90 [0m[8Cqml.CNOT(wires=[[31m6[0m,[31m5[0m])
[33m 91 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m9[0m)
[33m 92 [0m[8Cqml.RY(theta_learn[[31m17[0m],wires=[31m10[0m)
[33m 93 [0m[8Cqml.CNOT(wires=[[31m9[0m,[31m10[0m])
[33m 94 [0m[8Cqml.RY(theta_learn[[31m18[0m],wires=[31m13[0m)
[33m 95 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m14[0m)
[33m 96 [0m[8Cqml.CNOT(wires=[[31m9[0m,[31m10[0m])
[33m 97 [0m[8C[34m# Third Layer[0m
[33m 98 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m2[0m)
[33m 99 [0m[8Cqml.RY(theta_learn[[31m20[0m],wires=[31m5[0m)
[33m100 [0m[8Cqml.CNOT(wires=[[31m2[0m,[31m5[0m])
[33m101 [0m[8Cqml.RY(theta_learn[[31m21[0m],wires=[31m10[0m)
[33m102 [0m[8Cqml.RY(theta_learn[[31m22[0m],wires=[31m13[0m)
[33m103 [0m[8Cqml.CNOT(wires=[[31m13[0m,[31m10[0m])
[33m104 [0m[8C[34m# Forth Layer[0m
[33m105 [0m[8Cqml.RY(theta_learn[[31m23[0m],wires=[31m5[0m)
[33m106 [0m[8Cqml.RY(theta_learn[[31m24[0m],wires=[31m10[0m)
[33m107 [0m[8C[34m# Fifth Layer[0m
[33m108 [0m[8Cqml.RY(theta_learn[[31m25[0m],wires=[31m0[0m)
[33m109 [0m[8Cqml.RY(theta_learn[[31m26[0m],wires=[31m5[0m)
[33m110 [0m[8Cqml.CNOT(wires=[[31m0[0m,[31m5[0m])
[33m111 [0m[8Cqml.RY(theta_learn[[31m27[0m],wires=[31m10[0m)
[33m112 [0m[8Cqml.RY(theta_learn[[31m28[0m],wires=[31m14[0m)
[33m113 [0m[8Cqml.CNOT(wires=[[31m14[0m,[31m10[0m])
[33m114 [0m[8C[34m# Last Layer[0m
[33m115 [0m[8Cqml.RY(theta_learn[[31m29[0m],wires=[31m5[0m)
[33m116 [0m[8Cqml.RY(theta_learn[[31m30[0m],wires=[31m10[0m)
[33m117 
118 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m5[0m)), qml.expval(qml.PauliZ(wires=[31m10[0m))
[33m119 [0m[34m#################################################[0m
[33m120 def[0m [36medge_forward[0m(edge_array,theta_learn):
[33m121 [0m[8C[34m# executes TTN_edge circuit for each edge in edge_array[0m
[33m122 [0m[8C[34m# To Do: can parallize the for loop[0m
[33m123 [0m[8Coutputs = []
[33m124 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(edge_array[:,[31m0[0m])):
[33m125 [0m[16Cout = tf.constant(([31m1[0m-TTN_edge_forward(edge_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)
[33m126 [0m[16Coutputs.append(out)
[33m127 [8Creturn[0m tf.stack(outputs)
[33m128 [0m[34m#################################################[0m
[33m129 def[0m [36mnode_forward[0m(node_array,theta_learn):
[33m130 [0m[8C[34m# executes TTN_node circuit for each node in node_array[0m
[33m131 [0m[8C[34m# To Do: can parallize the for loop[0m
[33m132 [0m[8Coutputs = []
[33m133 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(node_array[:,[31m0[0m])):
[33m134 [0m[16Cout = tf.constant([31m4[0m*np.pi*([31m1[0m-TTN_node_forward(node_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)
[33m135 [0m[16Coutputs.append(out)
[33m136 [8Creturn[0m tf.stack(outputs) [34m# output is between [0,4*pi][0m
[33m137 [0m[34m#################################################[0m
[33m138 class[0m [36mEdgeNet[0m(tf.keras.layers.Layer):
[33m139 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'EdgeNet'[0m):
[33m140 [0m[16C[36msuper[0m(EdgeNet, self).__init__(name=name)
[33m141 [0m[16C[34m# can only work with hid_dim = 2[0m
[33m142 [0m[16C[34m# read parameters of the network from file[0m
[33m143 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m144 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('EN',config)[0])[0m
[33m145 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m19[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m146 [8Cdef[0m [36mcall[0m(self,X, Ri, Ro):
[33m147 [0m[16Cbo = tf.matmul(Ro,X,transpose_a=[36mTrue[0m)
[33m148 [0m[16Cbi = tf.matmul(Ri,X,transpose_a=[36mTrue[0m)
[33m149 [0m[16C[34m# Shape of B = N_edges x 6 (2x (3 coordinates))[0m
[33m150 [0m[16C[34m# each row consists of two node that are possibly connected.[0m
[33m151 [0m[16CB  = tf.concat([bo, bi], axis=[31m1[0m)[71;118H116,2-9[7C60%[35;13H[34h[?25h[?25l[71;120H7,0-1[36;5H[34h[?25h[?25l[71;120H8,2-9[37;13H[34h[?25h[?25l[71;120H9,9  [38;13H[34h[?25h[?25l[71;119H20[39;13H[34h[?25h[?25l[71;120H1,2-9[40;13H[34h[?25h[?25l[71;120H2[41;13H[34h[?25h[?25l[71;120H3[42;13H[34h[?25h[?25l[71;120H4[43;13H[34h[?25h[?25l[71;120H5,2-16[44;20H[34h[?25h[?25l[71;120H6[45;20H[34h[?25h[?25l[71;120H7,2-9 [46;13H[34h[?25h[?25l[71;120H8,9  [47;13H[34h[?25h[?25l[71;120H9[48;13H[34h[?25h[?25l[71;119H30,2-9[49;13H[34h[?25h[?25l[71;120H1[50;13H[34h[?25h[?25l[71;120H2[51;13H[34h[?25h[?25l[71;120H3[52;13H[34h[?25h[?25l[71;120H4,2-16[53;20H[34h[?25h[?25l[71;120H5[54;20H[34h[?25h[?25l[71;120H6,2-9 [55;13H[34h[?25h[?25l[71;120H7,9  [56;13H[34h[?25h[?25l[71;120H8[57;13H[34h[?25h[?25l[71;120H9,2-9[58;13H[34h[?25h[?25l[71;119H40,2-16[59;20H[34h[?25h[?25l[71;120H1[60;20H[34h[?25h[?25l[71;120H2[61;20H[34h[?25h[?25l[71;120H3[62;20H[34h[?25h[?25l[71;120H4[63;20H[34h[?25h[?25l[71;120H5[64;20H[34h[?25h[?25l[71;120H6,2-9 [65;13H[34h[?25h[?25l[71;120H7,2-16[66;20H[34h[?25h[?25l[71;120H8[67;20H[34h[?25h[?25l[71;120H9[68;20H[34h[?25h[?25l[71;119H50[69;20H[34h[?25h[?25l[71;120H1[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m152 [16Creturn[0m edge_forward(B,self.theta_learn)[71;1H[K[71;118H152,2-16      61%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m153 [0m[34m#################################################[0m[71;118H[K[71;118H153,9[9C62%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m154 class[0m [36mNodeNet[0m(tf.keras.layers.Layer):[71;118H[K[71;118H154,9[9C63%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m155 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'NodeNet'[0m):[71;118H[K[71;118H155,2-9[7C63%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m156 [0m[16C[36msuper[0m(NodeNet, self).__init__(name=name)[71;118H[K[71;118H156,2-16      64%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m157 [0m[16C[34m# can only work with hid_dim = 1[0m[71;118H[K[71;118H157,2-16      65%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m158 [0m[16C[34m# read parameters of the network from file[0m[71;118H[K[71;118H158,2-16      66%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m159 [0m[16C[34m# params are created using tools/init_params.py[0m[71;118H[K[71;118H159,2-16      66%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m160 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('NN',config)[0])[0m[71;118H[K[71;118H160,2-16      67%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m161 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m31[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))[71;118H[K[71;118H161,2-16      68%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m162 [8Cdef[0m [36mcall[0m(self, X, e, Ri, Ro):[71;118H[K[71;118H162,2-9[7C69%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m163 [0m[16Cbo  = tf.matmul(Ro, X, transpose_a=[36mTrue[0m)[71;118H[K[71;118H163,2-16      69%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m164 [0m[16Cbi  = tf.matmul(Ri, X, transpose_a=[36mTrue[0m)[71;118H[K[71;118H164,2-16      70%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m165 [0m[16CRwo = tf.math.multiply(Ro,e)[71;118H[K[71;118H165,2-16      71%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m166 [0m[16CRwi = tf.math.multiply(Ri,e)[71;118H[K[71;118H166,2-16      72%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m167 [0m[16Cmi = tf.matmul(Rwi, bo)[71;118H[K[71;118H167,2-16      72%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m168 [0m[16Cmo = tf.matmul(Rwo, bi)[71;118H[K[71;118H168,2-16      73%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m169 [0m[16CM = tf.concat([mi, mo, X], axis=[31m1[0m)[71;118H[K[71;118H169,2-16      74%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m170 [16Creturn[0m node_forward(M,self.theta_learn)[71;118H[K[71;118H170,2-16      75%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m171 [0m[34m#################################################[0m[71;118H[K[71;118H171,9[9C75%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m172 class[0m [36mInputNet[0m(tf.keras.layers.Layer):[71;118H[K[71;118H172,9[9C76%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m173 [8Cdef[0m [36m__init__[0m(self, config, name):[71;118H[K[71;118H173,2-9[7C77%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m174 [0m[16C[36msuper[0m(InputNet, self).__init__(name=name)[71;118H[K[71;118H174,2-16      78%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m175 [0m[16Cself.num_outputs = config[[31m'hid_dim'[0m] [34m# num_outputs = number of hidden dimensions[0m[71;118H[K[71;118H175,2-16      78%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m176 [0m[16C[34m# read parameters of the network from file[0m[71;118H[K[71;118H176,2-16      79%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m177 [0m[16C[34m# params are created using tools/init_params.py[0m[71;118H[K[71;118H177,2-16      80%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m178 [0m[16C[34m#init = tf.constant_initializer(get_params('IN',config)[0])[0m[71;118H[K[71;118H178,2-16      81%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m179 [0m[16C[34m# setup a Dense layer with the given config[0m[71;118H[K[71;118H179,2-16      81%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m180 [0m[16Cself.layer = tf.keras.layers.Dense(self.num_outputs,input_shape=([31m3[0m,),activation=[31m'sigmoid'[0m)[71;118H[K[71;118H180,2-16      82%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m181 [8Cdef[0m [36mcall[0m(self, arr):[71;118H[K[71;118H181,2-9[7C83%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m182 [16Creturn[0m self.layer(arr)*[31m4[0m*np.pi [34m# to map to output to [0,2*pi][0m[71;118H[K[71;118H182,2-16      84%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m183 [0m[34m#################################################[0m[71;118H[K[71;118H183,9[9C84%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m184 class[0m [36mGNN[0m(tf.keras.Model):[71;118H[K[71;118H184,9[9C85%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m185 [8Cdef[0m [36m__init__[0m(self, config):[71;118H[K[71;118H185,2-9[7C86%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m186 [0m[16C[36msuper[0m(GNN, self).__init__(name=[31m'GNN'[0m)[71;118H[K[71;118H186,2-16      87%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m187 [0m[71;118H[K[71;118H187,0-1[7C87%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m188 [0m[16Cself.InputNet = InputNet(config,name=[31m'InputNet'[0m)[71;118H[K[71;118H188,2-16      88%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m189 [0m[16Cself.EdgeNet  = EdgeNet(config,name=[31m'EdgeNet'[0m)[71;118H[K[71;118H189,2-16      89%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m190 [0m[16Cself.NodeNet  = NodeNet(config,name=[31m'NodeNet'[0m)[71;118H[K[71;118H190,2-16      90%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m191 [0m[16Cself.n_iters = config[[31m'n_iters'[0m][71;118H[K[71;118H191,2-16      90%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m192 [0m[71;118H[K[71;118H192,0-1[7C91%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m193 [8Cdef[0m [36mcall[0m(self, edge_array):[71;118H[K[71;118H193,2-9[7C92%[70;13H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m194 [0m[16CX,Ri,Ro = edge_array[71;118H[K[71;118H194,2-16      93%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m195 [0m[16CH = self.InputNet(X)[71;118H[K[71;118H195,2-16      93%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m196 [0m[16CH = tf.concat([H,X],axis=[31m1[0m)[71;118H[K[71;118H196,2-16      94%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m197 [16Cfor[0m i [33min[0m [36mrange[0m(self.n_iters):[71;118H[K[71;118H197,2-16      95%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m198 [0m[24Ce = self.EdgeNet(H, Ri, Ro)[71;118H[K[71;118H198,2-16      96%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m199 [0m[24CH = self.NodeNet(H, e, Ri, Ro)[71;118H[K[71;118H199,2-16      96%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m200 [0m[24CH = tf.concat([H,X], axis=[31m1[0m)[71;118H[K[71;118H200,2-16      97%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m201 [0m[16Ce = self.EdgeNet(H, Ri, Ro)[71;118H[K[71;118H201,2-16      98%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m202 [16Creturn[0m e[71;118H[K[71;118H202,2-16      99%[70;20H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m203 [0m[34m#################################################[0m[71;118H[K[71;118H203,9[9CBot[70;13H[34h[?25h[?25l[71;120H2,2-16[69;20H[34h[?25h[?25l[71;120H1[68;20H[34h[?25h[?25l[71;120H0[67;20H[34h[?25h[?25l[71;122H3-24[67;28H[34h[?25h[?25l[71;122H4-25[67;29H[34h[?25h[?25l[71;122H5-26[67;30H[34h[?25h[?25l[71;122H6-27[67;31H[34h[?25h[?25l[71;122H7-28[67;32H[34h[?25h[?25l[71;122H8-29[67;33H[34h[?25h[?25l[71;122H9-30[67;34H[34h[?25h[?25l[71;122H10-31[67;35H[34h[?25h[?25l[71;123H1-32[67;36H[34h[?25h[?25l[71;123H2-33[67;37H[34h[?25h[?25l[71;123H3-34[67;38H[34h[?25h[?25l[71;123H4-35[67;39H[34h[?25h[?25l[71;123H5-36[67;40H[34h[?25h[?25l[71;118H199[66;40H[34h[?25h[?25l[71;120H8[65;40H[34h[?25h[?25l[71;120H7,22[64;40H[34h[?25h[?25l[71;120H6[63;40H[34h[?25h[?25l[46m[[0mH,X[46m][0m[71;123H1-35[63;39H[34h[?25h[?25l[H,X][71;123H0-34[63;38H[34h[?25h[?25l[71;122H19-33[63;37H[34h[?25h[?25l[71;1H[1m-- INSERT --[0m[71;118H[K[71;118H196,19-33     Bot[63;37H[34h[?25h[?25l[,X],axis=[31m1[0m)[63;37H[46m[[0m,X[46m][0m[71;122H20-34[63;38H[34h[?25h[?25l],[46mX[0m],axis=[31m1[0m)[63;38H[46m][0m,X[71;123H1-35[63;39H[34h[?25h[?25l[71;123H0-34[63;38H[34h[?25h[?25l[46m:[0m],X],axis=[31m1[0m)[63;38H:[46m][0m[71;123H1-35[63;39H[34h[?25h[?25l[46m,[0m],X],axis=[31m1[0m)[63;39H,[46m][0m[71;123H2-36[63;40H[34h[?25h[?25l[46mN[0m],X],axis=[31m1[0m)[63;40HN[46m][0m[71;123H3-37[63;41H[34h[?25h[?25l[46mo[0m],X],axis=[31m1[0m)[63;41Ho[46m][0m[71;123H4-38[63;42H[34h[?25h[?25l[46mn[0m],X],axis=[31m1[0m)[63;42Hn[46m][0m[71;123H5-39[63;43H[34h[?25h[?25l[36mNon[0m[36m[46me[0m],X],axis=[31m1[0m)[63;43H[36me[0m[46m][0m[71;123H6-40[63;44H[34h[?25h[?25l[63;37H[[6C][71;120H7[64;44H[34h[?25h[?25l[71;120H8,19[65;44H[34h[?25h[?25l[71;120H9[66;44H[34h[?25h[?25l[67;43H[46m[[0mH,X[46m][0m[71;118H200[67;44H[34h[?25h[?25l[H,X][71;122H20-41[67;45H[34h[?25h[?25l[,X], axis=[31m1[0m)[67;45H[46m[[0m,X[46m][0m[71;123H1-42[67;46H[34h[?25h[?25l],[46mX[0m], axis=[31m1[0m)[67;46H[46m][0m,X[71;123H2-43[67;47H[34h[?25h[?25l[71;123H1-42[67;46H[34h[?25h[?25l[46m:[0m],X], axis=[31m1[0m)[67;46H:[46m][0m[71;123H2-43[67;47H[34h[?25h[?25l[46m,[0m],X], axis=[31m1[0m)[67;47H,[46m][0m[71;123H3-44[67;48H[34h[?25h[?25l[46mN[0m],X], axis=[31m1[0m)[67;48HN[46m][0m[71;123H4-45[67;49H[34h[?25h[?25l[46mo[0m],X], axis=[31m1[0m)[67;49Ho[46m][0m[71;123H5-46[67;50H[34h[?25h[?25l[46mn[0m],X], axis=[31m1[0m)[67;50Hn[46m][0m[71;123H6-47[67;51H[34h[?25h[?25l[36mNon[0m[36m[46me[0m],X], axis=[31m1[0m)[67;51H[36me[0m[46m][0m[71;123H7-48[67;52H[34h[?25h[71;1H[K[?25l[67;45H[[6C][71;118H200,26-47     Bot[67;51H[34h[?25h[?25l[71;118H[K[71;1H:[34h[?25hwq[?25l"qnetworks/GNN2.py" 203L, 7270C written
[?1l>[34h[?25h[?1049lSingularity> vim qnetworks/GNN2.py Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
Traceback (most recent call last):
  File "train.py", line 68, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 196, in call
    H = tf.concat([H[:,None],X],axis=1)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/dispatch.py", line 180, in wrapper
    return target(*args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1517, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py", line 1118, in concat_v2
    _ops.raise_from_not_ok_status(e, name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Ranks of all input tensors should match: shape[0] = [259,1,2] vs. shape[1] = [259,3] [Op:ConcatV2] name: concat
Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml Singularity> vim qnetworks/GNN2.py [KSingularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
Traceback (most recent call last):
  File "train.py", line 68, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 196, in call
    H = tf.concat([H[:,None],X],axis=1)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/dispatch.py", line 180, in wrapper
    return target(*args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1517, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py", line 1118, in concat_v2
    _ops.raise_from_not_ok_status(e, name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Ranks of all input tensors should match: shape[0] = [259,1,2] vs. shape[1] = [259,3] [Op:ConcatV2] name: concat
Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml Singularity> vim qnetworks/GNN2.py [K
[?1049h[?1h=[1;71r[34l[34h[?25h[23m[24m[0m[H[J[?25l[71;1H"qnetworks/GNN2.py" 203L, 7270C[1;1H[33m134 [0m[16Cout = tf.constant([31m4[0m*np.pi*([31m1[0m-TTN_node_forward(node_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)
[33m135 [0m[16Coutputs.append(out)
[33m136 [8Creturn[0m tf.stack(outputs) [34m# output is between [0,4*pi][0m
[33m137 [0m[34m#################################################[0m
[33m138 class[0m [36mEdgeNet[0m(tf.keras.layers.Layer):
[33m139 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'EdgeNet'[0m):
[33m140 [0m[16C[36msuper[0m(EdgeNet, self).__init__(name=name)
[33m141 [0m[16C[34m# can only work with hid_dim = 2[0m
[33m142 [0m[16C[34m# read parameters of the network from file[0m
[33m143 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m144 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('EN',config)[0])[0m
[33m145 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m19[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m146 [8Cdef[0m [36mcall[0m(self,X, Ri, Ro):
[33m147 [0m[16Cbo = tf.matmul(Ro,X,transpose_a=[36mTrue[0m)
[33m148 [0m[16Cbi = tf.matmul(Ri,X,transpose_a=[36mTrue[0m)
[33m149 [0m[16C[34m# Shape of B = N_edges x 6 (2x (3 coordinates))[0m
[33m150 [0m[16C[34m# each row consists of two node that are possibly connected.[0m
[33m151 [0m[16CB  = tf.concat([bo, bi], axis=[31m1[0m)
[33m152 [16Creturn[0m edge_forward(B,self.theta_learn)
[33m153 [0m[34m#################################################[0m
[33m154 class[0m [36mNodeNet[0m(tf.keras.layers.Layer):
[33m155 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'NodeNet'[0m):
[33m156 [0m[16C[36msuper[0m(NodeNet, self).__init__(name=name)
[33m157 [0m[16C[34m# can only work with hid_dim = 1[0m
[33m158 [0m[16C[34m# read parameters of the network from file[0m
[33m159 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m160 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('NN',config)[0])[0m
[33m161 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m31[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m162 [8Cdef[0m [36mcall[0m(self, X, e, Ri, Ro):
[33m163 [0m[16Cbo  = tf.matmul(Ro, X, transpose_a=[36mTrue[0m)
[33m164 [0m[16Cbi  = tf.matmul(Ri, X, transpose_a=[36mTrue[0m)
[33m165 [0m[16CRwo = tf.math.multiply(Ro,e)
[33m166 [0m[16CRwi = tf.math.multiply(Ri,e)
[33m167 [0m[16Cmi = tf.matmul(Rwi, bo)
[33m168 [0m[16Cmo = tf.matmul(Rwo, bi)
[33m169 [0m[16CM = tf.concat([mi, mo, X], axis=[31m1[0m)
[33m170 [16Creturn[0m node_forward(M,self.theta_learn)
[33m171 [0m[34m#################################################[0m
[33m172 class[0m [36mInputNet[0m(tf.keras.layers.Layer):
[33m173 [8Cdef[0m [36m__init__[0m(self, config, name):
[33m174 [0m[16C[36msuper[0m(InputNet, self).__init__(name=name)
[33m175 [0m[16Cself.num_outputs = config[[31m'hid_dim'[0m] [34m# num_outputs = number of hidden dimensions[0m
[33m176 [0m[16C[34m# read parameters of the network from file[0m
[33m177 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m178 [0m[16C[34m#init = tf.constant_initializer(get_params('IN',config)[0])[0m
[33m179 [0m[16C[34m# setup a Dense layer with the given config[0m
[33m180 [0m[16Cself.layer = tf.keras.layers.Dense(self.num_outputs,input_shape=([31m3[0m,),activation=[31m'sigmoid'[0m)
[33m181 [8Cdef[0m [36mcall[0m(self, arr):
[33m182 [16Creturn[0m self.layer(arr)*[31m4[0m*np.pi [34m# to map to output to [0,2*pi][0m
[33m183 [0m[34m#################################################[0m
[33m184 class[0m [36mGNN[0m(tf.keras.Model):
[33m185 [8Cdef[0m [36m__init__[0m(self, config):
[33m186 [0m[16C[36msuper[0m(GNN, self).__init__(name=[31m'GNN'[0m)
[33m187 
188 [0m[16Cself.InputNet = InputNet(config,name=[31m'InputNet'[0m)
[33m189 [0m[16Cself.EdgeNet  = EdgeNet(config,name=[31m'EdgeNet'[0m)
[33m190 [0m[16Cself.NodeNet  = NodeNet(config,name=[31m'NodeNet'[0m)
[33m191 [0m[16Cself.n_iters = config[[31m'n_iters'[0m]
[33m192 
193 [8Cdef[0m [36mcall[0m(self, edge_array):
[33m194 [0m[16CX,Ri,Ro = edge_array
[33m195 [0m[16CH = self.InputNet(X)
[33m196 [0m[16CH = tf.concat([H[:,[36mNone[0m],X],axis=[31m1[0m)
[33m197 [16Cfor[0m i [33min[0m [36mrange[0m(self.n_iters):
[33m198 [0m[24Ce = self.EdgeNet(H, Ri, Ro)
[33m199 [0m[24CH = self.NodeNet(H, e, Ri, Ro)
[33m200 [0m[24CH = tf.concat([H[:,[36mNone[0m],X], axis=[31m1[0m)
[33m201 [0m[16Ce = self.EdgeNet(H, Ri, Ro)
[33m202 [16Creturn[0m e
[33m203 [0m[34m#################################################[0m[71;118H200,4-25      Bot[67;29H[34h[?25h[?25l[71;120H1,11-25[68;29H[34h[?25h[?25l[71;120H2,10-24[69;28H[34h[?25h[?25l[71;120H3,25   [70;29H[34h[?25h[?25l[71;120H2,10-24[69;28H[34h[?25h[?25l[71;120H1,11-25[68;29H[34h[?25h[?25l[71;120H0,4-25 [67;29H[34h[?25h[?25l[71;118H199[66;29H[34h[?25h[?25l[71;120H8[65;29H[34h[?25h[?25l[71;120H7,11-25[64;29H[34h[?25h[?25l[71;120H6[63;29H[34h[?25h[?25l[71;123H2-26[63;30H[34h[?25h[?25l[71;123H3-27[63;31H[34h[?25h[?25l[71;123H4-28[63;32H[34h[?25h[?25l[71;123H5-29[63;33H[34h[?25h[?25lt[46m([20C)[0m[71;123H6-30[63;34H[34h[?25h[?25l([46m[[11C][0m[7C)[71;123H7-31[63;35H[34h[?25h[?25l[[11C][71;123H8-32[63;36H[34h[?25h[?25lH[46m[[6C][0m[71;123H9-33[63;37H[34h[?25h[?25l[[6C][71;122H20-34[63;38H[34h[?25h[?25l[71;123H1-35[63;39H[34h[?25h[?25l[71;123H2-36[63;40H[34h[?25h[?25l[71;123H3-37[63;41H[34h[?25h[?25l[71;123H4-38[63;42H[34h[?25h[?25l[71;123H5-39[63;43H[34h[?25h[?25l[46m[[6C][0m[71;123H6-40[63;44H[34h[?25h[?25l[63;37H[[6C][71;123H7-41[63;45H[34h[?25h[?25l[71;1H[1m-- INSERT --[0m[71;13H[K[71;118H196,27-41     Bot[63;45H[34h[?25h[?25l,X],axis=[31m1[0m)[63;55H[K[71;123H6-40[63;44H[34h[?25h[?25lNon,X],axis=[31m1[0m)[63;54H[K[71;123H5-39[63;43H[34h[?25h[?25l,X],axis=[31m1[0m)[63;53H[K[71;123H4-38[63;42H[34h[?25h[?25l,X],axis=[31m1[0m)[63;52H[K[71;123H3-37[63;41H[34h[?25h[?25l,X],axis=[31m1[0m)[63;51H[K[71;123H2-36[63;40H[34h[?25h[?25lX],axis=[31m1[0m)[63;50H[K[71;123H1-35[63;39H[34h[?25h[?25l,X],axis=[31m1[0m)[63;49H[K[63;37H[46m[[0m,X[46m][0m[71;123H0-34[63;38H[34h[?25h[?25l[46m,[0mX][46m,[0maxis=[31m1[0m)[63;48H[K[63;37H,X],[71;122H19-33[63;37H[34h[?25h[?25l[71;120H7[64;37H[34h[?25h[?25l[71;120H8,12[65;37H[34h[?25h[?25l[71;120H9[66;37H[34h[?25h[?25l[71;118H200[67;37H[34h[?25h[?25l[71;123H3-34[67;38H[34h[?25h[?25l[71;123H4-35[67;39H[34h[?25h[?25l[71;123H5-36[67;40H[34h[?25h[?25l[71;123H6-37[67;41H[34h[?25h[?25lt[46m([21C)[0m[71;123H7-38[67;42H[34h[?25h[?25l([46m[[11C][0m[8C)[71;123H8-39[67;43H[34h[?25h[?25l[71;123H9-40[67;44H[34h[?25h[?25l[H[46m[[6C][0m,X][71;122H20-41[67;45H[34h[?25h[?25l[71;123H1-42[67;46H[34h[?25h[?25l[[6C][71;123H2-43[67;47H[34h[?25h[?25l[71;123H3-44[67;48H[34h[?25h[?25l[71;123H4-45[67;49H[34h[?25h[?25l[71;123H5-46[67;50H[34h[?25h[?25l[71;123H6-47[67;51H[34h[?25h[?25l[46m[[6C][0m[71;123H7-48[67;52H[34h[?25h[?25l[71;123H8-49[67;53H[34h[?25h[?25l[46m,[0mX], axis=[31m1[0m)[67;64H[K[67;45H[[6C,[71;123H7-48[67;52H[34h[?25h[?25lNon,X], axis=[31m1[0m)[67;63H[K[71;123H6-47[67;51H[34h[?25h[?25l,X], axis=[31m1[0m)[67;62H[K[71;123H5-46[67;50H[34h[?25h[?25l,X], axis=[31m1[0m)[67;61H[K[71;123H4-45[67;49H[34h[?25h[?25l,X], axis=[31m1[0m)[67;60H[K[71;123H3-44[67;48H[34h[?25h[?25lX], axis=[31m1[0m)[67;59H[K[71;123H2-43[67;47H[34h[?25h[?25l,X], axis=[31m1[0m)[67;58H[K[67;45H[46m[[0m,X[46m][0m[71;123H1-42[67;46H[34h[?25h[?25l[46m,[0mX][46m,[0m axis=[31m1[0m)[67;57H[K[67;45H,X],[71;123H0-41[67;45H[34h[?25h[?25l[66;45H[46m([12C)[0m[71;118H199[66;45H[34h[?25h[?25l[65;45H[46m([9C)[0m[66;45H([12C)[71;120H8[65;45H[34h[?25h[?25l([9C)[71;120H7,27[64;45H[34h[?25h[?25l[71;120H6[63;45H[34h[?25h[?25l[71;120H5,24-38[62;42H[34h[?25h[?25l[71;120H4,23-37[61;41H[34h[?25h[?25l[71;120H3,29-36[60;40H[34h[?25h[?25l[71;120H2,1    [59;5H[34h[?25h[?25l[71;120H1,27-41[58;45H[34h[?25h[?25l[57;44H[46m([21C)[0m[71;120H0[57;45H[34h[?25h[?25l([21C)[71;119H83,41   [50;45H[34h[?25h[?25l[71;120H2,27-41[49;45H[34h[?25h[?25l[71;120H1,22-29[48;33H[34h[?25h[?25l[71;120H0,27-41[47;45H[34h[?25h[?25l[71;119H79[46;45H[34h[?25h[?25l[71;120H8[45;45H[34h[?25h[?25l[71;120H7[44;45H[34h[?25h[?25l[71;120H6[43;45H[34h[?25h[?25l[71;120H5[42;45H[34h[?25h[?25l[71;120H4[41;45H[34h[?25h[?25l[40;25H[46m([18C)[0m[71;120H3,34[40;45H[34h[?25h[?25l[40;25H([18C)[71;120H2,39   [39;43H[34h[?25h[?25l[71;120H1,41[38;45H[34h[?25h[?25l[71;120H0,27-41[37;45H[34h[?25h[?25l[36;35H[46m[[9C][0m[71;119H69[36;45H[34h[?25h[?25l[35;35H[46m([7C)[0m[36;35H[[9C][71;120H8,26-40[35;44H[34h[?25h[?25l[34;35H[46m([7C)[0m[35;35H([7C)[71;120H7[34;44H[34h[?25h[?25l[33;43H[46m([0mRi,e[46m)[0m[34;35H([7C)[71;120H6[33;44H[34h[?25h[?25l[32;43H[46m([0mRo,e[46m)[0m[33;43H(Ri,e)[71;120H5[32;44H[34h[?25h[?25l(Ro,e)[71;120H4[31;44H[34h[?25h[?25l[71;120H3[30;44H[34h[?25h[?25l[71;120H2,31-38[29;42H[34h[?25h[?25l[71;120H1,26-40[28;44H[34h[?25h[?25l[71;120H0[27;44H[34h[?25h[?25l[71;119H59[26;44H[34h[?25h[?25l[71;119H47[14;44H[34h[?25h[?25l[71;120H6,27-34[13;38H[34h[?25h[?25l[71;120H5,26-40[12;44H[34h[?25h[?25l[71;120H4[11;44H[34h[?25h[?25l[71;120H3[10;44H[34h[?25h[?25l[71;120H2[9;44H[34h[?25h[?25l[71;120H1[8;44H[34h[?25h[?25l[71;120H0[7;44H[34h[?25h[?25l[71;119H39,33[6;44H[34h[?25h[?25l[71;120H8,38   [5;42H[34h[?25h[?25l[71;120H7,40[4;44H[34h[?25h[?25l[71;120H6,33-40[3;44H[34h[?25h[?25l[2;35H[46m([0mout[46m)[0m[71;120H5,22-36[2;40H[34h[?25h[?25l(out)[71;120H4[1;40H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m133 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(node_array[:,[31m0[0m])):[71;118H[K[71;118H133,29-36     99%[1;40H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m132 [0m[8Coutputs = [][71;118H[K[71;118H132,14-21     98%[1;23H[46m[][34h[?25h[?25l[1;70r[0m[1;1H[L[1;71r[1;1H[33m131 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H131,14-21     97%[2;23H[][1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m130 [0m[8C[34m# executes TTN_node circuit for each node in node_array[0m[71;118H[K[71;118H130,14-21     96%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m129 def[0m [36mnode_forward[0m(node_array,theta_learn):[71;118H[K[71;118H129,21[8C96%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m128 [0m[34m#################################################[0m[71;118H[K[71;118H128,21[8C95%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m127 [8Creturn[0m tf.stack(outputs)[71;118H[K[71;118H127,14-21     94%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m126 [0m[16Coutputs.append(out)[71;118H[K[71;118H126,7-21      93%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m125 [0m[16Cout = tf.constant(([31m1[0m-TTN_edge_forward(edge_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)[71;118H[K[71;118H125,7-21      93%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m124 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(edge_array[:,[31m0[0m])):[71;118H[K[71;118H124,14-21     92%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m123 [0m[8Coutputs = [][71;118H[K[71;118H123,14-21     91%[1;23H[46m[][34h[?25h[?25l[1;70r[0m[1;1H[2L[1;71r[1;1H[33m121 [0m[8C[34m# executes TTN_edge circuit for each edge in edge_array[0m
[33m122 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H121,14-21     90%[3;23H[][1;25H[34h[?25h[?25l[1;70r[1;1H[2L[1;71r[1;1H[33m119 [0m[34m#################################################[0m
[33m120 def[0m [36medge_forward[0m(edge_array,theta_learn):[71;118H[K[71;118H119,21[8C88%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m118 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m5[0m)), qml.expval(qml.PauliZ(wires=[31m10[0m))[71;118H[K[71;118H118,14-21     87%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m117 [0m[71;118H[K[71;118H117,1[9C87%[1;5H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m116 [0m[8Cqml.RY(theta_learn[[31m30[0m],wires=[31m10[0m)[71;118H[K[71;118H116,14-21     86%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m115 [0m[8Cqml.RY(theta_learn[[31m29[0m],wires=[31m5[0m)[71;118H[K[71;118H115,14-21     85%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m114 [0m[8C[34m# Last Layer[0m[71;118H[K[71;118H114,14-21     84%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m113 [0m[8Cqml.CNOT(wires=[[31m14[0m,[31m10[0m])[71;118H[K[71;118H113,14-21     84%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m112 [0m[8Cqml.RY(theta_learn[[31m28[0m],wires=[31m14[0m)[71;118H[K[71;118H112,14-21     83%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m111 [0m[8Cqml.RY(theta_learn[[31m27[0m],wires=[31m10[0m)[71;118H[K[71;118H111,14-21     82%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m110 [0m[8Cqml.CNOT(wires=[[31m0[0m,[31m5[0m])[71;118H[K[71;118H110,14-21     81%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m109 [0m[8Cqml.RY(theta_learn[[31m26[0m],wires=[31m5[0m)[71;118H[K[71;118H109,14-21     81%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m108 [0m[8Cqml.RY(theta_learn[[31m25[0m],wires=[31m0[0m)[71;118H[K[71;118H108,14-21     80%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m107 [0m[8C[34m# Fifth Layer[0m[71;118H[K[71;118H107,14-21     79%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m106 [0m[8Cqml.RY(theta_learn[[31m24[0m],wires=[31m10[0m)[71;118H[K[71;118H106,14-21     78%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m105 [0m[8Cqml.RY(theta_learn[[31m23[0m],wires=[31m5[0m)[71;118H[K[71;118H105,14-21     78%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m104 [0m[8C[34m# Forth Layer[0m[71;118H[K[71;118H104,14-21     77%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m103 [0m[8Cqml.CNOT(wires=[[31m13[0m,[31m10[0m])[71;118H[K[71;118H103,14-21     76%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m102 [0m[8Cqml.RY(theta_learn[[31m22[0m],wires=[31m13[0m)[71;118H[K[71;118H102,14-21     75%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m101 [0m[8Cqml.RY(theta_learn[[31m21[0m],wires=[31m10[0m)[71;118H[K[71;118H101,14-21     75%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m100 [0m[8Cqml.CNOT(wires=[[31m2[0m,[31m5[0m])[71;118H[K[71;118H100,14-21     74%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 99 [0m[8Cqml.RY(theta_learn[[31m20[0m],wires=[31m5[0m)[71;118H[K[71;118H99,14-21      73%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 98 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m2[0m)[71;118H[K[71;118H98,14-21      72%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 97 [0m[8C[34m# Third Layer[0m[71;118H[K[71;118H97,14-21      72%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 96 [0m[8Cqml.CNOT(wires=[[31m9[0m,[31m10[0m])[71;118H[K[71;118H96,14-21      71%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 95 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m14[0m)[71;118H[K[71;118H95,14-21      70%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 94 [0m[8Cqml.RY(theta_learn[[31m18[0m],wires=[31m13[0m)[71;118H[K[71;118H94,14-21      69%[1;25H[34h[?25h[?25l[1;70r[1;1H[2L[1;71r[1;1H[33m 92 [0m[8Cqml.RY(theta_learn[[31m17[0m],wires=[31m10[0m)
[33m 93 [0m[8Cqml.CNOT(wires=[[31m9[0m,[31m10[0m])[71;118H[K[71;118H92,14-21      68%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 91 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m9[0m)[71;118H[K[71;118H91,14-21      67%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 90 [0m[8Cqml.CNOT(wires=[[31m6[0m,[31m5[0m])[71;118H[K[71;118H90,14-21      66%[1;25H[34h[?25h[?25l[1;70r[1;1H[2L[1;71r[1;1H[33m 88 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m5[0m)
[33m 89 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m6[0m)[71;118H[K[71;118H88,14-21      65%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 87 [0m[8Cqml.CNOT(wires=[[31m1[0m,[31m2[0m])[71;118H[K[71;118H87,14-21      64%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 86 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m2[0m)[71;118H[K[71;118H86,14-21      63%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 85 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m1[0m)[71;118H[K[71;118H85,14-21      63%[1;25H[34h[?25h[?25l[1;70r[1;1H[2L[1;71r[1;1H[33m 83 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m14[0m)
[33m 84 [0m[8C[34m# Second Layer[0m[71;118H[K[71;118H83,14-21      61%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 82 [0m[8Cqml.CNOT(wires=[[31m8[0m,[31m9[0m])[71;118H[K[71;118H82,14-21      60%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 81 [0m[8Cqml.RY(theta_learn[[31m13[0m],wires=[31m13[0m)[71;118H[K[71;118H81,14-21      60%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 80 [0m[8Cqml.RY(theta_learn[[31m12[0m],wires=[31m12[0m)[71;118H[K[71;118H80,14-21      59%[1;25H[1;70r[1;1H[L[1;71r[1;1H[33m 79 [0m[8Cqml.CNOT(wires=[[31m11[0m,[31m10[0m])[71;118H[K[71;118H79,14-21      58%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 78 [0m[8Cqml.RY(theta_learn[[31m11[0m],wires=[31m11[0m)[71;118H[K[71;118H78,14-21      57%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 77 [0m[8Cqml.RY(theta_learn[[31m10[0m],wires=[31m10[0m)[71;118H[K[71;118H77,14-21      57%[1;25H[1;70r[1;1H[L[1;71r[1;1H[33m 76 [0m[8Cqml.CNOT(wires=[[31m8[0m,[31m9[0m])[71;118H[K[71;118H76,14-21      56%[1;25H[34h[?25h[?25l[1;70r[1;1H[17L[1;71r[1;1H[33m 59 [0m[16Cqml.RY(edge[i],wires=i)
[33m 60 [0m[8C[34m# APPLY forward sequence[0m
[33m 61 [0m[8C[34m# First Layer[0m
[33m 62 [0m[8Cqml.RY(theta_learn[[31m0[0m],wires=[31m0[0m)
[33m 63 [0m[8Cqml.RY(theta_learn[[31m1[0m],wires=[31m1[0m)
[33m 64 [0m[8Cqml.CNOT(wires=[[31m0[0m,[31m1[0m])
[33m 65 [0m[8Cqml.RY(theta_learn[[31m2[0m],wires=[31m2[0m)
[33m 66 [0m[8Cqml.RY(theta_learn[[31m3[0m],wires=[31m3[0m)
[33m 67 [0m[8Cqml.CNOT(wires=[[31m3[0m,[31m2[0m])
[33m 68 [0m[8Cqml.RY(theta_learn[[31m4[0m],wires=[31m4[0m)
[33m 69 [0m[8Cqml.RY(theta_learn[[31m5[0m],wires=[31m5[0m)
[33m 70 [0m[8Cqml.CNOT(wires=[[31m4[0m,[31m5[0m])
[33m 71 [0m[8Cqml.RY(theta_learn[[31m6[0m],wires=[31m6[0m)
[33m 72 [0m[8Cqml.RY(theta_learn[[31m7[0m],wires=[31m7[0m)
[33m 73 [0m[8Cqml.CNOT(wires=[[31m7[0m,[31m6[0m])
[33m 74 [0m[8Cqml.RY(theta_learn[[31m8[0m],wires=[31m8[0m)
[33m 75 [0m[8Cqml.RY(theta_learn[[31m9[0m],wires=[31m9[0m)[71;118H[K[71;118H59,7-21[7C43%[1;25HO[34h[?25hR[?25l[1;70r[1;1H[5L[1;71r[1;1H[33m 54 [0mdev2 = qml.device([31m"qulacs.simulator"[0m, wires=[31m15[0m,gpu=[36mTrue[0m)
[33m 55 [0m[35m@[0m[36mqml.qnode[0m(dev2,interface=[31m'tf'[0m)
[33m 56 def[0m [36mTTN_node_forward[0m(edge,theta_learn):
[33m 57 [0m[8C[34m# STATE PREPARATION[0m
[33m 58 [8Cfor[0m i [33min[0m [36mrange[0m([31m15[0m):[71;118H[K[71;118H54,21[9C39%[1;25H[34h[?25h[?25l[1;70r[1;1H[2L[1;71r[1;1H[33m 52 [0m[34m# Use default.qubit for default pennylane simulation[0m
[33m 53 [0m[34m# use tf.interface for TF integration[0m[71;118H[K[71;118H52,21[9C38%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 51 [0m[34m##################################################################################################[0m[71;118H[K[71;118H51,21[9C37%[1;25H[34h[?25h[?25l[1;70r[1;1H[5L[1;71r[1;1H[33m 46 [0m[8Cqml.RY(theta_learn[[31m17[0m],wires=[31m7[0m)
[33m 47 [0m[8Cqml.CNOT(wires=[[31m4[0m,[31m7[0m])
[33m 48 [0m[8C[34m#Last Layer[0m
[33m 49 [0m[8Cqml.RY(theta_learn[[31m18[0m],wires=[31m7[0m)
[33m 50 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m7[0m))[71;118H[K[71;118H46,14-21      33%[1;25H[34h[?25h[?25l[1;70r[1;1H[3L[1;71r[1;1H[33m 43 [0m[8Cqml.CNOT(wires=[[31m2[0m,[31m4[0m])
[33m 44 [0m[8C[34m# Forth Layer[0m
[33m 45 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m4[0m)[71;118H[K[71;118H43,14-21      31%[1;25H[34h[?25h[?25l[1;70r[1;1H[3L[1;71r[1;1H[33m 40 [0m[8C[34m# Third Layer[0m
[33m 41 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m2[0m)
[33m 42 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m4[0m)[71;118H[K[71;118H40,14-21      29%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 39 [0m[8Cqml.CNOT(wires=[[31m9[0m,[31m7[0m])[71;118H[K[71;118H39,14-21      28%[1;25H[34h[?25h[?25l[1;70r[1;1H[2L[1;71r[1;1H[33m 37 [0m[8Cqml.RY(theta_learn[[31m12[0m],wires=[31m7[0m)
[33m 38 [0m[8Cqml.RY(theta_learn[[31m13[0m],wires=[31m9[0m)[71;118H[K[71;118H37,14-21      27%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 36 [0m[8Cqml.CNOT(wires=[[31m1[0m,[31m2[0m])[71;118H[K[71;118H36,14-21      26%[1;25H[34h[?25h[?25l[1;70r[1;1H[3L[1;71r[1;1H[33m 33 [0m[8C[34m# Second Layer[0m
[33m 34 [0m[8Cqml.RY(theta_learn[[31m10[0m],wires=[31m1[0m)
[33m 35 [0m[8Cqml.RY(theta_learn[[31m11[0m],wires=[31m2[0m)[71;118H[K[71;118H33,14-21      24%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 32 [0m[8Cqml.CNOT(wires=[[31m8[0m,[31m9[0m])[71;118H[K[71;118H32,14-21      23%[1;25H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 31 [0m[8Cqml.RY(theta_learn[[31m9[0m],wires=[31m9[0m)[71;118H[K[71;118H31,14-21      22%[1;25H[34h[?25h[?25l[1;70r[1;1H[2L[1;71r[1;1H[33m 29 [0m[8Cqml.CNOT(wires=[[31m6[0m,[31m7[0m])
[33m 30 [0m[8Cqml.RY(theta_learn[[31m8[0m],wires=[31m8[0m)[71;118H[K[71;118H29,14-21      21%[1;25H[34h[?25h[?25l[71;118H30[2;25H[34h[?25h[?25l[71;119H1[3;25H[34h[?25h[?25l[71;119H2[4;25H[34h[?25h[?25l[71;119H3[5;25H[34h[?25h[?25l[71;119H5[7;25H[34h[?25h[?25l[71;119H6[8;25H[34h[?25h[?25l[71;119H7[9;25H[34h[?25h[?25l[71;119H8[10;25H[34h[?25h[?25l[71;119H9[11;25H[34h[?25h[?25l[71;118H40[12;25H[34h[?25h[?25l[71;118H54,21   [26;25H[34h[?25h[?25l[71;119H5[27;25H[34h[?25h[?25l[71;119H9,7-21[31;25H[34h[?25h[?25l[71;118H62,14-21[34;25H[34h[?25h[?25l[71;119H3[35;25H[34h[?25h[?25l[71;119H6[38;25H[34h[?25h[?25l[71;119H8[40;25H[34h[?25h[?25l[71;118H70[42;25H[34h[?25h[?25l[71;119H1[43;25H[34h[?25h[?25l[71;119H3[45;25H[34h[?25h[?25l[71;119H4[46;25H[34h[?25h[?25l[71;119H5[47;25H[34h[?25h[?25l[71;119H8[50;25H[34h[?25h[?25l[71;118H81[53;25H[34h[?25h[?25l[71;119H3[55;25H[34h[?25h[?25l[71;119H7[59;25H[34h[?25h[?25l[71;119H8[60;25H[34h[?25h[?25l[71;118H90[62;25H[34h[?25h[?25l[71;119H1[63;25H[34h[?25h[?25l[71;119H3[65;25H[34h[?25h[?25l[71;119H4[66;25H[34h[?25h[?25l[71;119H5[67;25H[34h[?25h[?25l[71;119H6[68;25H[34h[?25h[?25l[71;119H7[69;25H[34h[?25h[?25l[71;119H8[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 99 [0m[8Cqml.RY(theta_learn[[31m20[0m],wires=[31m5[0m)[71;118H[K[71;118H99,14-21      21%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m100 [0m[8Cqml.CNOT(wires=[[31m2[0m,[31m5[0m])[71;118H[K[71;118H100,14-21     22%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m101 [0m[8Cqml.RY(theta_learn[[31m21[0m],wires=[31m10[0m)[71;118H[K[71;118H101,14-21     23%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m102 [0m[8Cqml.RY(theta_learn[[31m22[0m],wires=[31m13[0m)[71;118H[K[71;118H102,14-21     24%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m103 [0m[8Cqml.CNOT(wires=[[31m13[0m,[31m10[0m])[71;118H[K[71;118H103,14-21     24%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m104 [0m[8C[34m# Forth Layer[0m[71;118H[K[71;118H104,14-21     25%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m105 [0m[8Cqml.RY(theta_learn[[31m23[0m],wires=[31m5[0m)[71;118H[K[71;118H105,14-21     26%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m106 [0m[8Cqml.RY(theta_learn[[31m24[0m],wires=[31m10[0m)[71;118H[K[71;118H106,14-21     27%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m107 [0m[8C[34m# Fifth Layer[0m[71;118H[K[71;118H107,14-21     27%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m108 [0m[8Cqml.RY(theta_learn[[31m25[0m],wires=[31m0[0m)[71;118H[K[71;118H108,14-21     28%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m109 [0m[8Cqml.RY(theta_learn[[31m26[0m],wires=[31m5[0m)[71;118H[K[71;118H109,14-21     29%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m110 [0m[8Cqml.CNOT(wires=[[31m0[0m,[31m5[0m])[71;118H[K[71;118H110,14-21     30%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m111 [0m[8Cqml.RY(theta_learn[[31m27[0m],wires=[31m10[0m)[71;118H[K[71;118H111,14-21     30%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m112 [0m[8Cqml.RY(theta_learn[[31m28[0m],wires=[31m14[0m)[71;118H[K[71;118H112,14-21     31%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m113 [0m[8Cqml.CNOT(wires=[[31m14[0m,[31m10[0m])[71;118H[K[71;118H113,14-21     32%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m114 [0m[8C[34m# Last Layer[0m[71;118H[K[71;118H114,14-21     33%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m115 [0m[8Cqml.RY(theta_learn[[31m29[0m],wires=[31m5[0m)[71;118H[K[71;118H115,14-21     33%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m116 [0m[8Cqml.RY(theta_learn[[31m30[0m],wires=[31m10[0m)[71;118H[K[71;118H116,14-21     34%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m117 [0m[71;118H[K[71;118H117,1[9C35%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m118 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m5[0m)), qml.expval(qml.PauliZ(wires=[31m10[0m))[71;118H[K[71;118H118,14-21     36%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m119 [0m[34m#################################################[0m[71;118H[K[71;118H119,21[8C36%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m120 def[0m [36medge_forward[0m(edge_array,theta_learn):[71;118H[K[71;118H120,21[8C37%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m121 [0m[8C[34m# executes TTN_edge circuit for each edge in edge_array[0m[71;118H[K[71;118H121,14-21     38%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m122 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H122,14-21     39%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m123 [0m[8Coutputs = [][71;118H[K[71;118H123,14-21     39%[70;23H[46m[][34h[?25h[?25l[1;70r[0m[70;1H
[1;71r[70;1H[33m124 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(edge_array[:,[31m0[0m])):[71;118H[K[71;118H124,14-21     40%[69;23H[][70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m125 [0m[16Cout = tf.constant(([31m1[0m-TTN_edge_forward(edge_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)[71;118H[K[71;118H125,7-21      41%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m126 [0m[16Coutputs.append(out)[71;118H[K[71;118H126,7-21      42%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m127 [8Creturn[0m tf.stack(outputs)[71;118H[K[71;118H127,14-21     42%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m128 [0m[34m#################################################[0m[71;118H[K[71;118H128,21[8C43%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m129 def[0m [36mnode_forward[0m(node_array,theta_learn):[71;118H[K[71;118H129,21[8C44%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m130 [0m[8C[34m# executes TTN_node circuit for each node in node_array[0m[71;118H[K[71;118H130,14-21     45%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m131 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H131,14-21     45%[70;25H[1;70r[70;1H
[1;71r[70;1H[33m132 [0m[8Coutputs = [][71;118H[K[71;118H132,14-21     46%[70;23H[46m[][34h[?25h[?25l[1;70r[0m[70;1H
[1;71r[70;1H[33m133 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(node_array[:,[31m0[0m])):[71;118H[K[71;118H133,14-21     47%[69;23H[][70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m134 [0m[16Cout = tf.constant([31m4[0m*np.pi*([31m1[0m-TTN_node_forward(node_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)[71;118H[K[71;118H134,7-21      48%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m135 [0m[16Coutputs.append(out)[71;118H[K[71;118H135,7-21      48%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m136 [8Creturn[0m tf.stack(outputs) [34m# output is between [0,4*pi][0m[71;118H[K[71;118H136,14-21     49%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m137 [0m[34m#################################################[0m[71;118H[K[71;118H137,21[8C50%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m138 class[0m [36mEdgeNet[0m(tf.keras.layers.Layer):[71;118H[K[71;118H138,21[8C51%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m139 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'EdgeNet'[0m):[71;118H[K[71;118H139,14-21     51%[70;25H[46m([26C)[70;25H[34h[?25h[?25l[1;70r[0m[70;1H
[1;71r[70;1H[33m140 [0m[16C[36msuper[0m(EdgeNet, self).__init__(name=name)[71;118H[K[71;118H140,7-21      52%[69;25H([26C)[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m141 [0m[16C[34m# can only work with hid_dim = 2[0m[71;118H[K[71;118H141,7-21      53%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m142 [0m[16C[34m# read parameters of the network from file[0m[71;118H[K[71;118H142,7-21      54%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m143 [0m[16C[34m# params are created using tools/init_params.py[0m[71;118H[K[71;118H143,7-21      54%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m144 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('EN',config)[0])[0m[71;118H[K[71;118H144,7-21      55%[70;25H[34h[?25h[?25l[1;70r[1;1H[2M[1;71r[69;1H[33m145 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m19[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m146 [8Cdef[0m [36mcall[0m(self,X, Ri, Ro):[71;118H[K[71;118H146,14-21     57%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m147 [0m[16Cbo = tf.matmul(Ro,X,transpose_a=[36mTrue[0m)[71;118H[K[71;118H147,7-21      57%[70;25H[34h[?25h[?25l[1;70r[1;1H[2M[1;71r[69;1H[33m148 [0m[16Cbi = tf.matmul(Ri,X,transpose_a=[36mTrue[0m)
[33m149 [0m[16C[34m# Shape of B = N_edges x 6 (2x (3 coordinates))[0m[71;118H[K[71;118H149,7-21      59%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m150 [0m[16C[34m# each row consists of two node that are possibly connected.[0m[71;118H[K[71;118H150,7-21      60%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m151 [0m[16CB  = tf.concat([bo, bi], axis=[31m1[0m)[71;118H[K[71;118H151,7-21      60%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m152 [16Creturn[0m edge_forward(B,self.theta_learn)[71;118H[K[71;118H152,7-21      61%[70;25H[34h[?25h[?25l[1;70r[1;1H[2M[1;71r[69;1H[33m153 [0m[34m#################################################[0m
[33m154 class[0m [36mNodeNet[0m(tf.keras.layers.Layer):[71;118H[K[71;118H154,21[8C63%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m155 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'NodeNet'[0m):[71;118H[K[71;118H155,14-21     63%[70;25H[46m([26C)[70;25H[34h[?25h[?25l[1;70r[0m[70;1H
[1;71r[70;1H[33m156 [0m[16C[36msuper[0m(NodeNet, self).__init__(name=name)[71;118H[K[71;118H156,7-21      64%[69;25H([26C)[70;25H[34h[?25h[?25l[1;70r[1;1H[3M[1;71r[68;1H[33m157 [0m[16C[34m# can only work with hid_dim = 1[0m
[33m158 [0m[16C[34m# read parameters of the network from file[0m
[33m159 [0m[16C[34m# params are created using tools/init_params.py[0m[71;118H[K[71;118H159,7-21      66%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m160 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('NN',config)[0])[0m[71;118H[K[71;118H160,7-21      67%[70;25H[34h[?25h[?25l[1;70r[1;1H[2M[1;71r[69;1H[33m161 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m31[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m162 [8Cdef[0m [36mcall[0m(self, X, e, Ri, Ro):[71;118H[K[71;118H162,14-21     69%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m163 [0m[16Cbo  = tf.matmul(Ro, X, transpose_a=[36mTrue[0m)[71;118H[K[71;118H163,7-21      69%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m164 [0m[16Cbi  = tf.matmul(Ri, X, transpose_a=[36mTrue[0m)[71;118H[K[71;118H164,7-21      70%[70;25H[34h[?25h[?25l[1;70r[1;1H[2M[1;71r[69;1H[33m165 [0m[16CRwo = tf.math.multiply(Ro,e)
[33m166 [0m[16CRwi = tf.math.multiply(Ri,e)[71;118H[K[71;118H166,7-21      72%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m167 [0m[16Cmi = tf.matmul(Rwi, bo)[71;118H[K[71;118H167,7-21      72%[70;25H[34h[?25h[?25l[1;70r[1;1H[3M[1;71r[68;1H[33m168 [0m[16Cmo = tf.matmul(Rwo, bi)
[33m169 [0m[16CM = tf.concat([mi, mo, X], axis=[31m1[0m)
[33m170 [16Creturn[0m node_forward(M,self.theta_learn)[71;118H[K[71;118H170,7-21      75%[70;25H[34h[?25h[?25l[1;70r[1;1H[3M[1;71r[68;1H[33m171 [0m[34m#################################################[0m
[33m172 class[0m [36mInputNet[0m(tf.keras.layers.Layer):
[33m173 [8Cdef[0m [36m__init__[0m(self, config, name):[71;118H[K[71;118H173,14-21     77%[70;25H[46m([18C)[70;25H[34h[?25h[?25l[1;70r[0m[70;1H
[1;71r[70;1H[33m174 [0m[16C[36msuper[0m(InputNet, self).__init__(name=name)[71;118H[K[71;118H174,7-21      78%[69;25H([18C)[70;25H[34h[?25h[?25l[1;70r[1;1H[2M[1;71r[69;1H[33m175 [0m[16Cself.num_outputs = config[[31m'hid_dim'[0m] [34m# num_outputs = number of hidden dimensions[0m
[33m176 [0m[16C[34m# read parameters of the network from file[0m[71;118H[K[71;118H176,7-21      79%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m177 [0m[16C[34m# params are created using tools/init_params.py[0m[71;118H[K[71;118H177,7-21      80%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m178 [0m[16C[34m#init = tf.constant_initializer(get_params('IN',config)[0])[0m[71;118H[K[71;118H178,7-21      81%[70;25H[34h[?25h[?25l[1;70r[1;1H[2M[1;71r[69;1H[33m179 [0m[16C[34m# setup a Dense layer with the given config[0m
[33m180 [0m[16Cself.layer = tf.keras.layers.Dense(self.num_outputs,input_shape=([31m3[0m,),activation=[31m'sigmoid'[0m)[71;118H[K[71;118H180,7-21      82%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m181 [8Cdef[0m [36mcall[0m(self, arr):[71;118H[K[71;118H181,14-21     83%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m182 [16Creturn[0m self.layer(arr)*[31m4[0m*np.pi [34m# to map to output to [0,2*pi][0m[71;118H[K[71;118H182,7-21      84%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m183 [0m[34m#################################################[0m[71;118H[K[71;118H183,21[8C84%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m184 class[0m [36mGNN[0m(tf.keras.Model):[71;118H[K[71;118H184,21[8C85%[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m185 [8Cdef[0m [36m__init__[0m(self, config):[71;118H[K[71;118H185,14-21     86%[70;25H[46m([12C)[70;25H[34h[?25h[?25l[1;70r[0m[70;1H
[1;71r[70;1H[33m186 [0m[16C[36msuper[0m(GNN, self).__init__(name=[31m'GNN'[0m)[71;118H[K[71;118H186,7-21      87%[69;25H([12C)[70;25H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m187 [0m[71;118H[K[71;118H187,1[9C87%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m188 [0m[16Cself.InputNet = InputNet(config,name=[31m'InputNet'[0m)[71;118H[K[71;118H188,7-21      88%[70;25H[1;70r[70;1H
[1;71r[70;1H[33m189 [0m[16Cself.EdgeNet  = EdgeNet(config,name=[31m'EdgeNet'[0m)[71;118H[K[71;118H189,7-21      89%[70;25H[34h[?25h[71;1H[K[?25l[71;118H189,6-20      89%[70;24H[34h[?25h[?25l[71;118H[K[71;1H:[34h[?25hwq[?25l"qnetworks/GNN2.py" 203L, 7254C written
[?1l>[34h[?25h[?1049lSingularity> vim qnetworks/GNN2.py Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
Traceback (most recent call last):
  File "train.py", line 68, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 198, in call
    e = self.EdgeNet(H, Ri, Ro)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 152, in call
    return edge_forward(B,self.theta_learn)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 125, in edge_forward
    out = tf.constant((1-TTN_edge_forward(edge_array[i,:],theta_learn[0,:]))/2.,dtype=tf.float64)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1138, in _SliceHelperVar
    return _slice_helper(var.value(), slice_spec, var)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 898, in _slice_helper
    name=name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1064, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py", line 9513, in strided_slice
    _ops.raise_from_not_ok_status(e, name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: GNN/EdgeNet/strided_slice/
Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml Singularity> vim qnetworks/GNN2.py [K
[?1049h[?1h=[1;71r[34l[34h[?25h[23m[24m[0m[H[J[?25l[71;1H"qnetworks/GNN2.py" 203L, 7254C[1;1H[33m134 [0m[16Cout = tf.constant([31m4[0m*np.pi*([31m1[0m-TTN_node_forward(node_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)
[33m135 [0m[16Coutputs.append(out)
[33m136 [8Creturn[0m tf.stack(outputs) [34m# output is between [0,4*pi][0m
[33m137 [0m[34m#################################################[0m
[33m138 class[0m [36mEdgeNet[0m(tf.keras.layers.Layer):
[33m139 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'EdgeNet'[0m):
[33m140 [0m[16C[36msuper[0m(EdgeNet, self).__init__(name=name)
[33m141 [0m[16C[34m# can only work with hid_dim = 2[0m
[33m142 [0m[16C[34m# read parameters of the network from file[0m
[33m143 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m144 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('EN',config)[0])[0m
[33m145 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m19[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m146 [8Cdef[0m [36mcall[0m(self,X, Ri, Ro):
[33m147 [0m[16Cbo = tf.matmul(Ro,X,transpose_a=[36mTrue[0m)
[33m148 [0m[16Cbi = tf.matmul(Ri,X,transpose_a=[36mTrue[0m)
[33m149 [0m[16C[34m# Shape of B = N_edges x 6 (2x (3 coordinates))[0m
[33m150 [0m[16C[34m# each row consists of two node that are possibly connected.[0m
[33m151 [0m[16CB  = tf.concat([bo, bi], axis=[31m1[0m)
[33m152 [16Creturn[0m edge_forward(B,self.theta_learn)
[33m153 [0m[34m#################################################[0m
[33m154 class[0m [36mNodeNet[0m(tf.keras.layers.Layer):
[33m155 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'NodeNet'[0m):
[33m156 [0m[16C[36msuper[0m(NodeNet, self).__init__(name=name)
[33m157 [0m[16C[34m# can only work with hid_dim = 1[0m
[33m158 [0m[16C[34m# read parameters of the network from file[0m
[33m159 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m160 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('NN',config)[0])[0m
[33m161 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m31[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m162 [8Cdef[0m [36mcall[0m(self, X, e, Ri, Ro):
[33m163 [0m[16Cbo  = tf.matmul(Ro, X, transpose_a=[36mTrue[0m)
[33m164 [0m[16Cbi  = tf.matmul(Ri, X, transpose_a=[36mTrue[0m)
[33m165 [0m[16CRwo = tf.math.multiply(Ro,e)
[33m166 [0m[16CRwi = tf.math.multiply(Ri,e)
[33m167 [0m[16Cmi = tf.matmul(Rwi, bo)
[33m168 [0m[16Cmo = tf.matmul(Rwo, bi)
[33m169 [0m[16CM = tf.concat([mi, mo, X], axis=[31m1[0m)
[33m170 [16Creturn[0m node_forward(M,self.theta_learn)
[33m171 [0m[34m#################################################[0m
[33m172 class[0m [36mInputNet[0m(tf.keras.layers.Layer):
[33m173 [8Cdef[0m [36m__init__[0m(self, config, name):
[33m174 [0m[16C[36msuper[0m(InputNet, self).__init__(name=name)
[33m175 [0m[16Cself.num_outputs = config[[31m'hid_dim'[0m] [34m# num_outputs = number of hidden dimensions[0m
[33m176 [0m[16C[34m# read parameters of the network from file[0m
[33m177 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m178 [0m[16C[34m#init = tf.constant_initializer(get_params('IN',config)[0])[0m
[33m179 [0m[16C[34m# setup a Dense layer with the given config[0m
[33m180 [0m[16Cself.layer = tf.keras.layers.Dense(self.num_outputs,input_shape=([31m3[0m,),activation=[31m'sigmoid'[0m)
[33m181 [8Cdef[0m [36mcall[0m(self, arr):
[33m182 [16Creturn[0m self.layer(arr)*[31m4[0m*np.pi [34m# to map to output to [0,2*pi][0m
[33m183 [0m[34m#################################################[0m
[33m184 class[0m [36mGNN[0m(tf.keras.Model):
[33m185 [8Cdef[0m [36m__init__[0m(self, config):
[33m186 [0m[16C[36msuper[0m(GNN, self).__init__(name=[31m'GNN'[0m)
[33m187 
188 [0m[16Cself.InputNet = InputNet(config,name=[31m'InputNet'[0m)
[33m189 [0m[16Cself.EdgeNet  = EdgeNet(config,name=[31m'EdgeNet'[0m)
[33m190 [0m[16Cself.NodeNet  = NodeNet(config,name=[31m'NodeNet'[0m)
[33m191 [0m[16Cself.n_iters = config[[31m'n_iters'[0m]
[33m192 
193 [8Cdef[0m [36mcall[0m(self, edge_array):
[33m194 [0m[16CX,Ri,Ro = edge_array
[33m195 [0m[16CH = self.InputNet(X)
[33m196 [0m[16CH = tf.concat([H,X],axis=[31m1[0m)
[33m197 [16Cfor[0m i [33min[0m [36mrange[0m(self.n_iters):
[33m198 [0m[24Ce = self.EdgeNet(H, Ri, Ro)
[33m199 [0m[24CH = self.NodeNet(H, e, Ri, Ro)
[33m200 [0m[24CH = tf.concat([H,X], axis=[31m1[0m)
[33m201 [0m[16Ce = self.EdgeNet(H, Ri, Ro)
[33m202 [16Creturn[0m e
[33m203 [0m[34m#################################################[0m[71;118H189,3-17      Bot[56;21H[34h[?25h[?25l[71;119H90[57;21H[34h[?25h[?25l[71;120H1[58;21H[34h[?25h[?25l[71;120H2,0-1 [59;5H[34h[?25h[?25l[60;21H[46m([16C)[0m[71;120H3,10-17[60;21H[34h[?25h[?25l([16C)[71;120H4,3-17 [61;21H[34h[?25h[?25l[71;120H5[62;21H[34h[?25h[?25l[71;120H6[63;21H[34h[?25h[?25l[71;120H7[64;21H[34h[?25h[?25l[71;120H8,3-24[65;28H[34h[?25h[?25l[71;120H9[66;28H[34h[?25h[?25l[71;120H8[65;28H[34h[?25h[?25l[71;120H7,3-17[64;21H[34h[?25h[?25l[71;120H6[63;21H[34h[?25h[?25l[71;120H5[62;21H[34h[?25h[?25l[71;120H4[61;21H[34h[?25h[?25l[60;21H[46m([16C)[0m[71;120H3,10-17[60;21H[34h[?25h[?25l([16C)[71;120H2,0-1  [59;5H[34h[?25h[?25l[71;120H1,3-17[58;21H[34h[?25h[?25l[71;120H0[57;21H[34h[?25h[?25l[71;119H89[56;21H[34h[?25h[?25l[71;120H8[55;21H[34h[?25h[?25l[71;120H7,0-1 [54;5H[34h[?25h[?25l[71;120H6,3-17[53;21H[34h[?25h[?25l[71;120H5,10-17[52;21H[34h[?25h[?25l[71;120H4,17   [51;21H[34h[?25h[?25l[71;120H3[50;21H[34h[?25h[?25l[71;120H2,3-17[49;21H[34h[?25h[?25l[48;21H[46m([9C)[0m[71;120H1,10-17[48;21H[34h[?25h[?25l([9C)[71;120H0,3-17 [47;21H[34h[?25h[?25l[71;119H79[46;21H[34h[?25h[?25l[71;120H8[45;21H[34h[?25h[?25l[71;120H7[44;21H[34h[?25h[?25l[71;120H6[43;21H[34h[?25h[?25l[71;120H5[42;21H[34h[?25h[?25l[71;120H4[41;21H[34h[?25h[?25l[71;120H3,10-17[40;21H[34h[?25h[?25l[71;120H2,17   [39;21H[34h[?25h[?25l[71;120H1[38;21H[34h[?25h[?25l[71;120H0,3-17[37;21H[34h[?25h[?25l[71;119H69[36;21H[34h[?25h[?25l[71;120H8[35;21H[34h[?25h[?25l[71;120H7[34;21H[34h[?25h[?25l[71;120H6[33;21H[34h[?25h[?25l[71;120H5[32;21H[34h[?25h[?25l[71;120H4[31;21H[34h[?25h[?25l[71;120H3[30;21H[34h[?25h[?25l[29;21H[46m([18C)[0m[71;120H2,10-17[29;21H[34h[?25h[?25l([18C)[71;120H1,3-17 [28;21H[34h[?25h[?25l[71;120H0[27;21H[34h[?25h[?25l[71;119H59[26;21H[34h[?25h[?25l[71;120H8[25;21H[34h[?25h[?25l[71;120H7[24;21H[34h[?25h[?25l[71;120H6[23;21H[34h[?25h[?25l[71;120H5,10-17[22;21H[34h[?25h[?25l[71;120H4,17   [21;21H[34h[?25h[?25l[71;120H3[20;21H[34h[?25h[?25l[71;120H2,3-17[19;21H[34h[?25h[?25l[71;120H1[18;21H[34h[?25h[?25l[71;120H0[17;21H[34h[?25h[?25l[71;119H49[16;21H[34h[?25h[?25l[71;120H8[15;21H[34h[?25h[?25l[71;120H7[14;21H[34h[?25h[?25l[13;21H[46m([14C)[0m[71;120H6,10-17[13;21H[34h[?25h[?25l([14C)[71;120H5,3-17 [12;21H[34h[?25h[?25l[71;120H4[11;21H[34h[?25h[?25l[71;120H3[10;21H[34h[?25h[?25l[71;120H2[9;21H[34h[?25h[?25l[71;120H1[8;21H[34h[?25h[?25l[71;120H0[7;21H[34h[?25h[?25l[71;119H39,10-17[6;21H[34h[?25h[?25l[71;120H8,17   [5;21H[34h[?25h[?25l[71;120H7[4;21H[34h[?25h[?25l[71;120H6,10-17[3;21H[34h[?25h[?25l[71;120H5,3-17 [2;21H[34h[?25h[?25l[71;120H4[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m133 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(node_array[:,[31m0[0m])):[71;1H[K[71;118H133,10-17     99%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m132 [0m[8Coutputs = [][71;118H[K[71;118H132,10-17     98%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m131 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H131,10-17     97%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m130 [0m[8C[34m# executes TTN_node circuit for each node in node_array[0m[71;118H[K[71;118H130,10-17     96%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m129 def[0m [36mnode_forward[0m[46m([0mnode_array,theta_learn[46m)[0m:[71;118H[K[71;118H129,17[8C96%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m128 [0m[34m#################################################[0m[2;21H([22C)[71;118H[K[71;118H128,17[8C95%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m127 [8Creturn[0m tf.stack(outputs)[71;118H[K[71;118H127,10-17     94%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m126 [0m[16Coutputs.append(out)[71;118H[K[71;118H126,3-17      93%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m125 [0m[16Cout = tf.constant(([31m1[0m-TTN_edge_forward(edge_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)[71;118H[K[71;118H125,3-17      93%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m124 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(edge_array[:,[31m0[0m])):[71;118H[K[71;118H124,10-17     92%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m123 [0m[8Coutputs = [][71;118H[K[71;118H123,10-17     91%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m122 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H122,10-17     90%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m121 [0m[8C[34m# executes TTN_edge circuit for each edge in edge_array[0m[71;118H[K[71;118H121,10-17     90%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m120 def[0m [36medge_forward[0m[46m([0medge_array,theta_learn[46m)[0m:[71;118H[K[71;118H120,17[8C89%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m119 [0m[34m#################################################[0m[2;21H([22C)[71;118H[K[71;118H119,17[8C88%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m118 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m5[0m)), qml.expval(qml.PauliZ(wires=[31m10[0m))[71;118H[K[71;118H118,10-17     87%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m117 [0m[71;118H[K[71;118H117,0-1[7C87%[1;5H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m116 [0m[8Cqml.RY(theta_learn[[31m30[0m],wires=[31m10[0m)[71;118H[K[71;118H116,10-17     86%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m115 [0m[8Cqml.RY(theta_learn[[31m29[0m],wires=[31m5[0m)[71;118H[K[71;118H115,10-17     85%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m114 [0m[8C[34m# Last Layer[0m[71;118H[K[71;118H114,10-17     84%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m113 [0m[8Cqml.CNOT[46m([0mwires=[[31m14[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H113,10-17     84%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m112 [0m[8Cqml.RY(theta_learn[[31m28[0m],wires=[31m14[0m)[2;21H([13C)[71;118H[K[71;118H112,10-17     83%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m111 [0m[8Cqml.RY(theta_learn[[31m27[0m],wires=[31m10[0m)[71;118H[K[71;118H111,10-17     82%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m110 [0m[8Cqml.CNOT[46m([0mwires=[[31m0[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H110,10-17     81%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m109 [0m[8Cqml.RY(theta_learn[[31m26[0m],wires=[31m5[0m)[2;21H([11C)[71;118H[K[71;118H109,10-17     81%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m108 [0m[8Cqml.RY(theta_learn[[31m25[0m],wires=[31m0[0m)[71;118H[K[71;118H108,10-17     80%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m107 [0m[8C[34m# Fifth Layer[0m[71;118H[K[71;118H107,10-17     79%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m106 [0m[8Cqml.RY(theta_learn[[31m24[0m],wires=[31m10[0m)[71;118H[K[71;118H106,10-17     78%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m105 [0m[8Cqml.RY(theta_learn[[31m23[0m],wires=[31m5[0m)[71;118H[K[71;118H105,10-17     78%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m104 [0m[8C[34m# Forth Layer[0m[71;118H[K[71;118H104,10-17     77%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m103 [0m[8Cqml.CNOT[46m([0mwires=[[31m13[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H103,10-17     76%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m102 [0m[8Cqml.RY(theta_learn[[31m22[0m],wires=[31m13[0m)[2;21H([13C)[71;118H[K[71;118H102,10-17     75%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m101 [0m[8Cqml.RY(theta_learn[[31m21[0m],wires=[31m10[0m)[71;118H[K[71;118H101,10-17     75%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m100 [0m[8Cqml.CNOT[46m([0mwires=[[31m2[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H100,10-17     74%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 99 [0m[8Cqml.RY(theta_learn[[31m20[0m],wires=[31m5[0m)[2;21H([11C)[71;118H[K[71;118H99,10-17      73%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 98 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m2[0m)[71;118H[K[71;118H98,10-17      72%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 97 [0m[8C[34m# Third Layer[0m[71;118H[K[71;118H97,10-17      72%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 96 [0m[8Cqml.CNOT[46m([0mwires=[[31m9[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H96,10-17      71%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 95 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m14[0m)[2;21H([12C)[71;118H[K[71;118H95,10-17      70%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 94 [0m[8Cqml.RY(theta_learn[[31m18[0m],wires=[31m13[0m)[71;118H[K[71;118H94,10-17      69%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 93 [0m[8Cqml.CNOT[46m([0mwires=[[31m9[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H93,10-17      69%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 92 [0m[8Cqml.RY(theta_learn[[31m17[0m],wires=[31m10[0m)[2;21H([12C)[71;118H[K[71;118H92,10-17      68%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 91 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m9[0m)[71;118H[K[71;118H91,10-17      67%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 90 [0m[8Cqml.CNOT[46m([0mwires=[[31m6[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H90,10-17      66%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 89 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m6[0m)[2;21H([11C)[71;118H[K[71;118H89,10-17      66%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 88 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m5[0m)[71;118H[K[71;118H88,10-17      65%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 87 [0m[8Cqml.CNOT[46m([0mwires=[[31m1[0m,[31m2[0m][46m)[0m[71;118H[K[71;118H87,10-17      64%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 86 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m2[0m)[2;21H([11C)[71;118H[K[71;118H86,10-17      63%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 85 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m1[0m)[71;118H[K[71;118H85,10-17      63%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 84 [0m[8C[34m# Second Layer[0m[71;118H[K[71;118H84,10-17      62%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 83 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m14[0m)[71;118H[K[71;118H83,10-17      61%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 82 [0m[8Cqml.CNOT[46m([0mwires=[[31m8[0m,[31m9[0m][46m)[0m[71;118H[K[71;118H82,10-17      60%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 81 [0m[8Cqml.RY(theta_learn[[31m13[0m],wires=[31m13[0m)[2;21H([11C)[71;118H[K[71;118H81,10-17      60%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 80 [0m[8Cqml.RY(theta_learn[[31m12[0m],wires=[31m12[0m)[71;118H[K[71;118H80,10-17      59%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 79 [0m[8Cqml.CNOT[46m([0mwires=[[31m11[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H79,10-17      58%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 78 [0m[8Cqml.RY(theta_learn[[31m11[0m],wires=[31m11[0m)[2;21H([13C)[71;118H[K[71;118H78,10-17      57%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 77 [0m[8Cqml.RY(theta_learn[[31m10[0m],wires=[31m10[0m)[71;118H[K[71;118H77,10-17      57%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 76 [0m[8Cqml.CNOT[46m([0mwires=[[31m8[0m,[31m9[0m][46m)[0m[71;118H[K[71;118H76,10-17      56%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 75 [0m[8Cqml.RY(theta_learn[[31m9[0m],wires=[31m9[0m)[2;21H([11C)[71;118H[K[71;118H75,10-17      55%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 74 [0m[8Cqml.RY(theta_learn[[31m8[0m],wires=[31m8[0m)[71;118H[K[71;118H74,10-17      54%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 73 [0m[8Cqml.CNOT[46m([0mwires=[[31m7[0m,[31m6[0m][46m)[0m[71;118H[K[71;118H73,10-17      54%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 72 [0m[8Cqml.RY(theta_learn[[31m7[0m],wires=[31m7[0m)[2;21H([11C)[71;118H[K[71;118H72,10-17      53%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 71 [0m[8Cqml.RY(theta_learn[[31m6[0m],wires=[31m6[0m)[71;118H[K[71;118H71,10-17      52%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 70 [0m[8Cqml.CNOT[46m([0mwires=[[31m4[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H70,10-17      51%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 69 [0m[8Cqml.RY(theta_learn[[31m5[0m],wires=[31m5[0m)[2;21H([11C)[71;118H[K[71;118H69,10-17      51%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 68 [0m[8Cqml.RY(theta_learn[[31m4[0m],wires=[31m4[0m)[71;118H[K[71;118H68,10-17      50%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 67 [0m[8Cqml.CNOT[46m([0mwires=[[31m3[0m,[31m2[0m][46m)[0m[71;118H[K[71;118H67,10-17      49%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 66 [0m[8Cqml.RY(theta_learn[[31m3[0m],wires=[31m3[0m)[2;21H([11C)[71;118H[K[71;118H66,10-17      48%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 65 [0m[8Cqml.RY(theta_learn[[31m2[0m],wires=[31m2[0m)[71;118H[K[71;118H65,10-17      48%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 64 [0m[8Cqml.CNOT[46m([0mwires=[[31m0[0m,[31m1[0m][46m)[0m[71;118H[K[71;118H64,10-17      47%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 63 [0m[8Cqml.RY(theta_learn[[31m1[0m],wires=[31m1[0m)[2;21H([11C)[71;118H[K[71;118H63,10-17      46%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 62 [0m[8Cqml.RY(theta_learn[[31m0[0m],wires=[31m0[0m)[71;118H[K[71;118H62,10-17      45%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 61 [0m[8C[34m# First Layer[0m[71;118H[K[71;118H61,10-17      45%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 60 [0m[8C[34m# APPLY forward sequence[0m[71;118H[K[71;118H60,10-17      44%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 59 [0m[16Cqml.RY(edge[i],wires=i)[71;118H[K[71;118H59,3-17[7C43%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 58 [8Cfor[0m i [33min[0m [36mrange[0m([31m15[0m):[71;118H[K[71;118H58,10-17      42%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 57 [0m[8C[34m# STATE PREPARATION[0m[71;118H[K[71;118H57,10-17      42%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 56 def[0m [36mTTN_node_forward[0m(edge,theta_learn):[71;118H[K[71;118H56,17[9C41%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 55 [0m[35m@[0m[36mqml.qnode[0m(dev2,interface=[31m'tf'[0m)[71;118H[K[71;118H55,17[9C40%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 54 [0mdev2 = qml.device([31m"qulacs.simulator"[0m, wires=[31m15[0m,gpu=[36mTrue[0m)[71;118H[K[71;118H54,17[9C39%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 53 [0m[34m# use tf.interface for TF integration[0m[71;118H[K[71;118H53,17[9C39%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 52 [0m[34m# Use default.qubit for default pennylane simulation[0m[71;118H[K[71;118H52,17[9C38%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 51 [0m[34m##################################################################################################[0m[71;118H[K[71;118H51,17[9C37%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 50 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m7[0m))[71;118H[K[71;118H50,10-17      36%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 49 [0m[8Cqml.RY(theta_learn[[31m18[0m],wires=[31m7[0m)[71;118H[K[71;118H49,10-17      36%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 48 [0m[8C[34m#Last Layer[0m[71;118H[K[71;118H48,10-17      35%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 47 [0m[8Cqml.CNOT[46m([0mwires=[[31m4[0m,[31m7[0m][46m)[0m[71;118H[K[71;118H47,10-17      34%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 46 [0m[8Cqml.RY(theta_learn[[31m17[0m],wires=[31m7[0m)[2;21H([11C)[71;118H[K[71;118H46,10-17      33%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 45 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m4[0m)[71;118H[K[71;118H45,10-17      33%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 44 [0m[8C[34m# Forth Layer[0m[71;118H[K[71;118H44,10-17      32%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 43 [0m[8Cqml.CNOT[46m([0mwires=[[31m2[0m,[31m4[0m][46m)[0m[71;118H[K[71;118H43,10-17      31%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 42 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m4[0m)[2;21H([11C)[71;118H[K[71;118H42,10-17      30%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 41 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m2[0m)[71;118H[K[71;118H41,10-17      30%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 40 [0m[8C[34m# Third Layer[0m[71;118H[K[71;118H40,10-17      29%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 39 [0m[8Cqml.CNOT[46m([0mwires=[[31m9[0m,[31m7[0m][46m)[0m[71;118H[K[71;118H39,10-17      28%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 38 [0m[8Cqml.RY(theta_learn[[31m13[0m],wires=[31m9[0m)[2;21H([11C)[71;118H[K[71;118H38,10-17      27%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 37 [0m[8Cqml.RY(theta_learn[[31m12[0m],wires=[31m7[0m)[71;118H[K[71;118H37,10-17      27%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 36 [0m[8Cqml.CNOT[46m([0mwires=[[31m1[0m,[31m2[0m][46m)[0m[71;118H[K[71;118H36,10-17      26%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 35 [0m[8Cqml.RY(theta_learn[[31m11[0m],wires=[31m2[0m)[2;21H([11C)[71;118H[K[71;118H35,10-17      25%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 34 [0m[8Cqml.RY(theta_learn[[31m10[0m],wires=[31m1[0m)[71;118H[K[71;118H34,10-17      24%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 33 [0m[8C[34m# Second Layer[0m[71;118H[K[71;118H33,10-17      24%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 32 [0m[8Cqml.CNOT[46m([0mwires=[[31m8[0m,[31m9[0m][46m)[0m[71;118H[K[71;118H32,10-17      23%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 31 [0m[8Cqml.RY(theta_learn[[31m9[0m],wires=[31m9[0m)[2;21H([11C)[71;118H[K[71;118H31,10-17      22%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 30 [0m[8Cqml.RY(theta_learn[[31m8[0m],wires=[31m8[0m)[71;118H[K[71;118H30,10-17      21%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 29 [0m[8Cqml.CNOT[46m([0mwires=[[31m6[0m,[31m7[0m][46m)[0m[71;118H[K[71;118H29,10-17      21%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 28 [0m[8Cqml.RY(theta_learn[[31m7[0m],wires=[31m7[0m)[2;21H([11C)[71;118H[K[71;118H28,10-17      20%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 27 [0m[8Cqml.RY(theta_learn[[31m6[0m],wires=[31m6[0m)[71;118H[K[71;118H27,10-17      19%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 26 [0m[8Cqml.CNOT[46m([0mwires=[[31m5[0m,[31m4[0m][46m)[0m[71;118H[K[71;118H26,10-17      18%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 25 [0m[8Cqml.RY(theta_learn[[31m5[0m],wires=[31m5[0m)[2;21H([11C)[71;118H[K[71;118H25,10-17      18%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 24 [0m[8Cqml.RY(theta_learn[[31m4[0m],wires=[31m4[0m)[71;118H[K[71;118H24,10-17      17%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 23 [0m[8Cqml.CNOT[46m([0mwires=[[31m3[0m,[31m2[0m][46m)[0m[71;118H[K[71;118H23,10-17      16%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 22 [0m[8Cqml.RY(theta_learn[[31m3[0m],wires=[31m3[0m)[2;21H([11C)[71;118H[K[71;118H22,10-17      15%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 21 [0m[8Cqml.RY(theta_learn[[31m2[0m],wires=[31m2[0m)[71;118H[K[71;118H21,10-17      15%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 20 [0m[8Cqml.CNOT[46m([0mwires=[[31m0[0m,[31m1[0m][46m)[0m[71;118H[K[71;118H20,10-17      14%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 19 [0m[8Cqml.RY(theta_learn[[31m1[0m],wires=[31m1[0m)[2;21H([11C)[71;118H[K[71;118H19,10-17      13%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 18 [0m[8Cqml.RY(theta_learn[[31m0[0m],wires=[31m0[0m)[71;118H[K[71;118H18,10-17      12%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 17 [0m[8C[34m# First Layer[0m[71;118H[K[71;118H17,10-17      12%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 16 [0m[8C[34m# APPLY forward sequence[0m[71;118H[K[71;118H16,10-17      11%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 15 [0m[16Cqml.RY(edge[i],wires=i)[71;118H[K[71;118H15,3-17[7C10%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 14 [8Cfor[0m i [33min[0m [36mrange[0m([31m10[0m):[71;118H[K[71;118H14,10-17[7C9%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 13 [0m[8C[34m# STATE PREPARATION[0m[71;118H[K[71;118H13,10-17[7C9%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 12 def[0m [36mTTN_edge_forward[0m(edge,theta_learn):[71;118H[K[71;118H12,17[10C8%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 11 [0m[35m@[0m[36mqml.qnode[0m(dev1,interface=[31m'tf'[0m)[71;118H[K[71;118H11,17[10C7%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 10 [0mdev1 = qml.device([31m"qulacs.simulator"[0m, wires=[31m10[0m, gpu=[36mTrue[0m)[71;118H[K[71;118H10,17[10C6%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m  9 [0m[34m# use tf.interface for TF integration[0m[71;118H[K[71;118H9,17[11C6%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m  8 [0m[34m# Use default.qubit for default pennylane simulation[0m[71;118H[K[71;118H8,17[11C5%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m  7 [0m[34m##################################################################################################[0m[71;118H[K[71;118H7,17[11C4%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m  6 [0m[35mimport[0m os[71;118H[K[71;118H6,9[12C3%[1;13H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m  5 [0m[35mimport[0m pennylane_qulacs[71;118H[K[71;118H5,17[11C3%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m  4 [0m[35mfrom[0m tools.tools [35mimport[0m get_params[71;118H[K[71;118H4,17[11C2%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m  3 [0m[35mimport[0m tensorflow [33mas[0m tf[71;118H[K[71;118H3,17[11C1%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m  2 [0m[35mfrom[0m pennylane [35mimport[0m numpy [33mas[0m np[71;118H[K[71;118H2,17[11C0%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m  1 [0m[35mimport[0m pennylane [33mas[0m qml[71;118H[K[71;118H1,17[10CTop[1;21H[34h[?25h[?25l[71;118H2[2;21H[34h[?25h[?25l[71;118H3[3;21H[34h[?25h[?25l[71;118H4[4;21H[34h[?25h[?25l[71;118H5[5;21H[34h[?25h[?25l[71;118H6,9 [6;13H[34h[?25h[?25l[71;118H7,17[7;21H[34h[?25h[?25l[71;118H8[8;21H[34h[?25h[?25l[71;118H9[9;21H[34h[?25h[?25l[71;118H10,17[10;21H[34h[?25h[?25l[71;119H1[11;21H[34h[?25h[?25l[71;119H2[12;21H[34h[?25h[?25l[71;119H3,10-17[13;21H[34h[?25h[?25l[71;119H4[14;21H[34h[?25h[?25l[71;119H5,3-17 [15;21H[34h[?25h[?25l[71;119H6,10-17[16;21H[34h[?25h[?25l[71;119H7[17;21H[34h[?25h[?25l[71;119H8[18;21H[34h[?25h[?25l[71;119H9[19;21H[34h[?25h[?25l[20;21H[46m([11C)[0m[71;118H20[20;21H[34h[?25h[?25l([11C)[71;119H1[21;21H[34h[?25h[?25l[71;119H2[22;21H[34h[?25h[?25l[23;21H[46m([11C)[0m[71;119H3[23;21H[34h[?25h[?25l([11C)[71;119H4[24;21H[34h[?25h[?25l[71;119H5[25;21H[34h[?25h[?25l[26;21H[46m([11C)[0m[71;119H6[26;21H[34h[?25h[?25l([11C)[71;119H7[27;21H[34h[?25h[?25l[71;119H8[28;21H[34h[?25h[?25l[29;21H[46m([11C)[0m[71;119H9[29;21H[34h[?25h[?25l([11C)[71;118H30[30;21H[34h[?25h[?25l[71;119H1[31;21H[34h[?25h[?25l[32;21H[46m([11C)[0m[71;119H2[32;21H[34h[?25h[?25l([11C)[71;119H3[33;21H[34h[?25h[?25l[71;119H4[34;21H[34h[?25h[?25l[71;119H5[35;21H[34h[?25h[?25l[36;21H[46m([11C)[0m[71;119H6[36;21H[34h[?25h[?25l([11C)[71;119H7[37;21H[34h[?25h[?25l[71;119H8[38;21H[34h[?25h[?25l[39;21H[46m([11C)[0m[71;119H9[39;21H[34h[?25h[?25l([11C)[71;118H40[40;21H[34h[?25h[?25l[71;119H1[41;21H[34h[?25h[?25l[71;119H2[42;21H[34h[?25h[?25l[43;21H[46m([11C)[0m[71;119H3[43;21H[34h[?25h[?25l([11C)[71;119H4[44;21H[34h[?25h[?25l[71;119H5[45;21H[34h[?25h[?25l[71;119H6[46;21H[34h[?25h[?25l[47;21H[46m([11C)[0m[71;119H7[47;21H[34h[?25h[?25l([11C)[71;119H8[48;21H[34h[?25h[?25l[71;119H9[49;21H[34h[?25h[?25l[71;118H50[50;21H[34h[?25h[?25l[71;119H1,17   [51;21H[34h[?25h[?25l[71;119H2[52;21H[34h[?25h[?25l[71;119H3[53;21H[34h[?25h[?25l[71;119H4[54;21H[34h[?25h[?25l[71;119H5[55;21H[34h[?25h[?25l[71;119H6[56;21H[34h[?25h[?25l[71;119H7,10-17[57;21H[34h[?25h[?25l[71;119H8[58;21H[34h[?25h[?25l[71;119H9,3-17 [59;21H[34h[?25h[?25l[71;118H60,10-17[60;21H[34h[?25h[?25l[71;119H1[61;21H[34h[?25h[?25l[71;119H2[62;21H[34h[?25h[?25l[71;119H3[63;21H[34h[?25h[?25l[64;21H[46m([11C)[0m[71;119H4[64;21H[34h[?25h[?25l([11C)[71;119H5[65;21H[34h[?25h[?25l[71;119H6[66;21H[34h[?25h[?25l[67;21H[46m([11C)[0m[71;119H7[67;21H[34h[?25h[?25l([11C)[71;119H8[68;21H[34h[?25h[?25l[71;119H9[69;21H[34h[?25h[?25l[70;21H[46m([11C)[0m[71;118H70[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([11C)
[33m 71 [0m[8Cqml.RY(theta_learn[[31m6[0m],wires=[31m6[0m)[71;118H[K[71;118H71,10-17[7C0%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 72 [0m[8Cqml.RY(theta_learn[[31m7[0m],wires=[31m7[0m)[71;118H[K[71;118H72,10-17[7C1%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 73 [0m[8Cqml.CNOT[46m([0mwires=[[31m7[0m,[31m6[0m][46m)[0m[71;118H[K[71;118H73,10-17[7C2%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([11C)
[33m 74 [0m[8Cqml.RY(theta_learn[[31m8[0m],wires=[31m8[0m)[71;118H[K[71;118H74,10-17[7C3%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 75 [0m[8Cqml.RY(theta_learn[[31m9[0m],wires=[31m9[0m)[71;118H[K[71;118H75,10-17[7C3%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 76 [0m[8Cqml.CNOT[46m([0mwires=[[31m8[0m,[31m9[0m][46m)[0m[71;118H[K[71;118H76,10-17[7C4%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([11C)
[33m 77 [0m[8Cqml.RY(theta_learn[[31m10[0m],wires=[31m10[0m)[71;118H[K[71;118H77,10-17[7C5%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 78 [0m[8Cqml.RY(theta_learn[[31m11[0m],wires=[31m11[0m)[71;118H[K[71;118H78,10-17[7C6%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 79 [0m[8Cqml.CNOT[46m([0mwires=[[31m11[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H79,10-17[7C6%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([13C)
[33m 80 [0m[8Cqml.RY(theta_learn[[31m12[0m],wires=[31m12[0m)[71;118H[K[71;118H80,10-17[7C7%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 81 [0m[8Cqml.RY(theta_learn[[31m13[0m],wires=[31m13[0m)[71;118H[K[71;118H81,10-17[7C8%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 82 [0m[8Cqml.CNOT[46m([0mwires=[[31m8[0m,[31m9[0m][46m)[0m[71;118H[K[71;118H82,10-17[7C9%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([11C)
[33m 83 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m14[0m)[71;118H[K[71;118H83,10-17[7C9%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 84 [0m[8C[34m# Second Layer[0m[71;118H[K[71;118H84,10-17      10%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 85 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m1[0m)[71;118H[K[71;118H85,10-17      11%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 86 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m2[0m)[71;118H[K[71;118H86,10-17      12%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 87 [0m[8Cqml.CNOT[46m([0mwires=[[31m1[0m,[31m2[0m][46m)[0m[71;118H[K[71;118H87,10-17      12%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([11C)
[33m 88 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m5[0m)[71;118H[K[71;118H88,10-17      13%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 89 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m6[0m)[71;118H[K[71;118H89,10-17      14%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 90 [0m[8Cqml.CNOT[46m([0mwires=[[31m6[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H90,10-17      15%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([11C)
[33m 91 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m9[0m)[71;118H[K[71;118H91,10-17      15%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 92 [0m[8Cqml.RY(theta_learn[[31m17[0m],wires=[31m10[0m)[71;118H[K[71;118H92,10-17      16%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 93 [0m[8Cqml.CNOT[46m([0mwires=[[31m9[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H93,10-17      17%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([12C)
[33m 94 [0m[8Cqml.RY(theta_learn[[31m18[0m],wires=[31m13[0m)[71;118H[K[71;118H94,10-17      18%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 95 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m14[0m)[71;118H[K[71;118H95,10-17      18%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 96 [0m[8Cqml.CNOT[46m([0mwires=[[31m9[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H96,10-17      19%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([12C)
[33m 97 [0m[8C[34m# Third Layer[0m[71;118H[K[71;118H97,10-17      20%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 98 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m2[0m)[71;118H[K[71;118H98,10-17      21%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m 99 [0m[8Cqml.RY(theta_learn[[31m20[0m],wires=[31m5[0m)[71;118H[K[71;118H99,10-17      21%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m100 [0m[8Cqml.CNOT[46m([0mwires=[[31m2[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H100,10-17     22%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([11C)
[33m101 [0m[8Cqml.RY(theta_learn[[31m21[0m],wires=[31m10[0m)[71;118H[K[71;118H101,10-17     23%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m102 [0m[8Cqml.RY(theta_learn[[31m22[0m],wires=[31m13[0m)[71;118H[K[71;118H102,10-17     24%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m103 [0m[8Cqml.CNOT[46m([0mwires=[[31m13[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H103,10-17     24%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([13C)
[33m104 [0m[8C[34m# Forth Layer[0m[71;118H[K[71;118H104,10-17     25%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m105 [0m[8Cqml.RY(theta_learn[[31m23[0m],wires=[31m5[0m)[71;118H[K[71;118H105,10-17     26%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m106 [0m[8Cqml.RY(theta_learn[[31m24[0m],wires=[31m10[0m)[71;118H[K[71;118H106,10-17     27%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m107 [0m[8C[34m# Fifth Layer[0m[71;118H[K[71;118H107,10-17     27%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m108 [0m[8Cqml.RY(theta_learn[[31m25[0m],wires=[31m0[0m)[71;118H[K[71;118H108,10-17     28%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m109 [0m[8Cqml.RY(theta_learn[[31m26[0m],wires=[31m5[0m)[71;118H[K[71;118H109,10-17     29%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m110 [0m[8Cqml.CNOT[46m([0mwires=[[31m0[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H110,10-17     30%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([11C)
[33m111 [0m[8Cqml.RY(theta_learn[[31m27[0m],wires=[31m10[0m)[71;118H[K[71;118H111,10-17     30%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m112 [0m[8Cqml.RY(theta_learn[[31m28[0m],wires=[31m14[0m)[71;118H[K[71;118H112,10-17     31%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m113 [0m[8Cqml.CNOT[46m([0mwires=[[31m14[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H113,10-17     32%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([13C)
[33m114 [0m[8C[34m# Last Layer[0m[71;118H[K[71;118H114,10-17     33%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m115 [0m[8Cqml.RY(theta_learn[[31m29[0m],wires=[31m5[0m)[71;118H[K[71;118H115,10-17     33%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m116 [0m[8Cqml.RY(theta_learn[[31m30[0m],wires=[31m10[0m)[71;118H[K[71;118H116,10-17     34%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m117 [0m[71;118H[K[71;118H117,0-1[7C35%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m118 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m5[0m)), qml.expval(qml.PauliZ(wires=[31m10[0m))[71;118H[K[71;118H118,10-17     36%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m119 [0m[34m#################################################[0m[71;118H[K[71;118H119,17[8C36%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m120 def[0m [36medge_forward[0m[46m([0medge_array,theta_learn[46m)[0m:[71;118H[K[71;118H120,17[8C37%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([22C)
[33m121 [0m[8C[34m# executes TTN_edge circuit for each edge in edge_array[0m[71;118H[K[71;118H121,10-17     38%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m122 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H122,10-17     39%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m123 [0m[8Coutputs = [][71;118H[K[71;118H123,10-17     39%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m124 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(edge_array[:,[31m0[0m])):[71;118H[K[71;118H124,10-17     40%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m125 [0m[16Cout = tf.constant(([31m1[0m-TTN_edge_forward(edge_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)[71;118H[K[71;118H125,3-17      41%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m126 [0m[16Coutputs.append(out)[71;118H[K[71;118H126,3-17      42%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m127 [8Creturn[0m tf.stack(outputs)[71;118H[K[71;118H127,10-17     42%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m128 [0m[34m#################################################[0m[71;118H[K[71;118H128,17[8C43%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m129 def[0m [36mnode_forward[0m[46m([0mnode_array,theta_learn[46m)[0m:[71;118H[K[71;118H129,17[8C44%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([22C)
[33m130 [0m[8C[34m# executes TTN_node circuit for each node in node_array[0m[71;118H[K[71;118H130,10-17     45%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m131 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H131,10-17     45%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m132 [0m[8Coutputs = [][71;118H[K[71;118H132,10-17     46%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m133 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(node_array[:,[31m0[0m])):[71;118H[K[71;118H133,10-17     47%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m134 [0m[16Cout = tf.constant([31m4[0m*np.pi*([31m1[0m-TTN_node_forward(node_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)[71;118H[K[71;118H134,3-17      48%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m135 [0m[16Coutputs.append(out)[71;118H[K[71;118H135,3-17      48%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m136 [8Creturn[0m tf.stack(outputs) [34m# output is between [0,4*pi][0m[71;118H[K[71;118H136,10-17     49%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m137 [0m[34m#################################################[0m[71;118H[K[71;118H137,17[8C50%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m138 class[0m [36mEdgeNet[0m(tf.keras.layers.Layer):[71;118H[K[71;118H138,17[8C51%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m139 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'EdgeNet'[0m):[71;118H[K[71;118H139,10-17     51%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m140 [0m[16C[36msuper[0m(EdgeNet, self).__init__(name=name)[71;118H[K[71;118H140,3-17      52%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m141 [0m[16C[34m# can only work with hid_dim = 2[0m[71;118H[K[71;118H141,3-17      53%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m142 [0m[16C[34m# read parameters of the network from file[0m[71;118H[K[71;118H142,3-17      54%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m143 [0m[16C[34m# params are created using tools/init_params.py[0m[71;118H[K[71;118H143,3-17      54%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m144 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('EN',config)[0])[0m[71;118H[K[71;118H144,3-17      55%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m145 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m19[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))[71;118H[K[71;118H145,3-17      56%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m146 [8Cdef[0m [36mcall[0m[46m([0mself,X, Ri, Ro[46m)[0m:[71;118H[K[71;118H146,10-17     57%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([14C)
[33m147 [0m[16Cbo = tf.matmul(Ro,X,transpose_a=[36mTrue[0m)[71;118H[K[71;118H147,3-17      57%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m148 [0m[16Cbi = tf.matmul(Ri,X,transpose_a=[36mTrue[0m)[71;118H[K[71;118H148,3-17      58%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m149 [0m[16C[34m# Shape of B = N_edges x 6 (2x (3 coordinates))[0m[71;118H[K[71;118H149,3-17      59%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m150 [0m[16C[34m# each row consists of two node that are possibly connected.[0m[71;118H[K[71;118H150,3-17      60%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m151 [0m[16CB  = tf.concat([bo, bi], axis=[31m1[0m)[71;118H[K[71;118H151,3-17      60%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m152 [16Creturn[0m edge_forward(B,self.theta_learn)[71;118H[K[71;118H152,3-17      61%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m153 [0m[34m#################################################[0m[71;118H[K[71;118H153,17[8C62%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m154 class[0m [36mNodeNet[0m(tf.keras.layers.Layer):[71;118H[K[71;118H154,17[8C63%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m155 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'NodeNet'[0m):[71;118H[K[71;118H155,10-17     63%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m156 [0m[16C[36msuper[0m(NodeNet, self).__init__(name=name)[71;118H[K[71;118H156,3-17      64%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m157 [0m[16C[34m# can only work with hid_dim = 1[0m[71;118H[K[71;118H157,3-17      65%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m158 [0m[16C[34m# read parameters of the network from file[0m[71;118H[K[71;118H158,3-17      66%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m159 [0m[16C[34m# params are created using tools/init_params.py[0m[71;118H[K[71;118H159,3-17      66%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m160 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('NN',config)[0])[0m[71;118H[K[71;118H160,3-17      67%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m161 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m31[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))[71;118H[K[71;118H161,3-17      68%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m162 [8Cdef[0m [36mcall[0m[46m([0mself, X, e, Ri, Ro[46m)[0m:[71;118H[K[71;118H162,10-17     69%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([18C)
[33m163 [0m[16Cbo  = tf.matmul(Ro, X, transpose_a=[36mTrue[0m)[71;118H[K[71;118H163,3-17      69%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m164 [0m[16Cbi  = tf.matmul(Ri, X, transpose_a=[36mTrue[0m)[71;118H[K[71;118H164,3-17      70%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m165 [0m[16CRwo = tf.math.multiply(Ro,e)[71;118H[K[71;118H165,3-17      71%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m166 [0m[16CRwi = tf.math.multiply(Ri,e)[71;118H[K[71;118H166,3-17      72%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m167 [0m[16Cmi = tf.matmul(Rwi, bo)[71;118H[K[71;118H167,3-17      72%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m168 [0m[16Cmo = tf.matmul(Rwo, bi)[71;118H[K[71;118H168,3-17      73%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m169 [0m[16CM = tf.concat([mi, mo, X], axis=[31m1[0m)[71;118H[K[71;118H169,3-17      74%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m170 [16Creturn[0m node_forward(M,self.theta_learn)[71;118H[K[71;118H170,3-17      75%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m171 [0m[34m#################################################[0m[71;118H[K[71;118H171,17[8C75%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m172 class[0m [36mInputNet[0m(tf.keras.layers.Layer):[71;118H[K[71;118H172,17[8C76%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m173 [8Cdef[0m [36m__init__[0m(self, config, name):[71;118H[K[71;118H173,10-17     77%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m174 [0m[16C[36msuper[0m(InputNet, self).__init__(name=name)[71;118H[K[71;118H174,3-17      78%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m175 [0m[16Cself.num_outputs = config[[31m'hid_dim'[0m] [34m# num_outputs = number of hidden dimensions[0m[71;118H[K[71;118H175,3-17      78%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m176 [0m[16C[34m# read parameters of the network from file[0m[71;118H[K[71;118H176,3-17      79%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m177 [0m[16C[34m# params are created using tools/init_params.py[0m[71;118H[K[71;118H177,3-17      80%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m178 [0m[16C[34m#init = tf.constant_initializer(get_params('IN',config)[0])[0m[71;118H[K[71;118H178,3-17      81%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m179 [0m[16C[34m# setup a Dense layer with the given config[0m[71;118H[K[71;118H179,3-17      81%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m180 [0m[16Cself.layer = tf.keras.layers.Dense(self.num_outputs,input_shape=([31m3[0m,),activation=[31m'sigmoid'[0m)[71;118H[K[71;118H180,3-17      82%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m181 [8Cdef[0m [36mcall[0m[46m([0mself, arr[46m)[0m:[71;118H[K[71;118H181,10-17     83%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([9C)
[33m182 [16Creturn[0m self.layer(arr)*[31m4[0m*np.pi [34m# to map to output to [0,2*pi][0m[71;118H[K[71;118H182,3-17      84%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m183 [0m[34m#################################################[0m[71;118H[K[71;118H183,17[8C84%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m184 class[0m [36mGNN[0m(tf.keras.Model):[71;118H[K[71;118H184,17[8C85%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m185 [8Cdef[0m [36m__init__[0m(self, config):[71;118H[K[71;118H185,10-17     86%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m186 [0m[16C[36msuper[0m(GNN, self).__init__(name=[31m'GNN'[0m)[71;118H[K[71;118H186,3-17      87%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m187 [0m[71;118H[K[71;118H187,0-1[7C87%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m188 [0m[16Cself.InputNet = InputNet(config,name=[31m'InputNet'[0m)[71;118H[K[71;118H188,3-17      88%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m189 [0m[16Cself.EdgeNet  = EdgeNet(config,name=[31m'EdgeNet'[0m)[71;118H[K[71;118H189,3-17      89%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m190 [0m[16Cself.NodeNet  = NodeNet(config,name=[31m'NodeNet'[0m)[71;118H[K[71;118H190,3-17      90%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m191 [0m[16Cself.n_iters = config[[31m'n_iters'[0m][71;118H[K[71;118H191,3-17      90%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m192 [0m[71;118H[K[71;118H192,0-1[7C91%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m193 [8Cdef[0m [36mcall[0m[46m([0mself, edge_array[46m)[0m:[71;118H[K[71;118H193,10-17     92%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([16C)
[33m194 [0m[16CX,Ri,Ro = edge_array[71;118H[K[71;118H194,3-17      93%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m195 [0m[16CH = self.InputNet(X)[71;118H[K[71;118H195,3-17      93%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m196 [0m[16CH = tf.concat([H,X],axis=[31m1[0m)[71;118H[K[71;118H196,3-17      94%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m197 [16Cfor[0m i [33min[0m [36mrange[0m(self.n_iters):[71;118H[K[71;118H197,3-17      95%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m198 [0m[24Ce = self.EdgeNet(H, Ri, Ro)[71;118H[K[71;118H198,3-24      96%[70;28H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m199 [0m[24CH = self.NodeNet(H, e, Ri, Ro)[71;118H[K[71;118H199,3-24      96%[70;28H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m200 [0m[24CH = tf.concat([H,X], axis=[31m1[0m)[71;118H[K[71;118H200,3-24      97%[70;28H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m201 [0m[16Ce = self.EdgeNet(H, Ri, Ro)[71;118H[K[71;118H201,3-17      98%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m202 [16Creturn[0m e[71;118H[K[71;118H202,3-17      99%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m203 [0m[34m#################################################[0m[71;118H[K[71;118H203,17[8CBot[70;21H[34h[?25h[?25l[71;120H2,3-17[69;21H[34h[?25h[?25l[71;120H1[68;21H[34h[?25h[?25l[71;120H0,3-24[67;28H[34h[?25h[?25l[71;118H199[66;28H[34h[?25h[?25l[71;120H8[65;28H[34h[?25h[?25l[71;120H7,3-17[64;21H[34h[?25h[?25l[71;120H6[63;21H[34h[?25h[?25l[71;122H4-18[63;22H[34h[?25h[?25l[71;122H5-19[63;23H[34h[?25h[?25l[71;122H6-20[63;24H[34h[?25h[?25l[71;122H7-21[63;25H[34h[?25h[?25l[71;122H8-22[63;26H[34h[?25h[?25l[71;122H9-23[63;27H[34h[?25h[?25l[71;122H10-24[63;28H[34h[?25h[?25l[71;123H1-25[63;29H[34h[?25h[?25l[71;123H2-26[63;30H[34h[?25h[?25l[71;123H3-27[63;31H[34h[?25h[?25l[71;123H4-28[63;32H[34h[?25h[?25l[71;123H5-29[63;33H[34h[?25h[?25lt[46m([12C)[0m[71;123H6-30[63;34H[34h[?25h[?25l([46m[[0mH,X[46m][0m[7C)[71;123H7-31[63;35H[34h[?25h[?25l[H,X][71;123H8-32[63;36H[34h[?25h[?25l[71;123H9-33[63;37H[34h[?25h[?25l[71;122H20-34[63;38H[34h[?25h[?25l[46m[[0mH,X[46m][0m[71;123H1-35[63;39H[34h[?25h[?25l[H,X][71;123H2-36[63;40H[34h[?25h[?25l[71;123H3-37[63;41H[34h[?25h[?25l[71;123H4-38[63;42H[34h[?25h[?25l[71;123H5-39[63;43H[34h[?25h[?25l[71;123H6-40[63;44H[34h[?25h[?25l[71;123H7-41[63;45H[34h[?25h[?25l[71;123H8-42[63;46H[34h[?25h[?25l[63;34H[46m([12C)[0m[71;123H9-43[63;47H[34h[?25h[?25l[71;1H[1m-- INSERT --[0m[71;118H[K[71;118H196,29-43     Bot[63;47H[34h[?25h[?25l[71;122H30-44[63;48H[34h[?25h[?25l[64;21H[K[65;21H[33mfor[0m i [33min[0m [36mrange[0m(self.n_iters):[65;50H[K[66;29He[8CEdg[8CRi, Ro)[66;56H[K[67;33Hself.NodeNet(H, e, Ri, Ro)[68;21H        H = tf.concat([H,X], axis=[31m1[0m)[69;21He = self.EdgeNet(H, Ri, Ro)[70;5H                [33mreturn[0m e[70;29H[K[63;34H([12C)[71;120H7,1         99%[64;5H[34h[?25h[?25l[71;122H2-9[64;13H[34h[?25h[?25l[71;122H3-17[64;21H[34h[?25h[?25lp[71;122H4-18[64;22H[34h[?25h[?25lr[71;122H5-19[64;23H[34h[?25h[?25li[71;122H6-20[64;24H[34h[?25h[?25ln[71;122H7-21[64;25H[34h[?25h[?25l[36mprint[0m[71;122H8-22[64;26H[34h[?25h[?25l([71;122H9-23[64;27H[34h[?25h[?25l)[46m()[0m[71;122H10-24[64;28H[34h[?25h[?25l[71;122H9-23 [64;27H[34h[?25h[?25l[46mH[0m)H[46m)[0m[71;122H10-24[64;28H[34h[?25h[?25l[46m.[0m).[46m)[0m[71;123H1-25[64;29H[34h[?25h[?25l[46ms[0m)s[46m)[0m[71;123H2-26[64;30H[34h[?25h[?25l[46ma[0m)a[46m)[0m[71;123H3-27[64;31H[34h[?25h[?25l[46mh[0m)h[46m)[0m[71;123H4-28[64;32H[34h[?25h[?25l)[46m )[0m[64;32H[K[71;123H3-27[64;31H[34h[?25h[?25l[46ma[0m)a[46m)[0m[71;123H4-28[64;32H[34h[?25h[?25l)[46m )[0m[64;32H[K[71;123H3-27[64;31H[34h[?25h[?25l)[46m )[0m[64;31H[K[71;123H2-26[64;30H[34h[?25h[?25l[46mh[0m)h[46m)[0m[71;123H3-27[64;31H[34h[?25h[?25l[46ma[0m)a[46m)[0m[71;123H4-28[64;32H[34h[?25h[?25l[46mo[0m)o[46m)[0m[71;123H5-29[64;33H[34h[?25h[?25l[46me[0m)e[46m)[0m[71;123H6-30[64;34H[34h[?25h[?25l)[46m )[0m[64;34H[K[71;123H5-29[64;33H[34h[?25h[?25l)[46m )[0m[64;33H[K[71;123H4-28[64;32H[34h[?25h[?25l[46mp[0m)p[46m)[0m[71;123H5-29[64;33H[34h[?25h[?25l[46me[0m)e[46m)[0m[71;123H6-30[64;34H[34h[?25h[?25l[71;123H7-31[64;35H[34h[?25h[?25l[65;21H[K[66;21H[33mfor[0m i [33min[0m [36mrange[0m(self.n_iters):[66;50H[K[67;29He[8CEdg[8CRi, Ro)[67;56H[K[68;33Hself.NodeNet(H, e, Ri, Ro)[69;21H        H = tf.concat([H,X], axis=[31m1[0m)[70;21He = self.EdgeNet(H, Ri, Ro)[64;26H([7C)[71;120H8,1         98[65;5H[34h[?25h[?25l[71;122H2-9[65;13H[34h[?25h[?25l[71;122H3-17[65;21H[34h[?25h[?25lp[71;122H4-18[65;22H[34h[?25h[?25lr[71;122H5-19[65;23H[34h[?25h[?25li[71;122H6-20[65;24H[34h[?25h[?25ln[71;122H7-21[65;25H[34h[?25h[?25l[36mprint[0m[71;122H8-22[65;26H[34h[?25h[?25l([71;122H9-23[65;27H[34h[?25h[?25l)[46m()[0m[71;122H10-24[65;28H[34h[?25h[71;1H[K[?25l[71;118H198,9-23      98%[65;27H[34h[?25h[?25l([65;27H[K[66;21H        e = self.EdgeNet(H, Ri, Ro)[67;29HH[8CNod[8Ce, Ri, Ro)[68;33Htf.concat([H,X], axis=[31m1[0m)[68;57H[K[69;21He = self.EdgeNet(H, Ri, Ro)[69;48H[K[70;21H[33mreturn[0m e[70;29H[K[71;122H8-22[7C9[65;26H[34h[?25h[?25l[71;1H[1m-- REPLACE --[0m[71;118H[K[71;118H198,8-22      99%[65;26H[34h[?25h[?25lprinti[71;122H9-23[65;27H[34h[?25h[?25l.[71;122H10-24[65;28H[34h[?25h[?25ls[71;123H1-25[65;29H[34h[?25h[?25lh[71;123H2-26[65;30H[34h[?25h[?25la[71;123H3-27[65;31H[34h[?25h[71;1H[K[65;30H[?25l[71;118H198,12-26     99%[65;30H[34h[?25h[?25l[71;1H1 change; before #5  2 seconds ago[71;118H[K[65;21H[36mprint[0m([65;27H[K[71;118H198,8-22      99%[65;26H[34h[?25h[?25l[71;3Hmore line; before #4  5 seconds ago[71;118H[K[65;26H[46m()[0m[66;21H[33mfor[0m i [33min[0m [36mrange[0m(self.n_iters):[66;50H[K[67;29He[8CEdg[8CRi, Ro)[67;56H[K[68;33Hself.NodeNet(H, e, Ri, Ro)[69;21H        H = tf.concat([H,X], axis=[31m1[0m)[70;21He = self.EdgeNet(H, Ri, Ro)[71;118H198,9-23      98%[65;27H[34h[?25h[?25l[71;1H[1m-- INSERT --[0m[71;14H[K[71;118H198,9-23      98%[65;27H[34h[?25h[?25l[71;122H10-24[65;28H[34h[?25h[?25l[71;122H9-23 [65;27H[34h[?25h[?25l[46mR[0m)R[46m)[0m[71;122H10-24[65;28H[34h[?25h[?25l[46mi[0m)i[46m)[0m[71;123H1-25[65;29H[34h[?25h[?25l[46m.[0m).[46m)[0m[71;123H2-26[65;30H[34h[?25h[?25l[46ms[0m)s[46m)[0m[71;123H3-27[65;31H[34h[?25h[?25l[46mh[0m)h[46m)[0m[71;123H4-28[65;32H[34h[?25h[?25l[46ma[0m)a[46m)[0m[71;123H5-29[65;33H[34h[?25h[?25l[46mp[0m)p[46m)[0m[71;123H6-30[65;34H[34h[?25h[?25l[46me[0m)e[46m)[0m[71;123H7-31[65;35H[34h[?25h[71;1H[K[?25l[65;26H([8C)[71;118H198,16-30     98%[65;34H[34h[?25h[?25l[71;118H[K[71;1H:[34h[?25hwq[?25l"qnetworks/GNN2.py" 205L, 7289C written
[?1l>[34h[?25h[?1049lSingularity> vim qnetworks/GNN2.py Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
(259, 5)
(259, 290)
Traceback (most recent call last):
  File "train.py", line 68, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 200, in call
    e = self.EdgeNet(H, Ri, Ro)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 152, in call
    return edge_forward(B,self.theta_learn)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 125, in edge_forward
    out = tf.constant((1-TTN_edge_forward(edge_array[i,:],theta_learn[0,:]))/2.,dtype=tf.float64)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1138, in _SliceHelperVar
    return _slice_helper(var.value(), slice_spec, var)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 898, in _slice_helper
    name=name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 1064, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py", line 9513, in strided_slice
    _ops.raise_from_not_ok_status(e, name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: GNN/EdgeNet/strided_slice/
Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml Singularity> vim qnetworks/GNN2.py [K
[?1049h[?1h=[1;71r[34l[34h[?25h[23m[24m[0m[H[J[?25l[71;1H"qnetworks/GNN2.py" 205L, 7289C[1;1H[33m136 [8Creturn[0m tf.stack(outputs) [34m# output is between [0,4*pi][0m
[33m137 [0m[34m#################################################[0m
[33m138 class[0m [36mEdgeNet[0m(tf.keras.layers.Layer):
[33m139 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'EdgeNet'[0m):
[33m140 [0m[16C[36msuper[0m(EdgeNet, self).__init__(name=name)
[33m141 [0m[16C[34m# can only work with hid_dim = 2[0m
[33m142 [0m[16C[34m# read parameters of the network from file[0m
[33m143 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m144 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('EN',config)[0])[0m
[33m145 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m19[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m146 [8Cdef[0m [36mcall[0m(self,X, Ri, Ro):
[33m147 [0m[16Cbo = tf.matmul(Ro,X,transpose_a=[36mTrue[0m)
[33m148 [0m[16Cbi = tf.matmul(Ri,X,transpose_a=[36mTrue[0m)
[33m149 [0m[16C[34m# Shape of B = N_edges x 6 (2x (3 coordinates))[0m
[33m150 [0m[16C[34m# each row consists of two node that are possibly connected.[0m
[33m151 [0m[16CB  = tf.concat([bo, bi], axis=[31m1[0m)
[33m152 [16Creturn[0m edge_forward(B,self.theta_learn)
[33m153 [0m[34m#################################################[0m
[33m154 class[0m [36mNodeNet[0m(tf.keras.layers.Layer):
[33m155 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'NodeNet'[0m):
[33m156 [0m[16C[36msuper[0m(NodeNet, self).__init__(name=name)
[33m157 [0m[16C[34m# can only work with hid_dim = 1[0m
[33m158 [0m[16C[34m# read parameters of the network from file[0m
[33m159 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m160 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('NN',config)[0])[0m
[33m161 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m31[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m162 [8Cdef[0m [36mcall[0m(self, X, e, Ri, Ro):
[33m163 [0m[16Cbo  = tf.matmul(Ro, X, transpose_a=[36mTrue[0m)
[33m164 [0m[16Cbi  = tf.matmul(Ri, X, transpose_a=[36mTrue[0m)
[33m165 [0m[16CRwo = tf.math.multiply(Ro,e)
[33m166 [0m[16CRwi = tf.math.multiply(Ri,e)
[33m167 [0m[16Cmi = tf.matmul(Rwi, bo)
[33m168 [0m[16Cmo = tf.matmul(Rwo, bi)
[33m169 [0m[16CM = tf.concat([mi, mo, X], axis=[31m1[0m)
[33m170 [16Creturn[0m node_forward(M,self.theta_learn)
[33m171 [0m[34m#################################################[0m
[33m172 class[0m [36mInputNet[0m(tf.keras.layers.Layer):
[33m173 [8Cdef[0m [36m__init__[0m(self, config, name):
[33m174 [0m[16C[36msuper[0m(InputNet, self).__init__(name=name)
[33m175 [0m[16Cself.num_outputs = config[[31m'hid_dim'[0m] [34m# num_outputs = number of hidden dimensions[0m
[33m176 [0m[16C[34m# read parameters of the network from file[0m
[33m177 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m178 [0m[16C[34m#init = tf.constant_initializer(get_params('IN',config)[0])[0m
[33m179 [0m[16C[34m# setup a Dense layer with the given config[0m
[33m180 [0m[16Cself.layer = tf.keras.layers.Dense(self.num_outputs,input_shape=([31m3[0m,),activation=[31m'sigmoid'[0m)
[33m181 [8Cdef[0m [36mcall[0m(self, arr):
[33m182 [16Creturn[0m self.layer(arr)*[31m4[0m*np.pi [34m# to map to output to [0,2*pi][0m
[33m183 [0m[34m#################################################[0m
[33m184 class[0m [36mGNN[0m(tf.keras.Model):
[33m185 [8Cdef[0m [36m__init__[0m(self, config):
[33m186 [0m[16C[36msuper[0m(GNN, self).__init__(name=[31m'GNN'[0m)
[33m187 
188 [0m[16Cself.InputNet = InputNet(config,name=[31m'InputNet'[0m)
[33m189 [0m[16Cself.EdgeNet  = EdgeNet(config,name=[31m'EdgeNet'[0m)
[33m190 [0m[16Cself.NodeNet  = NodeNet(config,name=[31m'NodeNet'[0m)
[33m191 [0m[16Cself.n_iters = config[[31m'n_iters'[0m]
[33m192 
193 [8Cdef[0m [36mcall[0m(self, edge_array):
[33m194 [0m[16CX,Ri,Ro = edge_array
[33m195 [0m[16CH = self.InputNet(X)
[33m196 [0m[16CH = tf.concat([H,X],axis=[31m1[0m)
[33m197 [0m[16C[36mprint[0m(H.shape)
[33m198 [0m[16C[36mprint[0m(Ri.shape)
[33m199 [16Cfor[0m i [33min[0m [36mrange[0m(self.n_iters):
[33m200 [0m[24Ce = self.EdgeNet(H, Ri, Ro)
[33m201 [0m[24CH = self.NodeNet(H, e, Ri, Ro)
[33m202 [0m[24CH = tf.concat([H,X], axis=[31m1[0m)
[33m203 [0m[16Ce = self.EdgeNet(H, Ri, Ro)
[33m204 [16Creturn[0m e
[33m205 [0m[34m#################################################[0m[71;118H198,3-17      Bot[63;21H[34h[?25h[?25l[71;120H7[62;21H[34h[?25h[?25l[71;120H6[61;21H[34h[?25h[?25l[71;120H5[60;21H[34h[?25h[?25l[71;120H4[59;21H[34h[?25h[?25l[58;21H[46m([16C)[0m[71;120H3,10-17[58;21H[34h[?25h[?25l([16C)[71;120H2,0-1  [57;5H[34h[?25h[?25l[71;120H1,3-17[56;21H[34h[?25h[?25l[71;120H0[55;21H[34h[?25h[?25l[71;119H89[54;21H[34h[?25h[?25l[71;120H8[53;21H[34h[?25h[?25l[71;120H7,0-1 [52;5H[34h[?25h[?25l[71;120H6,3-17[51;21H[34h[?25h[?25l[71;120H5,10-17[50;21H[34h[?25h[?25l[71;120H4,17   [49;21H[34h[?25h[?25l[71;120H3[48;21H[34h[?25h[?25l[71;120H2,3-17[47;21H[34h[?25h[?25l[46;21H[46m([9C)[0m[71;120H1,10-17[46;21H[34h[?25h[?25l([9C)[71;120H0,3-17 [45;21H[34h[?25h[?25l[71;119H79[44;21H[34h[?25h[?25l[71;120H8[43;21H[34h[?25h[?25l[71;120H7[42;21H[34h[?25h[?25l[71;120H6[41;21H[34h[?25h[?25l[71;120H5[40;21H[34h[?25h[?25l[71;120H4[39;21H[34h[?25h[?25l[71;120H3,10-17[38;21H[34h[?25h[?25l[71;120H2,17   [37;21H[34h[?25h[?25l[71;120H1[36;21H[34h[?25h[?25l[71;120H0,3-17[35;21H[34h[?25h[?25l[71;119H69[34;21H[34h[?25h[?25l[71;120H8[33;21H[34h[?25h[?25l[71;120H7[32;21H[34h[?25h[?25l[71;120H6[31;21H[34h[?25h[?25l[71;120H5[30;21H[34h[?25h[?25l[71;120H4[29;21H[34h[?25h[?25l[71;120H3[28;21H[34h[?25h[?25l[27;21H[46m([18C)[0m[71;120H2,10-17[27;21H[34h[?25h[?25l([18C)[71;120H1,3-17 [26;21H[34h[?25h[?25l[71;120H0[25;21H[34h[?25h[?25l[71;119H59[24;21H[34h[?25h[?25l[71;120H8[23;21H[34h[?25h[?25l[71;120H7[22;21H[34h[?25h[?25l[71;120H6[21;21H[34h[?25h[?25l[71;120H5,10-17[20;21H[34h[?25h[?25l[71;120H4,17   [19;21H[34h[?25h[?25l[71;120H3[18;21H[34h[?25h[?25l[71;120H2,3-17[17;21H[34h[?25h[?25l[71;120H1[16;21H[34h[?25h[?25l[71;120H0[15;21H[34h[?25h[?25l[71;119H49[14;21H[34h[?25h[?25l[71;120H8[13;21H[34h[?25h[?25l[71;120H7[12;21H[34h[?25h[?25l[11;21H[46m([14C)[0m[71;120H6,10-17[11;21H[34h[?25h[?25l([14C)[71;120H5,3-17 [10;21H[34h[?25h[?25l[71;120H4[9;21H[34h[?25h[?25l[71;120H3[8;21H[34h[?25h[?25l[71;120H2[7;21H[34h[?25h[?25l[71;120H1[6;21H[34h[?25h[?25l[71;120H0[5;21H[34h[?25h[?25l[71;119H39,10-17[4;21H[34h[?25h[?25l[71;120H8,17   [3;21H[34h[?25h[?25l[71;120H7[2;21H[34h[?25h[?25l[71;120H6,10-17[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m135 [0m[16Coutputs.append(out)[71;1H[K[71;118H135,3-17      99%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m134 [0m[16Cout = tf.constant([31m4[0m*np.pi*([31m1[0m-TTN_node_forward(node_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)[71;118H[K[71;118H134,3-17      98%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m133 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(node_array[:,[31m0[0m])):[71;118H[K[71;118H133,10-17     97%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m132 [0m[8Coutputs = [][71;118H[K[71;118H132,10-17     97%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m131 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H131,10-17     96%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m130 [0m[8C[34m# executes TTN_node circuit for each node in node_array[0m[71;118H[K[71;118H130,10-17     95%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m129 def[0m [36mnode_forward[0m[46m([0mnode_array,theta_learn[46m)[0m:[71;118H[K[71;118H129,17[8C94%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m128 [0m[34m#################################################[0m[2;21H([22C)[71;118H[K[71;118H128,17[8C94%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m127 [8Creturn[0m tf.stack(outputs)[71;118H[K[71;118H127,10-17     93%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m126 [0m[16Coutputs.append(out)[71;118H[K[71;118H126,3-17      92%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m125 [0m[16Cout = tf.constant(([31m1[0m-TTN_edge_forward(edge_array[i,:],theta_learn[[31m0[0m,:]))/[31m2.[0m,dtype=tf.float64)[71;118H[K[71;118H125,3-17      91%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m124 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(edge_array[:,[31m0[0m])):[71;118H[K[71;118H124,10-17     91%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m123 [0m[8Coutputs = [][71;118H[K[71;118H123,10-17     90%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m122 [0m[8C[34m# To Do: can parallize the for loop[0m[71;118H[K[71;118H122,10-17     89%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m121 [0m[8C[34m# executes TTN_edge circuit for each edge in edge_array[0m[71;118H[K[71;118H121,10-17     88%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m120 def[0m [36medge_forward[0m[46m([0medge_array,theta_learn[46m)[0m:[71;118H[K[71;118H120,17[8C88%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m119 [0m[34m#################################################[0m[2;21H([22C)[71;118H[K[71;118H119,17[8C87%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m118 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m5[0m)), qml.expval(qml.PauliZ(wires=[31m10[0m))[71;118H[K[71;118H118,10-17     86%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m117 [0m[71;118H[K[71;118H117,0-1[7C85%[1;5H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m116 [0m[8Cqml.RY(theta_learn[[31m30[0m],wires=[31m10[0m)[71;118H[K[71;118H116,10-17     85%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m115 [0m[8Cqml.RY(theta_learn[[31m29[0m],wires=[31m5[0m)[71;118H[K[71;118H115,10-17     84%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m114 [0m[8C[34m# Last Layer[0m[71;118H[K[71;118H114,10-17     83%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m113 [0m[8Cqml.CNOT[46m([0mwires=[[31m14[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H113,10-17     82%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m112 [0m[8Cqml.RY(theta_learn[[31m28[0m],wires=[31m14[0m)[2;21H([13C)[71;118H[K[71;118H112,10-17     82%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m111 [0m[8Cqml.RY(theta_learn[[31m27[0m],wires=[31m10[0m)[71;118H[K[71;118H111,10-17     81%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m110 [0m[8Cqml.CNOT[46m([0mwires=[[31m0[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H110,10-17     80%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m109 [0m[8Cqml.RY(theta_learn[[31m26[0m],wires=[31m5[0m)[2;21H([11C)[71;118H[K[71;118H109,10-17     80%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m108 [0m[8Cqml.RY(theta_learn[[31m25[0m],wires=[31m0[0m)[71;118H[K[71;118H108,10-17     79%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m107 [0m[8C[34m# Fifth Layer[0m[71;118H[K[71;118H107,10-17     78%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m106 [0m[8Cqml.RY(theta_learn[[31m24[0m],wires=[31m10[0m)[71;118H[K[71;118H106,10-17     77%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m105 [0m[8Cqml.RY(theta_learn[[31m23[0m],wires=[31m5[0m)[71;118H[K[71;118H105,10-17     77%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m104 [0m[8C[34m# Forth Layer[0m[71;118H[K[71;118H104,10-17     76%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m103 [0m[8Cqml.CNOT[46m([0mwires=[[31m13[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H103,10-17     75%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m102 [0m[8Cqml.RY(theta_learn[[31m22[0m],wires=[31m13[0m)[2;21H([13C)[71;118H[K[71;118H102,10-17     74%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m101 [0m[8Cqml.RY(theta_learn[[31m21[0m],wires=[31m10[0m)[71;118H[K[71;118H101,10-17     74%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m100 [0m[8Cqml.CNOT[46m([0mwires=[[31m2[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H100,10-17     73%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 99 [0m[8Cqml.RY(theta_learn[[31m20[0m],wires=[31m5[0m)[2;21H([11C)[71;118H[K[71;118H99,10-17      72%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 98 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m2[0m)[71;118H[K[71;118H98,10-17      71%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 97 [0m[8C[34m# Third Layer[0m[71;118H[K[71;118H97,10-17      71%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 96 [0m[8Cqml.CNOT[46m([0mwires=[[31m9[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H96,10-17      70%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 95 [0m[8Cqml.RY(theta_learn[[31m19[0m],wires=[31m14[0m)[2;21H([12C)[71;118H[K[71;118H95,10-17      69%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 94 [0m[8Cqml.RY(theta_learn[[31m18[0m],wires=[31m13[0m)[71;118H[K[71;118H94,10-17      68%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 93 [0m[8Cqml.CNOT[46m([0mwires=[[31m9[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H93,10-17      68%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 92 [0m[8Cqml.RY(theta_learn[[31m17[0m],wires=[31m10[0m)[2;21H([12C)[71;118H[K[71;118H92,10-17      67%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 91 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m9[0m)[71;118H[K[71;118H91,10-17      66%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 90 [0m[8Cqml.CNOT[46m([0mwires=[[31m6[0m,[31m5[0m][46m)[0m[71;118H[K[71;118H90,10-17      65%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 89 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m6[0m)[2;21H([11C)[71;118H[K[71;118H89,10-17      65%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 88 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m5[0m)[71;118H[K[71;118H88,10-17      64%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 87 [0m[8Cqml.CNOT[46m([0mwires=[[31m1[0m,[31m2[0m][46m)[0m[71;118H[K[71;118H87,10-17      63%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 86 [0m[8Cqml.RY(theta_learn[[31m16[0m],wires=[31m2[0m)[2;21H([11C)[71;118H[K[71;118H86,10-17      62%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 85 [0m[8Cqml.RY(theta_learn[[31m15[0m],wires=[31m1[0m)[71;118H[K[71;118H85,10-17      62%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 84 [0m[8C[34m# Second Layer[0m[71;118H[K[71;118H84,10-17      61%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 83 [0m[8Cqml.RY(theta_learn[[31m14[0m],wires=[31m14[0m)[71;118H[K[71;118H83,10-17      60%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 82 [0m[8Cqml.CNOT[46m([0mwires=[[31m8[0m,[31m9[0m][46m)[0m[71;118H[K[71;118H82,10-17      60%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 81 [0m[8Cqml.RY(theta_learn[[31m13[0m],wires=[31m13[0m)[2;21H([11C)[71;118H[K[71;118H81,10-17      59%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 80 [0m[8Cqml.RY(theta_learn[[31m12[0m],wires=[31m12[0m)[71;118H[K[71;118H80,10-17      58%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 79 [0m[8Cqml.CNOT[46m([0mwires=[[31m11[0m,[31m10[0m][46m)[0m[71;118H[K[71;118H79,10-17      57%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 78 [0m[8Cqml.RY(theta_learn[[31m11[0m],wires=[31m11[0m)[2;21H([13C)[71;118H[K[71;118H78,10-17      57%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 77 [0m[8Cqml.RY(theta_learn[[31m10[0m],wires=[31m10[0m)[71;118H[K[71;118H77,10-17      56%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 76 [0m[8Cqml.CNOT[46m([0mwires=[[31m8[0m,[31m9[0m][46m)[0m[71;118H[K[71;118H76,10-17      55%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 75 [0m[8Cqml.RY(theta_learn[[31m9[0m],wires=[31m9[0m)[2;21H([11C)[71;118H[K[71;118H75,10-17      54%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 74 [0m[8Cqml.RY(theta_learn[[31m8[0m],wires=[31m8[0m)[71;118H[K[71;118H74,10-17      54%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 73 [0m[8Cqml.CNOT[46m([0mwires=[[31m7[0m,[31m6[0m][46m)[0m[71;118H[K[71;118H73,10-17      53%[1;21H[34h[?25h[?25l[1;70r[1;1H[L[1;71r[1;1H[33m 72 [0m[8Cqml.RY(theta_learn[[31m7[0m],wires=[31m7[0m)[2;21H([11C)[71;118H[K[71;118H72,10-17      52%[1;21H[34h[?25h[?25l[2;21H[46m([11C)[0m[71;119H3[2;21H[34h[?25h[?25l([11C)[71;119H4[3;21H[34h[?25h[?25l[71;119H5[4;21H[34h[?25h[?25l[5;21H[46m([11C)[0m[71;119H6[5;21H[34h[?25h[?25l([11C)[71;119H7[6;21H[34h[?25h[?25l[71;119H8[7;21H[34h[?25h[?25l[8;21H[46m([13C)[0m[71;119H9[8;21H[34h[?25h[?25l([13C)[71;118H80[9;21H[34h[?25h[?25l[71;119H1[10;21H[34h[?25h[?25l[11;21H[46m([11C)[0m[71;119H2[11;21H[34h[?25h[?25l([11C)[71;119H3[12;21H[34h[?25h[?25l[71;119H4[13;21H[34h[?25h[?25l[71;119H5[14;21H[34h[?25h[?25l[71;119H6[15;21H[34h[?25h[?25l[16;21H[46m([11C)[0m[71;119H7[16;21H[34h[?25h[?25l([11C)[71;119H8[17;21H[34h[?25h[?25l[71;119H9[18;21H[34h[?25h[?25l[19;21H[46m([11C)[0m[71;118H90[19;21H[34h[?25h[?25l([11C)[71;119H1[20;21H[34h[?25h[?25l[71;119H2[21;21H[34h[?25h[?25l[22;21H[46m([12C)[0m[71;119H3[22;21H[34h[?25h[?25l([12C)[71;119H4[23;21H[34h[?25h[?25l[71;119H5[24;21H[34h[?25h[?25l[25;21H[46m([12C)[0m[71;119H6[25;21H[34h[?25h[?25l([12C)[71;119H7[26;21H[34h[?25h[?25l[71;119H8[27;21H[34h[?25h[?25l[71;119H9[28;21H[34h[?25h[?25l[29;21H[46m([11C)[0m[71;118H100,10-17[29;21H[34h[?25h[?25l([11C)[71;120H1[30;21H[34h[?25h[?25l[71;120H2[31;21H[34h[?25h[?25l[32;21H[46m([13C)[0m[71;120H3[32;21H[34h[?25h[?25l([13C)[71;120H4[33;21H[34h[?25h[?25l[71;120H5[34;21H[34h[?25h[?25l[71;120H6[35;21H[34h[?25h[?25l[71;120H7[36;21H[34h[?25h[?25l[71;120H8[37;21H[34h[?25h[?25l[71;120H9[38;21H[34h[?25h[?25l[39;21H[46m([11C)[0m[71;119H10[39;21H[34h[?25h[?25l([11C)[71;120H1[40;21H[34h[?25h[?25l[71;120H2[41;21H[34h[?25h[?25l[42;21H[46m([13C)[0m[71;120H3[42;21H[34h[?25h[?25l([13C)[71;120H4[43;21H[34h[?25h[?25l[71;120H5[44;21H[34h[?25h[?25l[71;120H6[45;21H[34h[?25h[?25l[71;120H7,0-1  [46;5H[34h[?25h[?25l[71;120H8,10-17[47;21H[34h[?25h[?25l[71;120H9,17   [48;21H[34h[?25h[?25l[49;21H[46m([22C)[0m[71;119H20[49;21H[34h[?25h[?25l([22C)[71;120H1,10-17[50;21H[34h[?25h[?25l[71;120H2[51;21H[34h[?25h[?25l[71;120H3[52;21H[34h[?25h[?25l[71;120H4[53;21H[34h[?25h[?25l[71;120H5,3-17 [54;21H[34h[?25h[?25l[71;120H6[55;21H[34h[?25h[?25l[71;122H4-18[55;22H[34h[?25h[?25l[71;120H5[54;22H[34h[?25h[?25l[71;122H5-19[54;23H[34h[?25h[?25l[71;122H6-20[54;24H[34h[?25h[?25l[71;122H7-21[54;25H[34h[?25h[?25l[71;122H8-22[54;26H[34h[?25h[?25l[71;122H9-23[54;27H[34h[?25h[?25l[71;122H10-24[54;28H[34h[?25h[?25l[71;123H1-25[54;29H[34h[?25h[?25l[71;123H2-26[54;30H[34h[?25h[?25l[71;123H3-27[54;31H[34h[?25h[?25l[71;123H4-28[54;32H[34h[?25h[?25l[71;123H5-29[54;33H[34h[?25h[?25l[71;123H6-30[54;34H[34h[?25h[?25l[71;123H7-31[54;35H[34h[?25h[?25l[71;123H8-32[54;36H[34h[?25h[?25l[71;123H9-33[54;37H[34h[?25h[?25lt[46m([74C)[0m[71;122H20-34[54;38H[34h[?25h[?25l([46m([52C)[0m[20C)[71;123H1-35[54;39H[34h[?25h[?25l([52C)[71;123H2-36[54;40H[34h[?25h[?25l[71;123H3-37[54;41H[34h[?25h[?25l[71;123H4-38[54;42H[34h[?25h[?25l[71;123H5-39[54;43H[34h[?25h[?25l[71;123H6-40[54;44H[34h[?25h[?25l[71;123H7-41[54;45H[34h[?25h[?25l[71;123H8-42[54;46H[34h[?25h[?25l[71;123H9-43[54;47H[34h[?25h[?25l[71;122H30-44[54;48H[34h[?25h[?25l[71;123H1-45[54;49H[34h[?25h[?25l[71;123H2-46[54;50H[34h[?25h[?25l[71;123H3-47[54;51H[34h[?25h[?25l[71;123H4-48[54;52H[34h[?25h[?25l[71;123H5-49[54;53H[34h[?25h[?25l[71;123H6-50[54;54H[34h[?25h[?25l[71;123H7-51[54;55H[34h[?25h[?25l[71;123H8-52[54;56H[34h[?25h[?25l[71;123H9-53[54;57H[34h[?25h[?25ld[46m([32C)[0m[71;122H40-54[54;58H[34h[?25h[?25l([32C)[71;123H1-55[54;59H[34h[?25h[?25l[71;123H2-56[54;60H[34h[?25h[?25l[71;123H3-57[54;61H[34h[?25h[?25l[71;123H4-58[54;62H[34h[?25h[?25l[71;123H5-59[54;63H[34h[?25h[?25l[71;123H6-60[54;64H[34h[?25h[?25l[71;123H7-61[54;65H[34h[?25h[?25l[71;123H8-62[54;66H[34h[?25h[?25l[71;123H9-63[54;67H[34h[?25h[?25l[71;122H50-64[54;68H[34h[?25h[?25ly[46m[[0mi,:[46m][0m[71;123H1-65[54;69H[34h[?25h[?25l[i,:][71;123H2-66[54;70H[34h[?25h[?25l[71;123H3-67[54;71H[34h[?25h[?25l[71;123H4-68[54;72H[34h[?25h[?25l[46m[[0mi,:[46m][0m[71;123H5-69[54;73H[34h[?25h[?25l[i,:][71;123H6-70[54;74H[34h[?25h[?25l[71;118H[K[71;1H:[34h[?25hwq[?25l[71;3H[K[71;3H[34h[?25h[?25l[71;2H[K[71;2H[34h[?25h [?25l[116C125,56-70     52%[54;74H[34h[?25h[?25l[71;123H7-71[54;75H[34h[?25h[?25l[71;123H8-72[54;76H[34h[?25h[?25l[71;123H9-73[54;77H[34h[?25h[?25l[71;122H60-74[54;78H[34h[?25h[?25l[71;123H1-75[54;79H[34h[?25h[?25l[71;123H2-76[54;80H[34h[?25h[?25l[71;123H3-77[54;81H[34h[?25h[?25l[71;123H4-78[54;82H[34h[?25h[?25l[71;123H5-79[54;83H[34h[?25h[?25l[71;123H6-80[54;84H[34h[?25h[?25l[71;123H7-81[54;85H[34h[?25h[?25ln[46m[[3C][0m[71;123H8-82[54;86H[34h[?25h[?25l[[3C][71;123H9-83[54;87H[34h[?25h[?25l[71;122H70-84[54;88H[34h[?25h[?25l[71;123H1-85[54;89H[34h[?25h[?25l[46m[[3C][0m[71;123H2-86[54;90H[34h[?25h[?25l[54;58H[46m([0m[27C[[3C][46m)[0m[71;123H3-87[54;91H[34h[?25h[?25l[71;1H[1m-- INSERT --[0m[71;118H[K[71;118H125,73-87     52%[54;91H[34h[?25h[?25l)[1C/[31m2.[0m,dtype=tf.float64)[54;113H[K[54;90H[46m)[0m)[71;123H2-86[54;90H[34h[?25h[?25l)[1C/[31m2.[0m,dtype=tf.float64)[54;112H[K[54;89H[46m)[0m)[71;123H1-85[54;89H[34h[?25h[?25l)[1C/[31m2.[0m,dtype=tf.float64)[54;111H[K[54;88H[46m)[0m)[71;123H0-84[54;88H[34h[?25h[?25l)[1C/[31m2.[0m,dtype=tf.float64)[54;110H[K[54;87H[46m)[0m)[71;122H69-83[54;87H[34h[?25h[?25l)[1C/[31m2.[0m,dtype=tf.float64)[54;109H[K[54;86H[46m)[0m)[71;123H8-82[54;86H[34h[?25h[?25l[54;58H([27C)[55;35H[46m([0mout[46m)[0m[71;120H6,22-36[55;40H[34h[?25h[?25l(out)[56;28H[46m([7C)[0m[71;120H7,26-33[56;37H[34h[?25h[?25l[56;28H([7C)[71;120H8,33   [57;37H[34h[?25h[?25l[71;120H9[58;37H[34h[?25h[?25l[71;119H30,26-33[59;37H[34h[?25h[?25l[71;120H1[60;37H[34h[?25h[?25l[61;23H[46m[][0m[71;120H2,14-21[61;25H[34h[?25h[?25l[][71;120H3[62;25H[34h[?25h[?25l[71;120H4,7-21 [63;25H[34h[?25h[?25l[71;122H8-22[63;26H[34h[?25h[?25l[71;122H9-23[63;27H[34h[?25h[?25l[71;122H10-24[63;28H[34h[?25h[?25l[71;123H1-25[63;29H[34h[?25h[?25l[71;123H2-26[63;30H[34h[?25h[?25l[71;123H4-28[63;32H[34h[?25h[?25l[71;123H5-29[63;33H[34h[?25h[?25l[71;123H6-30[63;34H[34h[?25h[?25l[71;123H7-31[63;35H[34h[?25h[?25l[71;123H8-32[63;36H[34h[?25h[?25l[71;123H9-33[63;37H[34h[?25h[?25lt[46m([82C)[0m[71;122H20-34[63;38H[34h[?25h[?25l[71;123H1-35[63;39H[34h[?25h[?25l([82C)[71;123H2-36[63;40H[34h[?25h[?25l[71;123H3-37[63;41H[34h[?25h[?25l[71;123H4-38[63;42H[34h[?25h[?25l[71;123H5-39[63;43H[34h[?25h[?25l[71;123H6-40[63;44H[34h[?25h[?25l[71;123H7-41[63;45H[34h[?25h[?25l[71;123H8-42[63;46H[34h[?25h[?25l*[46m([52C)[0m[71;123H9-43[63;47H[34h[?25h[?25l[71;122H30-44[63;48H[34h[?25h[?25l([52C)[71;123H1-45[63;49H[34h[?25h[?25l[71;123H2-46[63;50H[34h[?25h[?25l[71;123H3-47[63;51H[34h[?25h[?25l[71;123H4-48[63;52H[34h[?25h[?25l[71;123H5-49[63;53H[34h[?25h[?25l[71;123H6-50[63;54H[34h[?25h[?25l[71;123H7-51[63;55H[34h[?25h[?25l[71;123H8-52[63;56H[34h[?25h[?25l[71;123H9-53[63;57H[34h[?25h[?25l[71;122H40-54[63;58H[34h[?25h[?25l[71;123H1-55[63;59H[34h[?25h[?25l[71;123H2-56[63;60H[34h[?25h[?25l[71;123H3-57[63;61H[34h[?25h[?25l[71;123H4-58[63;62H[34h[?25h[?25l[71;123H5-59[63;63H[34h[?25h[?25l[71;123H6-60[63;64H[34h[?25h[?25l[71;123H7-61[63;65H[34h[?25h[?25ld[46m([32C)[0m[71;123H8-62[63;66H[34h[?25h[?25l[71;123H9-63[63;67H[34h[?25h[?25l([32C)[71;122H50-64[63;68H[34h[?25h[?25l[71;123H1-65[63;69H[34h[?25h[?25l[71;123H2-66[63;70H[34h[?25h[?25l[71;123H3-67[63;71H[34h[?25h[?25l[71;123H4-68[63;72H[34h[?25h[?25l[71;123H5-69[63;73H[34h[?25h[?25l[71;123H6-70[63;74H[34h[?25h[?25l[71;123H7-71[63;75H[34h[?25h[?25l[71;123H8-72[63;76H[34h[?25h[?25ly[46m[[0mi,:[46m][0m[71;123H9-73[63;77H[34h[?25h[?25l[71;122H60-74[63;78H[34h[?25h[?25l[i,:][71;123H1-75[63;79H[34h[?25h[?25l[71;123H2-76[63;80H[34h[?25h[?25l[46m[[0mi,:[46m][0m[71;123H3-77[63;81H[34h[?25h[?25l[71;123H4-78[63;82H[34h[?25h[?25l[i,:][71;123H5-79[63;83H[34h[?25h[?25l[71;123H6-80[63;84H[34h[?25h[?25l[71;123H7-81[63;85H[34h[?25h[?25l[71;123H8-82[63;86H[34h[?25h[?25l[71;123H9-83[63;87H[34h[?25h[?25l[71;122H70-84[63;88H[34h[?25h[?25l[71;123H2-86[63;90H[34h[?25h[?25l[71;123H3-87[63;91H[34h[?25h[?25l[71;123H4-88[63;92H[34h[?25h[?25l[71;123H5-89[63;93H[34h[?25h[?25ln[46m[[3C][0m[71;123H6-90[63;94H[34h[?25h[?25l[71;123H7-91[63;95H[34h[?25h[?25l[[3C][71;123H8-92[63;96H[34h[?25h[?25l[71;123H9-93[63;97H[34h[?25h[?25l[46m[[3C][0m[71;122H80-94[63;98H[34h[?25h[?25l[63;66H[46m([0m[27C[[3C][46m)[0m[71;123H1-95[63;99H[34h[?25h[?25l)[1C/[31m2.[0m,dtype=tf.float64)[63;121H[K[63;98H[46m)[0m)[71;123H0-94[63;98H[34h[?25h[?25l)[1C/[31m2.[0m,dtype=tf.float64)[63;120H[K[63;97H[46m)[0m)[71;122H79-93[63;97H[34h[?25h[?25l)[1C/[31m2.[0m,dtype=tf.float64)[63;119H[K[63;96H[46m)[0m)[71;123H8-92[63;96H[34h[?25h[?25l)[1C/[31m2.[0m,dtype=tf.float64)[63;118H[K[63;95H[46m)[0m)[71;123H7-91[63;95H[34h[?25h[?25l)[1C/[31m2.[0m,dtype=tf.float64)[63;117H[K[63;94H[46m)[0m)[71;123H6-90[63;94H[34h[?25h[71;1H[K[?25l[63;66H([27C)[71;118H134,75-89     52%[63;93H[34h[?25h[?25l[71;118H[K[71;1H:[34h[?25hwq[?25l"qnetworks/GNN2.py" 205L, 7279C written
[?1l>[34h[?25h[?1049lSingularity> vim qnetworks/GNN2.py Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
(259, 5)
(259, 290)
^CTraceback (most recent call last):
  File "train.py", line 68, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 201, in call
    H = self.NodeNet(H, e, Ri, Ro)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 170, in call
    return node_forward(M,self.theta_learn)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN2.py", line 134, in node_forward
    out = tf.constant(4*np.pi*(1-TTN_node_forward(node_array[i,:],theta_learn))/2.,dtype=tf.float64)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 256, in __call__
    return self._d(self._f, a, k)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 210, in decorated
    return _eager_mode_decorator(wrapped, args, kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 406, in _eager_mode_decorator
    result, grad_fn = f(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/interfaces/tf.py", line 77, in _TFQNode
    res = qnode(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 606, in __call__
    return self.evaluate(args, **kwargs)  # args as one tuple
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/autograd/tracer.py", line 48, in f_wrapped
    return f_raw(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 680, in evaluate
    check_op(op)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 673, in check_op
    for w in _flatten(op.wires):
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/utils.py", line 47, in _flatten
    yield from _flatten(item)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/utils.py", line 45, in _flatten
    elif isinstance(x, Iterable) and not isinstance(x, (str, bytes)):
  File "/usr/lib64/python3.6/abc.py", line 184, in __instancecheck__
    if subclass in cls._abc_cache:
  File "/usr/lib64/python3.6/_weakrefset.py", line 70, in __contains__
    def __contains__(self, item):
KeyboardInterrupt
Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml Singularity> vim qnetworks/GNN2.py [K
[?1049h[?1h=[1;71r[34l[34h[?25h[23m[24m[0m[H[J[?25l[71;1H"qnetworks/GNN2.py" 205L, 7279C[1;1H[33m100 [0m[8Cqml.CNOT(wires=[[31m2[0m,[31m5[0m])
[33m101 [0m[8Cqml.RY(theta_learn[[31m21[0m],wires=[31m10[0m)
[33m102 [0m[8Cqml.RY(theta_learn[[31m22[0m],wires=[31m13[0m)
[33m103 [0m[8Cqml.CNOT(wires=[[31m13[0m,[31m10[0m])
[33m104 [0m[8C[34m# Forth Layer[0m
[33m105 [0m[8Cqml.RY(theta_learn[[31m23[0m],wires=[31m5[0m)
[33m106 [0m[8Cqml.RY(theta_learn[[31m24[0m],wires=[31m10[0m)
[33m107 [0m[8C[34m# Fifth Layer[0m
[33m108 [0m[8Cqml.RY(theta_learn[[31m25[0m],wires=[31m0[0m)
[33m109 [0m[8Cqml.RY(theta_learn[[31m26[0m],wires=[31m5[0m)
[33m110 [0m[8Cqml.CNOT(wires=[[31m0[0m,[31m5[0m])
[33m111 [0m[8Cqml.RY(theta_learn[[31m27[0m],wires=[31m10[0m)
[33m112 [0m[8Cqml.RY(theta_learn[[31m28[0m],wires=[31m14[0m)
[33m113 [0m[8Cqml.CNOT(wires=[[31m14[0m,[31m10[0m])
[33m114 [0m[8C[34m# Last Layer[0m
[33m115 [0m[8Cqml.RY(theta_learn[[31m29[0m],wires=[31m5[0m)
[33m116 [0m[8Cqml.RY(theta_learn[[31m30[0m],wires=[31m10[0m)
[33m117 
118 [8Creturn[0m qml.expval(qml.PauliZ(wires=[31m5[0m)), qml.expval(qml.PauliZ(wires=[31m10[0m))
[33m119 [0m[34m#################################################[0m
[33m120 def[0m [36medge_forward[0m(edge_array,theta_learn):
[33m121 [0m[8C[34m# executes TTN_edge circuit for each edge in edge_array[0m
[33m122 [0m[8C[34m# To Do: can parallize the for loop[0m
[33m123 [0m[8Coutputs = []
[33m124 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(edge_array[:,[31m0[0m])):
[33m125 [0m[16Cout = tf.constant(([31m1[0m-TTN_edge_forward(edge_array[i,:],theta_learn))/[31m2.[0m,dtype=tf.float64)
[33m126 [0m[16Coutputs.append(out)
[33m127 [8Creturn[0m tf.stack(outputs)
[33m128 [0m[34m#################################################[0m
[33m129 def[0m [36mnode_forward[0m(node_array,theta_learn):
[33m130 [0m[8C[34m# executes TTN_node circuit for each node in node_array[0m
[33m131 [0m[8C[34m# To Do: can parallize the for loop[0m
[33m132 [0m[8Coutputs = []
[33m133 [8Cfor[0m i [33min[0m [36mrange[0m([36mlen[0m(node_array[:,[31m0[0m])):
[33m134 [0m[16Cout = tf.constant([31m4[0m*np.pi*([31m1[0m-TTN_node_forward(node_array[i,:],theta_learn))/[31m2.[0m,dtype=tf.float64)
[33m135 [0m[16Coutputs.append(out)
[33m136 [8Creturn[0m tf.stack(outputs) [34m# output is between [0,4*pi][0m
[33m137 [0m[34m#################################################[0m
[33m138 class[0m [36mEdgeNet[0m(tf.keras.layers.Layer):
[33m139 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'EdgeNet'[0m):
[33m140 [0m[16C[36msuper[0m(EdgeNet, self).__init__(name=name)
[33m141 [0m[16C[34m# can only work with hid_dim = 2[0m
[33m142 [0m[16C[34m# read parameters of the network from file[0m
[33m143 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m144 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('EN',config)[0])[0m
[33m145 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m19[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m146 [8Cdef[0m [36mcall[0m(self,X, Ri, Ro):
[33m147 [0m[16Cbo = tf.matmul(Ro,X,transpose_a=[36mTrue[0m)
[33m148 [0m[16Cbi = tf.matmul(Ri,X,transpose_a=[36mTrue[0m)
[33m149 [0m[16C[34m# Shape of B = N_edges x 6 (2x (3 coordinates))[0m
[33m150 [0m[16C[34m# each row consists of two node that are possibly connected.[0m
[33m151 [0m[16CB  = tf.concat([bo, bi], axis=[31m1[0m)
[33m152 [16Creturn[0m edge_forward(B,self.theta_learn)
[33m153 [0m[34m#################################################[0m
[33m154 class[0m [36mNodeNet[0m(tf.keras.layers.Layer):
[33m155 [8Cdef[0m [36m__init__[0m(self,config,name=[31m'NodeNet'[0m):
[33m156 [0m[16C[36msuper[0m(NodeNet, self).__init__(name=name)
[33m157 [0m[16C[34m# can only work with hid_dim = 1[0m
[33m158 [0m[16C[34m# read parameters of the network from file[0m
[33m159 [0m[16C[34m# params are created using tools/init_params.py[0m
[33m160 [0m[16C[34m#self.theta_learn = tf.Variable(get_params('NN',config)[0])[0m
[33m161 [0m[16Cself.theta_learn =  tf.Variable(tf.random.uniform(shape=[[31m31[0m,],minval=[31m0[0m,maxval=np.pi*[31m2[0m,dtype=tf.float64))
[33m162 [8Cdef[0m [36mcall[0m(self, X, e, Ri, Ro):
[33m163 [0m[16Cbo  = tf.matmul(Ro, X, transpose_a=[36mTrue[0m)
[33m164 [0m[16Cbi  = tf.matmul(Ri, X, transpose_a=[36mTrue[0m)
[33m165 [0m[16CRwo = tf.math.multiply(Ro,e)
[33m166 [0m[16CRwi = tf.math.multiply(Ri,e)
[33m167 [0m[16Cmi = tf.matmul(Rwi, bo)
[33m168 [0m[16Cmo = tf.matmul(Rwo, bi)
[33m169 [0m[16CM = tf.concat([mi, mo, X], axis=[31m1[0m)[71;118H134,3-17      73%[35;21H[34h[?25h[?25l[71;120H5[36;21H[34h[?25h[?25l[71;120H6,10-17[37;21H[34h[?25h[?25l[71;120H7,17   [38;21H[34h[?25h[?25l[71;120H8[39;21H[34h[?25h[?25l[71;120H9,10-17[40;21H[34h[?25h[?25l[71;119H40,3-17 [41;21H[34h[?25h[?25l[71;120H1[42;21H[34h[?25h[?25l[71;120H2[43;21H[34h[?25h[?25l[71;120H3[44;21H[34h[?25h[?25l[71;120H4[45;21H[34h[?25h[?25l[71;120H5[46;21H[34h[?25h[?25l[47;21H[46m([14C)[0m[71;120H6,10-17[47;21H[34h[?25h[?25l([14C)[71;120H7,3-17 [48;21H[34h[?25h[?25l[71;120H8[49;21H[34h[?25h[?25l[71;120H9[50;21H[34h[?25h[?25l[71;119H50[51;21H[34h[?25h[?25l[71;120H1[52;21H[34h[?25h[?25l[71;120H2[53;21H[34h[?25h[?25l[71;120H3,17  [54;21H[34h[?25h[?25l[71;120H4[55;21H[34h[?25h[?25l[71;120H5,10-17[56;21H[34h[?25h[?25l[71;120H6,3-17 [57;21H[34h[?25h[?25l[71;120H7[58;21H[34h[?25h[?25l[71;120H8[59;21H[34h[?25h[?25l[71;120H9[60;21H[34h[?25h[?25l[71;119H60[61;21H[34h[?25h[?25l[71;120H1[62;21H[34h[?25h[?25l[63;21H[46m([18C)[0m[71;120H2,10-17[63;21H[34h[?25h[?25l([18C)[71;120H3,3-17 [64;21H[34h[?25h[?25l[71;120H4[65;21H[34h[?25h[?25l[71;120H5[66;21H[34h[?25h[?25l[71;120H6[67;21H[34h[?25h[?25l[71;120H7[68;21H[34h[?25h[?25l[71;120H8[69;21H[34h[?25h[?25l[71;120H9[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m170 [16Creturn[0m node_forward(M,self.theta_learn)[71;1H[K[71;118H170,3-17      74%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m171 [0m[34m#################################################[0m[71;118H[K[71;118H171,17[8C74%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m172 class[0m [36mInputNet[0m(tf.keras.layers.Layer):[71;118H[K[71;118H172,17[8C75%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m173 [8Cdef[0m [36m__init__[0m(self, config, name):[71;118H[K[71;118H173,10-17     76%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m174 [0m[16C[36msuper[0m(InputNet, self).__init__(name=name)[71;118H[K[71;118H174,3-17      77%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m175 [0m[16Cself.num_outputs = config[[31m'hid_dim'[0m] [34m# num_outputs = number of hidden dimensions[0m[71;118H[K[71;118H175,3-17      77%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m176 [0m[16C[34m# read parameters of the network from file[0m[71;118H[K[71;118H176,3-17      78%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m177 [0m[16C[34m# params are created using tools/init_params.py[0m[71;118H[K[71;118H177,3-17      79%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m178 [0m[16C[34m#init = tf.constant_initializer(get_params('IN',config)[0])[0m[71;118H[K[71;118H178,3-17      80%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m179 [0m[16C[34m# setup a Dense layer with the given config[0m[71;118H[K[71;118H179,3-17      80%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m180 [0m[16Cself.layer = tf.keras.layers.Dense(self.num_outputs,input_shape=([31m3[0m,),activation=[31m'sigmoid'[0m)[71;118H[K[71;118H180,3-17      81%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m181 [8Cdef[0m [36mcall[0m[46m([0mself, arr[46m)[0m:[71;118H[K[71;118H181,10-17     82%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([9C)
[33m182 [16Creturn[0m self.layer(arr)*[31m4[0m*np.pi [34m# to map to output to [0,2*pi][0m[71;118H[K[71;118H182,3-17      82%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m183 [0m[34m#################################################[0m[71;118H[K[71;118H183,17[8C83%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m184 class[0m [36mGNN[0m(tf.keras.Model):[71;118H[K[71;118H184,17[8C84%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m185 [8Cdef[0m [36m__init__[0m(self, config):[71;118H[K[71;118H185,10-17     85%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m186 [0m[16C[36msuper[0m(GNN, self).__init__(name=[31m'GNN'[0m)[71;118H[K[71;118H186,3-17      85%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m187 [0m[71;118H[K[71;118H187,0-1[7C86%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m188 [0m[16Cself.InputNet = InputNet(config,name=[31m'InputNet'[0m)[71;118H[K[71;118H188,3-17      87%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m189 [0m[16Cself.EdgeNet  = EdgeNet(config,name=[31m'EdgeNet'[0m)[71;118H[K[71;118H189,3-17      88%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m190 [0m[16Cself.NodeNet  = NodeNet(config,name=[31m'NodeNet'[0m)[71;118H[K[71;118H190,3-17      88%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m191 [0m[16Cself.n_iters = config[[31m'n_iters'[0m][71;118H[K[71;118H191,3-17      89%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m192 [0m[71;118H[K[71;118H192,0-1[7C90%[70;5H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m193 [8Cdef[0m [36mcall[0m[46m([0mself, edge_array[46m)[0m:[71;118H[K[71;118H193,10-17     91%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[69;21H([16C)
[33m194 [0m[16CX,Ri,Ro = edge_array[71;118H[K[71;118H194,3-17      91%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m195 [0m[16CH = self.InputNet(X)[71;118H[K[71;118H195,3-17      92%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m196 [0m[16CH = tf.concat([H,X],axis=[31m1[0m)[71;118H[K[71;118H196,3-17      93%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m197 [0m[16C[36mprint[0m(H.shape)[71;118H[K[71;118H197,3-17      94%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m198 [0m[16C[36mprint[0m(Ri.shape)[71;118H[K[71;118H198,3-17      94%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m199 [16Cfor[0m i [33min[0m [36mrange[0m(self.n_iters):[71;118H[K[71;118H199,3-17      95%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m200 [0m[24Ce = self.EdgeNet(H, Ri, Ro)[71;118H[K[71;118H200,3-24      96%[70;28H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m201 [0m[24CH = self.NodeNet(H, e, Ri, Ro)[71;118H[K[71;118H201,3-24      97%[70;28H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m202 [0m[24CH = tf.concat([H,X], axis=[31m1[0m)[71;118H[K[71;118H202,3-24      97%[70;28H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m203 [0m[16Ce = self.EdgeNet(H, Ri, Ro)[71;118H[K[71;118H203,3-17      98%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m204 [16Creturn[0m e[71;118H[K[71;118H204,3-17      99%[70;21H[34h[?25h[?25l[1;70r[70;1H
[1;71r[70;1H[33m205 [0m[34m#################################################[0m[71;118H[K[71;118H205,17[8CBot[70;21H[34h[?25h[?25l[71;120H4,3-17[69;21H[34h[?25h[?25l[71;120H5,17  [70;21H[34h[?25h[?25l[71;120H4,3-17[69;21H[34h[?25h[?25l[71;120H5,17  [70;21H[34h[?25h[?25l[71;120H4,3-17[69;21H[34h[?25h[?25l[71;120H3[68;21H[34h[?25h[?25l[71;120H2,3-24[67;28H[34h[?25h[?25l[71;120H1[66;28H[34h[?25h[?25l[71;120H0[65;28H[34h[?25h[?25l[71;118H199,3-17[64;21H[34h[?25h[?25l[71;120H8[63;21H[34h[?25h[?25l[63;5H[7m                [0m[1C[7m[36mrint[0m[7m(Ri.shape) [0m[71;1H[1m-- VISUAL LINE --[0m[71;118H[K[71;118H198,3-17      Bot[63;21H[34h[?25h[?25l[62;5H[7m                [0m[1C[7m[36mrint[0m[7m(H.shape) [0m[63;21H[7m[36mp[0m[71;120H7[62;21H[34h[?25h[?25l[62;5H                [33mfor[0m i [33min[0m [36mrange[0m(self.n_iters):[63;5H                        e = self.EdgeNet(H, Ri, Ro)[64;21H        H = self.NodeNet(H, e, Ri, Ro)[65;29HH = tf.concat([H,X], axis=[31m1[0m)[66;21He = self.EdgeNet(H, Ri, Ro)[66;49H[K[67;21H[33mreturn[0m e[67;29H[K[68;5H[34m#################################################[0m
[1m[34m~                                                                                                                                      [70;1H~                                                                                                                                      [0m[71;1H[K[71;118H197,3-17      Bot[62;21H[34h[?25h[?25l[71;118H[K[71;1H:[34h[?25hwq[?25l"qnetworks/GNN2.py" 203L, 7244C written
[?1l>[34h[?25h[?1049lSingularity> clear
[H[JSingularity> clearvim qnetworks/GNN2.py Singularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
2020-05-04 05:13:19.517334: Validation Test:  Loss: 0.7428,  Acc: 50.6832, AUC: 0.5122, Precision: 0.5545 -- Elapsed: 35m57s
2020-05-04 05:13:19.517430: Training is starting!
2020-05-04 05:13:54.287666: Validation Test:  Loss: 0.7948,  Acc: 59.6263, AUC: 0.6118, Precision: 0.6410 -- Elapsed: 36m13s
2020-05-04 05:13:54.287767: Training is starting!
2020-05-04 05:18:29.726472: Epoch: 1, Batch: 1, Loss: 0.7538, Elapsed: 5m10s
2020-05-04 05:21:21.438711: Epoch: 1, Batch: 2, Loss: 0.7407, Elapsed: 2m51s
2020-05-04 05:21:37.280608: Epoch: 1, Batch: 1, Loss: 0.8092, Elapsed: 7m42s
2020-05-04 05:25:30.778419: Epoch: 1, Batch: 2, Loss: 0.8236, Elapsed: 3m53s
2020-05-04 05:27:32.815040: Epoch: 1, Batch: 3, Loss: 0.7787, Elapsed: 6m11s
2020-05-04 05:31:13.216059: Epoch: 1, Batch: 3, Loss: 0.7932, Elapsed: 5m42s
2020-05-04 05:34:18.029012: Epoch: 1, Batch: 4, Loss: 0.7599, Elapsed: 6m45s
2020-05-04 05:34:31.941602: Validation Test:  Loss: 0.7807,  Acc: 50.5503, AUC: 0.5119, Precision: 0.5783 -- Elapsed: 48m59s
2020-05-04 05:34:31.941702: Training is starting!
2020-05-04 05:39:14.854085: Epoch: 1, Batch: 4, Loss: 0.7518, Elapsed: 8m1s
2020-05-04 05:40:54.845767: Epoch: 1, Batch: 5, Loss: 0.7345, Elapsed: 6m36s
2020-05-04 05:46:01.420753: Epoch: 1, Batch: 6, Loss: 0.7290, Elapsed: 5m6s
2020-05-04 05:46:13.480673: Epoch: 1, Batch: 1, Loss: 0.7755, Elapsed: 11m41s
Traceback (most recent call last):
  File "train.py", line 102, in <module>
    f.write('%f,\n' %grads[1].numpy())
TypeError: only size-1 arrays can be converted to Python scalars
Singularity> 2020-05-04 05:49:23.198406: Epoch: 1, Batch: 5, Loss: 0.7289, Elapsed: 10m8s
2020-05-04 05:51:43.554678: Epoch: 1, Batch: 7, Loss: 0.7608, Elapsed: 5m42s
2020-05-04 05:54:35.601707: Epoch: 1, Batch: 6, Loss: 0.7295, Elapsed: 5m12s
2020-05-04 05:56:55.030308: Epoch: 1, Batch: 7, Loss: 0.8098, Elapsed: 2m19s
2020-05-04 05:57:45.760649: Epoch: 1, Batch: 8, Loss: 0.7686, Elapsed: 6m2s
2020-05-04 06:02:13.734497: Epoch: 1, Batch: 9, Loss: 0.7521, Elapsed: 4m27s
2020-05-04 06:02:35.585136: Epoch: 1, Batch: 8, Loss: 0.7078, Elapsed: 5m40s
2020-05-04 06:06:13.708190: Epoch: 1, Batch: 9, Loss: 0.7205, Elapsed: 3m38s
2020-05-04 06:06:49.198448: Epoch: 1, Batch: 10, Loss: 0.7424, Elapsed: 4m35s
2020-05-04 06:12:07.421418: Epoch: 1, Batch: 10, Loss: 0.7282, Elapsed: 5m53s
2020-05-04 06:12:59.892175: Epoch: 1, Batch: 11, Loss: 0.7405, Elapsed: 6m10s
2020-05-04 06:16:34.310254: Epoch: 1, Batch: 11, Loss: 0.7369, Elapsed: 4m26s
2020-05-04 06:18:40.244520: Epoch: 1, Batch: 12, Loss: 0.7188, Elapsed: 2m5s
2020-05-04 06:20:31.857427: Epoch: 1, Batch: 12, Loss: 0.7382, Elapsed: 7m31s
2020-05-04 06:21:53.824454: Epoch: 1, Batch: 13, Loss: 0.7431, Elapsed: 3m13s
2020-05-04 06:23:22.508549: Epoch: 1, Batch: 13, Loss: 0.7465, Elapsed: 2m50s
2020-05-04 06:26:47.926943: Epoch: 1, Batch: 14, Loss: 0.7418, Elapsed: 4m54s
2020-05-04 06:30:06.896360: Epoch: 1, Batch: 15, Loss: 0.7556, Elapsed: 3m18s
2020-05-04 06:30:17.372334: Epoch: 1, Batch: 14, Loss: 0.7483, Elapsed: 6m54s
2020-05-04 06:35:41.347249: Epoch: 1, Batch: 15, Loss: 0.7805, Elapsed: 5m23s
2020-05-04 06:39:48.040981: Epoch: 1, Batch: 16, Loss: 0.6787, Elapsed: 9m41s
2020-05-04 06:40:16.315603: Epoch: 1, Batch: 16, Loss: 0.7491, Elapsed: 4m34s
2020-05-04 06:44:11.032984: Epoch: 1, Batch: 17, Loss: 0.7160, Elapsed: 4m22s
2020-05-04 06:46:22.196405: Epoch: 1, Batch: 17, Loss: 0.7859, Elapsed: 6m5s
2020-05-04 06:50:08.346279: Epoch: 1, Batch: 18, Loss: 0.7486, Elapsed: 3m46s
2020-05-04 06:50:31.670699: Epoch: 1, Batch: 18, Loss: 0.7079, Elapsed: 6m20s
2020-05-04 06:55:56.550683: Epoch: 1, Batch: 19, Loss: 0.7220, Elapsed: 5m48s
2020-05-04 06:56:45.347836: Epoch: 1, Batch: 19, Loss: 0.7326, Elapsed: 6m13s
2020-05-04 07:01:57.155173: Epoch: 1, Batch: 20, Loss: 0.7274, Elapsed: 6m0s
[KSingularity> vim configs/comparisons/
dnn/  qgnn/ 
Singularity> vim configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
[?1049h[?1h=[1;73r[34l[34h[?25h[23m[24m[0m[H[J[?25l[73;1H"configs/comparisons/qgnn/dimension_comparison/dim2.yaml" 18L, 402C[1;1H[33m  1 [0m[36mtrain_dir[0m[35m   :[0m [31m'data/graph_data/train'[0m
[33m  2 [0m[36mvalid_dir[0m[35m   :[0m [31m'data/graph_data/valid'[0m
[33m  3 [0m[36mparam_dir[0m[35m   :[0m [31m'params/'[0m
[33m  4 [0m[36mlog_dir[0m[35m     :[0m [31m'logs/comparisons/qgnn/dimension_comparison/dim2/'[0m
[33m  5 [0m[36mrun_type[0m[35m    :[0m [31m'new_run'[0m
[33m  6 [0m[36mgpu[0m[35m         :[0m [31m'7'[0m
[33m  7 [0m[36mn_files[0m[35m     :[0m [31m1600[0m
[33m  8 [0m[36mn_valid[0m[35m     :[0m [31m200[0m
[33m  9 [0m[36mn_train[0m[35m     :[0m [31m1400[0m
[33m 10 [0m[36mlr[0m[35m          :[0m [31m0.03[0m
[33m 11 [0m[36mn_iters[0m[35m     :[0m [31m2[0m
[33m 12 [0m[36mn_epoch[0m[35m     :[0m [31m1[0m
[33m 13 [0m[36mTEST_every[0m[35m  :[0m [31m50[0m
[33m 14 [0m[36mhid_dim[0m[35m     :[0m [31m2[0m
[33m 15 [0m[36mnetwork[0m[35m     :[0m [31m'QGNN'[0m
[33m 16 [0m[36mn_thread[0m[35m    :[0m [31m4[0m
[33m 17 [0m[36mlog_verbosity[0m[35m:[0m [31m2[0m
[33m 18 [0m
[1m[34m~                                                                                                                                      [20;1H~                                                                                                                                      [21;1H~                                                                                                                                      [22;1H~                                                                                                                                      [23;1H~                                                                                                                                      [24;1H~                                                                                                                                      [25;1H~                                                                                                                                      [26;1H~                                                                                                                                      [27;1H~                                                                                                                                      [28;1H~                                                                                                                                      [29;1H~                                                                                                                                      [30;1H~                                                                                                                                      [31;1H~                                                                                                                                      [32;1H~                                                                                                                                      [33;1H~                                                                                                                                      [34;1H~                                                                                                                                      [35;1H~                                                                                                                                      [36;1H~                                                                                                                                      [37;1H~                                                                                                                                      [38;1H~                                                                                                                                      [39;1H~                                                                                                                                      [40;1H~                                                                                                                                      [41;1H~                                                                                                                                      [42;1H~                                                                                                                                      [43;1H~                                                                                                                                      [44;1H~                                                                                                                                      [45;1H~                                                                                                                                      [46;1H~                                                                                                                                      [47;1H~                                                                                                                                      [48;1H~                                                                                                                                      [49;1H~                                                                                                                                      [50;1H~                                                                                                                                      [51;1H~                                                                                                                                      [52;1H~                                                                                                                                      [53;1H~                                                                                                                                      [54;1H~                                                                                                                                      [55;1H~                                                                                                                                      [56;1H~                                                                                                                                      [57;1H~                                                                                                                                      [58;1H~                                                                                                                                      [59;1H~                                                                                                                                      [60;1H~                                                                                                                                      [61;1H~                                                                                                                                      [62;1H~                                                                                                                                      [63;1H~                                                                                                                                      [64;1H~                                                                                                                                      [65;1H~                                                                                                                                      [66;1H~                                                                                                                                      [67;1H~                                                                                                                                      [68;1H~                                                                                                                                      [69;1H~                                                                                                                                      [70;1H~                                                                                                                                      [71;1H~                                                                                                                                      [72;1H~                                                                                                                                      [0m[73;118H4,1[11CAll[4;5H[34h[?25h[?25l[73;118H5[5;5H[34h[?25h[?25l[73;118H6[6;5H[34h[?25h[?25l[73;118H7[7;5H[34h[?25h[?25l[73;118H8[8;5H[34h[?25h[?25l[73;118H9[9;5H[34h[?25h[?25l[73;118H10,1[10;5H[34h[?25h[?25l[73;119H1[11;5H[34h[?25h[?25l[73;119H2[12;5H[34h[?25h[?25l[73;119H3[13;5H[34h[?25h[?25l[73;119H4[14;5H[34h[?25h[?25l[73;119H5[15;5H[34h[?25h[?25l[73;119H6[16;5H[34h[?25h[?25l[73;119H7[17;5H[34h[?25h[?25l[73;119H8,0-1[18;5H[34h[?25h[?25l[73;119H7,1  [17;5H[34h[?25h[?25l[73;121H2[17;6H[34h[?25h[?25l[73;121H3[17;7H[34h[?25h[?25l[73;121H4[17;8H[34h[?25h[?25l[73;121H5[17;9H[34h[?25h[?25l[73;121H6[17;10H[34h[?25h[?25l[73;121H7[17;11H[34h[?25h[?25l[73;121H8[17;12H[34h[?25h[?25l[73;121H9[17;13H[34h[?25h[?25l[73;121H10[17;14H[34h[?25h[?25l[73;122H1[17;15H[34h[?25h[?25l[73;122H2[17;16H[34h[?25h[?25l[73;122H3[17;17H[34h[?25h[?25l[73;122H4[17;18H[34h[?25h[?25l[73;122H5[17;19H[34h[?25h[?25l[73;122H6[17;20H[34h[?25h[?25l[73;1H[1m-- INSERT --[0m[73;13H[K[73;118H17,16[9CAll[17;20H[34h[?25h[?25l[73;122H7[17;21H[34h[?25h[?25l[17;20H[K[73;122H6[17;20H[34h[?25h[?25l[31m1[0m[73;122H7[17;21H[34h[?25h[73;1H[K[?25l[73;118H17,16[9CAll[17;20H[34h[?25h[?25l[73;118H[K[73;1H:[34h[?25hwq[?25l"configs/comparisons/qgnn/dimension_comparison/dim2.yaml" 18L, 402C written
[?1l>[34h[?25h[?1049lSingularity> clear
[H[JSingularity> clearvim configs/comparisons/qgnn/dimension_comparison/dim2.yaml Singularity> [13@python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 1
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
2020-05-04 07:02:55.390646 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/summary.csv
2020-05-04 07:02:55.390974 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_grads_IN.csv
2020-05-04 07:02:55.391265 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_validation.csv
2020-05-04 07:02:55.391522 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_loss.csv
Starting testing the validation set with 200 subgraphs!
2020-05-04 07:04:59.711496: Epoch: 1, Batch: 20, Loss: 0.6864, Elapsed: 8m14s
2020-05-04 07:07:53.472430: Epoch: 1, Batch: 21, Loss: 0.7009, Elapsed: 2m53s
2020-05-04 07:09:17.490852: Epoch: 1, Batch: 21, Loss: 0.7446, Elapsed: 7m20s
2020-05-04 07:13:29.266943: Epoch: 1, Batch: 22, Loss: 0.7219, Elapsed: 5m35s
2020-05-04 07:16:43.635233: Epoch: 1, Batch: 22, Loss: 0.7465, Elapsed: 7m26s
2020-05-04 07:20:40.287235: Epoch: 1, Batch: 23, Loss: 0.7075, Elapsed: 7m10s
2020-05-04 07:22:45.143222: Epoch: 1, Batch: 23, Loss: 0.7451, Elapsed: 6m1s
2020-05-04 07:28:49.947664: Epoch: 1, Batch: 24, Loss: 0.7627, Elapsed: 6m4s
2020-05-04 07:28:48.108832: Epoch: 1, Batch: 24, Loss: 0.6841, Elapsed: 8m7s
2020-05-04 07:34:50.040325: Epoch: 1, Batch: 25, Loss: 0.7338, Elapsed: 6m0s
2020-05-04 07:37:17.115741: Epoch: 1, Batch: 25, Loss: 0.7030, Elapsed: 8m28s
2020-05-04 07:44:37.832709: Epoch: 1, Batch: 26, Loss: 0.6873, Elapsed: 7m20s
2020-05-04 07:44:56.492665: Epoch: 1, Batch: 26, Loss: 0.7472, Elapsed: 10m6s
2020-05-04 07:50:01.806642: Epoch: 1, Batch: 27, Loss: 0.6898, Elapsed: 5m23s
2020-05-04 07:51:22.532268: Epoch: 1, Batch: 27, Loss: 0.7428, Elapsed: 6m26s
2020-05-04 07:57:18.033406: Validation Test:  Loss: 0.7700,  Acc: 56.4025, AUC: 0.5622, Precision: 0.6050 -- Elapsed: 54m21s
2020-05-04 07:57:18.033504: Training is starting!
2020-05-04 07:57:39.712696: Epoch: 1, Batch: 28, Loss: 0.6863, Elapsed: 7m37s
2020-05-04 07:58:38.543470: Epoch: 1, Batch: 28, Loss: 0.7384, Elapsed: 7m15s
2020-05-04 08:03:46.046480: Epoch: 1, Batch: 29, Loss: 0.6936, Elapsed: 6m6s
2020-05-04 08:06:32.835488: Epoch: 1, Batch: 29, Loss: 0.7520, Elapsed: 7m54s
2020-05-04 08:09:55.673378: Epoch: 1, Batch: 1, Loss: 0.7879, Elapsed: 12m37s
2020-05-04 08:12:12.217004: Epoch: 1, Batch: 30, Loss: 0.6929, Elapsed: 8m26s
2020-05-04 08:15:58.022086: Epoch: 1, Batch: 30, Loss: 0.6945, Elapsed: 9m25s
2020-05-04 08:18:03.431922: Epoch: 1, Batch: 31, Loss: 0.7042, Elapsed: 5m51s
2020-05-04 08:21:51.557538: Epoch: 1, Batch: 31, Loss: 0.7284, Elapsed: 5m53s
2020-05-04 08:23:08.464370: Epoch: 1, Batch: 2, Loss: 0.7669, Elapsed: 13m12s
2020-05-04 08:27:50.426707: Epoch: 1, Batch: 32, Loss: 0.7286, Elapsed: 5m58s
2020-05-04 08:28:58.120421: Epoch: 1, Batch: 32, Loss: 0.6863, Elapsed: 10m54s
2020-05-04 08:35:13.281353: Epoch: 1, Batch: 33, Loss: 0.7507, Elapsed: 7m22s
2020-05-04 08:39:15.842588: Epoch: 1, Batch: 33, Loss: 0.7003, Elapsed: 10m17s
2020-05-04 08:44:10.454498: Epoch: 1, Batch: 3, Loss: 0.8018, Elapsed: 21m1s
2020-05-04 08:45:44.524053: Epoch: 1, Batch: 34, Loss: 0.7282, Elapsed: 10m31s
2020-05-04 08:45:47.402271: Epoch: 1, Batch: 34, Loss: 0.6955, Elapsed: 6m31s
2020-05-04 08:54:00.268239: Epoch: 1, Batch: 4, Loss: 0.7564, Elapsed: 9m49s
2020-05-04 08:54:13.216723: Epoch: 1, Batch: 35, Loss: 0.7152, Elapsed: 8m28s
2020-05-04 08:57:41.362243: Epoch: 1, Batch: 35, Loss: 0.6995, Elapsed: 11m53s
2020-05-04 09:00:48.399110: Epoch: 1, Batch: 36, Loss: 0.7237, Elapsed: 6m35s
2020-05-04 09:04:53.788034: Epoch: 1, Batch: 36, Loss: 0.6855, Elapsed: 7m12s
2020-05-04 09:05:09.264238: Epoch: 1, Batch: 5, Loss: 0.7568, Elapsed: 11m8s
2020-05-04 09:06:58.363394: Epoch: 1, Batch: 37, Loss: 0.7257, Elapsed: 6m9s
2020-05-04 09:14:13.896533: Epoch: 1, Batch: 37, Loss: 0.6768, Elapsed: 9m20s
2020-05-04 09:14:32.766175: Epoch: 1, Batch: 6, Loss: 0.7620, Elapsed: 9m23s
2020-05-04 09:16:20.729409: Epoch: 1, Batch: 38, Loss: 0.7070, Elapsed: 9m22s
2020-05-04 09:23:18.842658: Epoch: 1, Batch: 7, Loss: 0.7763, Elapsed: 8m46s
2020-05-04 09:23:29.732088: Epoch: 1, Batch: 38, Loss: 0.6972, Elapsed: 9m15s
2020-05-04 09:23:55.393558: Epoch: 1, Batch: 39, Loss: 0.6967, Elapsed: 7m34s
2020-05-04 09:30:16.816610: Epoch: 1, Batch: 40, Loss: 0.7342, Elapsed: 6m21s
2020-05-04 09:30:41.327383: Epoch: 1, Batch: 39, Loss: 0.6897, Elapsed: 7m11s
2020-05-04 09:33:46.692010: Epoch: 1, Batch: 8, Loss: 0.7521, Elapsed: 10m27s
2020-05-04 09:39:52.962759: Epoch: 1, Batch: 40, Loss: 0.7031, Elapsed: 9m11s
2020-05-04 09:40:23.830138: Epoch: 1, Batch: 41, Loss: 0.7219, Elapsed: 10m6s
2020-05-04 09:44:48.952403: Epoch: 1, Batch: 42, Loss: 0.7445, Elapsed: 4m25s
2020-05-04 09:48:38.255182: Epoch: 1, Batch: 41, Loss: 0.6785, Elapsed: 8m45s
2020-05-04 09:49:26.534371: Epoch: 1, Batch: 9, Loss: 0.7710, Elapsed: 15m39s
2020-05-04 09:50:29.498177: Epoch: 1, Batch: 43, Loss: 0.7264, Elapsed: 5m40s
2020-05-04 09:55:14.473110: Epoch: 1, Batch: 42, Loss: 0.6838, Elapsed: 6m36s
2020-05-04 09:55:59.451920: Epoch: 1, Batch: 44, Loss: 0.7528, Elapsed: 5m29s
2020-05-04 09:59:40.736806: Epoch: 1, Batch: 45, Loss: 0.7303, Elapsed: 3m41s
2020-05-04 10:01:06.519837: Epoch: 1, Batch: 43, Loss: 0.7087, Elapsed: 5m52s
2020-05-04 10:04:12.475045: Epoch: 1, Batch: 10, Loss: 0.7541, Elapsed: 14m45s
2020-05-04 10:06:50.975276: Epoch: 1, Batch: 44, Loss: 0.6916, Elapsed: 5m44s
2020-05-04 10:07:41.064127: Epoch: 1, Batch: 46, Loss: 0.7302, Elapsed: 8m0s
2020-05-04 10:16:59.841664: Epoch: 1, Batch: 45, Loss: 0.6970, Elapsed: 10m8s
2020-05-04 10:18:04.365669: Epoch: 1, Batch: 47, Loss: 0.7164, Elapsed: 10m23s
2020-05-04 10:19:42.302138: Epoch: 1, Batch: 11, Loss: 0.7463, Elapsed: 15m29s
2020-05-04 10:23:08.561107: Epoch: 1, Batch: 48, Loss: 0.7530, Elapsed: 5m4s
2020-05-04 10:26:38.007436: Epoch: 1, Batch: 46, Loss: 0.6916, Elapsed: 9m38s
2020-05-04 10:27:41.255721: Epoch: 1, Batch: 49, Loss: 0.7083, Elapsed: 4m32s
2020-05-04 10:31:56.896366: Epoch: 1, Batch: 47, Loss: 0.6868, Elapsed: 5m18s
2020-05-04 10:32:10.074032: Epoch: 1, Batch: 12, Loss: 0.7417, Elapsed: 12m27s
2020-05-04 10:34:57.186747: Epoch: 1, Batch: 50, Loss: 0.7099, Elapsed: 7m15s
Starting testing the validation set with 200 subgraphs!
2020-05-04 10:37:09.417046: Epoch: 1, Batch: 48, Loss: 0.7008, Elapsed: 5m12s
2020-05-04 10:45:11.627372: Epoch: 1, Batch: 13, Loss: 0.7633, Elapsed: 13m1s
2020-05-04 10:45:24.898454: Epoch: 1, Batch: 49, Loss: 0.6802, Elapsed: 8m15s
2020-05-04 10:52:30.246261: Epoch: 1, Batch: 50, Loss: 0.6863, Elapsed: 7m5s
Starting testing the validation set with 200 subgraphs!
2020-05-04 10:56:23.642091: Epoch: 1, Batch: 14, Loss: 0.7309, Elapsed: 11m11s
2020-05-04 11:05:59.520992: Epoch: 1, Batch: 15, Loss: 0.7764, Elapsed: 9m35s
2020-05-04 11:11:36.319895: Epoch: 1, Batch: 16, Loss: 0.7292, Elapsed: 5m36s
2020-05-04 11:14:55.676095: Validation Test:  Loss: 0.7270,  Acc: 49.6110, AUC: 0.5062, Precision: 0.5575 -- Elapsed: 39m58s
2020-05-04 11:19:34.823375: Epoch: 1, Batch: 51, Loss: 0.7336, Elapsed: 4m39s
2020-05-04 11:23:09.915249: Epoch: 1, Batch: 17, Loss: 0.7214, Elapsed: 11m33s
2020-05-04 11:27:23.052752: Epoch: 1, Batch: 52, Loss: 0.7250, Elapsed: 7m48s
2020-05-04 11:32:45.576234: Validation Test:  Loss: 0.6887,  Acc: 55.9511, AUC: 0.5801, Precision: 0.6190 -- Elapsed: 40m15s
2020-05-04 11:33:49.574489: Epoch: 1, Batch: 53, Loss: 0.7009, Elapsed: 6m26s
2020-05-04 11:34:56.923246: Epoch: 1, Batch: 18, Loss: 0.7693, Elapsed: 11m47s
2020-05-04 11:38:38.611800: Epoch: 1, Batch: 51, Loss: 0.6810, Elapsed: 5m53s
2020-05-04 11:43:21.244745: Epoch: 1, Batch: 19, Loss: 0.7355, Elapsed: 8m24s
2020-05-04 11:43:42.148874: Epoch: 1, Batch: 54, Loss: 0.7668, Elapsed: 9m52s
2020-05-04 11:45:53.674452: Epoch: 1, Batch: 52, Loss: 0.6903, Elapsed: 7m15s
2020-05-04 11:49:59.894260: Epoch: 1, Batch: 53, Loss: 0.6851, Elapsed: 4m6s
2020-05-04 11:50:09.690551: Epoch: 1, Batch: 55, Loss: 0.7392, Elapsed: 6m27s
2020-05-04 11:52:23.284696: Epoch: 1, Batch: 20, Loss: 0.7507, Elapsed: 9m2s
2020-05-04 11:55:34.058504: Epoch: 1, Batch: 56, Loss: 0.7234, Elapsed: 5m24s
2020-05-04 11:57:06.984374: Epoch: 1, Batch: 54, Loss: 0.6863, Elapsed: 7m7s
2020-05-04 12:01:26.856576: Epoch: 1, Batch: 21, Loss: 0.7485, Elapsed: 9m3s
2020-05-04 12:02:02.562101: Epoch: 1, Batch: 55, Loss: 0.6928, Elapsed: 4m55s
2020-05-04 12:02:06.825667: Epoch: 1, Batch: 57, Loss: 0.7235, Elapsed: 6m32s
2020-05-04 12:08:43.373158: Epoch: 1, Batch: 58, Loss: 0.7144, Elapsed: 6m36s
2020-05-04 12:12:17.963260: Epoch: 1, Batch: 22, Loss: 0.7360, Elapsed: 10m51s
2020-05-04 12:14:07.891040: Epoch: 1, Batch: 56, Loss: 0.6977, Elapsed: 12m5s
2020-05-04 12:17:36.578005: Epoch: 1, Batch: 59, Loss: 0.7066, Elapsed: 8m53s
2020-05-04 12:22:03.813029: Epoch: 1, Batch: 57, Loss: 0.6866, Elapsed: 7m55s
2020-05-04 12:27:59.717545: Epoch: 1, Batch: 23, Loss: 0.7249, Elapsed: 15m41s
2020-05-04 12:28:19.287661: Epoch: 1, Batch: 60, Loss: 0.7413, Elapsed: 10m42s
2020-05-04 12:31:16.935973: Epoch: 1, Batch: 61, Loss: 0.7216, Elapsed: 2m57s
2020-05-04 12:32:39.310677: Epoch: 1, Batch: 58, Loss: 0.6937, Elapsed: 10m35s
2020-05-04 12:40:57.372066: Epoch: 1, Batch: 59, Loss: 0.7109, Elapsed: 8m18s
2020-05-04 12:41:43.487869: Epoch: 1, Batch: 62, Loss: 0.7047, Elapsed: 10m26s
2020-05-04 12:44:29.959176: Epoch: 1, Batch: 24, Loss: 0.7272, Elapsed: 16m30s
2020-05-04 12:49:12.220114: Epoch: 1, Batch: 60, Loss: 0.6942, Elapsed: 8m14s
2020-05-04 12:51:48.726606: Epoch: 1, Batch: 63, Loss: 0.7328, Elapsed: 10m5s
2020-05-04 12:55:44.267259: Epoch: 1, Batch: 61, Loss: 0.6803, Elapsed: 6m32s
2020-05-04 12:56:24.715527: Epoch: 1, Batch: 64, Loss: 0.7148, Elapsed: 4m35s
2020-05-04 13:00:20.297386: Epoch: 1, Batch: 25, Loss: 0.7266, Elapsed: 15m50s
2020-05-04 13:02:34.829874: Epoch: 1, Batch: 62, Loss: 0.7027, Elapsed: 6m50s
2020-05-04 13:03:50.020755: Epoch: 1, Batch: 65, Loss: 0.7411, Elapsed: 7m25s
2020-05-04 13:07:09.640234: Epoch: 1, Batch: 66, Loss: 0.7287, Elapsed: 3m19s
2020-05-04 13:10:07.884887: Epoch: 1, Batch: 26, Loss: 0.7532, Elapsed: 9m47s
2020-05-04 13:10:42.195817: Epoch: 1, Batch: 63, Loss: 0.6895, Elapsed: 8m7s
2020-05-04 13:12:36.036155: Epoch: 1, Batch: 67, Loss: 0.7603, Elapsed: 5m26s
2020-05-04 13:19:22.148448: Epoch: 1, Batch: 64, Loss: 0.6795, Elapsed: 8m39s
2020-05-04 13:20:03.156802: Epoch: 1, Batch: 68, Loss: 0.7380, Elapsed: 7m27s
2020-05-04 13:23:46.238499: Epoch: 1, Batch: 65, Loss: 0.6824, Elapsed: 4m24s
2020-05-04 13:23:45.468352: Epoch: 1, Batch: 27, Loss: 0.7241, Elapsed: 13m37s
2020-05-04 13:25:48.828554: Epoch: 1, Batch: 69, Loss: 0.7387, Elapsed: 5m45s
2020-05-04 13:30:47.144546: Epoch: 1, Batch: 66, Loss: 0.6873, Elapsed: 7m0s
2020-05-04 13:34:23.741972: Epoch: 1, Batch: 70, Loss: 0.7389, Elapsed: 8m34s
2020-05-04 13:41:44.726474: Epoch: 1, Batch: 67, Loss: 0.6888, Elapsed: 10m57s
2020-05-04 13:42:17.710640: Epoch: 1, Batch: 71, Loss: 0.7481, Elapsed: 7m53s
2020-05-04 13:45:33.152858: Epoch: 1, Batch: 28, Loss: 0.7354, Elapsed: 21m47s
2020-05-04 13:46:48.034362: Epoch: 1, Batch: 72, Loss: 0.7132, Elapsed: 4m30s
2020-05-04 13:49:27.490608: Epoch: 1, Batch: 68, Loss: 0.6969, Elapsed: 7m42s
2020-05-04 13:53:06.278932: Epoch: 1, Batch: 73, Loss: 0.7284, Elapsed: 6m18s
2020-05-04 13:53:38.609041: Epoch: 1, Batch: 29, Loss: 0.7361, Elapsed: 8m5s
2020-05-04 13:55:44.013635: Epoch: 1, Batch: 69, Loss: 0.6854, Elapsed: 6m16s
2020-05-04 14:00:30.958360: Epoch: 1, Batch: 74, Loss: 0.7376, Elapsed: 7m24s
2020-05-04 14:01:53.860287: Epoch: 1, Batch: 30, Loss: 0.7442, Elapsed: 8m15s
2020-05-04 14:07:07.512974: Epoch: 1, Batch: 70, Loss: 0.6889, Elapsed: 11m23s
2020-05-04 14:08:06.857470: Epoch: 1, Batch: 75, Loss: 0.7366, Elapsed: 7m35s
2020-05-04 14:11:21.403027: Epoch: 1, Batch: 31, Loss: 0.7175, Elapsed: 9m27s
2020-05-04 14:15:33.880335: Epoch: 1, Batch: 76, Loss: 0.7177, Elapsed: 7m27s
2020-05-04 14:18:08.589356: Epoch: 1, Batch: 71, Loss: 0.6993, Elapsed: 11m1s
2020-05-04 14:21:16.165993: Epoch: 1, Batch: 77, Loss: 0.7397, Elapsed: 5m42s
2020-05-04 14:21:29.023043: Epoch: 1, Batch: 32, Loss: 0.7412, Elapsed: 10m7s
2020-05-04 14:24:00.374703: Epoch: 1, Batch: 72, Loss: 0.6679, Elapsed: 5m51s
2020-05-04 14:28:12.177953: Epoch: 1, Batch: 33, Loss: 0.7202, Elapsed: 6m43s
2020-05-04 14:29:24.177253: Epoch: 1, Batch: 73, Loss: 0.6615, Elapsed: 5m23s
2020-05-04 14:29:41.237383: Epoch: 1, Batch: 78, Loss: 0.7249, Elapsed: 8m25s
2020-05-04 14:33:59.697390: Epoch: 1, Batch: 74, Loss: 0.7003, Elapsed: 4m35s
2020-05-04 14:37:07.863364: Epoch: 1, Batch: 34, Loss: 0.7440, Elapsed: 8m55s
2020-05-04 14:37:29.070902: Epoch: 1, Batch: 79, Loss: 0.7470, Elapsed: 7m47s
2020-05-04 14:44:58.724107: Epoch: 1, Batch: 35, Loss: 0.7364, Elapsed: 7m50s
2020-05-04 14:47:37.669031: Epoch: 1, Batch: 75, Loss: 0.6979, Elapsed: 13m37s
2020-05-04 14:47:56.434388: Epoch: 1, Batch: 80, Loss: 0.7497, Elapsed: 10m27s
2020-05-04 14:52:48.923379: Epoch: 1, Batch: 81, Loss: 0.7437, Elapsed: 4m52s
2020-05-04 14:55:44.636126: Epoch: 1, Batch: 76, Loss: 0.6989, Elapsed: 8m6s
2020-05-04 14:58:12.471264: Epoch: 1, Batch: 36, Loss: 0.7107, Elapsed: 13m13s
2020-05-04 15:00:43.552205: Epoch: 1, Batch: 82, Loss: 0.7651, Elapsed: 7m54s
2020-05-04 15:02:02.542434: Epoch: 1, Batch: 77, Loss: 0.6845, Elapsed: 6m17s
2020-05-04 15:06:40.986079: Epoch: 1, Batch: 83, Loss: 0.7365, Elapsed: 5m57s
2020-05-04 15:08:09.431896: Epoch: 1, Batch: 78, Loss: 0.6976, Elapsed: 6m6s
2020-05-04 15:13:56.246252: Epoch: 1, Batch: 79, Loss: 0.6887, Elapsed: 5m46s
2020-05-04 15:16:02.136322: Epoch: 1, Batch: 37, Loss: 0.7238, Elapsed: 17m49s
2020-05-04 15:16:38.879742: Epoch: 1, Batch: 84, Loss: 0.7474, Elapsed: 9m57s
2020-05-04 15:19:14.464875: Epoch: 1, Batch: 80, Loss: 0.6854, Elapsed: 5m18s
2020-05-04 15:23:54.446460: Epoch: 1, Batch: 85, Loss: 0.7271, Elapsed: 7m15s
2020-05-04 15:27:23.485335: Epoch: 1, Batch: 38, Loss: 0.7306, Elapsed: 11m21s
2020-05-04 15:28:40.762706: Epoch: 1, Batch: 86, Loss: 0.7292, Elapsed: 4m46s
2020-05-04 15:31:41.362377: Epoch: 1, Batch: 81, Loss: 0.7028, Elapsed: 12m26s
2020-05-04 15:38:20.092771: Epoch: 1, Batch: 87, Loss: 0.7409, Elapsed: 9m39s
2020-05-04 15:38:23.825049: Epoch: 1, Batch: 82, Loss: 0.6817, Elapsed: 6m42s
2020-05-04 15:42:43.160630: Epoch: 1, Batch: 39, Loss: 0.7082, Elapsed: 15m19s
2020-05-04 15:43:51.846481: Epoch: 1, Batch: 83, Loss: 0.7148, Elapsed: 5m27s
2020-05-04 15:45:11.977140: Epoch: 1, Batch: 88, Loss: 0.7491, Elapsed: 6m51s
2020-05-04 15:47:39.019474: Epoch: 1, Batch: 84, Loss: 0.6996, Elapsed: 3m47s
2020-05-04 15:50:11.321294: Epoch: 1, Batch: 89, Loss: 0.7204, Elapsed: 4m59s
2020-05-04 15:52:51.305598: Epoch: 1, Batch: 40, Loss: 0.7105, Elapsed: 10m8s
2020-05-04 15:53:15.648480: Epoch: 1, Batch: 85, Loss: 0.7019, Elapsed: 5m36s
2020-05-04 15:55:38.371254: Epoch: 1, Batch: 90, Loss: 0.7189, Elapsed: 5m27s
2020-05-04 15:58:37.463718: Epoch: 1, Batch: 86, Loss: 0.6663, Elapsed: 5m21s
2020-05-04 16:02:01.018398: Epoch: 1, Batch: 91, Loss: 0.7506, Elapsed: 6m22s
2020-05-04 16:03:49.956998: Epoch: 1, Batch: 41, Loss: 0.7006, Elapsed: 10m58s
2020-05-04 16:08:38.833866: Epoch: 1, Batch: 92, Loss: 0.7427, Elapsed: 6m37s
2020-05-04 16:10:23.165289: Epoch: 1, Batch: 87, Loss: 0.7228, Elapsed: 11m45s
2020-05-04 16:16:32.170385: Epoch: 1, Batch: 93, Loss: 0.7255, Elapsed: 7m53s
2020-05-04 16:18:58.377269: Epoch: 1, Batch: 88, Loss: 0.6994, Elapsed: 8m35s
2020-05-04 16:23:19.751326: Epoch: 1, Batch: 89, Loss: 0.6752, Elapsed: 4m21s
2020-05-04 16:23:17.404892: Epoch: 1, Batch: 42, Loss: 0.7311, Elapsed: 19m27s
2020-05-04 16:23:27.217526: Epoch: 1, Batch: 94, Loss: 0.7452, Elapsed: 6m55s
2020-05-04 16:31:06.230500: Epoch: 1, Batch: 43, Loss: 0.7201, Elapsed: 7m48s
2020-05-04 16:31:43.142308: Epoch: 1, Batch: 95, Loss: 0.7493, Elapsed: 8m15s
2020-05-04 16:31:43.106504: Epoch: 1, Batch: 90, Loss: 0.6929, Elapsed: 8m23s
2020-05-04 16:37:50.246357: Epoch: 1, Batch: 96, Loss: 0.7170, Elapsed: 6m7s
2020-05-04 16:40:21.703672: Epoch: 1, Batch: 91, Loss: 0.7044, Elapsed: 8m38s
2020-05-04 16:42:03.936193: Epoch: 1, Batch: 97, Loss: 0.7563, Elapsed: 4m13s
2020-05-04 16:46:00.672765: Epoch: 1, Batch: 92, Loss: 0.6674, Elapsed: 5m38s
2020-05-04 16:47:39.468066: Epoch: 1, Batch: 98, Loss: 0.7352, Elapsed: 5m35s
2020-05-04 16:47:47.935513: Epoch: 1, Batch: 44, Loss: 0.7241, Elapsed: 16m41s
2020-05-04 16:52:33.606620: Epoch: 1, Batch: 99, Loss: 0.7609, Elapsed: 4m54s
2020-05-04 16:54:22.703023: Epoch: 1, Batch: 93, Loss: 0.6835, Elapsed: 8m22s
2020-05-04 16:56:47.827690: Epoch: 1, Batch: 100, Loss: 0.7402, Elapsed: 4m14s
Starting testing the validation set with 200 subgraphs!
2020-05-04 17:00:48.281957: Epoch: 1, Batch: 94, Loss: 0.6941, Elapsed: 6m25s
2020-05-04 17:06:44.486705: Epoch: 1, Batch: 45, Loss: 0.7221, Elapsed: 18m56s
2020-05-04 17:07:23.986654: Epoch: 1, Batch: 95, Loss: 0.6769, Elapsed: 6m35s
2020-05-04 17:12:42.599269: Epoch: 1, Batch: 96, Loss: 0.6804, Elapsed: 5m18s
2020-05-04 17:17:16.799186: Epoch: 1, Batch: 46, Loss: 0.7205, Elapsed: 10m32s
2020-05-04 17:18:46.246490: Epoch: 1, Batch: 97, Loss: 0.6891, Elapsed: 6m3s
2020-05-04 17:25:50.901840: Epoch: 1, Batch: 47, Loss: 0.7413, Elapsed: 8m34s
2020-05-04 17:28:19.233889: Epoch: 1, Batch: 98, Loss: 0.7071, Elapsed: 9m32s
2020-05-04 17:33:22.670969: Epoch: 1, Batch: 48, Loss: 0.7184, Elapsed: 7m31s
2020-05-04 17:34:51.127553: Epoch: 1, Batch: 99, Loss: 0.6854, Elapsed: 6m31s
2020-05-04 17:38:41.030547: Validation Test:  Loss: 0.7331,  Acc: 49.3158, AUC: 0.4991, Precision: 0.5511 -- Elapsed: 41m53s
2020-05-04 17:43:51.918795: Epoch: 1, Batch: 100, Loss: 0.6969, Elapsed: 9m0s
Starting testing the validation set with 200 subgraphs!
2020-05-04 17:46:25.843762: Epoch: 1, Batch: 101, Loss: 0.7514, Elapsed: 7m44s
2020-05-04 17:47:37.322104: Epoch: 1, Batch: 49, Loss: 0.7209, Elapsed: 14m14s
2020-05-04 17:54:44.732044: Epoch: 1, Batch: 102, Loss: 0.7114, Elapsed: 8m18s
2020-05-04 18:00:10.703982: Epoch: 1, Batch: 103, Loss: 0.7346, Elapsed: 5m25s
2020-05-04 18:02:41.099892: Epoch: 1, Batch: 50, Loss: 0.7192, Elapsed: 15m3s
Starting testing the validation set with 200 subgraphs!
2020-05-04 18:08:39.390692: Epoch: 1, Batch: 104, Loss: 0.7231, Elapsed: 8m28s
2020-05-04 18:17:12.245734: Epoch: 1, Batch: 105, Loss: 0.7377, Elapsed: 8m32s
2020-05-04 18:21:11.783871: Epoch: 1, Batch: 106, Loss: 0.7220, Elapsed: 3m59s
2020-05-04 18:24:47.769418: Validation Test:  Loss: 0.6887,  Acc: 56.5719, AUC: 0.5726, Precision: 0.6004 -- Elapsed: 40m55s
2020-05-04 18:26:41.591504: Epoch: 1, Batch: 107, Loss: 0.7407, Elapsed: 5m29s
2020-05-04 18:32:03.910639: Epoch: 1, Batch: 101, Loss: 0.6760, Elapsed: 7m16s
2020-05-04 18:32:42.236499: Epoch: 1, Batch: 108, Loss: 0.7535, Elapsed: 6m0s
2020-05-04 18:37:16.143348: Epoch: 1, Batch: 102, Loss: 0.6784, Elapsed: 5m12s
2020-05-04 18:39:27.657748: Epoch: 1, Batch: 109, Loss: 0.7095, Elapsed: 6m45s
2020-05-04 18:41:50.592059: Epoch: 1, Batch: 103, Loss: 0.6997, Elapsed: 4m34s
2020-05-04 18:42:27.361606: Epoch: 1, Batch: 110, Loss: 0.7389, Elapsed: 2m59s
2020-05-04 18:46:45.233667: Epoch: 1, Batch: 104, Loss: 0.6859, Elapsed: 4m54s
2020-05-04 18:49:26.241043: Epoch: 1, Batch: 111, Loss: 0.7364, Elapsed: 6m58s
2020-05-04 18:52:38.614253: Epoch: 1, Batch: 105, Loss: 0.7018, Elapsed: 5m53s
2020-05-04 18:53:19.714585: Epoch: 1, Batch: 112, Loss: 0.7306, Elapsed: 3m53s
2020-05-04 18:54:28.156454: Validation Test:  Loss: 0.7168,  Acc: 51.6652, AUC: 0.5176, Precision: 0.5689 -- Elapsed: 51m47s
2020-05-04 18:57:04.662954: Epoch: 1, Batch: 113, Loss: 0.7110, Elapsed: 3m44s
2020-05-04 19:00:15.719173: Epoch: 1, Batch: 106, Loss: 0.7274, Elapsed: 7m37s
2020-05-04 19:01:47.227200: Epoch: 1, Batch: 114, Loss: 0.7493, Elapsed: 4m42s
2020-05-04 19:05:39.456620: Epoch: 1, Batch: 107, Loss: 0.6837, Elapsed: 5m23s
2020-05-04 19:08:03.036381: Epoch: 1, Batch: 51, Loss: 0.7357, Elapsed: 13m34s
2020-05-04 19:11:04.521503: Epoch: 1, Batch: 115, Loss: 0.7551, Elapsed: 9m17s
2020-05-04 19:16:01.836627: Epoch: 1, Batch: 108, Loss: 0.6906, Elapsed: 10m22s
2020-05-04 19:18:34.314317: Epoch: 1, Batch: 116, Loss: 0.7147, Elapsed: 7m29s
2020-05-04 19:23:14.729307: Epoch: 1, Batch: 109, Loss: 0.6847, Elapsed: 7m12s
2020-05-04 19:25:03.753817: Epoch: 1, Batch: 52, Loss: 0.7141, Elapsed: 17m0s
2020-05-04 19:26:22.685885: Epoch: 1, Batch: 117, Loss: 0.7253, Elapsed: 7m48s
2020-05-04 19:26:58.772415: Epoch: 1, Batch: 110, Loss: 0.6700, Elapsed: 3m44s
2020-05-04 19:31:56.741510: Epoch: 1, Batch: 118, Loss: 0.7291, Elapsed: 5m34s
2020-05-04 19:31:51.870147: Epoch: 1, Batch: 111, Loss: 0.6755, Elapsed: 4m53s
2020-05-04 19:38:18.098505: Epoch: 1, Batch: 119, Loss: 0.7229, Elapsed: 6m21s
2020-05-04 19:43:47.363687: Epoch: 1, Batch: 112, Loss: 0.6895, Elapsed: 11m55s
2020-05-04 19:46:47.490619: Epoch: 1, Batch: 120, Loss: 0.6997, Elapsed: 8m29s
2020-05-04 19:51:10.405563: Epoch: 1, Batch: 113, Loss: 0.6883, Elapsed: 7m23s
2020-05-04 19:51:12.970636: Epoch: 1, Batch: 53, Loss: 0.7156, Elapsed: 26m9s
2020-05-04 19:56:55.321363: Epoch: 1, Batch: 121, Loss: 0.7518, Elapsed: 10m7s
2020-05-04 19:57:02.035286: Epoch: 1, Batch: 114, Loss: 0.6793, Elapsed: 5m51s
2020-05-04 20:02:36.842542: Epoch: 1, Batch: 115, Loss: 0.6955, Elapsed: 5m34s
2020-05-04 20:02:40.551413: Epoch: 1, Batch: 122, Loss: 0.7134, Elapsed: 5m45s
2020-05-04 20:06:39.132037: Epoch: 1, Batch: 54, Loss: 0.7111, Elapsed: 15m26s
2020-05-04 20:10:41.843180: Epoch: 1, Batch: 116, Loss: 0.6884, Elapsed: 8m4s
2020-05-04 20:11:35.145611: Epoch: 1, Batch: 123, Loss: 0.7291, Elapsed: 8m54s
2020-05-04 20:12:40.389632: Epoch: 1, Batch: 55, Loss: 0.7132, Elapsed: 6m1s
2020-05-04 20:19:06.935199: Epoch: 1, Batch: 124, Loss: 0.7186, Elapsed: 7m31s
2020-05-04 20:20:32.883476: Epoch: 1, Batch: 117, Loss: 0.6853, Elapsed: 9m51s
2020-05-04 20:22:02.297964: Epoch: 1, Batch: 56, Loss: 0.6905, Elapsed: 9m21s
2020-05-04 20:25:28.923941: Epoch: 1, Batch: 125, Loss: 0.7319, Elapsed: 6m21s
2020-05-04 20:26:43.653541: Epoch: 1, Batch: 118, Loss: 0.6732, Elapsed: 6m10s
2020-05-04 20:29:49.299308: Epoch: 1, Batch: 126, Loss: 0.6992, Elapsed: 4m20s
2020-05-04 20:34:01.736067: Epoch: 1, Batch: 57, Loss: 0.7174, Elapsed: 11m59s
2020-05-04 20:34:28.962884: Epoch: 1, Batch: 119, Loss: 0.7086, Elapsed: 7m45s
2020-05-04 20:35:10.718536: Epoch: 1, Batch: 127, Loss: 0.7344, Elapsed: 5m21s
2020-05-04 20:41:29.455166: Epoch: 1, Batch: 128, Loss: 0.7069, Elapsed: 6m18s
2020-05-04 20:43:48.762971: Epoch: 1, Batch: 120, Loss: 0.6829, Elapsed: 9m19s
2020-05-04 20:46:13.915673: Epoch: 1, Batch: 129, Loss: 0.7354, Elapsed: 4m44s
2020-05-04 20:50:01.562579: Epoch: 1, Batch: 130, Loss: 0.7537, Elapsed: 3m47s
2020-05-04 20:51:16.845958: Epoch: 1, Batch: 121, Loss: 0.6846, Elapsed: 7m28s
2020-05-04 20:52:54.808748: Epoch: 1, Batch: 58, Loss: 0.7101, Elapsed: 18m53s
2020-05-04 20:57:51.011039: Epoch: 1, Batch: 131, Loss: 0.7271, Elapsed: 7m49s
2020-05-04 20:59:53.611056: Epoch: 1, Batch: 59, Loss: 0.7366, Elapsed: 6m58s
2020-05-04 21:02:56.996164: Epoch: 1, Batch: 132, Loss: 0.7392, Elapsed: 5m5s
2020-05-04 21:04:25.303706: Epoch: 1, Batch: 122, Loss: 0.6791, Elapsed: 13m8s
2020-05-04 21:09:25.428934: Epoch: 1, Batch: 133, Loss: 0.7302, Elapsed: 6m28s
2020-05-04 21:13:19.576807: Epoch: 1, Batch: 123, Loss: 0.6875, Elapsed: 8m54s
2020-05-04 21:15:09.549161: Epoch: 1, Batch: 60, Loss: 0.7151, Elapsed: 15m15s
2020-05-04 21:16:28.527184: Epoch: 1, Batch: 134, Loss: 0.7347, Elapsed: 7m3s
2020-05-04 21:18:00.680269: Epoch: 1, Batch: 124, Loss: 0.6680, Elapsed: 4m41s
2020-05-04 21:28:23.548027: Epoch: 1, Batch: 125, Loss: 0.6869, Elapsed: 10m22s
2020-05-04 21:28:42.494072: Epoch: 1, Batch: 135, Loss: 0.7260, Elapsed: 12m13s
2020-05-04 21:30:24.648389: Epoch: 1, Batch: 61, Loss: 0.7161, Elapsed: 15m15s
2020-05-04 21:36:48.144340: Epoch: 1, Batch: 136, Loss: 0.7254, Elapsed: 8m5s
2020-05-04 21:36:58.971468: Epoch: 1, Batch: 126, Loss: 0.6761, Elapsed: 8m35s
2020-05-04 21:42:02.574539: Epoch: 1, Batch: 62, Loss: 0.7037, Elapsed: 11m37s
2020-05-04 21:43:19.267179: Epoch: 1, Batch: 137, Loss: 0.7186, Elapsed: 6m31s
2020-05-04 21:46:25.610386: Epoch: 1, Batch: 127, Loss: 0.6867, Elapsed: 9m26s
2020-05-04 21:48:51.795089: Epoch: 1, Batch: 138, Loss: 0.7167, Elapsed: 5m32s
2020-05-04 21:50:26.448616: Epoch: 1, Batch: 128, Loss: 0.6519, Elapsed: 4m0s
2020-05-04 21:52:46.264356: Epoch: 1, Batch: 63, Loss: 0.6989, Elapsed: 10m43s
2020-05-04 21:55:02.210613: Epoch: 1, Batch: 139, Loss: 0.7086, Elapsed: 6m10s
2020-05-04 21:58:45.123110: Epoch: 1, Batch: 129, Loss: 0.7031, Elapsed: 8m18s
2020-05-04 21:59:19.995193: Epoch: 1, Batch: 140, Loss: 0.7449, Elapsed: 4m17s
2020-05-04 22:03:34.332617: Epoch: 1, Batch: 64, Loss: 0.7078, Elapsed: 10m48s
2020-05-04 22:05:58.256019: Epoch: 1, Batch: 130, Loss: 0.6910, Elapsed: 7m13s
2020-05-04 22:06:23.773783: Epoch: 1, Batch: 141, Loss: 0.7385, Elapsed: 7m3s
2020-05-04 22:13:37.351138: Epoch: 1, Batch: 131, Loss: 0.6789, Elapsed: 7m39s
2020-05-04 22:16:06.010968: Epoch: 1, Batch: 142, Loss: 0.7304, Elapsed: 9m42s
2020-05-04 22:20:40.294373: Epoch: 1, Batch: 132, Loss: 0.6916, Elapsed: 7m2s
2020-05-04 22:21:03.923379: Epoch: 1, Batch: 65, Loss: 0.7156, Elapsed: 17m29s
2020-05-04 22:22:16.637708: Epoch: 1, Batch: 143, Loss: 0.7099, Elapsed: 6m10s
2020-05-04 22:28:39.084190: Epoch: 1, Batch: 144, Loss: 0.7136, Elapsed: 6m22s
2020-05-04 22:29:09.881318: Epoch: 1, Batch: 133, Loss: 0.7162, Elapsed: 8m29s
2020-05-04 22:33:19.688072: Epoch: 1, Batch: 66, Loss: 0.7139, Elapsed: 12m15s
2020-05-04 22:35:02.752482: Epoch: 1, Batch: 134, Loss: 0.6715, Elapsed: 5m52s
2020-05-04 22:37:09.531206: Epoch: 1, Batch: 145, Loss: 0.7074, Elapsed: 8m30s
2020-05-04 22:44:12.444546: Epoch: 1, Batch: 146, Loss: 0.7362, Elapsed: 7m2s
2020-05-04 22:48:17.949085: Epoch: 1, Batch: 135, Loss: 0.6804, Elapsed: 13m15s
2020-05-04 22:49:25.386745: Epoch: 1, Batch: 147, Loss: 0.7115, Elapsed: 5m12s
2020-05-04 22:50:57.170923: Epoch: 1, Batch: 67, Loss: 0.6960, Elapsed: 17m37s
2020-05-04 22:55:41.608578: Epoch: 1, Batch: 136, Loss: 0.6794, Elapsed: 7m23s
2020-05-04 22:59:13.797646: Epoch: 1, Batch: 148, Loss: 0.7184, Elapsed: 9m48s
2020-05-04 23:00:09.861870: Epoch: 1, Batch: 68, Loss: 0.7095, Elapsed: 9m12s
2020-05-04 23:01:12.357467: Epoch: 1, Batch: 137, Loss: 0.6919, Elapsed: 5m30s
2020-05-04 23:05:01.982430: Epoch: 1, Batch: 149, Loss: 0.7128, Elapsed: 5m48s
2020-05-04 23:10:09.120717: Epoch: 1, Batch: 150, Loss: 0.7403, Elapsed: 5m7s
Starting testing the validation set with 200 subgraphs!
2020-05-04 23:11:34.718789: Epoch: 1, Batch: 69, Loss: 0.7115, Elapsed: 11m24s
2020-05-04 23:11:56.214559: Epoch: 1, Batch: 138, Loss: 0.6936, Elapsed: 10m43s
2020-05-04 23:18:02.601937: Epoch: 1, Batch: 139, Loss: 0.6521, Elapsed: 6m6s
2020-05-04 23:22:38.931550: Epoch: 1, Batch: 140, Loss: 0.6916, Elapsed: 4m36s
2020-05-04 23:24:53.312693: Epoch: 1, Batch: 70, Loss: 0.7148, Elapsed: 13m18s
2020-05-04 23:30:41.625558: Epoch: 1, Batch: 141, Loss: 0.6748, Elapsed: 8m2s
2020-05-04 23:33:53.391472: Epoch: 1, Batch: 71, Loss: 0.7024, Elapsed: 9m0s
2020-05-04 23:36:29.858465: Epoch: 1, Batch: 142, Loss: 0.6917, Elapsed: 5m48s
2020-05-04 23:41:19.397657: Epoch: 1, Batch: 143, Loss: 0.6825, Elapsed: 4m49s
2020-05-04 23:45:25.667232: Epoch: 1, Batch: 72, Loss: 0.7331, Elapsed: 11m32s
2020-05-04 23:49:02.681614: Epoch: 1, Batch: 144, Loss: 0.6670, Elapsed: 7m43s
2020-05-04 23:52:27.043009: Validation Test:  Loss: 0.7288,  Acc: 50.9287, AUC: 0.5142, Precision: 0.5668 -- Elapsed: 42m17s
2020-05-04 23:54:06.521062: Epoch: 1, Batch: 145, Loss: 0.6858, Elapsed: 5m3s
2020-05-04 23:57:47.374147: Epoch: 1, Batch: 151, Loss: 0.7252, Elapsed: 5m20s
2020-05-04 23:59:18.513163: Epoch: 1, Batch: 73, Loss: 0.7340, Elapsed: 13m52s
2020-05-05 00:02:44.360366: Epoch: 1, Batch: 146, Loss: 0.6964, Elapsed: 8m37s
2020-05-05 00:03:31.149236: Epoch: 1, Batch: 152, Loss: 0.7345, Elapsed: 5m43s
2020-05-05 00:07:53.630631: Epoch: 1, Batch: 153, Loss: 0.7266, Elapsed: 4m22s
2020-05-05 00:08:15.418892: Epoch: 1, Batch: 147, Loss: 0.6757, Elapsed: 5m31s
2020-05-05 00:09:11.674978: Epoch: 1, Batch: 74, Loss: 0.7287, Elapsed: 9m53s
2020-05-05 00:15:56.735050: Epoch: 1, Batch: 148, Loss: 0.6779, Elapsed: 7m41s
2020-05-05 00:16:17.999528: Epoch: 1, Batch: 154, Loss: 0.7098, Elapsed: 8m24s
2020-05-05 00:21:41.143542: Epoch: 1, Batch: 155, Loss: 0.7050, Elapsed: 5m23s
2020-05-05 00:23:33.235920: Epoch: 1, Batch: 149, Loss: 0.6986, Elapsed: 7m36s
2020-05-05 00:26:22.508673: Epoch: 1, Batch: 75, Loss: 0.7178, Elapsed: 17m10s
2020-05-05 00:30:15.477983: Epoch: 1, Batch: 150, Loss: 0.6879, Elapsed: 6m42s
Starting testing the validation set with 200 subgraphs!
2020-05-05 00:34:46.714152: Epoch: 1, Batch: 156, Loss: 0.7319, Elapsed: 13m5s
2020-05-05 00:37:17.197728: Epoch: 1, Batch: 76, Loss: 0.7141, Elapsed: 10m54s
2020-05-05 00:43:27.810062: Epoch: 1, Batch: 157, Loss: 0.7263, Elapsed: 8m41s
2020-05-05 00:47:34.716706: Epoch: 1, Batch: 158, Loss: 0.7363, Elapsed: 4m6s
2020-05-05 00:49:29.329635: Epoch: 1, Batch: 77, Loss: 0.7231, Elapsed: 12m12s
2020-05-05 00:55:06.838787: Epoch: 1, Batch: 159, Loss: 0.7448, Elapsed: 7m32s
2020-05-05 01:00:58.209220: Epoch: 1, Batch: 78, Loss: 0.6927, Elapsed: 11m28s
2020-05-05 01:01:34.269958: Epoch: 1, Batch: 160, Loss: 0.7424, Elapsed: 6m27s
2020-05-05 01:07:35.094803: Epoch: 1, Batch: 161, Loss: 0.7320, Elapsed: 6m0s
2020-05-05 01:11:21.856544: Epoch: 1, Batch: 162, Loss: 0.7386, Elapsed: 3m46s
2020-05-05 01:12:42.438030: Validation Test:  Loss: 0.6842,  Acc: 56.8732, AUC: 0.5821, Precision: 0.6060 -- Elapsed: 42m26s
2020-05-05 01:18:07.998052: Epoch: 1, Batch: 151, Loss: 0.6791, Elapsed: 5m25s
2020-05-05 01:18:31.562129: Epoch: 1, Batch: 79, Loss: 0.7205, Elapsed: 17m33s
2020-05-05 01:21:08.902096: Epoch: 1, Batch: 163, Loss: 0.7322, Elapsed: 9m47s
2020-05-05 01:22:35.342082: Epoch: 1, Batch: 152, Loss: 0.6722, Elapsed: 4m27s
2020-05-05 01:28:13.795810: Epoch: 1, Batch: 164, Loss: 0.7076, Elapsed: 7m4s
2020-05-05 01:29:40.099326: Epoch: 1, Batch: 80, Loss: 0.7088, Elapsed: 11m8s
2020-05-05 01:33:04.382611: Epoch: 1, Batch: 153, Loss: 0.6917, Elapsed: 10m29s
2020-05-05 01:34:07.098873: Epoch: 1, Batch: 165, Loss: 0.7305, Elapsed: 5m53s
2020-05-05 01:38:59.054507: Epoch: 1, Batch: 154, Loss: 0.6889, Elapsed: 5m54s
2020-05-05 01:39:34.374098: Epoch: 1, Batch: 166, Loss: 0.7222, Elapsed: 5m27s
2020-05-05 01:44:35.395223: Epoch: 1, Batch: 167, Loss: 0.7301, Elapsed: 5m0s
2020-05-05 01:45:57.168545: Epoch: 1, Batch: 81, Loss: 0.6967, Elapsed: 16m17s
2020-05-05 01:47:54.063304: Epoch: 1, Batch: 155, Loss: 0.6814, Elapsed: 8m54s
2020-05-05 01:50:22.230808: Epoch: 1, Batch: 168, Loss: 0.7238, Elapsed: 5m46s
2020-05-05 01:55:56.346263: Epoch: 1, Batch: 169, Loss: 0.7719, Elapsed: 5m34s
2020-05-05 01:57:28.092382: Epoch: 1, Batch: 156, Loss: 0.6732, Elapsed: 9m34s
2020-05-05 01:58:26.360978: Epoch: 1, Batch: 82, Loss: 0.7127, Elapsed: 12m29s
2020-05-05 02:03:46.232814: Epoch: 1, Batch: 170, Loss: 0.7089, Elapsed: 7m49s
2020-05-05 02:05:29.350762: Epoch: 1, Batch: 157, Loss: 0.6741, Elapsed: 8m1s
2020-05-05 02:11:38.246868: Epoch: 1, Batch: 171, Loss: 0.7030, Elapsed: 7m51s
2020-05-05 02:13:34.194003: Epoch: 1, Batch: 83, Loss: 0.7015, Elapsed: 15m7s
2020-05-05 02:16:29.504877: Epoch: 1, Batch: 158, Loss: 0.6916, Elapsed: 11m0s
2020-05-05 02:19:20.031207: Epoch: 1, Batch: 172, Loss: 0.7105, Elapsed: 7m41s
2020-05-05 02:22:27.642441: Epoch: 1, Batch: 173, Loss: 0.7022, Elapsed: 3m7s
2020-05-05 02:24:50.808509: Epoch: 1, Batch: 159, Loss: 0.6865, Elapsed: 8m21s
2020-05-05 02:29:02.380160: Epoch: 1, Batch: 84, Loss: 0.6938, Elapsed: 15m28s
2020-05-05 02:31:27.803304: Epoch: 1, Batch: 174, Loss: 0.7345, Elapsed: 9m0s
2020-05-05 02:35:53.424980: Epoch: 1, Batch: 160, Loss: 0.6897, Elapsed: 11m2s
2020-05-05 02:40:11.098743: Epoch: 1, Batch: 175, Loss: 0.7146, Elapsed: 8m43s
2020-05-05 02:41:25.785040: Epoch: 1, Batch: 85, Loss: 0.6849, Elapsed: 12m23s
2020-05-05 02:42:14.393426: Epoch: 1, Batch: 161, Loss: 0.6831, Elapsed: 6m20s
2020-05-05 02:46:43.075559: Epoch: 1, Batch: 162, Loss: 0.6775, Elapsed: 4m28s
2020-05-05 02:46:56.478127: Epoch: 1, Batch: 176, Loss: 0.7472, Elapsed: 6m45s
2020-05-05 02:52:06.952377: Epoch: 1, Batch: 177, Loss: 0.7171, Elapsed: 5m10s
2020-05-05 02:53:13.546166: Epoch: 1, Batch: 86, Loss: 0.6963, Elapsed: 11m47s
2020-05-05 02:55:15.337055: Epoch: 1, Batch: 163, Loss: 0.6841, Elapsed: 8m32s
2020-05-05 03:00:57.449917: Epoch: 1, Batch: 178, Loss: 0.7355, Elapsed: 8m50s
2020-05-05 03:00:57.098636: Epoch: 1, Batch: 164, Loss: 0.6802, Elapsed: 5m41s
2020-05-05 03:05:47.370789: Epoch: 1, Batch: 87, Loss: 0.7162, Elapsed: 12m33s
2020-05-05 03:09:37.721351: Epoch: 1, Batch: 165, Loss: 0.6910, Elapsed: 8m40s
2020-05-05 03:15:00.866908: Epoch: 1, Batch: 179, Loss: 0.7397, Elapsed: 14m3s
2020-05-05 03:16:17.233977: Epoch: 1, Batch: 88, Loss: 0.7150, Elapsed: 10m29s
2020-05-05 03:16:32.348999: Epoch: 1, Batch: 166, Loss: 0.6833, Elapsed: 6m54s
2020-05-05 03:21:21.239686: Epoch: 1, Batch: 180, Loss: 0.7051, Elapsed: 6m20s
2020-05-05 03:24:23.686684: Epoch: 1, Batch: 167, Loss: 0.6972, Elapsed: 7m51s
2020-05-05 03:25:58.918881: Epoch: 1, Batch: 181, Loss: 0.7297, Elapsed: 4m37s
2020-05-05 03:28:41.224974: Epoch: 1, Batch: 168, Loss: 0.7092, Elapsed: 4m17s
2020-05-05 03:31:25.372505: Epoch: 1, Batch: 182, Loss: 0.7417, Elapsed: 5m26s
2020-05-05 03:33:49.726655: Epoch: 1, Batch: 169, Loss: 0.6740, Elapsed: 5m8s
2020-05-05 03:34:14.483419: Epoch: 1, Batch: 89, Loss: 0.6941, Elapsed: 17m57s
2020-05-05 03:40:35.522349: Epoch: 1, Batch: 170, Loss: 0.7051, Elapsed: 6m45s
2020-05-05 03:42:38.572471: Epoch: 1, Batch: 183, Loss: 0.7317, Elapsed: 11m13s
2020-05-05 03:48:18.507444: Epoch: 1, Batch: 90, Loss: 0.6931, Elapsed: 14m4s
2020-05-05 03:50:19.980393: Epoch: 1, Batch: 184, Loss: 0.7041, Elapsed: 7m41s
2020-05-05 03:54:00.596697: Epoch: 1, Batch: 171, Loss: 0.6836, Elapsed: 13m25s
2020-05-05 03:57:54.824892: Epoch: 1, Batch: 185, Loss: 0.7180, Elapsed: 7m34s
2020-05-05 04:01:01.345466: Epoch: 1, Batch: 172, Loss: 0.7170, Elapsed: 7m0s
2020-05-05 04:03:50.091931: Epoch: 1, Batch: 186, Loss: 0.7107, Elapsed: 5m55s
2020-05-05 04:06:29.362615: Epoch: 1, Batch: 173, Loss: 0.6823, Elapsed: 5m27s
2020-05-05 04:07:05.285841: Epoch: 1, Batch: 91, Loss: 0.7030, Elapsed: 18m46s
2020-05-05 04:11:40.675463: Epoch: 1, Batch: 187, Loss: 0.7350, Elapsed: 7m50s
2020-05-05 04:15:18.115862: Epoch: 1, Batch: 174, Loss: 0.6988, Elapsed: 8m48s
2020-05-05 04:19:17.803247: Epoch: 1, Batch: 92, Loss: 0.7125, Elapsed: 12m12s
2020-05-05 04:19:24.652027: Epoch: 1, Batch: 188, Loss: 0.7087, Elapsed: 7m43s
2020-05-05 04:23:14.863650: Epoch: 1, Batch: 175, Loss: 0.6686, Elapsed: 7m56s
2020-05-05 04:27:18.343731: Epoch: 1, Batch: 189, Loss: 0.6902, Elapsed: 7m53s
2020-05-05 04:29:44.192532: Epoch: 1, Batch: 176, Loss: 0.6921, Elapsed: 6m29s
2020-05-05 04:31:29.257976: Epoch: 1, Batch: 93, Loss: 0.6962, Elapsed: 12m11s
2020-05-05 04:31:50.073400: Epoch: 1, Batch: 190, Loss: 0.7265, Elapsed: 4m31s
2020-05-05 04:35:54.982016: Epoch: 1, Batch: 177, Loss: 0.6826, Elapsed: 6m10s
2020-05-05 04:40:14.487879: Epoch: 1, Batch: 94, Loss: 0.6890, Elapsed: 8m45s
2020-05-05 04:40:41.515895: Epoch: 1, Batch: 178, Loss: 0.6722, Elapsed: 4m46s
2020-05-05 04:41:43.630108: Epoch: 1, Batch: 191, Loss: 0.7095, Elapsed: 9m53s
2020-05-05 04:46:25.290024: Epoch: 1, Batch: 179, Loss: 0.6809, Elapsed: 5m43s
2020-05-05 04:47:25.497673: Epoch: 1, Batch: 192, Loss: 0.7235, Elapsed: 5m41s
2020-05-05 04:52:29.447710: Epoch: 1, Batch: 95, Loss: 0.7150, Elapsed: 12m14s
2020-05-05 04:52:46.430072: Epoch: 1, Batch: 180, Loss: 0.6848, Elapsed: 6m21s
2020-05-05 04:55:44.413944: Epoch: 1, Batch: 193, Loss: 0.7272, Elapsed: 8m18s
2020-05-05 05:02:55.477857: Epoch: 1, Batch: 181, Loss: 0.6654, Elapsed: 10m9s
2020-05-05 05:05:46.220197: Epoch: 1, Batch: 96, Loss: 0.7019, Elapsed: 13m16s
2020-05-05 05:07:58.290908: Epoch: 1, Batch: 194, Loss: 0.7276, Elapsed: 12m13s
2020-05-05 05:11:41.286227: Epoch: 1, Batch: 182, Loss: 0.7072, Elapsed: 8m45s
2020-05-05 05:14:02.740033: Epoch: 1, Batch: 195, Loss: 0.7198, Elapsed: 6m4s
2020-05-05 05:19:18.298972: Epoch: 1, Batch: 183, Loss: 0.6889, Elapsed: 7m36s
2020-05-05 05:20:28.640529: Epoch: 1, Batch: 97, Loss: 0.7006, Elapsed: 14m42s
2020-05-05 05:23:03.185946: Epoch: 1, Batch: 196, Loss: 0.7253, Elapsed: 9m0s
2020-05-05 05:27:59.668825: Epoch: 1, Batch: 184, Loss: 0.6795, Elapsed: 8m41s
2020-05-05 05:29:50.378976: Epoch: 1, Batch: 98, Loss: 0.6976, Elapsed: 9m21s
2020-05-05 05:32:01.301423: Epoch: 1, Batch: 197, Loss: 0.7076, Elapsed: 8m58s
2020-05-05 05:37:40.139319: Epoch: 1, Batch: 185, Loss: 0.6882, Elapsed: 9m40s
2020-05-05 05:42:41.513742: Epoch: 1, Batch: 186, Loss: 0.6963, Elapsed: 5m1s
2020-05-05 05:43:56.945015: Epoch: 1, Batch: 198, Loss: 0.7308, Elapsed: 11m55s
2020-05-05 05:46:45.412881: Epoch: 1, Batch: 187, Loss: 0.6533, Elapsed: 4m3s
2020-05-05 05:48:15.671309: Epoch: 1, Batch: 99, Loss: 0.7093, Elapsed: 18m25s
2020-05-05 05:50:09.750630: Epoch: 1, Batch: 199, Loss: 0.7187, Elapsed: 6m12s
2020-05-05 05:50:11.702881: Epoch: 1, Batch: 188, Loss: 0.6628, Elapsed: 3m26s
2020-05-05 05:54:37.805277: Epoch: 1, Batch: 189, Loss: 0.6844, Elapsed: 4m26s
2020-05-05 05:58:32.193207: Epoch: 1, Batch: 200, Loss: 0.7165, Elapsed: 8m22s
Starting testing the validation set with 200 subgraphs!
2020-05-05 05:59:09.619774: Epoch: 1, Batch: 100, Loss: 0.7018, Elapsed: 10m53s
Starting testing the validation set with 200 subgraphs!
2020-05-05 05:59:59.275462: Epoch: 1, Batch: 190, Loss: 0.6823, Elapsed: 5m21s
2020-05-05 06:05:52.422073: Epoch: 1, Batch: 191, Loss: 0.6836, Elapsed: 5m53s
2020-05-05 06:10:55.364937: Epoch: 1, Batch: 192, Loss: 0.6977, Elapsed: 5m2s
2020-05-05 06:18:02.684286: Epoch: 1, Batch: 193, Loss: 0.7121, Elapsed: 7m7s
2020-05-05 06:21:56.164788: Epoch: 1, Batch: 194, Loss: 0.7061, Elapsed: 3m53s
2020-05-05 06:24:26.461028: Epoch: 1, Batch: 195, Loss: 0.6716, Elapsed: 2m30s
2020-05-05 06:29:09.344117: Epoch: 1, Batch: 196, Loss: 0.6920, Elapsed: 4m42s
2020-05-05 06:36:32.904847: Epoch: 1, Batch: 197, Loss: 0.6901, Elapsed: 7m23s
2020-05-05 06:37:55.384940: Validation Test:  Loss: 0.7174,  Acc: 52.3174, AUC: 0.5274, Precision: 0.5808 -- Elapsed: 39m23s
2020-05-05 06:42:01.723388: Epoch: 1, Batch: 198, Loss: 0.6994, Elapsed: 5m28s
2020-05-05 06:44:46.321394: Epoch: 1, Batch: 201, Loss: 0.7115, Elapsed: 6m50s
2020-05-05 06:49:05.256434: Epoch: 1, Batch: 199, Loss: 0.6766, Elapsed: 7m3s
2020-05-05 06:49:25.608677: Validation Test:  Loss: 0.7009,  Acc: 51.1387, AUC: 0.5176, Precision: 0.5617 -- Elapsed: 50m15s
2020-05-05 06:54:12.617418: Epoch: 1, Batch: 202, Loss: 0.7115, Elapsed: 9m26s
2020-05-05 06:56:03.955262: Epoch: 1, Batch: 200, Loss: 0.6811, Elapsed: 6m58s
Starting testing the validation set with 200 subgraphs!
2020-05-05 06:59:05.393216: Epoch: 1, Batch: 203, Loss: 0.6994, Elapsed: 4m52s
2020-05-05 07:02:17.351846: Epoch: 1, Batch: 101, Loss: 0.7073, Elapsed: 12m51s
2020-05-05 07:05:20.004385: Epoch: 1, Batch: 204, Loss: 0.7001, Elapsed: 6m14s
2020-05-05 07:14:05.559528: Epoch: 1, Batch: 102, Loss: 0.6985, Elapsed: 11m48s
2020-05-05 07:16:59.178111: Epoch: 1, Batch: 205, Loss: 0.7472, Elapsed: 11m39s
2020-05-05 07:21:40.604106: Epoch: 1, Batch: 103, Loss: 0.7067, Elapsed: 7m35s
2020-05-05 07:24:52.140805: Epoch: 1, Batch: 206, Loss: 0.7177, Elapsed: 7m52s
2020-05-05 07:31:00.439242: Epoch: 1, Batch: 104, Loss: 0.7072, Elapsed: 9m19s
2020-05-05 07:33:46.968856: Epoch: 1, Batch: 207, Loss: 0.6973, Elapsed: 8m54s
2020-05-05 07:38:22.158561: Validation Test:  Loss: 0.6821,  Acc: 57.4757, AUC: 0.5928, Precision: 0.6124 -- Elapsed: 42m18s
2020-05-05 07:41:03.529235: Epoch: 1, Batch: 208, Loss: 0.7103, Elapsed: 7m16s
2020-05-05 07:47:34.483498: Epoch: 1, Batch: 201, Loss: 0.6989, Elapsed: 9m12s
2020-05-05 07:50:36.047899: Epoch: 1, Batch: 105, Loss: 0.6992, Elapsed: 19m35s
2020-05-05 07:54:37.227857: Epoch: 1, Batch: 209, Loss: 0.6981, Elapsed: 13m33s
2020-05-05 07:56:37.421302: Epoch: 1, Batch: 202, Loss: 0.6778, Elapsed: 9m2s
2020-05-05 08:01:07.576191: Epoch: 1, Batch: 210, Loss: 0.7028, Elapsed: 6m30s
2020-05-05 08:06:32.548792: Epoch: 1, Batch: 203, Loss: 0.6898, Elapsed: 9m55s
2020-05-05 08:08:48.866012: Epoch: 1, Batch: 106, Loss: 0.7027, Elapsed: 18m12s
2020-05-05 08:08:57.819907: Epoch: 1, Batch: 211, Loss: 0.7372, Elapsed: 7m50s
2020-05-05 08:16:54.217245: Epoch: 1, Batch: 204, Loss: 0.6959, Elapsed: 10m21s
2020-05-05 08:17:53.412017: Epoch: 1, Batch: 212, Loss: 0.7080, Elapsed: 8m55s
2020-05-05 08:23:13.225154: Epoch: 1, Batch: 205, Loss: 0.6806, Elapsed: 6m18s
2020-05-05 08:24:46.597072: Epoch: 1, Batch: 213, Loss: 0.7025, Elapsed: 6m53s
2020-05-05 08:28:26.692386: Epoch: 1, Batch: 107, Loss: 0.7033, Elapsed: 19m37s
2020-05-05 08:30:06.898907: Epoch: 1, Batch: 214, Loss: 0.7436, Elapsed: 5m20s
2020-05-05 08:30:50.637301: Epoch: 1, Batch: 206, Loss: 0.6862, Elapsed: 7m37s
2020-05-05 08:36:47.990874: Epoch: 1, Batch: 108, Loss: 0.7076, Elapsed: 8m21s
2020-05-05 08:37:37.278938: Epoch: 1, Batch: 207, Loss: 0.6829, Elapsed: 6m46s
2020-05-05 08:37:56.088495: Epoch: 1, Batch: 215, Loss: 0.7217, Elapsed: 7m49s
2020-05-05 08:42:46.202258: Epoch: 1, Batch: 216, Loss: 0.7117, Elapsed: 4m50s
2020-05-05 08:42:48.834074: Epoch: 1, Batch: 208, Loss: 0.6927, Elapsed: 5m11s
2020-05-05 08:48:37.177057: Epoch: 1, Batch: 209, Loss: 0.6584, Elapsed: 5m48s
2020-05-05 08:50:45.706189: Epoch: 1, Batch: 109, Loss: 0.6960, Elapsed: 13m57s
2020-05-05 08:54:43.632247: Epoch: 1, Batch: 217, Loss: 0.7056, Elapsed: 11m57s
2020-05-05 08:59:36.857729: Epoch: 1, Batch: 210, Loss: 0.7167, Elapsed: 10m59s
2020-05-05 09:03:11.710739: Epoch: 1, Batch: 218, Loss: 0.7189, Elapsed: 8m28s
2020-05-05 09:05:49.702930: Epoch: 1, Batch: 110, Loss: 0.7094, Elapsed: 15m3s
2020-05-05 09:06:31.305195: Epoch: 1, Batch: 211, Loss: 0.6734, Elapsed: 6m54s
2020-05-05 09:07:54.876652: Epoch: 1, Batch: 219, Loss: 0.7335, Elapsed: 4m43s
2020-05-05 09:13:15.651692: Epoch: 1, Batch: 212, Loss: 0.6769, Elapsed: 6m44s
2020-05-05 09:15:50.685576: Epoch: 1, Batch: 220, Loss: 0.7121, Elapsed: 7m55s
2020-05-05 09:16:27.892563: Epoch: 1, Batch: 111, Loss: 0.6906, Elapsed: 10m38s
2020-05-05 09:20:18.081193: Epoch: 1, Batch: 213, Loss: 0.6829, Elapsed: 7m2s
2020-05-05 09:22:21.564911: Epoch: 1, Batch: 221, Loss: 0.7095, Elapsed: 6m30s
2020-05-05 09:25:01.653036: Epoch: 1, Batch: 112, Loss: 0.7077, Elapsed: 8m33s
2020-05-05 09:25:05.963854: Epoch: 1, Batch: 214, Loss: 0.6873, Elapsed: 4m47s
2020-05-05 09:29:12.131243: Epoch: 1, Batch: 222, Loss: 0.7166, Elapsed: 6m50s
2020-05-05 09:34:26.051197: Epoch: 1, Batch: 215, Loss: 0.6812, Elapsed: 9m20s
2020-05-05 09:34:36.487566: Epoch: 1, Batch: 113, Loss: 0.6940, Elapsed: 9m34s
2020-05-05 09:36:16.585883: Epoch: 1, Batch: 223, Loss: 0.7241, Elapsed: 7m4s
2020-05-05 09:38:58.157177: Epoch: 1, Batch: 216, Loss: 0.6973, Elapsed: 4m32s
2020-05-05 09:45:24.781097: Epoch: 1, Batch: 224, Loss: 0.7149, Elapsed: 9m8s
2020-05-05 09:45:29.803587: Epoch: 1, Batch: 217, Loss: 0.6798, Elapsed: 6m31s
2020-05-05 09:49:52.734838: Epoch: 1, Batch: 225, Loss: 0.6984, Elapsed: 4m27s
2020-05-05 09:51:20.268705: Epoch: 1, Batch: 218, Loss: 0.6756, Elapsed: 5m50s
2020-05-05 09:56:30.943681: Epoch: 1, Batch: 226, Loss: 0.7150, Elapsed: 6m38s
2020-05-05 09:59:14.120762: Epoch: 1, Batch: 114, Loss: 0.7040, Elapsed: 24m37s
2020-05-05 09:59:40.275521: Epoch: 1, Batch: 219, Loss: 0.6753, Elapsed: 8m19s
2020-05-05 10:02:25.343933: Epoch: 1, Batch: 227, Loss: 0.6993, Elapsed: 5m54s
2020-05-05 10:09:37.933841: Epoch: 1, Batch: 228, Loss: 0.7231, Elapsed: 7m12s
2020-05-05 10:09:54.906674: Epoch: 1, Batch: 220, Loss: 0.6938, Elapsed: 10m14s
2020-05-05 10:13:17.893111: Epoch: 1, Batch: 115, Loss: 0.7009, Elapsed: 14m3s
2020-05-05 10:15:12.802058: Epoch: 1, Batch: 221, Loss: 0.6644, Elapsed: 5m17s
2020-05-05 10:16:05.598296: Epoch: 1, Batch: 229, Loss: 0.7034, Elapsed: 6m27s
2020-05-05 10:23:56.490920: Epoch: 1, Batch: 222, Loss: 0.6722, Elapsed: 8m43s
2020-05-05 10:24:20.778052: Epoch: 1, Batch: 230, Loss: 0.7020, Elapsed: 8m15s
2020-05-05 10:24:30.024493: Epoch: 1, Batch: 116, Loss: 0.6938, Elapsed: 11m12s
2020-05-05 10:30:41.993981: Epoch: 1, Batch: 231, Loss: 0.7179, Elapsed: 6m21s
2020-05-05 10:31:54.783802: Epoch: 1, Batch: 223, Loss: 0.6881, Elapsed: 7m58s
2020-05-05 10:34:36.172425: Epoch: 1, Batch: 224, Loss: 0.7128, Elapsed: 2m41s
2020-05-05 10:36:17.213332: Epoch: 1, Batch: 117, Loss: 0.7004, Elapsed: 11m47s
2020-05-05 10:36:22.272725: Epoch: 1, Batch: 232, Loss: 0.7299, Elapsed: 5m40s
2020-05-05 10:42:20.893081: Epoch: 1, Batch: 225, Loss: 0.6822, Elapsed: 7m44s
2020-05-05 10:43:53.414593: Epoch: 1, Batch: 233, Loss: 0.7015, Elapsed: 7m31s
2020-05-05 10:50:46.100048: Epoch: 1, Batch: 118, Loss: 0.6941, Elapsed: 14m28s
2020-05-05 10:51:04.438088: Epoch: 1, Batch: 226, Loss: 0.6892, Elapsed: 8m43s
2020-05-05 10:53:18.895297: Epoch: 1, Batch: 234, Loss: 0.6966, Elapsed: 9m25s
2020-05-05 10:57:51.480624: Epoch: 1, Batch: 227, Loss: 0.6710, Elapsed: 6m47s
2020-05-05 11:03:04.088782: Epoch: 1, Batch: 119, Loss: 0.6960, Elapsed: 12m17s
2020-05-05 11:05:29.601728: Epoch: 1, Batch: 235, Loss: 0.7377, Elapsed: 12m10s
2020-05-05 11:08:14.034638: Epoch: 1, Batch: 228, Loss: 0.6743, Elapsed: 10m22s
2020-05-05 11:11:32.895143: Epoch: 1, Batch: 236, Loss: 0.7016, Elapsed: 6m3s
2020-05-05 11:13:06.881852: Epoch: 1, Batch: 120, Loss: 0.6982, Elapsed: 10m2s
2020-05-05 11:14:18.631047: Epoch: 1, Batch: 229, Loss: 0.6762, Elapsed: 6m4s
2020-05-05 11:18:52.207460: Epoch: 1, Batch: 230, Loss: 0.6843, Elapsed: 4m33s
2020-05-05 11:19:49.409432: Epoch: 1, Batch: 237, Loss: 0.7214, Elapsed: 8m16s
2020-05-05 11:24:40.485569: Epoch: 1, Batch: 231, Loss: 0.6811, Elapsed: 5m48s
2020-05-05 11:26:33.454710: Epoch: 1, Batch: 238, Loss: 0.7137, Elapsed: 6m44s
2020-05-05 11:28:59.321196: Epoch: 1, Batch: 121, Loss: 0.7116, Elapsed: 15m52s
2020-05-05 11:33:12.147617: Epoch: 1, Batch: 232, Loss: 0.6633, Elapsed: 8m31s
2020-05-05 11:33:53.993002: Epoch: 1, Batch: 239, Loss: 0.7240, Elapsed: 7m20s
2020-05-05 11:39:44.981199: Epoch: 1, Batch: 122, Loss: 0.7073, Elapsed: 10m45s
2020-05-05 11:39:56.853480: Epoch: 1, Batch: 233, Loss: 0.6746, Elapsed: 6m44s
2020-05-05 11:42:55.761745: Epoch: 1, Batch: 240, Loss: 0.7101, Elapsed: 9m1s
2020-05-05 11:44:41.685327: Epoch: 1, Batch: 234, Loss: 0.6769, Elapsed: 4m44s
2020-05-05 11:47:04.549069: Epoch: 1, Batch: 241, Loss: 0.6804, Elapsed: 4m8s
2020-05-05 11:51:05.534632: Epoch: 1, Batch: 123, Loss: 0.6915, Elapsed: 11m20s
2020-05-05 11:53:55.596720: Epoch: 1, Batch: 235, Loss: 0.6672, Elapsed: 9m13s
2020-05-05 11:56:21.372208: Epoch: 1, Batch: 242, Loss: 0.7312, Elapsed: 9m16s
2020-05-05 11:57:25.551400: Epoch: 1, Batch: 236, Loss: 0.6822, Elapsed: 3m29s
2020-05-05 12:00:58.953841: Epoch: 1, Batch: 124, Loss: 0.7018, Elapsed: 9m53s
2020-05-05 12:04:12.155044: Epoch: 1, Batch: 243, Loss: 0.7198, Elapsed: 7m50s
2020-05-05 12:06:31.304957: Epoch: 1, Batch: 237, Loss: 0.6811, Elapsed: 9m5s
2020-05-05 12:09:35.380535: Epoch: 1, Batch: 244, Loss: 0.7119, Elapsed: 5m23s
2020-05-05 12:11:34.455890: Epoch: 1, Batch: 125, Loss: 0.6828, Elapsed: 10m35s
2020-05-05 12:12:30.709652: Epoch: 1, Batch: 238, Loss: 0.6651, Elapsed: 5m59s
2020-05-05 12:17:22.163902: Epoch: 1, Batch: 245, Loss: 0.6886, Elapsed: 7m46s
2020-05-05 12:19:17.297729: Epoch: 1, Batch: 239, Loss: 0.6859, Elapsed: 6m46s
2020-05-05 12:24:38.931500: Epoch: 1, Batch: 240, Loss: 0.6685, Elapsed: 5m21s
2020-05-05 12:24:52.169387: Epoch: 1, Batch: 126, Loss: 0.6989, Elapsed: 13m17s
2020-05-05 12:25:33.887074: Epoch: 1, Batch: 246, Loss: 0.6967, Elapsed: 8m11s
2020-05-05 12:29:15.834666: Epoch: 1, Batch: 241, Loss: 0.6833, Elapsed: 4m36s
2020-05-05 12:31:30.659360: Epoch: 1, Batch: 247, Loss: 0.7142, Elapsed: 5m56s
2020-05-05 12:39:31.557311: Epoch: 1, Batch: 248, Loss: 0.7202, Elapsed: 8m0s
2020-05-05 12:43:00.195160: Epoch: 1, Batch: 242, Loss: 0.6921, Elapsed: 13m44s
2020-05-05 12:43:50.604287: Epoch: 1, Batch: 127, Loss: 0.7023, Elapsed: 18m58s
2020-05-05 12:45:10.813097: Epoch: 1, Batch: 249, Loss: 0.7097, Elapsed: 5m39s
2020-05-05 12:50:07.854356: Epoch: 1, Batch: 243, Loss: 0.6732, Elapsed: 7m7s
2020-05-05 12:55:25.459026: Epoch: 1, Batch: 244, Loss: 0.6860, Elapsed: 5m17s
2020-05-05 12:56:34.649982: Epoch: 1, Batch: 250, Loss: 0.7087, Elapsed: 11m23s
Starting testing the validation set with 200 subgraphs!
2020-05-05 13:01:22.929378: Epoch: 1, Batch: 245, Loss: 0.6917, Elapsed: 5m57s
2020-05-05 13:05:35.751056: Epoch: 1, Batch: 128, Loss: 0.6987, Elapsed: 21m45s
2020-05-05 13:09:16.095414: Epoch: 1, Batch: 246, Loss: 0.6825, Elapsed: 7m53s
2020-05-05 13:17:22.713362: Epoch: 1, Batch: 129, Loss: 0.7080, Elapsed: 11m46s
2020-05-05 13:21:38.644976: Epoch: 1, Batch: 247, Loss: 0.7009, Elapsed: 12m22s
2020-05-05 13:29:40.073596: Epoch: 1, Batch: 248, Loss: 0.6806, Elapsed: 8m1s
2020-05-05 13:31:17.287574: Epoch: 1, Batch: 130, Loss: 0.6988, Elapsed: 13m54s
2020-05-05 13:38:19.495690: Epoch: 1, Batch: 249, Loss: 0.6889, Elapsed: 8m39s
2020-05-05 13:38:29.702759: Validation Test:  Loss: 0.7090,  Acc: 52.9342, AUC: 0.5345, Precision: 0.5903 -- Elapsed: 41m55s
2020-05-05 13:43:21.218541: Epoch: 1, Batch: 251, Loss: 0.7169, Elapsed: 4m51s
2020-05-05 13:43:30.302588: Epoch: 1, Batch: 131, Loss: 0.7113, Elapsed: 12m13s
2020-05-05 13:44:05.082344: Epoch: 1, Batch: 250, Loss: 0.6669, Elapsed: 5m45s
Starting testing the validation set with 200 subgraphs!
2020-05-05 13:50:35.718589: Epoch: 1, Batch: 252, Loss: 0.6870, Elapsed: 7m14s
2020-05-05 13:56:01.239942: Epoch: 1, Batch: 253, Loss: 0.7279, Elapsed: 5m25s
2020-05-05 13:58:00.629723: Epoch: 1, Batch: 132, Loss: 0.6910, Elapsed: 14m30s
2020-05-05 14:03:35.068888: Epoch: 1, Batch: 254, Loss: 0.7204, Elapsed: 7m33s
2020-05-05 14:09:00.625941: Epoch: 1, Batch: 255, Loss: 0.7093, Elapsed: 5m25s
2020-05-05 14:15:21.969109: Epoch: 1, Batch: 256, Loss: 0.7187, Elapsed: 6m21s
2020-05-05 14:17:48.542723: Epoch: 1, Batch: 133, Loss: 0.7119, Elapsed: 19m47s
2020-05-05 14:21:29.240927: Epoch: 1, Batch: 257, Loss: 0.7251, Elapsed: 6m7s
2020-05-05 14:26:21.053207: Validation Test:  Loss: 0.6813,  Acc: 57.7699, AUC: 0.5943, Precision: 0.6093 -- Elapsed: 42m15s
2020-05-05 14:27:00.797240: Epoch: 1, Batch: 134, Loss: 0.7205, Elapsed: 9m12s
2020-05-05 14:27:37.273127: Epoch: 1, Batch: 258, Loss: 0.7521, Elapsed: 6m8s
2020-05-05 14:32:04.726520: Epoch: 1, Batch: 251, Loss: 0.6923, Elapsed: 5m43s
2020-05-05 14:34:08.215994: Epoch: 1, Batch: 259, Loss: 0.7130, Elapsed: 6m30s
2020-05-05 14:38:00.053428: Epoch: 1, Batch: 252, Loss: 0.6884, Elapsed: 5m55s
2020-05-05 14:38:04.226484: Epoch: 1, Batch: 135, Loss: 0.7078, Elapsed: 11m3s
2020-05-05 14:40:17.077376: Epoch: 1, Batch: 260, Loss: 0.6961, Elapsed: 6m8s
2020-05-05 14:47:21.140247: Epoch: 1, Batch: 261, Loss: 0.7014, Elapsed: 7m4s
2020-05-05 14:47:47.678471: Epoch: 1, Batch: 136, Loss: 0.6946, Elapsed: 9m43s
2020-05-05 14:48:45.344350: Epoch: 1, Batch: 253, Loss: 0.6666, Elapsed: 10m45s
2020-05-05 14:54:00.080334: Epoch: 1, Batch: 262, Loss: 0.7199, Elapsed: 6m38s
2020-05-05 14:56:54.604531: Epoch: 1, Batch: 254, Loss: 0.6741, Elapsed: 8m9s
2020-05-05 15:02:15.273400: Epoch: 1, Batch: 263, Loss: 0.6978, Elapsed: 8m15s
2020-05-05 15:03:12.147008: Epoch: 1, Batch: 255, Loss: 0.6627, Elapsed: 6m17s
2020-05-05 15:06:54.618995: Epoch: 1, Batch: 264, Loss: 0.7127, Elapsed: 4m39s
2020-05-05 15:08:22.065047: Epoch: 1, Batch: 137, Loss: 0.7039, Elapsed: 20m34s
2020-05-05 15:12:12.128323: Epoch: 1, Batch: 265, Loss: 0.7032, Elapsed: 5m17s
2020-05-05 15:12:28.768226: Epoch: 1, Batch: 256, Loss: 0.6997, Elapsed: 9m16s
2020-05-05 15:20:21.322239: Epoch: 1, Batch: 266, Loss: 0.7097, Elapsed: 8m9s
2020-05-05 15:20:28.261776: Epoch: 1, Batch: 138, Loss: 0.6936, Elapsed: 12m6s
2020-05-05 15:21:26.979070: Epoch: 1, Batch: 257, Loss: 0.6933, Elapsed: 8m58s
2020-05-05 15:28:16.158474: Epoch: 1, Batch: 267, Loss: 0.6901, Elapsed: 7m54s
2020-05-05 15:29:59.865575: Epoch: 1, Batch: 258, Loss: 0.6820, Elapsed: 8m32s
2020-05-05 15:34:22.476779: Epoch: 1, Batch: 259, Loss: 0.6563, Elapsed: 4m22s
2020-05-05 15:34:34.175740: Epoch: 1, Batch: 139, Loss: 0.7115, Elapsed: 14m5s
2020-05-05 15:35:27.040309: Epoch: 1, Batch: 268, Loss: 0.7148, Elapsed: 7m10s
2020-05-05 15:39:17.354655: Epoch: 1, Batch: 269, Loss: 0.7446, Elapsed: 3m50s
2020-05-05 15:39:40.807610: Epoch: 1, Batch: 260, Loss: 0.6833, Elapsed: 5m18s
2020-05-05 15:46:27.403794: Epoch: 1, Batch: 261, Loss: 0.6813, Elapsed: 6m46s
2020-05-05 15:47:21.313295: Epoch: 1, Batch: 140, Loss: 0.6880, Elapsed: 12m47s
2020-05-05 15:48:39.200742: Epoch: 1, Batch: 270, Loss: 0.6973, Elapsed: 9m21s
2020-05-05 15:53:41.918968: Epoch: 1, Batch: 262, Loss: 0.6760, Elapsed: 7m14s
2020-05-05 15:53:49.072835: Epoch: 1, Batch: 271, Loss: 0.7138, Elapsed: 5m9s
2020-05-05 15:58:54.043322: Epoch: 1, Batch: 272, Loss: 0.7336, Elapsed: 5m4s
2020-05-05 16:00:37.855397: Epoch: 1, Batch: 141, Loss: 0.6949, Elapsed: 13m16s
2020-05-05 16:03:53.263492: Epoch: 1, Batch: 263, Loss: 0.6940, Elapsed: 10m11s
2020-05-05 16:07:42.040294: Epoch: 1, Batch: 273, Loss: 0.7196, Elapsed: 8m47s
2020-05-05 16:08:21.434110: Epoch: 1, Batch: 264, Loss: 0.6689, Elapsed: 4m28s
2020-05-05 16:10:38.283571: Epoch: 1, Batch: 142, Loss: 0.6901, Elapsed: 10m0s
2020-05-05 16:11:35.688466: Epoch: 1, Batch: 274, Loss: 0.7423, Elapsed: 3m53s
2020-05-05 16:17:16.506221: Epoch: 1, Batch: 265, Loss: 0.6942, Elapsed: 8m55s
2020-05-05 16:17:50.968487: Epoch: 1, Batch: 275, Loss: 0.6940, Elapsed: 6m15s
2020-05-05 16:24:06.476723: Epoch: 1, Batch: 266, Loss: 0.6893, Elapsed: 6m49s
2020-05-05 16:24:08.737585: Epoch: 1, Batch: 143, Loss: 0.7058, Elapsed: 13m30s
2020-05-05 16:24:20.361224: Epoch: 1, Batch: 276, Loss: 0.7191, Elapsed: 6m29s
2020-05-05 16:28:45.297867: Epoch: 1, Batch: 267, Loss: 0.6916, Elapsed: 4m38s
2020-05-05 16:33:24.300066: Epoch: 1, Batch: 268, Loss: 0.6585, Elapsed: 4m38s
2020-05-05 16:36:31.172279: Epoch: 1, Batch: 144, Loss: 0.6843, Elapsed: 12m22s
2020-05-05 16:37:46.554854: Epoch: 1, Batch: 277, Loss: 0.6980, Elapsed: 13m26s
2020-05-05 16:40:22.860358: Epoch: 1, Batch: 269, Loss: 0.6723, Elapsed: 6m58s
2020-05-05 16:44:45.877526: Epoch: 1, Batch: 278, Loss: 0.7126, Elapsed: 6m59s
2020-05-05 16:49:33.745840: Epoch: 1, Batch: 270, Loss: 0.6748, Elapsed: 9m10s
2020-05-05 16:50:26.986609: Epoch: 1, Batch: 145, Loss: 0.6977, Elapsed: 13m55s
2020-05-05 16:52:17.758923: Epoch: 1, Batch: 279, Loss: 0.7051, Elapsed: 7m31s
2020-05-05 16:56:56.778073: Epoch: 1, Batch: 280, Loss: 0.7036, Elapsed: 4m38s
2020-05-05 16:58:35.275767: Epoch: 1, Batch: 271, Loss: 0.6776, Elapsed: 9m1s
2020-05-05 17:02:40.174155: Epoch: 1, Batch: 281, Loss: 0.7259, Elapsed: 5m43s
2020-05-05 17:07:55.942966: Epoch: 1, Batch: 272, Loss: 0.6713, Elapsed: 9m20s
2020-05-05 17:12:12.475993: Epoch: 1, Batch: 282, Loss: 0.7129, Elapsed: 9m32s
2020-05-05 17:12:46.463968: Epoch: 1, Batch: 146, Loss: 0.6863, Elapsed: 22m19s
2020-05-05 17:14:29.609651: Epoch: 1, Batch: 273, Loss: 0.6813, Elapsed: 6m33s
2020-05-05 17:19:56.045742: Epoch: 1, Batch: 283, Loss: 0.7142, Elapsed: 7m43s
2020-05-05 17:20:46.062056: Epoch: 1, Batch: 274, Loss: 0.6823, Elapsed: 6m16s
2020-05-05 17:22:25.509184: Epoch: 1, Batch: 147, Loss: 0.7090, Elapsed: 9m39s
2020-05-05 17:26:05.503079: Epoch: 1, Batch: 275, Loss: 0.6606, Elapsed: 5m19s
2020-05-05 17:26:35.166058: Epoch: 1, Batch: 284, Loss: 0.7207, Elapsed: 6m39s
2020-05-05 17:35:03.271597: Epoch: 1, Batch: 148, Loss: 0.7016, Elapsed: 12m37s
2020-05-05 17:35:13.066593: Epoch: 1, Batch: 276, Loss: 0.6798, Elapsed: 9m7s
2020-05-05 17:35:45.811892: Epoch: 1, Batch: 285, Loss: 0.7016, Elapsed: 9m10s
2020-05-05 17:41:25.298535: Epoch: 1, Batch: 277, Loss: 0.6643, Elapsed: 6m12s
2020-05-05 17:41:26.788397: Epoch: 1, Batch: 286, Loss: 0.7220, Elapsed: 5m40s
2020-05-05 17:45:01.017169: Epoch: 1, Batch: 149, Loss: 0.7032, Elapsed: 9m57s
2020-05-05 17:47:46.725106: Epoch: 1, Batch: 287, Loss: 0.7005, Elapsed: 6m19s
2020-05-05 17:48:28.744549: Epoch: 1, Batch: 278, Loss: 0.6624, Elapsed: 7m3s
2020-05-05 17:52:32.162733: Epoch: 1, Batch: 288, Loss: 0.7072, Elapsed: 4m45s
2020-05-05 17:53:40.185280: Epoch: 1, Batch: 150, Loss: 0.6998, Elapsed: 8m39s
Starting testing the validation set with 200 subgraphs!
2020-05-05 17:55:05.257117: Epoch: 1, Batch: 279, Loss: 0.6732, Elapsed: 6m36s
2020-05-05 17:56:46.757912: Epoch: 1, Batch: 289, Loss: 0.7068, Elapsed: 4m14s
2020-05-05 18:02:50.251192: Epoch: 1, Batch: 280, Loss: 0.6821, Elapsed: 7m44s
2020-05-05 18:03:06.379202: Epoch: 1, Batch: 290, Loss: 0.6944, Elapsed: 6m19s
2020-05-05 18:08:52.846245: Epoch: 1, Batch: 291, Loss: 0.7092, Elapsed: 5m46s
2020-05-05 18:10:09.826363: Epoch: 1, Batch: 281, Loss: 0.6804, Elapsed: 7m19s
2020-05-05 18:13:17.148646: Epoch: 1, Batch: 292, Loss: 0.7046, Elapsed: 4m24s
2020-05-05 18:19:12.335814: Epoch: 1, Batch: 282, Loss: 0.6760, Elapsed: 9m2s
2020-05-05 18:24:48.467634: Epoch: 1, Batch: 293, Loss: 0.7052, Elapsed: 11m31s
2020-05-05 18:25:00.096101: Epoch: 1, Batch: 283, Loss: 0.6667, Elapsed: 5m47s
2020-05-05 18:28:51.381261: Epoch: 1, Batch: 284, Loss: 0.6813, Elapsed: 3m51s
2020-05-05 18:29:41.698414: Epoch: 1, Batch: 294, Loss: 0.6968, Elapsed: 4m53s
2020-05-05 18:36:26.295092: Epoch: 1, Batch: 285, Loss: 0.6683, Elapsed: 7m34s
2020-05-05 18:37:12.441738: Epoch: 1, Batch: 295, Loss: 0.7117, Elapsed: 7m30s
2020-05-05 18:42:07.118321: Epoch: 1, Batch: 286, Loss: 0.6815, Elapsed: 5m40s
2020-05-05 18:45:36.922243: Epoch: 1, Batch: 296, Loss: 0.7170, Elapsed: 8m24s
2020-05-05 18:46:50.088135: Epoch: 1, Batch: 287, Loss: 0.6867, Elapsed: 4m42s
2020-05-05 18:47:35.728864: Validation Test:  Loss: 0.7073,  Acc: 51.0443, AUC: 0.5073, Precision: 0.5475 -- Elapsed: 53m55s
2020-05-05 18:51:47.688903: Epoch: 1, Batch: 288, Loss: 0.6641, Elapsed: 4m57s
2020-05-05 18:52:12.645045: Epoch: 1, Batch: 297, Loss: 0.7320, Elapsed: 6m35s
2020-05-05 18:59:55.456468: Epoch: 1, Batch: 289, Loss: 0.6677, Elapsed: 8m7s
2020-05-05 19:00:01.605517: Epoch: 1, Batch: 298, Loss: 0.6959, Elapsed: 7m48s
2020-05-05 19:05:23.933795: Epoch: 1, Batch: 151, Loss: 0.7128, Elapsed: 17m48s
2020-05-05 19:05:55.005466: Epoch: 1, Batch: 290, Loss: 0.6732, Elapsed: 5m59s
2020-05-05 19:09:00.393917: Epoch: 1, Batch: 299, Loss: 0.6941, Elapsed: 8m58s
2020-05-05 19:10:52.915296: Epoch: 1, Batch: 291, Loss: 0.6980, Elapsed: 4m57s
2020-05-05 19:18:06.558194: Epoch: 1, Batch: 300, Loss: 0.7033, Elapsed: 9m6s
Starting testing the validation set with 200 subgraphs!
2020-05-05 19:18:50.218056: Epoch: 1, Batch: 152, Loss: 0.6862, Elapsed: 13m26s
2020-05-05 19:19:29.545378: Epoch: 1, Batch: 292, Loss: 0.6872, Elapsed: 8m36s
2020-05-05 19:24:54.684221: Epoch: 1, Batch: 293, Loss: 0.7005, Elapsed: 5m25s
2020-05-05 19:27:56.612293: Epoch: 1, Batch: 153, Loss: 0.6994, Elapsed: 9m6s
2020-05-05 19:30:24.675379: Epoch: 1, Batch: 294, Loss: 0.7045, Elapsed: 5m29s
2020-05-05 19:34:52.589799: Epoch: 1, Batch: 295, Loss: 0.7025, Elapsed: 4m27s
2020-05-05 19:39:23.595810: Epoch: 1, Batch: 154, Loss: 0.7090, Elapsed: 11m26s
2020-05-05 19:44:08.244193: Epoch: 1, Batch: 296, Loss: 0.6899, Elapsed: 9m15s
2020-05-05 19:47:43.695546: Epoch: 1, Batch: 297, Loss: 0.6969, Elapsed: 3m35s
2020-05-05 19:51:58.354906: Epoch: 1, Batch: 155, Loss: 0.7106, Elapsed: 12m34s
2020-05-05 19:54:58.481291: Epoch: 1, Batch: 298, Loss: 0.6749, Elapsed: 7m14s
2020-05-05 19:59:19.994301: Epoch: 1, Batch: 299, Loss: 0.6651, Elapsed: 4m21s
2020-05-05 20:00:07.479300: Validation Test:  Loss: 0.7046,  Acc: 53.7244, AUC: 0.5463, Precision: 0.5982 -- Elapsed: 42m0s
2020-05-05 20:00:20.477154: Epoch: 1, Batch: 156, Loss: 0.7009, Elapsed: 8m22s
2020-05-05 20:04:02.633906: Epoch: 1, Batch: 300, Loss: 0.6936, Elapsed: 4m42s
Starting testing the validation set with 200 subgraphs!
2020-05-05 20:06:28.815094: Epoch: 1, Batch: 301, Loss: 0.7038, Elapsed: 6m21s
2020-05-05 20:08:33.095895: Epoch: 1, Batch: 157, Loss: 0.7028, Elapsed: 8m12s
2020-05-05 20:10:04.542357: Epoch: 1, Batch: 302, Loss: 0.7274, Elapsed: 3m35s
2020-05-05 20:15:47.078333: Epoch: 1, Batch: 303, Loss: 0.7178, Elapsed: 5m42s
2020-05-05 20:18:23.842307: Epoch: 1, Batch: 158, Loss: 0.7230, Elapsed: 9m50s
2020-05-05 20:23:49.675515: Epoch: 1, Batch: 304, Loss: 0.7474, Elapsed: 8m2s
2020-05-05 20:24:54.205893: Epoch: 1, Batch: 159, Loss: 0.6899, Elapsed: 6m30s
2020-05-05 20:28:25.621243: Epoch: 1, Batch: 305, Loss: 0.6994, Elapsed: 4m35s
2020-05-05 20:32:17.479360: Epoch: 1, Batch: 306, Loss: 0.7036, Elapsed: 3m51s
2020-05-05 20:35:00.421016: Epoch: 1, Batch: 160, Loss: 0.6987, Elapsed: 10m6s
2020-05-05 20:39:46.141875: Epoch: 1, Batch: 307, Loss: 0.6976, Elapsed: 7m28s
2020-05-05 20:43:36.707783: Epoch: 1, Batch: 161, Loss: 0.7126, Elapsed: 8m36s
2020-05-05 20:46:09.809037: Validation Test:  Loss: 0.6825,  Acc: 58.6291, AUC: 0.6089, Precision: 0.6344 -- Elapsed: 42m7s
2020-05-05 20:50:36.797080: Epoch: 1, Batch: 308, Loss: 0.7381, Elapsed: 10m50s
2020-05-05 20:52:30.417499: Epoch: 1, Batch: 301, Loss: 0.6891, Elapsed: 6m20s
2020-05-05 20:53:20.537138: Epoch: 1, Batch: 162, Loss: 0.7182, Elapsed: 9m43s
2020-05-05 20:56:36.695563: Epoch: 1, Batch: 309, Loss: 0.7170, Elapsed: 5m59s
2020-05-05 21:01:19.327802: Epoch: 1, Batch: 310, Loss: 0.7058, Elapsed: 4m42s
2020-05-05 21:02:05.820598: Epoch: 1, Batch: 302, Loss: 0.6914, Elapsed: 9m35s
2020-05-05 21:06:04.639714: Epoch: 1, Batch: 311, Loss: 0.7066, Elapsed: 4m45s
2020-05-05 21:09:13.065344: Epoch: 1, Batch: 163, Loss: 0.7082, Elapsed: 15m52s
2020-05-05 21:10:20.563904: Epoch: 1, Batch: 303, Loss: 0.7018, Elapsed: 8m14s
2020-05-05 21:10:27.269775: Epoch: 1, Batch: 312, Loss: 0.6886, Elapsed: 4m22s
2020-05-05 21:15:41.354957: Epoch: 1, Batch: 313, Loss: 0.6981, Elapsed: 5m14s
2020-05-05 21:15:42.229189: Epoch: 1, Batch: 304, Loss: 0.6713, Elapsed: 5m21s
2020-05-05 21:20:14.326183: Epoch: 1, Batch: 164, Loss: 0.7056, Elapsed: 11m1s
2020-05-05 21:21:12.983161: Epoch: 1, Batch: 305, Loss: 0.6833, Elapsed: 5m30s
2020-05-05 21:24:56.644161: Epoch: 1, Batch: 314, Loss: 0.6941, Elapsed: 9m15s
2020-05-05 21:28:00.882119: Epoch: 1, Batch: 306, Loss: 0.6875, Elapsed: 6m47s
2020-05-05 21:34:46.204054: Epoch: 1, Batch: 315, Loss: 0.7020, Elapsed: 9m49s
2020-05-05 21:34:42.881598: Epoch: 1, Batch: 307, Loss: 0.6823, Elapsed: 6m41s
2020-05-05 21:37:02.803760: Epoch: 1, Batch: 165, Loss: 0.7017, Elapsed: 16m48s
2020-05-05 21:40:31.815903: Epoch: 1, Batch: 308, Loss: 0.6684, Elapsed: 5m48s
2020-05-05 21:47:17.547399: Epoch: 1, Batch: 316, Loss: 0.6911, Elapsed: 12m31s
2020-05-05 21:50:28.903228: Epoch: 1, Batch: 309, Loss: 0.6859, Elapsed: 9m57s
2020-05-05 21:52:41.312163: Epoch: 1, Batch: 317, Loss: 0.7338, Elapsed: 5m23s
2020-05-05 21:56:42.130835: Epoch: 1, Batch: 166, Loss: 0.6967, Elapsed: 19m39s
2020-05-05 22:00:12.742265: Epoch: 1, Batch: 318, Loss: 0.7118, Elapsed: 7m31s
2020-05-05 22:00:28.587493: Epoch: 1, Batch: 310, Loss: 0.7060, Elapsed: 9m59s
2020-05-05 22:05:40.771750: Epoch: 1, Batch: 319, Loss: 0.6800, Elapsed: 5m28s
2020-05-05 22:06:12.394226: Epoch: 1, Batch: 311, Loss: 0.6869, Elapsed: 5m43s
2020-05-05 22:10:08.024298: Epoch: 1, Batch: 312, Loss: 0.6754, Elapsed: 3m55s
2020-05-05 22:14:10.543992: Epoch: 1, Batch: 320, Loss: 0.6857, Elapsed: 8m29s
2020-05-05 22:19:13.612243: Epoch: 1, Batch: 321, Loss: 0.6941, Elapsed: 5m3s
2020-05-05 22:19:43.481512: Epoch: 1, Batch: 167, Loss: 0.7092, Elapsed: 23m1s
2020-05-05 22:19:59.515093: Epoch: 1, Batch: 313, Loss: 0.7005, Elapsed: 9m51s
2020-05-05 22:23:27.615686: Epoch: 1, Batch: 322, Loss: 0.6933, Elapsed: 4m13s
2020-05-05 22:28:49.617934: Epoch: 1, Batch: 323, Loss: 0.7208, Elapsed: 5m21s
2020-05-05 22:32:48.938157: Epoch: 1, Batch: 314, Loss: 0.6980, Elapsed: 12m49s
2020-05-05 22:34:21.154894: Epoch: 1, Batch: 324, Loss: 0.7023, Elapsed: 5m31s
2020-05-05 22:36:06.130135: Epoch: 1, Batch: 168, Loss: 0.7230, Elapsed: 16m22s
2020-05-05 22:36:51.365948: Epoch: 1, Batch: 315, Loss: 0.6897, Elapsed: 4m2s
2020-05-05 22:39:46.832573: Epoch: 1, Batch: 325, Loss: 0.7161, Elapsed: 5m25s
2020-05-05 22:43:49.445404: Epoch: 1, Batch: 316, Loss: 0.6937, Elapsed: 6m58s
2020-05-05 22:47:05.628033: Epoch: 1, Batch: 169, Loss: 0.6854, Elapsed: 10m59s
2020-05-05 22:47:54.647903: Epoch: 1, Batch: 326, Loss: 0.6895, Elapsed: 8m7s
2020-05-05 22:52:52.070459: Epoch: 1, Batch: 317, Loss: 0.6815, Elapsed: 9m2s
2020-05-05 22:56:53.995657: Epoch: 1, Batch: 327, Loss: 0.7267, Elapsed: 8m59s
2020-05-05 22:59:02.410008: Epoch: 1, Batch: 318, Loss: 0.6848, Elapsed: 6m10s
2020-05-05 23:03:18.797926: Epoch: 1, Batch: 328, Loss: 0.6892, Elapsed: 6m24s
2020-05-05 23:04:12.615020: Epoch: 1, Batch: 170, Loss: 0.6886, Elapsed: 17m6s
2020-05-05 23:10:10.503138: Epoch: 1, Batch: 329, Loss: 0.7058, Elapsed: 6m51s
2020-05-05 23:10:27.057330: Epoch: 1, Batch: 319, Loss: 0.6624, Elapsed: 11m24s
2020-05-05 23:11:00.257509: Epoch: 1, Batch: 171, Loss: 0.6948, Elapsed: 6m47s
2020-05-05 23:16:45.343975: Epoch: 1, Batch: 320, Loss: 0.6978, Elapsed: 6m18s
2020-05-05 23:18:45.707205: Epoch: 1, Batch: 330, Loss: 0.7062, Elapsed: 8m35s
2020-05-05 23:22:15.153894: Epoch: 1, Batch: 321, Loss: 0.6825, Elapsed: 5m29s
2020-05-05 23:25:44.922086: Epoch: 1, Batch: 331, Loss: 0.6985, Elapsed: 6m59s
2020-05-05 23:27:13.984343: Epoch: 1, Batch: 172, Loss: 0.7011, Elapsed: 16m13s
2020-05-05 23:28:49.646543: Epoch: 1, Batch: 322, Loss: 0.6813, Elapsed: 6m34s
2020-05-05 23:36:37.715801: Epoch: 1, Batch: 332, Loss: 0.7153, Elapsed: 10m52s
2020-05-05 23:36:50.655644: Epoch: 1, Batch: 323, Loss: 0.6654, Elapsed: 8m0s
2020-05-05 23:40:45.723708: Epoch: 1, Batch: 173, Loss: 0.7003, Elapsed: 13m31s
2020-05-05 23:43:00.297779: Epoch: 1, Batch: 333, Loss: 0.7058, Elapsed: 6m22s
2020-05-05 23:45:54.773202: Epoch: 1, Batch: 324, Loss: 0.6916, Elapsed: 9m4s
2020-05-05 23:48:17.290531: Epoch: 1, Batch: 174, Loss: 0.7011, Elapsed: 7m31s
2020-05-05 23:49:04.885364: Epoch: 1, Batch: 334, Loss: 0.7346, Elapsed: 6m4s
2020-05-05 23:50:56.292703: Epoch: 1, Batch: 325, Loss: 0.6807, Elapsed: 5m1s
2020-05-05 23:56:43.538312: Epoch: 1, Batch: 326, Loss: 0.6979, Elapsed: 5m47s
2020-05-06 00:01:31.769170: Epoch: 1, Batch: 327, Loss: 0.6959, Elapsed: 4m48s
2020-05-06 00:04:21.729689: Epoch: 1, Batch: 335, Loss: 0.7503, Elapsed: 15m16s
2020-05-06 00:06:44.533714: Epoch: 1, Batch: 175, Loss: 0.6980, Elapsed: 18m27s
2020-05-06 00:07:23.274157: Epoch: 1, Batch: 328, Loss: 0.6816, Elapsed: 5m51s
2020-05-06 00:12:37.663552: Epoch: 1, Batch: 336, Loss: 0.7010, Elapsed: 8m15s
2020-05-06 00:16:07.135131: Epoch: 1, Batch: 329, Loss: 0.6969, Elapsed: 8m43s
2020-05-06 00:17:29.734636: Epoch: 1, Batch: 176, Loss: 0.7110, Elapsed: 10m45s
2020-05-06 00:22:50.263220: Epoch: 1, Batch: 337, Loss: 0.6902, Elapsed: 10m12s
2020-05-06 00:23:02.055568: Epoch: 1, Batch: 330, Loss: 0.6992, Elapsed: 6m54s
2020-05-06 00:27:58.411156: Epoch: 1, Batch: 338, Loss: 0.6969, Elapsed: 5m8s
2020-05-06 00:28:57.719253: Epoch: 1, Batch: 331, Loss: 0.6774, Elapsed: 5m55s
2020-05-06 00:33:49.093537: Epoch: 1, Batch: 177, Loss: 0.6992, Elapsed: 16m19s
2020-05-06 00:35:42.819411: Epoch: 1, Batch: 332, Loss: 0.6731, Elapsed: 6m45s
2020-05-06 00:36:33.432734: Epoch: 1, Batch: 339, Loss: 0.6968, Elapsed: 8m35s
2020-05-06 00:41:42.523169: Epoch: 1, Batch: 333, Loss: 0.6859, Elapsed: 5m59s
2020-05-06 00:46:32.492677: Epoch: 1, Batch: 340, Loss: 0.7110, Elapsed: 9m59s
2020-05-06 00:47:23.314081: Epoch: 1, Batch: 178, Loss: 0.7157, Elapsed: 13m34s
2020-05-06 00:48:25.333246: Epoch: 1, Batch: 334, Loss: 0.6791, Elapsed: 6m42s
2020-05-06 00:56:07.320599: Epoch: 1, Batch: 341, Loss: 0.7430, Elapsed: 9m34s
2020-05-06 00:56:31.789207: Epoch: 1, Batch: 335, Loss: 0.6991, Elapsed: 8m6s
2020-05-06 01:01:16.082355: Epoch: 1, Batch: 342, Loss: 0.7265, Elapsed: 5m8s
2020-05-06 01:04:25.768605: Epoch: 1, Batch: 336, Loss: 0.6750, Elapsed: 7m53s
2020-05-06 01:05:03.028200: Epoch: 1, Batch: 179, Loss: 0.7068, Elapsed: 17m39s
2020-05-06 01:06:38.959839: Epoch: 1, Batch: 343, Loss: 0.6929, Elapsed: 5m22s
2020-05-06 01:11:37.334763: Epoch: 1, Batch: 337, Loss: 0.6697, Elapsed: 7m11s
2020-05-06 01:14:38.538979: Epoch: 1, Batch: 344, Loss: 0.7034, Elapsed: 7m59s
2020-05-06 01:19:14.566592: Epoch: 1, Batch: 345, Loss: 0.7215, Elapsed: 4m36s
2020-05-06 01:21:20.795812: Epoch: 1, Batch: 338, Loss: 0.6857, Elapsed: 9m43s
2020-05-06 01:32:51.392925: Epoch: 1, Batch: 346, Loss: 0.7286, Elapsed: 13m36s
2020-05-06 01:34:26.417277: Epoch: 1, Batch: 339, Loss: 0.6733, Elapsed: 13m5s
2020-05-06 01:39:10.665184: Epoch: 1, Batch: 347, Loss: 0.7334, Elapsed: 6m19s
2020-05-06 01:45:53.527628: Epoch: 1, Batch: 340, Loss: 0.6841, Elapsed: 11m27s
2020-05-06 01:47:27.832244: Epoch: 1, Batch: 348, Loss: 0.6977, Elapsed: 8m17s
2020-05-06 01:51:26.362794: Epoch: 1, Batch: 349, Loss: 0.7105, Elapsed: 3m58s
2020-05-06 01:53:03.932759: Epoch: 1, Batch: 341, Loss: 0.6796, Elapsed: 7m10s
2020-05-06 01:54:49.756867: Epoch: 1, Batch: 180, Loss: 0.7147, Elapsed: 49m46s
2020-05-06 02:00:21.380242: Epoch: 1, Batch: 342, Loss: 0.6822, Elapsed: 7m17s
2020-05-06 02:02:00.641465: Epoch: 1, Batch: 350, Loss: 0.7044, Elapsed: 10m34s
Starting testing the validation set with 200 subgraphs!
2020-05-06 02:04:58.901015: Epoch: 1, Batch: 181, Loss: 0.7093, Elapsed: 10m9s
2020-05-06 02:07:34.539362: Epoch: 1, Batch: 343, Loss: 0.6646, Elapsed: 7m13s
2020-05-06 02:12:07.600945: Epoch: 1, Batch: 344, Loss: 0.6702, Elapsed: 4m33s
2020-05-06 02:15:54.108774: Epoch: 1, Batch: 345, Loss: 0.6965, Elapsed: 3m46s
2020-05-06 02:17:13.979935: Epoch: 1, Batch: 182, Loss: 0.7546, Elapsed: 12m15s
2020-05-06 02:22:33.944648: Epoch: 1, Batch: 346, Loss: 0.6661, Elapsed: 6m39s
2020-05-06 02:23:27.918851: Epoch: 1, Batch: 183, Loss: 0.6798, Elapsed: 6m13s
2020-05-06 02:25:51.534503: Epoch: 1, Batch: 347, Loss: 0.6582, Elapsed: 3m17s
2020-05-06 02:31:00.406355: Epoch: 1, Batch: 348, Loss: 0.6702, Elapsed: 5m8s
2020-05-06 02:34:07.307457: Epoch: 1, Batch: 184, Loss: 0.6863, Elapsed: 10m39s
2020-05-06 02:37:34.932409: Epoch: 1, Batch: 349, Loss: 0.6728, Elapsed: 6m34s
2020-05-06 02:43:28.277229: Epoch: 1, Batch: 185, Loss: 0.7133, Elapsed: 9m20s
2020-05-06 02:44:03.643523: Validation Test:  Loss: 0.7100,  Acc: 53.8553, AUC: 0.5481, Precision: 0.5985 -- Elapsed: 42m2s
2020-05-06 02:45:27.308462: Epoch: 1, Batch: 350, Loss: 0.6886, Elapsed: 7m52s
Starting testing the validation set with 200 subgraphs!
2020-05-06 02:52:14.162554: Epoch: 1, Batch: 186, Loss: 0.7300, Elapsed: 8m45s
2020-05-06 02:52:37.230193: Epoch: 1, Batch: 351, Loss: 0.7112, Elapsed: 8m33s
2020-05-06 02:59:54.286956: Epoch: 1, Batch: 352, Loss: 0.7056, Elapsed: 7m17s
2020-05-06 03:02:35.209845: Epoch: 1, Batch: 187, Loss: 0.7130, Elapsed: 10m21s
2020-05-06 03:06:27.138322: Epoch: 1, Batch: 353, Loss: 0.7016, Elapsed: 6m32s
2020-05-06 03:11:23.075547: Epoch: 1, Batch: 188, Loss: 0.7073, Elapsed: 8m47s
2020-05-06 03:12:49.745439: Epoch: 1, Batch: 354, Loss: 0.7263, Elapsed: 6m22s
2020-05-06 03:19:44.913599: Epoch: 1, Batch: 355, Loss: 0.7118, Elapsed: 6m55s
2020-05-06 03:23:17.112524: Epoch: 1, Batch: 189, Loss: 0.7181, Elapsed: 11m54s
2020-05-06 03:25:41.628562: Epoch: 1, Batch: 356, Loss: 0.7251, Elapsed: 5m56s
2020-05-06 03:27:48.903440: Validation Test:  Loss: 0.6793,  Acc: 58.2852, AUC: 0.5989, Precision: 0.6256 -- Elapsed: 42m21s
2020-05-06 03:31:52.327718: Epoch: 1, Batch: 357, Loss: 0.7099, Elapsed: 6m10s
2020-05-06 03:33:46.885757: Epoch: 1, Batch: 190, Loss: 0.6994, Elapsed: 10m29s
2020-05-06 03:36:17.601275: Epoch: 1, Batch: 358, Loss: 0.7011, Elapsed: 4m25s
2020-05-06 03:39:38.560604: Epoch: 1, Batch: 351, Loss: 0.6861, Elapsed: 11m49s
2020-05-06 03:41:17.703353: Epoch: 1, Batch: 359, Loss: 0.7125, Elapsed: 5m0s
2020-05-06 03:42:48.754061: Epoch: 1, Batch: 191, Loss: 0.7123, Elapsed: 9m1s
2020-05-06 03:48:36.644722: Epoch: 1, Batch: 352, Loss: 0.6787, Elapsed: 8m58s
2020-05-06 03:48:58.249534: Epoch: 1, Batch: 360, Loss: 0.7060, Elapsed: 7m40s
2020-05-06 03:52:49.650921: Epoch: 1, Batch: 353, Loss: 0.6673, Elapsed: 4m12s
2020-05-06 03:55:07.467134: Epoch: 1, Batch: 361, Loss: 0.7117, Elapsed: 6m9s
2020-05-06 03:56:55.631241: Epoch: 1, Batch: 192, Loss: 0.7080, Elapsed: 14m6s
2020-05-06 03:58:34.358277: Epoch: 1, Batch: 354, Loss: 0.6822, Elapsed: 5m44s
2020-05-06 04:04:06.607219: Epoch: 1, Batch: 362, Loss: 0.7030, Elapsed: 8m59s
2020-05-06 04:07:37.410052: Epoch: 1, Batch: 355, Loss: 0.6711, Elapsed: 9m3s
2020-05-06 04:09:08.688662: Epoch: 1, Batch: 363, Loss: 0.7015, Elapsed: 5m2s
2020-05-06 04:12:24.243863: Epoch: 1, Batch: 193, Loss: 0.7132, Elapsed: 15m28s
2020-05-06 04:14:05.386667: Epoch: 1, Batch: 356, Loss: 0.6669, Elapsed: 6m27s
2020-05-06 04:18:12.840428: Epoch: 1, Batch: 364, Loss: 0.7130, Elapsed: 9m4s
2020-05-06 04:20:17.177422: Epoch: 1, Batch: 194, Loss: 0.7217, Elapsed: 7m52s
2020-05-06 04:24:37.806145: Epoch: 1, Batch: 365, Loss: 0.7201, Elapsed: 6m24s
2020-05-06 04:26:35.310568: Epoch: 1, Batch: 357, Loss: 0.6878, Elapsed: 12m29s
2020-05-06 04:31:40.856233: Epoch: 1, Batch: 366, Loss: 0.7053, Elapsed: 7m3s
2020-05-06 04:33:55.808910: Epoch: 1, Batch: 195, Loss: 0.7215, Elapsed: 13m38s
2020-05-06 04:35:46.624580: Epoch: 1, Batch: 358, Loss: 0.6758, Elapsed: 9m11s
2020-05-06 04:39:15.977292: Epoch: 1, Batch: 367, Loss: 0.7091, Elapsed: 7m35s
2020-05-06 04:41:48.570766: Epoch: 1, Batch: 196, Loss: 0.6990, Elapsed: 7m52s
2020-05-06 04:41:57.617274: Epoch: 1, Batch: 359, Loss: 0.6755, Elapsed: 6m10s
2020-05-06 04:45:34.221602: Epoch: 1, Batch: 368, Loss: 0.7074, Elapsed: 6m18s
2020-05-06 04:49:27.025624: Epoch: 1, Batch: 360, Loss: 0.6866, Elapsed: 7m29s
2020-05-06 04:54:20.764419: Epoch: 1, Batch: 369, Loss: 0.7089, Elapsed: 8m46s
2020-05-06 04:55:16.283069: Epoch: 1, Batch: 361, Loss: 0.6723, Elapsed: 5m49s
2020-05-06 04:59:07.501155: Epoch: 1, Batch: 197, Loss: 0.6907, Elapsed: 17m18s
2020-05-06 05:00:50.883040: Epoch: 1, Batch: 362, Loss: 0.6625, Elapsed: 5m34s
2020-05-06 05:01:30.685267: Epoch: 1, Batch: 370, Loss: 0.7067, Elapsed: 7m9s
2020-05-06 05:04:06.600487: Epoch: 1, Batch: 363, Loss: 0.6672, Elapsed: 3m15s
2020-05-06 05:06:58.202253: Epoch: 1, Batch: 198, Loss: 0.7094, Elapsed: 7m50s
2020-05-06 05:10:51.392050: Epoch: 1, Batch: 364, Loss: 0.6663, Elapsed: 6m44s
2020-05-06 05:12:04.220785: Epoch: 1, Batch: 371, Loss: 0.7236, Elapsed: 10m33s
2020-05-06 05:16:27.566833: Epoch: 1, Batch: 199, Loss: 0.6993, Elapsed: 9m29s
2020-05-06 05:20:02.828410: Epoch: 1, Batch: 365, Loss: 0.6856, Elapsed: 9m11s
2020-05-06 05:20:45.930593: Epoch: 1, Batch: 372, Loss: 0.7007, Elapsed: 8m41s
2020-05-06 05:27:58.178064: Epoch: 1, Batch: 366, Loss: 0.6787, Elapsed: 7m55s
2020-05-06 05:31:41.930992: Epoch: 1, Batch: 373, Loss: 0.6897, Elapsed: 10m55s
2020-05-06 05:32:36.412976: Epoch: 1, Batch: 367, Loss: 0.6698, Elapsed: 4m38s
2020-05-06 05:34:47.397419: Epoch: 1, Batch: 200, Loss: 0.7018, Elapsed: 18m19s
Starting testing the validation set with 200 subgraphs!
2020-05-06 05:37:47.116945: Epoch: 1, Batch: 368, Loss: 0.6690, Elapsed: 5m10s
2020-05-06 05:42:32.063301: Epoch: 1, Batch: 374, Loss: 0.6826, Elapsed: 10m50s
2020-05-06 05:45:04.753331: Epoch: 1, Batch: 369, Loss: 0.6636, Elapsed: 7m17s
2020-05-06 05:49:54.048932: Epoch: 1, Batch: 375, Loss: 0.7051, Elapsed: 7m21s
2020-05-06 05:50:52.207096: Epoch: 1, Batch: 370, Loss: 0.6648, Elapsed: 5m47s
2020-05-06 05:56:26.082751: Epoch: 1, Batch: 376, Loss: 0.6852, Elapsed: 6m32s
2020-05-06 05:57:20.202531: Epoch: 1, Batch: 371, Loss: 0.6824, Elapsed: 6m27s
2020-05-06 06:02:13.441542: Epoch: 1, Batch: 377, Loss: 0.7027, Elapsed: 5m47s
2020-05-06 06:06:30.800975: Epoch: 1, Batch: 372, Loss: 0.6839, Elapsed: 9m10s
2020-05-06 06:08:12.770971: Epoch: 1, Batch: 378, Loss: 0.7133, Elapsed: 5m59s
2020-05-06 06:12:14.065485: Epoch: 1, Batch: 373, Loss: 0.6794, Elapsed: 5m43s
2020-05-06 06:13:27.062347: Epoch: 1, Batch: 379, Loss: 0.6901, Elapsed: 5m14s
2020-05-06 06:18:44.241632: Epoch: 1, Batch: 374, Loss: 0.6876, Elapsed: 6m30s
2020-05-06 06:21:04.748982: Epoch: 1, Batch: 380, Loss: 0.7023, Elapsed: 7m37s
2020-05-06 06:26:25.490332: Epoch: 1, Batch: 381, Loss: 0.7069, Elapsed: 5m20s
2020-05-06 06:28:57.870492: Validation Test:  Loss: 0.7077,  Acc: 51.9624, AUC: 0.5220, Precision: 0.5714 -- Elapsed: 54m10s
2020-05-06 06:29:27.635907: Epoch: 1, Batch: 375, Loss: 0.6952, Elapsed: 10m43s
2020-05-06 06:36:09.609790: Epoch: 1, Batch: 376, Loss: 0.6790, Elapsed: 6m41s
2020-05-06 06:36:44.515585: Epoch: 1, Batch: 382, Loss: 0.7296, Elapsed: 10m19s
2020-05-06 06:41:11.156561: Epoch: 1, Batch: 377, Loss: 0.6810, Elapsed: 5m1s
2020-05-06 06:41:26.957569: Epoch: 1, Batch: 383, Loss: 0.7317, Elapsed: 4m42s
2020-05-06 06:43:20.369790: Epoch: 1, Batch: 201, Loss: 0.7054, Elapsed: 14m22s
2020-05-06 06:46:15.018504: Epoch: 1, Batch: 384, Loss: 0.7083, Elapsed: 4m48s
2020-05-06 06:53:26.198411: Epoch: 1, Batch: 378, Loss: 0.6924, Elapsed: 12m15s
2020-05-06 06:56:25.331658: Epoch: 1, Batch: 385, Loss: 0.7328, Elapsed: 10m10s
2020-05-06 06:59:12.835959: Epoch: 1, Batch: 202, Loss: 0.7114, Elapsed: 15m52s
2020-05-06 07:01:59.354117: Epoch: 1, Batch: 379, Loss: 0.6772, Elapsed: 8m33s
2020-05-06 07:03:21.721132: Epoch: 1, Batch: 386, Loss: 0.7038, Elapsed: 6m56s
2020-05-06 07:07:48.003303: Epoch: 1, Batch: 387, Loss: 0.7210, Elapsed: 4m26s
2020-05-06 07:10:22.367955: Epoch: 1, Batch: 203, Loss: 0.7093, Elapsed: 11m9s
2020-05-06 07:11:30.068857: Epoch: 1, Batch: 380, Loss: 0.6780, Elapsed: 9m30s
2020-05-06 07:18:02.537036: Epoch: 1, Batch: 388, Loss: 0.7100, Elapsed: 10m14s
2020-05-06 07:18:20.016729: Epoch: 1, Batch: 381, Loss: 0.6772, Elapsed: 6m49s
2020-05-06 07:20:20.794076: Epoch: 1, Batch: 204, Loss: 0.7175, Elapsed: 9m58s
2020-05-06 07:22:00.483263: Epoch: 1, Batch: 382, Loss: 0.6604, Elapsed: 3m40s
2020-05-06 07:25:16.758955: Epoch: 1, Batch: 389, Loss: 0.7103, Elapsed: 7m14s
2020-05-06 07:30:18.349195: Epoch: 1, Batch: 383, Loss: 0.6695, Elapsed: 8m17s
2020-05-06 07:33:10.006484: Epoch: 1, Batch: 390, Loss: 0.7096, Elapsed: 7m53s
2020-05-06 07:36:23.210228: Epoch: 1, Batch: 205, Loss: 0.7047, Elapsed: 16m2s
2020-05-06 07:37:16.376729: Epoch: 1, Batch: 384, Loss: 0.6616, Elapsed: 6m57s
2020-05-06 07:39:56.319646: Epoch: 1, Batch: 391, Loss: 0.6910, Elapsed: 6m46s
2020-05-06 07:47:24.198437: Epoch: 1, Batch: 385, Loss: 0.6717, Elapsed: 10m7s
2020-05-06 07:48:18.494864: Epoch: 1, Batch: 206, Loss: 0.6975, Elapsed: 11m55s
2020-05-06 07:50:03.752692: Epoch: 1, Batch: 392, Loss: 0.7017, Elapsed: 10m7s
2020-05-06 07:55:33.618638: Epoch: 1, Batch: 386, Loss: 0.6844, Elapsed: 8m9s
2020-05-06 07:56:34.903413: Epoch: 1, Batch: 207, Loss: 0.7129, Elapsed: 8m16s
2020-05-06 07:58:34.414273: Epoch: 1, Batch: 393, Loss: 0.6902, Elapsed: 8m30s
2020-05-06 08:01:41.996167: Epoch: 1, Batch: 387, Loss: 0.6720, Elapsed: 6m8s
2020-05-06 08:02:36.916713: Epoch: 1, Batch: 394, Loss: 0.6930, Elapsed: 4m2s
2020-05-06 08:09:06.313698: Epoch: 1, Batch: 208, Loss: 0.7275, Elapsed: 12m31s
2020-05-06 08:11:14.367967: Epoch: 1, Batch: 395, Loss: 0.7182, Elapsed: 8m37s
2020-05-06 08:13:28.968369: Epoch: 1, Batch: 388, Loss: 0.7060, Elapsed: 11m46s
2020-05-06 08:20:39.882423: Epoch: 1, Batch: 389, Loss: 0.6788, Elapsed: 7m10s
2020-05-06 08:20:48.637983: Epoch: 1, Batch: 209, Loss: 0.7335, Elapsed: 11m42s
2020-05-06 08:21:12.038579: Epoch: 1, Batch: 396, Loss: 0.7159, Elapsed: 9m57s
2020-05-06 08:27:25.625647: Epoch: 1, Batch: 397, Loss: 0.6985, Elapsed: 6m13s
2020-05-06 08:29:56.655027: Epoch: 1, Batch: 390, Loss: 0.6711, Elapsed: 9m16s
2020-05-06 08:33:41.315111: Epoch: 1, Batch: 210, Loss: 0.7037, Elapsed: 12m52s
2020-05-06 08:34:43.406528: Epoch: 1, Batch: 398, Loss: 0.7089, Elapsed: 7m17s
2020-05-06 08:36:18.509982: Epoch: 1, Batch: 391, Loss: 0.6880, Elapsed: 6m21s
2020-05-06 08:39:10.590979: Epoch: 1, Batch: 399, Loss: 0.7036, Elapsed: 4m27s
2020-05-06 08:44:40.404954: Epoch: 1, Batch: 400, Loss: 0.7146, Elapsed: 5m29s
Starting testing the validation set with 200 subgraphs!
2020-05-06 08:44:39.842020: Epoch: 1, Batch: 392, Loss: 0.6818, Elapsed: 8m21s
2020-05-06 08:50:46.932220: Epoch: 1, Batch: 211, Loss: 0.6981, Elapsed: 17m5s
2020-05-06 08:51:22.282889: Epoch: 1, Batch: 393, Loss: 0.7061, Elapsed: 6m42s
2020-05-06 08:59:31.099961: Epoch: 1, Batch: 394, Loss: 0.6793, Elapsed: 8m8s
2020-05-06 09:04:46.713813: Epoch: 1, Batch: 395, Loss: 0.6698, Elapsed: 5m15s
2020-05-06 09:08:25.914937: Epoch: 1, Batch: 396, Loss: 0.6813, Elapsed: 3m39s
2020-05-06 09:09:07.643501: Epoch: 1, Batch: 212, Loss: 0.7085, Elapsed: 18m20s
2020-05-06 09:13:23.946062: Epoch: 1, Batch: 397, Loss: 0.6706, Elapsed: 4m58s
2020-05-06 09:19:05.269906: Epoch: 1, Batch: 398, Loss: 0.6706, Elapsed: 5m41s
2020-05-06 09:19:43.258047: Epoch: 1, Batch: 213, Loss: 0.7076, Elapsed: 10m35s
2020-05-06 09:24:55.995751: Epoch: 1, Batch: 399, Loss: 0.6770, Elapsed: 5m50s
2020-05-06 09:26:54.905716: Validation Test:  Loss: 0.7039,  Acc: 54.3615, AUC: 0.5554, Precision: 0.6043 -- Elapsed: 42m14s
2020-05-06 09:28:01.900885: Epoch: 1, Batch: 214, Loss: 0.6966, Elapsed: 8m18s
2020-05-06 09:31:25.929276: Epoch: 1, Batch: 400, Loss: 0.6845, Elapsed: 6m29s
Starting testing the validation set with 200 subgraphs!
2020-05-06 09:33:19.601142: Epoch: 1, Batch: 401, Loss: 0.7117, Elapsed: 6m24s
2020-05-06 09:34:54.568089: Epoch: 1, Batch: 215, Loss: 0.6908, Elapsed: 6m52s
2020-05-06 09:40:36.435032: Epoch: 1, Batch: 402, Loss: 0.7000, Elapsed: 7m16s
2020-05-06 09:44:11.643518: Epoch: 1, Batch: 216, Loss: 0.7326, Elapsed: 9m17s
2020-05-06 09:45:26.756777: Epoch: 1, Batch: 403, Loss: 0.6919, Elapsed: 4m50s
2020-05-06 09:51:25.217173: Epoch: 1, Batch: 404, Loss: 0.7101, Elapsed: 5m58s
2020-05-06 09:55:09.376394: Epoch: 1, Batch: 217, Loss: 0.7075, Elapsed: 10m57s
2020-05-06 09:58:46.381350: Epoch: 1, Batch: 405, Loss: 0.7096, Elapsed: 7m21s
2020-05-06 10:03:45.275787: Epoch: 1, Batch: 218, Loss: 0.7064, Elapsed: 8m35s
2020-05-06 10:06:12.742894: Epoch: 1, Batch: 406, Loss: 0.6923, Elapsed: 7m26s
2020-05-06 10:09:37.808045: Epoch: 1, Batch: 407, Loss: 0.6873, Elapsed: 3m25s
2020-05-06 10:13:41.824750: Validation Test:  Loss: 0.6791,  Acc: 58.9649, AUC: 0.6079, Precision: 0.6307 -- Elapsed: 42m15s
2020-05-06 10:14:44.413857: Epoch: 1, Batch: 219, Loss: 0.6836, Elapsed: 10m59s
2020-05-06 10:15:29.368854: Epoch: 1, Batch: 408, Loss: 0.7045, Elapsed: 5m51s
2020-05-06 10:22:41.079052: Epoch: 1, Batch: 401, Loss: 0.6882, Elapsed: 8m59s
2020-05-06 10:23:32.616108: Epoch: 1, Batch: 409, Loss: 0.7123, Elapsed: 8m3s
2020-05-06 10:24:24.972767: Epoch: 1, Batch: 220, Loss: 0.7123, Elapsed: 9m40s
2020-05-06 10:29:07.976954: Epoch: 1, Batch: 410, Loss: 0.7005, Elapsed: 5m35s
2020-05-06 10:37:26.631688: Epoch: 1, Batch: 221, Loss: 0.7034, Elapsed: 13m1s
2020-05-06 10:37:53.921411: Epoch: 1, Batch: 411, Loss: 0.7175, Elapsed: 8m45s
2020-05-06 10:40:50.298481: Epoch: 1, Batch: 402, Loss: 0.6708, Elapsed: 18m9s
2020-05-06 10:45:27.778094: Epoch: 1, Batch: 412, Loss: 0.6958, Elapsed: 7m33s
2020-05-06 10:46:19.609580: Epoch: 1, Batch: 403, Loss: 0.6767, Elapsed: 5m29s
2020-05-06 10:50:21.668100: Epoch: 1, Batch: 222, Loss: 0.7029, Elapsed: 12m55s
2020-05-06 10:50:38.919883: Epoch: 1, Batch: 404, Loss: 0.6720, Elapsed: 4m19s
2020-05-06 10:53:02.902638: Epoch: 1, Batch: 413, Loss: 0.7257, Elapsed: 7m35s
2020-05-06 10:58:34.036639: Epoch: 1, Batch: 414, Loss: 0.7002, Elapsed: 5m31s
2020-05-06 10:58:37.507327: Epoch: 1, Batch: 405, Loss: 0.6822, Elapsed: 7m58s
2020-05-06 11:04:41.287982: Epoch: 1, Batch: 415, Loss: 0.6821, Elapsed: 6m7s
2020-05-06 11:05:47.143444: Epoch: 1, Batch: 223, Loss: 0.7061, Elapsed: 15m25s
2020-05-06 11:07:27.246801: Epoch: 1, Batch: 406, Loss: 0.6763, Elapsed: 8m49s
2020-05-06 11:12:20.981635: Epoch: 1, Batch: 416, Loss: 0.7111, Elapsed: 7m39s
2020-05-06 11:14:06.464286: Epoch: 1, Batch: 407, Loss: 0.6883, Elapsed: 6m39s
2020-05-06 11:17:47.259769: Epoch: 1, Batch: 224, Loss: 0.6903, Elapsed: 12m0s
2020-05-06 11:18:51.307940: Epoch: 1, Batch: 417, Loss: 0.6996, Elapsed: 6m30s
2020-05-06 11:22:32.818098: Epoch: 1, Batch: 408, Loss: 0.6789, Elapsed: 8m26s
2020-05-06 11:27:08.119903: Epoch: 1, Batch: 409, Loss: 0.6649, Elapsed: 4m35s
2020-05-06 11:27:32.954644: Epoch: 1, Batch: 418, Loss: 0.6744, Elapsed: 8m41s
2020-05-06 11:31:38.807101: Epoch: 1, Batch: 419, Loss: 0.6793, Elapsed: 4m5s
2020-05-06 11:33:17.972366: Epoch: 1, Batch: 225, Loss: 0.7143, Elapsed: 15m30s
2020-05-06 11:35:00.150598: Epoch: 1, Batch: 410, Loss: 0.6825, Elapsed: 7m52s
2020-05-06 11:38:17.970014: Epoch: 1, Batch: 420, Loss: 0.6891, Elapsed: 6m39s
2020-05-06 11:43:25.376478: Epoch: 1, Batch: 411, Loss: 0.6679, Elapsed: 8m25s
2020-05-06 11:43:39.086840: Epoch: 1, Batch: 421, Loss: 0.7027, Elapsed: 5m21s
2020-05-06 11:44:28.727036: Epoch: 1, Batch: 226, Loss: 0.7119, Elapsed: 11m10s
2020-05-06 11:48:52.344559: Epoch: 1, Batch: 422, Loss: 0.6843, Elapsed: 5m13s
2020-05-06 11:50:25.695561: Epoch: 1, Batch: 412, Loss: 0.6756, Elapsed: 7m0s
2020-05-06 11:52:42.316496: Epoch: 1, Batch: 227, Loss: 0.7013, Elapsed: 8m13s
2020-05-06 11:55:55.608253: Epoch: 1, Batch: 423, Loss: 0.7028, Elapsed: 7m3s
2020-05-06 11:58:36.680300: Epoch: 1, Batch: 228, Loss: 0.6948, Elapsed: 5m54s
2020-05-06 12:01:41.012066: Epoch: 1, Batch: 424, Loss: 0.7060, Elapsed: 5m45s
2020-05-06 12:01:44.305220: Epoch: 1, Batch: 413, Loss: 0.6687, Elapsed: 11m18s
2020-05-06 12:06:17.030408: Epoch: 1, Batch: 414, Loss: 0.6718, Elapsed: 4m32s
2020-05-06 12:11:46.371888: Epoch: 1, Batch: 229, Loss: 0.7374, Elapsed: 13m9s
2020-05-06 12:13:14.828574: Epoch: 1, Batch: 425, Loss: 0.7063, Elapsed: 11m33s
2020-05-06 12:17:07.968848: Epoch: 1, Batch: 415, Loss: 0.6619, Elapsed: 10m50s
2020-05-06 12:20:51.380576: Epoch: 1, Batch: 230, Loss: 0.7013, Elapsed: 9m5s
2020-05-06 12:21:28.464219: Epoch: 1, Batch: 426, Loss: 0.7015, Elapsed: 8m13s
2020-05-06 12:24:53.242188: Epoch: 1, Batch: 416, Loss: 0.6825, Elapsed: 7m45s
2020-05-06 12:27:22.376965: Epoch: 1, Batch: 427, Loss: 0.7149, Elapsed: 5m53s
2020-05-06 12:35:03.142176: Epoch: 1, Batch: 417, Loss: 0.6552, Elapsed: 10m9s
2020-05-06 12:35:25.687720: Epoch: 1, Batch: 428, Loss: 0.6795, Elapsed: 8m3s
2020-05-06 12:36:24.001662: Epoch: 1, Batch: 231, Loss: 0.7039, Elapsed: 15m32s
2020-05-06 12:41:07.481580: Epoch: 1, Batch: 429, Loss: 0.7014, Elapsed: 5m41s
2020-05-06 12:45:05.097495: Epoch: 1, Batch: 232, Loss: 0.7026, Elapsed: 8m41s
2020-05-06 12:45:14.676571: Epoch: 1, Batch: 418, Loss: 0.6856, Elapsed: 10m11s
2020-05-06 12:47:18.272054: Epoch: 1, Batch: 430, Loss: 0.7035, Elapsed: 6m10s
2020-05-06 12:54:06.286990: Epoch: 1, Batch: 431, Loss: 0.6930, Elapsed: 6m47s
2020-05-06 12:56:12.412169: Epoch: 1, Batch: 419, Loss: 0.6694, Elapsed: 10m57s
2020-05-06 12:58:55.353843: Epoch: 1, Batch: 432, Loss: 0.7230, Elapsed: 4m49s
2020-05-06 13:02:15.163996: Epoch: 1, Batch: 420, Loss: 0.6746, Elapsed: 6m2s
2020-05-06 13:02:29.654276: Epoch: 1, Batch: 233, Loss: 0.7004, Elapsed: 17m24s
2020-05-06 13:03:09.349401: Epoch: 1, Batch: 433, Loss: 0.7099, Elapsed: 4m13s
2020-05-06 13:08:08.602514: Epoch: 1, Batch: 421, Loss: 0.6691, Elapsed: 5m53s
2020-05-06 13:11:41.081634: Epoch: 1, Batch: 434, Loss: 0.7187, Elapsed: 8m31s
2020-05-06 13:15:19.611688: Epoch: 1, Batch: 234, Loss: 0.7120, Elapsed: 12m49s
2020-05-06 13:15:49.090569: Epoch: 1, Batch: 422, Loss: 0.6582, Elapsed: 7m40s
2020-05-06 13:16:13.481915: Epoch: 1, Batch: 435, Loss: 0.7049, Elapsed: 4m32s
2020-05-06 13:25:31.912309: Epoch: 1, Batch: 423, Loss: 0.6841, Elapsed: 9m42s
2020-05-06 13:25:44.054470: Epoch: 1, Batch: 436, Loss: 0.6868, Elapsed: 9m30s
2020-05-06 13:26:42.438396: Epoch: 1, Batch: 235, Loss: 0.6985, Elapsed: 11m22s
2020-05-06 13:34:39.623172: Epoch: 1, Batch: 424, Loss: 0.6843, Elapsed: 9m7s
2020-05-06 13:36:39.343538: Epoch: 1, Batch: 437, Loss: 0.6893, Elapsed: 10m55s
2020-05-06 13:38:40.230284: Epoch: 1, Batch: 425, Loss: 0.6563, Elapsed: 4m0s
2020-05-06 13:39:37.178501: Epoch: 1, Batch: 236, Loss: 0.7320, Elapsed: 12m54s
2020-05-06 13:45:31.880613: Epoch: 1, Batch: 438, Loss: 0.7213, Elapsed: 8m52s
2020-05-06 13:45:50.627675: Epoch: 1, Batch: 426, Loss: 0.6639, Elapsed: 7m10s
2020-05-06 13:52:22.277274: Epoch: 1, Batch: 237, Loss: 0.7202, Elapsed: 12m45s
2020-05-06 13:53:06.956362: Epoch: 1, Batch: 439, Loss: 0.6895, Elapsed: 7m35s
2020-05-06 13:57:17.340505: Epoch: 1, Batch: 427, Loss: 0.6862, Elapsed: 11m26s
2020-05-06 14:02:38.268598: Epoch: 1, Batch: 440, Loss: 0.6929, Elapsed: 9m31s
2020-05-06 14:03:31.636122: Epoch: 1, Batch: 428, Loss: 0.6773, Elapsed: 6m14s
2020-05-06 14:05:14.924622: Epoch: 1, Batch: 238, Loss: 0.6894, Elapsed: 12m52s
2020-05-06 14:06:34.261638: Epoch: 1, Batch: 441, Loss: 0.7162, Elapsed: 3m55s
2020-05-06 14:07:32.969134: Epoch: 1, Batch: 429, Loss: 0.6601, Elapsed: 4m1s
2020-05-06 14:14:49.014361: Epoch: 1, Batch: 239, Loss: 0.7000, Elapsed: 9m34s
2020-05-06 14:14:58.781435: Epoch: 1, Batch: 430, Loss: 0.6938, Elapsed: 7m25s
2020-05-06 14:16:49.723744: Epoch: 1, Batch: 442, Loss: 0.7236, Elapsed: 10m15s
2020-05-06 14:22:50.211151: Epoch: 1, Batch: 443, Loss: 0.7046, Elapsed: 6m0s
2020-05-06 14:25:01.789450: Epoch: 1, Batch: 431, Loss: 0.6811, Elapsed: 10m2s
2020-05-06 14:27:55.581360: Epoch: 1, Batch: 444, Loss: 0.7097, Elapsed: 5m5s
2020-05-06 14:28:28.787148: Epoch: 1, Batch: 240, Loss: 0.7197, Elapsed: 13m39s
2020-05-06 14:29:34.358751: Epoch: 1, Batch: 432, Loss: 0.6727, Elapsed: 4m32s
2020-05-06 14:32:28.595140: Epoch: 1, Batch: 445, Loss: 0.6928, Elapsed: 4m32s
2020-05-06 14:39:33.220903: Epoch: 1, Batch: 433, Loss: 0.6849, Elapsed: 9m58s
2020-05-06 14:41:06.557552: Epoch: 1, Batch: 241, Loss: 0.7034, Elapsed: 12m37s
2020-05-06 14:43:33.257261: Epoch: 1, Batch: 446, Loss: 0.7113, Elapsed: 11m4s
2020-05-06 14:51:41.150605: Epoch: 1, Batch: 242, Loss: 0.7184, Elapsed: 10m34s
2020-05-06 14:52:40.840977: Epoch: 1, Batch: 447, Loss: 0.7165, Elapsed: 9m7s
2020-05-06 14:53:08.444695: Epoch: 1, Batch: 434, Loss: 0.6939, Elapsed: 13m35s
2020-05-06 15:00:33.496612: Epoch: 1, Batch: 435, Loss: 0.6815, Elapsed: 7m25s
2020-05-06 15:00:58.557004: Epoch: 1, Batch: 448, Loss: 0.7175, Elapsed: 8m17s
2020-05-06 15:03:56.237553: Epoch: 1, Batch: 243, Loss: 0.7380, Elapsed: 12m15s
2020-05-06 15:08:32.587259: Epoch: 1, Batch: 449, Loss: 0.7169, Elapsed: 7m34s
2020-05-06 15:11:09.502067: Epoch: 1, Batch: 244, Loss: 0.6994, Elapsed: 7m13s
2020-05-06 15:11:23.615540: Epoch: 1, Batch: 436, Loss: 0.6879, Elapsed: 10m50s
2020-05-06 15:16:02.858099: Epoch: 1, Batch: 450, Loss: 0.6853, Elapsed: 7m30s
Starting testing the validation set with 200 subgraphs!
2020-05-06 15:18:54.161094: Epoch: 1, Batch: 437, Loss: 0.6861, Elapsed: 7m30s
2020-05-06 15:19:01.891181: Epoch: 1, Batch: 245, Loss: 0.7007, Elapsed: 7m52s
2020-05-06 15:25:25.555208: Epoch: 1, Batch: 438, Loss: 0.6761, Elapsed: 6m31s
2020-05-06 15:28:23.837159: Epoch: 1, Batch: 246, Loss: 0.7344, Elapsed: 9m21s
2020-05-06 15:29:37.628863: Epoch: 1, Batch: 439, Loss: 0.6513, Elapsed: 4m12s
2020-05-06 15:37:22.440814: Epoch: 1, Batch: 440, Loss: 0.6909, Elapsed: 7m44s
2020-05-06 15:41:33.353137: Epoch: 1, Batch: 247, Loss: 0.7080, Elapsed: 13m9s
2020-05-06 15:42:15.045769: Epoch: 1, Batch: 441, Loss: 0.6736, Elapsed: 4m52s
2020-05-06 15:48:48.355346: Epoch: 1, Batch: 248, Loss: 0.7151, Elapsed: 7m14s
2020-05-06 15:50:00.915509: Epoch: 1, Batch: 442, Loss: 0.6792, Elapsed: 7m45s
2020-05-06 15:54:28.203547: Epoch: 1, Batch: 443, Loss: 0.6708, Elapsed: 4m27s
2020-05-06 15:58:06.526030: Validation Test:  Loss: 0.6994,  Acc: 54.9488, AUC: 0.5591, Precision: 0.6076 -- Elapsed: 42m3s
2020-05-06 15:58:35.485936: Epoch: 1, Batch: 249, Loss: 0.6991, Elapsed: 9m47s
2020-05-06 16:04:19.274697: Epoch: 1, Batch: 444, Loss: 0.6923, Elapsed: 9m51s
2020-05-06 16:05:43.227269: Epoch: 1, Batch: 451, Loss: 0.7052, Elapsed: 7m36s
2020-05-06 16:10:20.184044: Epoch: 1, Batch: 445, Loss: 0.6700, Elapsed: 6m0s
2020-05-06 16:11:32.459436: Epoch: 1, Batch: 452, Loss: 0.6760, Elapsed: 5m49s
2020-05-06 16:13:00.186632: Epoch: 1, Batch: 250, Loss: 0.7086, Elapsed: 14m24s
Starting testing the validation set with 200 subgraphs!
2020-05-06 16:15:54.689673: Epoch: 1, Batch: 453, Loss: 0.6864, Elapsed: 4m22s
2020-05-06 16:18:49.111267: Epoch: 1, Batch: 446, Loss: 0.6876, Elapsed: 8m28s
2020-05-06 16:21:23.930079: Epoch: 1, Batch: 454, Loss: 0.7074, Elapsed: 5m29s
2020-05-06 16:24:22.960766: Epoch: 1, Batch: 447, Loss: 0.6687, Elapsed: 5m33s
2020-05-06 16:29:05.820546: Epoch: 1, Batch: 455, Loss: 0.7026, Elapsed: 7m41s
2020-05-06 16:30:16.380898: Epoch: 1, Batch: 448, Loss: 0.6864, Elapsed: 5m53s
2020-05-06 16:37:06.444952: Epoch: 1, Batch: 449, Loss: 0.6983, Elapsed: 6m50s
2020-05-06 16:38:41.928372: Epoch: 1, Batch: 456, Loss: 0.7637, Elapsed: 9m36s
2020-05-06 16:42:31.960170: Epoch: 1, Batch: 450, Loss: 0.6848, Elapsed: 5m25s
Starting testing the validation set with 200 subgraphs!
2020-05-06 16:47:44.339649: Epoch: 1, Batch: 457, Loss: 0.7192, Elapsed: 9m2s
2020-05-06 16:53:11.442990: Epoch: 1, Batch: 458, Loss: 0.7103, Elapsed: 5m27s
2020-05-06 16:59:06.545467: Epoch: 1, Batch: 459, Loss: 0.6784, Elapsed: 5m55s
2020-05-06 17:03:21.015837: Epoch: 1, Batch: 460, Loss: 0.6717, Elapsed: 4m14s
2020-05-06 17:04:51.933622: Validation Test:  Loss: 0.7063,  Acc: 52.1541, AUC: 0.5282, Precision: 0.5771 -- Elapsed: 51m51s
2020-05-06 17:10:41.390507: Epoch: 1, Batch: 461, Loss: 0.7194, Elapsed: 7m20s
2020-05-06 17:11:56.240147: Epoch: 1, Batch: 251, Loss: 0.6956, Elapsed: 7m4s
2020-05-06 17:17:35.667899: Epoch: 1, Batch: 462, Loss: 0.6785, Elapsed: 6m54s
2020-05-06 17:23:14.064082: Validation Test:  Loss: 0.6835,  Acc: 55.9429, AUC: 0.5718, Precision: 0.5949 -- Elapsed: 40m42s
2020-05-06 17:23:19.127596: Epoch: 1, Batch: 463, Loss: 0.6885, Elapsed: 5m43s
2020-05-06 17:27:42.875682: Epoch: 1, Batch: 252, Loss: 0.7073, Elapsed: 15m46s
2020-05-06 17:30:26.686724: Epoch: 1, Batch: 451, Loss: 0.6774, Elapsed: 7m12s
2020-05-06 17:31:22.334413: Epoch: 1, Batch: 464, Loss: 0.7030, Elapsed: 8m3s
2020-05-06 17:38:04.184158: Epoch: 1, Batch: 452, Loss: 0.6726, Elapsed: 7m37s
2020-05-06 17:40:09.136659: Epoch: 1, Batch: 253, Loss: 0.7089, Elapsed: 12m26s
2020-05-06 17:41:23.544121: Epoch: 1, Batch: 465, Loss: 0.7076, Elapsed: 10m1s
2020-05-06 17:46:25.437584: Epoch: 1, Batch: 453, Loss: 0.6728, Elapsed: 8m21s
2020-05-06 17:48:33.404109: Epoch: 1, Batch: 254, Loss: 0.7014, Elapsed: 8m24s
2020-05-06 17:49:33.741765: Epoch: 1, Batch: 466, Loss: 0.6812, Elapsed: 8m10s
2020-05-06 17:53:28.326614: Epoch: 1, Batch: 454, Loss: 0.6742, Elapsed: 7m2s
2020-05-06 17:57:38.038741: Epoch: 1, Batch: 467, Loss: 0.7013, Elapsed: 8m4s
2020-05-06 18:01:39.305391: Epoch: 1, Batch: 455, Loss: 0.6773, Elapsed: 8m10s
2020-05-06 18:03:18.406085: Epoch: 1, Batch: 468, Loss: 0.6873, Elapsed: 5m40s
2020-05-06 18:05:35.721312: Epoch: 1, Batch: 255, Loss: 0.7041, Elapsed: 17m2s
2020-05-06 18:07:00.557345: Epoch: 1, Batch: 456, Loss: 0.6762, Elapsed: 5m21s
2020-05-06 18:10:06.766860: Epoch: 1, Batch: 469, Loss: 0.6815, Elapsed: 6m48s
2020-05-06 18:16:50.432355: Epoch: 1, Batch: 457, Loss: 0.6824, Elapsed: 9m49s
2020-05-06 18:17:11.553544: Epoch: 1, Batch: 470, Loss: 0.6966, Elapsed: 7m4s
2020-05-06 18:23:17.011356: Epoch: 1, Batch: 256, Loss: 0.7294, Elapsed: 17m41s
2020-05-06 18:23:48.347075: Epoch: 1, Batch: 458, Loss: 0.6801, Elapsed: 6m57s
2020-05-06 18:26:35.994805: Epoch: 1, Batch: 471, Loss: 0.6886, Elapsed: 9m24s
2020-05-06 18:30:54.941070: Epoch: 1, Batch: 459, Loss: 0.6707, Elapsed: 7m6s
2020-05-06 18:34:10.754293: Epoch: 1, Batch: 472, Loss: 0.7036, Elapsed: 7m34s
2020-05-06 18:35:48.364499: Epoch: 1, Batch: 257, Loss: 0.7248, Elapsed: 12m31s
2020-05-06 18:36:54.015743: Epoch: 1, Batch: 460, Loss: 0.6717, Elapsed: 5m59s
2020-05-06 18:43:34.496837: Epoch: 1, Batch: 461, Loss: 0.6602, Elapsed: 6m40s
2020-05-06 18:44:05.096674: Epoch: 1, Batch: 258, Loss: 0.7049, Elapsed: 8m16s
2020-05-06 18:47:42.411763: Epoch: 1, Batch: 473, Loss: 0.7191, Elapsed: 13m31s
2020-05-06 18:49:35.225031: Epoch: 1, Batch: 462, Loss: 0.6657, Elapsed: 6m0s
2020-05-06 18:54:17.485194: Epoch: 1, Batch: 463, Loss: 0.6828, Elapsed: 4m42s
2020-05-06 18:58:34.125203: Epoch: 1, Batch: 474, Loss: 0.7076, Elapsed: 10m51s
2020-05-06 19:00:35.152459: Epoch: 1, Batch: 259, Loss: 0.7028, Elapsed: 16m30s
2020-05-06 19:02:37.260243: Epoch: 1, Batch: 464, Loss: 0.6775, Elapsed: 8m19s
2020-05-06 19:06:19.379327: Epoch: 1, Batch: 475, Loss: 0.6930, Elapsed: 7m45s
2020-05-06 19:11:03.384375: Epoch: 1, Batch: 465, Loss: 0.6509, Elapsed: 8m26s
2020-05-06 19:13:48.935024: Epoch: 1, Batch: 260, Loss: 0.6920, Elapsed: 13m13s
2020-05-06 19:14:30.403428: Epoch: 1, Batch: 476, Loss: 0.7128, Elapsed: 8m10s
2020-05-06 19:16:57.491005: Epoch: 1, Batch: 466, Loss: 0.6813, Elapsed: 5m54s
2020-05-06 19:22:17.475376: Epoch: 1, Batch: 467, Loss: 0.6711, Elapsed: 5m19s
2020-05-06 19:24:31.312407: Epoch: 1, Batch: 477, Loss: 0.6882, Elapsed: 10m0s
2020-05-06 19:27:02.798481: Epoch: 1, Batch: 468, Loss: 0.6795, Elapsed: 4m45s
2020-05-06 19:29:18.248341: Epoch: 1, Batch: 478, Loss: 0.6946, Elapsed: 4m46s
2020-05-06 19:33:23.579339: Epoch: 1, Batch: 469, Loss: 0.6649, Elapsed: 6m20s
2020-05-06 19:33:33.820424: Epoch: 1, Batch: 261, Loss: 0.7116, Elapsed: 19m44s
2020-05-06 19:38:12.380522: Epoch: 1, Batch: 479, Loss: 0.6996, Elapsed: 8m54s
2020-05-06 19:42:01.753125: Epoch: 1, Batch: 470, Loss: 0.6870, Elapsed: 8m38s
2020-05-06 19:44:50.332709: Epoch: 1, Batch: 480, Loss: 0.6845, Elapsed: 6m37s
2020-05-06 19:47:36.793523: Epoch: 1, Batch: 262, Loss: 0.7015, Elapsed: 14m2s
2020-05-06 19:49:57.276039: Epoch: 1, Batch: 471, Loss: 0.6806, Elapsed: 7m55s
2020-05-06 19:50:10.990798: Epoch: 1, Batch: 481, Loss: 0.6963, Elapsed: 5m20s
2020-05-06 19:57:40.676479: Epoch: 1, Batch: 263, Loss: 0.7006, Elapsed: 10m3s
2020-05-06 19:57:37.173563: Epoch: 1, Batch: 482, Loss: 0.6931, Elapsed: 7m26s
2020-05-06 19:58:29.796291: Epoch: 1, Batch: 472, Loss: 0.6640, Elapsed: 8m32s
2020-05-06 20:05:16.779384: Epoch: 1, Batch: 483, Loss: 0.6873, Elapsed: 7m39s
2020-05-06 20:07:55.583243: Epoch: 1, Batch: 473, Loss: 0.6648, Elapsed: 9m25s
2020-05-06 20:10:47.936932: Epoch: 1, Batch: 264, Loss: 0.7079, Elapsed: 13m7s
2020-05-06 20:15:26.806933: Epoch: 1, Batch: 484, Loss: 0.6866, Elapsed: 10m9s
2020-05-06 20:18:17.665787: Epoch: 1, Batch: 474, Loss: 0.6826, Elapsed: 10m22s
2020-05-06 20:20:33.796479: Epoch: 1, Batch: 485, Loss: 0.6969, Elapsed: 5m6s
2020-05-06 20:22:31.665326: Epoch: 1, Batch: 475, Loss: 0.6711, Elapsed: 4m13s
2020-05-06 20:26:50.277363: Epoch: 1, Batch: 476, Loss: 0.6601, Elapsed: 4m18s
2020-05-06 20:27:04.754914: Epoch: 1, Batch: 486, Loss: 0.7014, Elapsed: 6m30s
2020-05-06 20:30:49.661015: Epoch: 1, Batch: 265, Loss: 0.7229, Elapsed: 20m1s
2020-05-06 20:31:30.402693: Epoch: 1, Batch: 477, Loss: 0.6930, Elapsed: 4m40s
2020-05-06 20:36:39.573690: Epoch: 1, Batch: 478, Loss: 0.6928, Elapsed: 5m9s
2020-05-06 20:42:11.026220: Epoch: 1, Batch: 487, Loss: 0.7097, Elapsed: 15m6s
2020-05-06 20:42:53.997496: Epoch: 1, Batch: 479, Loss: 0.6672, Elapsed: 6m14s
2020-05-06 20:42:58.275597: Epoch: 1, Batch: 266, Loss: 0.7041, Elapsed: 12m8s
2020-05-06 20:47:35.686361: Epoch: 1, Batch: 488, Loss: 0.6789, Elapsed: 5m24s
2020-05-06 20:51:39.092042: Epoch: 1, Batch: 480, Loss: 0.6738, Elapsed: 8m45s
2020-05-06 20:56:36.334795: Epoch: 1, Batch: 267, Loss: 0.7294, Elapsed: 13m38s
2020-05-06 20:57:05.855637: Epoch: 1, Batch: 489, Loss: 0.7214, Elapsed: 9m30s
2020-05-06 20:57:13.158602: Epoch: 1, Batch: 481, Loss: 0.6723, Elapsed: 5m34s
2020-05-06 21:03:29.921770: Epoch: 1, Batch: 482, Loss: 0.6489, Elapsed: 6m16s
2020-05-06 21:07:16.550812: Epoch: 1, Batch: 490, Loss: 0.7004, Elapsed: 10m10s
2020-05-06 21:07:35.283578: Epoch: 1, Batch: 268, Loss: 0.7253, Elapsed: 10m58s
2020-05-06 21:08:07.643442: Epoch: 1, Batch: 483, Loss: 0.6788, Elapsed: 4m37s
2020-05-06 21:12:57.543990: Epoch: 1, Batch: 491, Loss: 0.7032, Elapsed: 5m40s
2020-05-06 21:16:12.176139: Epoch: 1, Batch: 484, Loss: 0.6775, Elapsed: 8m4s
2020-05-06 21:17:06.928764: Epoch: 1, Batch: 269, Loss: 0.7353, Elapsed: 9m31s
2020-05-06 21:18:53.847031: Epoch: 1, Batch: 492, Loss: 0.6873, Elapsed: 5m56s
2020-05-06 21:22:46.826087: Epoch: 1, Batch: 485, Loss: 0.6625, Elapsed: 6m34s
2020-05-06 21:26:45.744668: Epoch: 1, Batch: 270, Loss: 0.7200, Elapsed: 9m38s
2020-05-06 21:27:12.255463: Epoch: 1, Batch: 493, Loss: 0.6953, Elapsed: 8m18s
2020-05-06 21:30:00.681937: Epoch: 1, Batch: 486, Loss: 0.6598, Elapsed: 7m13s
2020-05-06 21:31:09.730785: Epoch: 1, Batch: 494, Loss: 0.6949, Elapsed: 3m57s
2020-05-06 21:39:09.436959: Epoch: 1, Batch: 487, Loss: 0.6857, Elapsed: 9m8s
2020-05-06 21:39:12.566556: Epoch: 1, Batch: 495, Loss: 0.7321, Elapsed: 8m2s
2020-05-06 21:40:12.850522: Epoch: 1, Batch: 271, Loss: 0.7246, Elapsed: 13m27s
2020-05-06 21:45:34.178588: Epoch: 1, Batch: 488, Loss: 0.6638, Elapsed: 6m24s
2020-05-06 21:46:45.577559: Epoch: 1, Batch: 496, Loss: 0.6861, Elapsed: 7m32s
2020-05-06 21:50:12.152995: Epoch: 1, Batch: 272, Loss: 0.7273, Elapsed: 9m59s
2020-05-06 21:50:17.994890: Epoch: 1, Batch: 489, Loss: 0.6623, Elapsed: 4m43s
2020-05-06 21:58:33.788530: Epoch: 1, Batch: 497, Loss: 0.7127, Elapsed: 11m48s
2020-05-06 22:01:51.259263: Epoch: 1, Batch: 273, Loss: 0.7236, Elapsed: 11m39s
2020-05-06 22:03:58.669416: Epoch: 1, Batch: 498, Loss: 0.6833, Elapsed: 5m24s
2020-05-06 22:06:17.274192: Epoch: 1, Batch: 490, Loss: 0.6723, Elapsed: 15m59s
2020-05-06 22:09:38.574539: Epoch: 1, Batch: 499, Loss: 0.6689, Elapsed: 5m39s
2020-05-06 22:10:12.367523: Epoch: 1, Batch: 491, Loss: 0.7011, Elapsed: 3m55s
2020-05-06 22:15:26.502172: Epoch: 1, Batch: 492, Loss: 0.6569, Elapsed: 5m14s
2020-05-06 22:16:27.746947: Epoch: 1, Batch: 500, Loss: 0.6904, Elapsed: 6m49s
Starting testing the validation set with 200 subgraphs!
2020-05-06 22:16:46.044772: Epoch: 1, Batch: 274, Loss: 0.7204, Elapsed: 14m54s
2020-05-06 22:24:09.227560: Epoch: 1, Batch: 493, Loss: 0.6704, Elapsed: 8m42s
2020-05-06 22:29:52.476878: Epoch: 1, Batch: 494, Loss: 0.6466, Elapsed: 5m43s
2020-05-06 22:30:11.608599: Epoch: 1, Batch: 275, Loss: 0.7028, Elapsed: 13m25s
2020-05-06 22:36:38.856272: Epoch: 1, Batch: 495, Loss: 0.6959, Elapsed: 6m46s
2020-05-06 22:41:26.176116: Epoch: 1, Batch: 276, Loss: 0.7111, Elapsed: 11m14s
2020-05-06 22:43:15.562412: Epoch: 1, Batch: 496, Loss: 0.6735, Elapsed: 6m36s
2020-05-06 22:47:27.757375: Epoch: 1, Batch: 497, Loss: 0.6872, Elapsed: 4m12s
2020-05-06 22:57:58.202362: Epoch: 1, Batch: 277, Loss: 0.7268, Elapsed: 16m32s
2020-05-06 22:59:04.581028: Epoch: 1, Batch: 498, Loss: 0.6801, Elapsed: 11m36s
2020-05-06 23:11:03.165227: Validation Test:  Loss: 0.6958,  Acc: 55.7279, AUC: 0.5637, Precision: 0.6079 -- Elapsed: 54m35s
2020-05-06 23:11:11.542283: Epoch: 1, Batch: 499, Loss: 0.6856, Elapsed: 12m6s
2020-05-06 23:14:52.661103: Epoch: 1, Batch: 278, Loss: 0.7272, Elapsed: 16m54s
2020-05-06 23:29:23.553601: Epoch: 1, Batch: 501, Loss: 0.7052, Elapsed: 18m20s
2020-05-06 23:31:07.265219: Epoch: 1, Batch: 500, Loss: 0.6906, Elapsed: 19m55s
Starting testing the validation set with 200 subgraphs!
2020-05-06 23:37:35.247242: Epoch: 1, Batch: 279, Loss: 0.7222, Elapsed: 22m42s
2020-05-06 23:42:52.354865: Epoch: 1, Batch: 502, Loss: 0.7080, Elapsed: 13m28s
2020-05-07 00:05:34.889291: Epoch: 1, Batch: 503, Loss: 0.7182, Elapsed: 22m42s
2020-05-07 00:17:07.897766: Epoch: 1, Batch: 504, Loss: 0.6924, Elapsed: 11m32s
2020-05-07 00:17:59.150784: Epoch: 1, Batch: 280, Loss: 0.7473, Elapsed: 40m23s
2020-05-07 00:36:40.496198: Epoch: 1, Batch: 505, Loss: 0.7282, Elapsed: 19m32s
2020-05-07 00:36:49.209611: Epoch: 1, Batch: 281, Loss: 0.7078, Elapsed: 18m50s
2020-05-07 00:54:04.241745: Epoch: 1, Batch: 506, Loss: 0.6892, Elapsed: 17m23s
2020-05-07 01:06:26.759940: Epoch: 1, Batch: 507, Loss: 0.6944, Elapsed: 12m22s
2020-05-07 01:09:53.233577: Validation Test:  Loss: 0.6757,  Acc: 59.3980, AUC: 0.6099, Precision: 0.6294 -- Elapsed: 98m45s
2020-05-07 01:11:19.350583: Epoch: 1, Batch: 282, Loss: 0.6904, Elapsed: 34m30s
2020-05-07 01:19:29.749995: Epoch: 1, Batch: 508, Loss: 0.6952, Elapsed: 13m2s
2020-05-07 01:21:37.544959: Epoch: 1, Batch: 501, Loss: 0.6792, Elapsed: 11m44s
2020-05-07 01:27:51.190531: Epoch: 1, Batch: 283, Loss: 0.7202, Elapsed: 16m31s
2020-05-07 01:29:01.911790: Epoch: 1, Batch: 502, Loss: 0.6860, Elapsed: 7m24s
2020-05-07 01:37:38.563538: Epoch: 1, Batch: 509, Loss: 0.6991, Elapsed: 18m8s
2020-05-07 01:43:39.597397: Epoch: 1, Batch: 503, Loss: 0.6688, Elapsed: 14m37s
2020-05-07 01:45:43.002249: Epoch: 1, Batch: 510, Loss: 0.6906, Elapsed: 8m4s
2020-05-07 01:49:57.979506: Epoch: 1, Batch: 284, Loss: 0.7334, Elapsed: 22m6s
2020-05-07 01:51:16.298612: Epoch: 1, Batch: 511, Loss: 0.6806, Elapsed: 5m33s
2020-05-07 01:51:41.425033: Epoch: 1, Batch: 504, Loss: 0.6837, Elapsed: 8m1s
2020-05-07 02:00:40.813026: Epoch: 1, Batch: 512, Loss: 0.6969, Elapsed: 9m24s
2020-05-07 02:01:43.685857: Epoch: 1, Batch: 285, Loss: 0.7273, Elapsed: 11m45s
2020-05-07 02:02:07.918141: Epoch: 1, Batch: 505, Loss: 0.6660, Elapsed: 10m26s
2020-05-07 02:09:30.650871: Epoch: 1, Batch: 513, Loss: 0.6706, Elapsed: 8m49s
2020-05-07 02:09:46.989941: Epoch: 1, Batch: 506, Loss: 0.6646, Elapsed: 7m39s
2020-05-07 02:10:34.218411: Epoch: 1, Batch: 286, Loss: 0.7151, Elapsed: 8m50s
2020-05-07 02:18:05.748970: Epoch: 1, Batch: 514, Loss: 0.7001, Elapsed: 8m35s
2020-05-07 02:18:28.655279: Epoch: 1, Batch: 507, Loss: 0.6682, Elapsed: 8m41s
2020-05-07 02:24:12.678995: Epoch: 1, Batch: 515, Loss: 0.6802, Elapsed: 6m6s
2020-05-07 02:24:47.246069: Epoch: 1, Batch: 287, Loss: 0.7138, Elapsed: 14m13s
2020-05-07 02:27:23.395936: Epoch: 1, Batch: 508, Loss: 0.6728, Elapsed: 8m54s
2020-05-07 02:33:05.113948: Epoch: 1, Batch: 516, Loss: 0.7039, Elapsed: 8m52s
2020-05-07 02:34:05.425367: Epoch: 1, Batch: 509, Loss: 0.6834, Elapsed: 6m42s
2020-05-07 02:38:16.824353: Epoch: 1, Batch: 517, Loss: 0.6707, Elapsed: 5m11s
2020-05-07 02:38:38.154473: Epoch: 1, Batch: 288, Loss: 0.7133, Elapsed: 13m50s
2020-05-07 02:47:11.117563: Epoch: 1, Batch: 510, Loss: 0.6885, Elapsed: 13m5s
2020-05-07 02:47:09.990815: Epoch: 1, Batch: 518, Loss: 0.6985, Elapsed: 8m53s
2020-05-07 02:49:10.063897: Epoch: 1, Batch: 289, Loss: 0.7266, Elapsed: 10m31s
2020-05-07 02:51:45.902757: Epoch: 1, Batch: 511, Loss: 0.6741, Elapsed: 4m34s
2020-05-07 02:54:44.569662: Epoch: 1, Batch: 519, Loss: 0.7012, Elapsed: 7m34s
2020-05-07 03:00:56.986281: Epoch: 1, Batch: 520, Loss: 0.7089, Elapsed: 6m12s
2020-05-07 03:05:30.580669: Epoch: 1, Batch: 512, Loss: 0.6602, Elapsed: 13m44s
2020-05-07 03:05:34.623498: Epoch: 1, Batch: 290, Loss: 0.6909, Elapsed: 16m24s
2020-05-07 03:07:54.791917: Epoch: 1, Batch: 521, Loss: 0.6891, Elapsed: 6m57s
2020-05-07 03:10:33.387032: Epoch: 1, Batch: 522, Loss: 0.7308, Elapsed: 2m38s
2020-05-07 03:12:02.995387: Epoch: 1, Batch: 513, Loss: 0.6752, Elapsed: 6m32s
2020-05-07 03:16:28.378157: Epoch: 1, Batch: 523, Loss: 0.6834, Elapsed: 5m54s
2020-05-07 03:17:31.439561: Epoch: 1, Batch: 291, Loss: 0.7247, Elapsed: 11m56s
2020-05-07 03:24:16.709653: Epoch: 1, Batch: 524, Loss: 0.7021, Elapsed: 7m48s
2020-05-07 03:24:47.622309: Epoch: 1, Batch: 514, Loss: 0.6803, Elapsed: 12m44s
2020-05-07 03:27:28.873167: Epoch: 1, Batch: 292, Loss: 0.7134, Elapsed: 9m57s
2020-05-07 03:31:55.302067: Epoch: 1, Batch: 525, Loss: 0.7134, Elapsed: 7m38s
2020-05-07 03:33:31.005718: Epoch: 1, Batch: 515, Loss: 0.6642, Elapsed: 8m43s
2020-05-07 03:38:04.285902: Epoch: 1, Batch: 293, Loss: 0.7075, Elapsed: 10m35s
2020-05-07 03:39:19.767518: Epoch: 1, Batch: 526, Loss: 0.7193, Elapsed: 7m24s
2020-05-07 03:45:10.168410: Epoch: 1, Batch: 516, Loss: 0.6669, Elapsed: 11m39s
2020-05-07 03:46:11.979230: Epoch: 1, Batch: 527, Loss: 0.7146, Elapsed: 6m52s
2020-05-07 03:51:18.396942: Epoch: 1, Batch: 528, Loss: 0.6902, Elapsed: 5m6s
2020-05-07 03:51:22.162941: Epoch: 1, Batch: 517, Loss: 0.6614, Elapsed: 6m11s
2020-05-07 03:58:19.039234: Epoch: 1, Batch: 294, Loss: 0.7201, Elapsed: 20m14s
2020-05-07 04:00:03.082323: Epoch: 1, Batch: 529, Loss: 0.7067, Elapsed: 8m44s
2020-05-07 04:01:05.950304: Epoch: 1, Batch: 518, Loss: 0.6645, Elapsed: 9m43s
2020-05-07 04:05:57.297104: Epoch: 1, Batch: 519, Loss: 0.6789, Elapsed: 4m51s
2020-05-07 04:12:03.015246: Epoch: 1, Batch: 530, Loss: 0.6960, Elapsed: 11m59s
2020-05-07 04:12:24.587870: Epoch: 1, Batch: 295, Loss: 0.6875, Elapsed: 14m5s
2020-05-07 04:13:51.247778: Epoch: 1, Batch: 520, Loss: 0.6734, Elapsed: 7m53s
2020-05-07 04:16:27.008816: Epoch: 1, Batch: 531, Loss: 0.6866, Elapsed: 4m23s
2020-05-07 04:20:29.266399: Epoch: 1, Batch: 521, Loss: 0.6742, Elapsed: 6m37s
2020-05-07 04:22:01.746553: Epoch: 1, Batch: 532, Loss: 0.6918, Elapsed: 5m34s
2020-05-07 04:28:15.176478: Epoch: 1, Batch: 522, Loss: 0.6748, Elapsed: 7m45s
2020-05-07 04:28:24.486445: Epoch: 1, Batch: 296, Loss: 0.6932, Elapsed: 15m59s
2020-05-07 04:30:45.480018: Epoch: 1, Batch: 533, Loss: 0.6975, Elapsed: 8m43s
2020-05-07 04:36:58.374380: Epoch: 1, Batch: 523, Loss: 0.6753, Elapsed: 8m43s
2020-05-07 04:37:01.413355: Epoch: 1, Batch: 297, Loss: 0.7279, Elapsed: 8m36s
2020-05-07 04:42:05.534176: Epoch: 1, Batch: 534, Loss: 0.7117, Elapsed: 11m20s
2020-05-07 04:43:02.457112: Epoch: 1, Batch: 524, Loss: 0.6696, Elapsed: 6m4s
2020-05-07 04:49:55.067270: Epoch: 1, Batch: 298, Loss: 0.6924, Elapsed: 12m53s
2020-05-07 04:49:58.998158: Epoch: 1, Batch: 535, Loss: 0.6892, Elapsed: 7m53s
2020-05-07 04:51:30.523717: Epoch: 1, Batch: 525, Loss: 0.6706, Elapsed: 8m28s
2020-05-07 04:56:38.411059: Epoch: 1, Batch: 536, Loss: 0.6977, Elapsed: 6m39s
2020-05-07 04:59:34.736832: Epoch: 1, Batch: 299, Loss: 0.7267, Elapsed: 9m39s
2020-05-07 05:00:44.808667: Epoch: 1, Batch: 537, Loss: 0.6904, Elapsed: 4m6s
2020-05-07 05:00:38.539945: Epoch: 1, Batch: 526, Loss: 0.6730, Elapsed: 9m7s
2020-05-07 05:07:43.482100: Epoch: 1, Batch: 538, Loss: 0.7058, Elapsed: 6m58s
2020-05-07 05:10:04.550486: Epoch: 1, Batch: 527, Loss: 0.6826, Elapsed: 9m25s
2020-05-07 05:11:00.302905: Epoch: 1, Batch: 300, Loss: 0.7235, Elapsed: 11m25s
Starting testing the validation set with 200 subgraphs!
2020-05-07 05:13:51.763903: Epoch: 1, Batch: 539, Loss: 0.6940, Elapsed: 6m8s
2020-05-07 05:16:41.149633: Epoch: 1, Batch: 528, Loss: 0.6737, Elapsed: 6m36s
2020-05-07 05:19:13.665111: Epoch: 1, Batch: 540, Loss: 0.6961, Elapsed: 5m21s
2020-05-07 05:23:20.383004: Epoch: 1, Batch: 529, Loss: 0.6757, Elapsed: 6m39s
2020-05-07 05:25:32.583010: Epoch: 1, Batch: 541, Loss: 0.6826, Elapsed: 6m18s
2020-05-07 05:30:59.900823: Epoch: 1, Batch: 530, Loss: 0.6729, Elapsed: 7m39s
2020-05-07 05:31:23.746925: Epoch: 1, Batch: 542, Loss: 0.6917, Elapsed: 5m51s
2020-05-07 05:34:17.407520: Epoch: 1, Batch: 543, Loss: 0.6617, Elapsed: 2m53s
2020-05-07 05:36:32.595934: Epoch: 1, Batch: 531, Loss: 0.6825, Elapsed: 5m32s
2020-05-07 05:40:38.477638: Epoch: 1, Batch: 544, Loss: 0.6891, Elapsed: 6m21s
2020-05-07 05:41:05.666152: Epoch: 1, Batch: 532, Loss: 0.6829, Elapsed: 4m33s
2020-05-07 05:45:10.994065: Epoch: 1, Batch: 545, Loss: 0.6810, Elapsed: 4m32s
2020-05-07 05:49:00.273360: Epoch: 1, Batch: 533, Loss: 0.6756, Elapsed: 7m54s
2020-05-07 05:50:40.821954: Epoch: 1, Batch: 546, Loss: 0.6897, Elapsed: 5m29s
2020-05-07 05:55:43.163740: Epoch: 1, Batch: 547, Loss: 0.6887, Elapsed: 5m2s
2020-05-07 05:57:31.795133: Epoch: 1, Batch: 534, Loss: 0.6881, Elapsed: 8m31s
2020-05-07 06:02:20.297617: Epoch: 1, Batch: 548, Loss: 0.6877, Elapsed: 6m37s
2020-05-07 06:04:56.077287: Validation Test:  Loss: 0.7143,  Acc: 55.4023, AUC: 0.5682, Precision: 0.6139 -- Elapsed: 53m55s
2020-05-07 06:05:11.608524: Epoch: 1, Batch: 535, Loss: 0.6825, Elapsed: 7m39s
2020-05-07 06:08:48.914093: Epoch: 1, Batch: 549, Loss: 0.6810, Elapsed: 6m28s
2020-05-07 06:09:46.509500: Epoch: 1, Batch: 536, Loss: 0.6620, Elapsed: 4m34s
2020-05-07 06:12:44.418883: Epoch: 1, Batch: 301, Loss: 0.7248, Elapsed: 7m48s
2020-05-07 06:15:00.641842: Epoch: 1, Batch: 550, Loss: 0.7022, Elapsed: 6m11s
Starting testing the validation set with 200 subgraphs!
2020-05-07 06:19:32.697767: Epoch: 1, Batch: 537, Loss: 0.6816, Elapsed: 9m46s
2020-05-07 06:22:49.984846: Epoch: 1, Batch: 302, Loss: 0.7195, Elapsed: 10m5s
2020-05-07 06:29:43.717923: Epoch: 1, Batch: 538, Loss: 0.6860, Elapsed: 10m10s
2020-05-07 06:34:02.745804: Epoch: 1, Batch: 303, Loss: 0.7035, Elapsed: 11m12s
2020-05-07 06:35:26.453940: Epoch: 1, Batch: 539, Loss: 0.6578, Elapsed: 5m42s
2020-05-07 06:40:26.468347: Epoch: 1, Batch: 540, Loss: 0.6565, Elapsed: 4m59s
2020-05-07 06:45:13.848907: Epoch: 1, Batch: 541, Loss: 0.6732, Elapsed: 4m47s
2020-05-07 06:47:35.152865: Epoch: 1, Batch: 304, Loss: 0.7075, Elapsed: 13m32s
2020-05-07 06:51:59.085271: Epoch: 1, Batch: 542, Loss: 0.6985, Elapsed: 6m45s
2020-05-07 06:54:16.493671: Epoch: 1, Batch: 305, Loss: 0.7295, Elapsed: 6m41s
2020-05-07 06:57:06.686456: Validation Test:  Loss: 0.6933,  Acc: 55.7654, AUC: 0.5657, Precision: 0.6068 -- Elapsed: 42m6s
2020-05-07 06:58:38.313088: Epoch: 1, Batch: 543, Loss: 0.6775, Elapsed: 6m39s
2020-05-07 07:02:52.317388: Epoch: 1, Batch: 306, Loss: 0.7150, Elapsed: 8m35s
2020-05-07 07:04:03.580163: Epoch: 1, Batch: 551, Loss: 0.6948, Elapsed: 6m56s
2020-05-07 07:04:24.776296: Epoch: 1, Batch: 544, Loss: 0.6711, Elapsed: 5m46s
2020-05-07 07:09:49.476251: Epoch: 1, Batch: 307, Loss: 0.7468, Elapsed: 6m57s
2020-05-07 07:10:00.654644: Epoch: 1, Batch: 552, Loss: 0.6709, Elapsed: 5m57s
2020-05-07 07:14:19.420531: Epoch: 1, Batch: 545, Loss: 0.6689, Elapsed: 9m54s
2020-05-07 07:17:08.942925: Epoch: 1, Batch: 553, Loss: 0.6847, Elapsed: 7m8s
2020-05-07 07:22:18.707335: Epoch: 1, Batch: 546, Loss: 0.6672, Elapsed: 7m59s
2020-05-07 07:22:37.433989: Epoch: 1, Batch: 554, Loss: 0.6895, Elapsed: 5m28s
2020-05-07 07:26:10.520867: Epoch: 1, Batch: 308, Loss: 0.7225, Elapsed: 16m21s
2020-05-07 07:26:36.497229: Epoch: 1, Batch: 547, Loss: 0.6749, Elapsed: 4m17s
2020-05-07 07:29:14.195459: Epoch: 1, Batch: 555, Loss: 0.6848, Elapsed: 6m36s
2020-05-07 07:29:23.516445: Epoch: 1, Batch: 548, Loss: 0.6686, Elapsed: 2m46s
2020-05-07 07:34:43.929644: Epoch: 1, Batch: 549, Loss: 0.6769, Elapsed: 5m20s
2020-05-07 07:35:10.400092: Epoch: 1, Batch: 556, Loss: 0.6927, Elapsed: 5m56s
2020-05-07 07:36:44.151888: Epoch: 1, Batch: 309, Loss: 0.7157, Elapsed: 10m33s
2020-05-07 07:40:51.315173: Epoch: 1, Batch: 550, Loss: 0.6891, Elapsed: 6m7s
Starting testing the validation set with 200 subgraphs!
2020-05-07 07:44:10.162191: Epoch: 1, Batch: 310, Loss: 0.7458, Elapsed: 7m26s
2020-05-07 07:44:22.562836: Epoch: 1, Batch: 557, Loss: 0.6903, Elapsed: 9m12s
2020-05-07 07:54:19.767863: Epoch: 1, Batch: 558, Loss: 0.7027, Elapsed: 9m57s
2020-05-07 07:57:27.635973: Epoch: 1, Batch: 311, Loss: 0.7205, Elapsed: 13m17s
2020-05-07 07:58:44.043807: Epoch: 1, Batch: 559, Loss: 0.6823, Elapsed: 4m24s
2020-05-07 08:03:28.331378: Epoch: 1, Batch: 560, Loss: 0.6965, Elapsed: 4m44s
2020-05-07 08:06:52.288692: Epoch: 1, Batch: 561, Loss: 0.6676, Elapsed: 3m23s
2020-05-07 08:08:16.309711: Epoch: 1, Batch: 312, Loss: 0.7474, Elapsed: 10m48s
2020-05-07 08:16:54.334300: Epoch: 1, Batch: 562, Loss: 0.7053, Elapsed: 10m2s
2020-05-07 08:18:13.831922: Epoch: 1, Batch: 313, Loss: 0.7246, Elapsed: 9m57s
2020-05-07 08:23:20.529990: Validation Test:  Loss: 0.6724,  Acc: 59.1810, AUC: 0.6233, Precision: 0.6361 -- Elapsed: 42m29s
2020-05-07 08:26:22.738216: Epoch: 1, Batch: 314, Loss: 0.7091, Elapsed: 8m8s
2020-05-07 08:26:46.272394: Epoch: 1, Batch: 563, Loss: 0.6737, Elapsed: 9m51s
2020-05-07 08:27:26.888244: Epoch: 1, Batch: 551, Loss: 0.6748, Elapsed: 4m6s
2020-05-07 08:32:38.850285: Epoch: 1, Batch: 552, Loss: 0.6766, Elapsed: 5m11s
2020-05-07 08:34:16.541098: Epoch: 1, Batch: 315, Loss: 0.7045, Elapsed: 7m53s
2020-05-07 08:34:31.964081: Epoch: 1, Batch: 564, Loss: 0.7009, Elapsed: 7m45s
2020-05-07 08:41:24.091627: Epoch: 1, Batch: 565, Loss: 0.6793, Elapsed: 6m52s
2020-05-07 08:43:03.427920: Epoch: 1, Batch: 553, Loss: 0.6757, Elapsed: 10m24s
2020-05-07 08:45:16.632800: Epoch: 1, Batch: 566, Loss: 0.6893, Elapsed: 3m52s
2020-05-07 08:46:44.550598: Epoch: 1, Batch: 316, Loss: 0.7145, Elapsed: 12m28s
2020-05-07 08:47:00.390263: Epoch: 1, Batch: 554, Loss: 0.6845, Elapsed: 3m56s
2020-05-07 08:51:43.760311: Epoch: 1, Batch: 567, Loss: 0.6673, Elapsed: 6m27s
2020-05-07 08:53:36.907754: Epoch: 1, Batch: 555, Loss: 0.6724, Elapsed: 6m36s
2020-05-07 08:57:25.096043: Epoch: 1, Batch: 568, Loss: 0.7088, Elapsed: 5m41s
2020-05-07 08:58:28.548178: Epoch: 1, Batch: 556, Loss: 0.6836, Elapsed: 4m51s
2020-05-07 09:04:48.778393: Epoch: 1, Batch: 317, Loss: 0.7008, Elapsed: 18m4s
2020-05-07 09:05:40.147618: Epoch: 1, Batch: 557, Loss: 0.6793, Elapsed: 7m11s
2020-05-07 09:07:51.379427: Epoch: 1, Batch: 569, Loss: 0.6902, Elapsed: 10m26s
2020-05-07 09:10:11.604641: Epoch: 1, Batch: 558, Loss: 0.6649, Elapsed: 4m31s
2020-05-07 09:15:46.652259: Epoch: 1, Batch: 318, Loss: 0.7139, Elapsed: 10m57s
2020-05-07 09:16:50.224848: Epoch: 1, Batch: 570, Loss: 0.6848, Elapsed: 8m58s
2020-05-07 09:18:50.881379: Epoch: 1, Batch: 559, Loss: 0.6685, Elapsed: 8m39s
2020-05-07 09:23:47.584511: Epoch: 1, Batch: 560, Loss: 0.6801, Elapsed: 4m56s
2020-05-07 09:25:41.240174: Epoch: 1, Batch: 319, Loss: 0.7212, Elapsed: 9m54s
2020-05-07 09:26:53.900727: Epoch: 1, Batch: 571, Loss: 0.6926, Elapsed: 10m3s
2020-05-07 09:29:53.151977: Epoch: 1, Batch: 561, Loss: 0.6763, Elapsed: 6m5s
2020-05-07 09:33:07.369895: Epoch: 1, Batch: 572, Loss: 0.6937, Elapsed: 6m13s
2020-05-07 09:37:47.448721: Epoch: 1, Batch: 562, Loss: 0.6678, Elapsed: 7m54s
2020-05-07 09:38:41.533066: Epoch: 1, Batch: 320, Loss: 0.6977, Elapsed: 13m0s
2020-05-07 09:44:38.218878: Epoch: 1, Batch: 573, Loss: 0.6897, Elapsed: 11m30s
2020-05-07 09:45:39.440709: Epoch: 1, Batch: 563, Loss: 0.6584, Elapsed: 7m51s
2020-05-07 09:50:59.567146: Epoch: 1, Batch: 574, Loss: 0.7138, Elapsed: 6m21s
2020-05-07 09:53:39.062393: Epoch: 1, Batch: 321, Loss: 0.7021, Elapsed: 14m57s
2020-05-07 09:55:43.004946: Epoch: 1, Batch: 564, Loss: 0.6628, Elapsed: 10m3s
2020-05-07 09:59:42.520257: Epoch: 1, Batch: 575, Loss: 0.7163, Elapsed: 8m42s
2020-05-07 10:03:12.124534: Epoch: 1, Batch: 322, Loss: 0.7215, Elapsed: 9m33s
2020-05-07 10:05:10.733664: Epoch: 1, Batch: 565, Loss: 0.6723, Elapsed: 9m27s
2020-05-07 10:07:18.405679: Epoch: 1, Batch: 576, Loss: 0.6796, Elapsed: 7m35s
2020-05-07 10:11:11.196659: Epoch: 1, Batch: 566, Loss: 0.6775, Elapsed: 6m0s
2020-05-07 10:11:26.572832: Epoch: 1, Batch: 577, Loss: 0.6930, Elapsed: 4m8s
2020-05-07 10:17:31.795288: Epoch: 1, Batch: 323, Loss: 0.7056, Elapsed: 14m19s
2020-05-07 10:17:32.006150: Epoch: 1, Batch: 578, Loss: 0.6918, Elapsed: 6m5s
2020-05-07 10:22:48.562977: Epoch: 1, Batch: 567, Loss: 0.6795, Elapsed: 11m37s
2020-05-07 10:29:04.281930: Epoch: 1, Batch: 324, Loss: 0.6956, Elapsed: 11m32s
2020-05-07 10:29:10.593074: Epoch: 1, Batch: 579, Loss: 0.7010, Elapsed: 11m38s
2020-05-07 10:30:45.264060: Epoch: 1, Batch: 568, Loss: 0.6634, Elapsed: 7m56s
2020-05-07 10:35:13.517505: Epoch: 1, Batch: 580, Loss: 0.7029, Elapsed: 6m2s
2020-05-07 10:39:18.726321: Epoch: 1, Batch: 569, Loss: 0.6708, Elapsed: 8m33s
2020-05-07 10:41:06.460897: Epoch: 1, Batch: 581, Loss: 0.6916, Elapsed: 5m52s
2020-05-07 10:46:54.488745: Epoch: 1, Batch: 582, Loss: 0.6931, Elapsed: 5m48s
2020-05-07 10:49:30.018564: Epoch: 1, Batch: 570, Loss: 0.6615, Elapsed: 10m11s
2020-05-07 10:50:35.199278: Epoch: 1, Batch: 583, Loss: 0.6767, Elapsed: 3m40s
2020-05-07 10:52:44.580959: Epoch: 1, Batch: 325, Loss: 0.7001, Elapsed: 23m40s
2020-05-07 10:56:12.525271: Epoch: 1, Batch: 571, Loss: 0.6770, Elapsed: 6m42s
2020-05-07 10:56:55.391257: Epoch: 1, Batch: 584, Loss: 0.6830, Elapsed: 6m20s
2020-05-07 11:01:32.764001: Epoch: 1, Batch: 585, Loss: 0.6897, Elapsed: 4m37s
2020-05-07 11:02:57.176476: Epoch: 1, Batch: 326, Loss: 0.7172, Elapsed: 10m12s
2020-05-07 11:03:45.244115: Epoch: 1, Batch: 572, Loss: 0.6796, Elapsed: 7m32s
2020-05-07 11:09:45.134709: Epoch: 1, Batch: 586, Loss: 0.6798, Elapsed: 8m12s
2020-05-07 11:12:02.457490: Epoch: 1, Batch: 573, Loss: 0.6814, Elapsed: 8m17s
2020-05-07 11:14:38.526824: Epoch: 1, Batch: 327, Loss: 0.7331, Elapsed: 11m41s
2020-05-07 11:15:51.505660: Epoch: 1, Batch: 587, Loss: 0.6897, Elapsed: 6m6s
2020-05-07 11:19:37.455598: Epoch: 1, Batch: 574, Loss: 0.6819, Elapsed: 7m34s
2020-05-07 11:22:54.851082: Epoch: 1, Batch: 588, Loss: 0.6804, Elapsed: 7m3s
2020-05-07 11:26:08.531302: Epoch: 1, Batch: 575, Loss: 0.6694, Elapsed: 6m31s
2020-05-07 11:29:58.621718: Epoch: 1, Batch: 328, Loss: 0.7026, Elapsed: 15m20s
2020-05-07 11:30:05.155833: Epoch: 1, Batch: 589, Loss: 0.6995, Elapsed: 7m10s
2020-05-07 11:34:19.988364: Epoch: 1, Batch: 590, Loss: 0.6855, Elapsed: 4m14s
2020-05-07 11:34:58.065028: Epoch: 1, Batch: 576, Loss: 0.6884, Elapsed: 8m49s
2020-05-07 11:37:41.437796: Epoch: 1, Batch: 591, Loss: 0.6779, Elapsed: 3m21s
2020-05-07 11:40:52.803761: Epoch: 1, Batch: 577, Loss: 0.6672, Elapsed: 5m54s
2020-05-07 11:43:22.004138: Epoch: 1, Batch: 329, Loss: 0.6816, Elapsed: 13m23s
2020-05-07 11:46:27.242561: Epoch: 1, Batch: 592, Loss: 0.6918, Elapsed: 8m45s
2020-05-07 11:47:01.718206: Epoch: 1, Batch: 578, Loss: 0.6761, Elapsed: 6m8s
2020-05-07 11:55:03.408300: Epoch: 1, Batch: 593, Loss: 0.6929, Elapsed: 8m36s
2020-05-07 11:55:26.589149: Epoch: 1, Batch: 579, Loss: 0.6631, Elapsed: 8m24s
2020-05-07 12:01:37.725594: Epoch: 1, Batch: 330, Loss: 0.6878, Elapsed: 18m15s
2020-05-07 12:01:38.138163: Epoch: 1, Batch: 594, Loss: 0.6784, Elapsed: 6m34s
2020-05-07 12:02:56.997125: Epoch: 1, Batch: 580, Loss: 0.6663, Elapsed: 7m30s
2020-05-07 12:08:07.419774: Epoch: 1, Batch: 595, Loss: 0.6747, Elapsed: 6m29s
2020-05-07 12:11:08.938227: Epoch: 1, Batch: 331, Loss: 0.7055, Elapsed: 9m31s
2020-05-07 12:11:42.779754: Epoch: 1, Batch: 581, Loss: 0.6921, Elapsed: 8m45s
2020-05-07 12:17:19.003320: Epoch: 1, Batch: 582, Loss: 0.6780, Elapsed: 5m36s
2020-05-07 12:20:08.468499: Epoch: 1, Batch: 596, Loss: 0.6864, Elapsed: 12m1s
2020-05-07 12:23:35.176184: Epoch: 1, Batch: 583, Loss: 0.6670, Elapsed: 6m16s
2020-05-07 12:26:56.739589: Epoch: 1, Batch: 332, Loss: 0.6970, Elapsed: 15m47s
2020-05-07 12:28:40.172964: Epoch: 1, Batch: 584, Loss: 0.6867, Elapsed: 5m4s
2020-05-07 12:29:58.347191: Epoch: 1, Batch: 597, Loss: 0.6933, Elapsed: 9m49s
2020-05-07 12:33:57.467126: Epoch: 1, Batch: 585, Loss: 0.6558, Elapsed: 5m17s
2020-05-07 12:35:10.919669: Epoch: 1, Batch: 333, Loss: 0.7304, Elapsed: 8m14s
2020-05-07 12:35:50.107506: Epoch: 1, Batch: 598, Loss: 0.6830, Elapsed: 5m51s
2020-05-07 12:41:12.216298: Epoch: 1, Batch: 586, Loss: 0.6566, Elapsed: 7m14s
2020-05-07 12:42:14.252028: Epoch: 1, Batch: 599, Loss: 0.6841, Elapsed: 6m24s
2020-05-07 12:45:18.624650: Epoch: 1, Batch: 587, Loss: 0.6504, Elapsed: 4m6s
2020-05-07 12:48:48.665252: Epoch: 1, Batch: 588, Loss: 0.6755, Elapsed: 3m30s
2020-05-07 12:48:43.966749: Epoch: 1, Batch: 334, Loss: 0.7041, Elapsed: 13m33s
2020-05-07 12:51:41.545077: Epoch: 1, Batch: 600, Loss: 0.7030, Elapsed: 9m27s
Starting testing the validation set with 200 subgraphs!
2020-05-07 12:53:12.375347: Epoch: 1, Batch: 589, Loss: 0.6595, Elapsed: 4m23s
2020-05-07 12:59:28.813491: Epoch: 1, Batch: 590, Loss: 0.6648, Elapsed: 6m16s
2020-05-07 13:03:04.896921: Epoch: 1, Batch: 335, Loss: 0.6907, Elapsed: 14m20s
2020-05-07 13:05:46.063715: Epoch: 1, Batch: 591, Loss: 0.6681, Elapsed: 6m17s
2020-05-07 13:11:12.773831: Epoch: 1, Batch: 336, Loss: 0.6763, Elapsed: 8m7s
2020-05-07 13:12:10.853809: Epoch: 1, Batch: 592, Loss: 0.6518, Elapsed: 6m24s
2020-05-07 13:21:15.474587: Epoch: 1, Batch: 593, Loss: 0.6668, Elapsed: 9m4s
2020-05-07 13:26:26.754553: Epoch: 1, Batch: 337, Loss: 0.6939, Elapsed: 15m13s
2020-05-07 13:28:23.516667: Epoch: 1, Batch: 594, Loss: 0.6724, Elapsed: 7m8s
2020-05-07 13:33:45.406494: Validation Test:  Loss: 0.6910,  Acc: 55.9541, AUC: 0.5692, Precision: 0.6073 -- Elapsed: 42m3s
2020-05-07 13:34:07.589340: Epoch: 1, Batch: 595, Loss: 0.6464, Elapsed: 5m44s
2020-05-07 13:40:08.094457: Epoch: 1, Batch: 601, Loss: 0.6811, Elapsed: 6m22s
2020-05-07 13:40:24.341759: Epoch: 1, Batch: 338, Loss: 0.6966, Elapsed: 13m57s
2020-05-07 13:41:11.272407: Epoch: 1, Batch: 596, Loss: 0.6660, Elapsed: 7m3s
2020-05-07 13:45:30.640920: Epoch: 1, Batch: 602, Loss: 0.6504, Elapsed: 5m22s
2020-05-07 13:50:11.038796: Epoch: 1, Batch: 597, Loss: 0.6636, Elapsed: 8m59s
2020-05-07 13:52:16.796123: Epoch: 1, Batch: 603, Loss: 0.6862, Elapsed: 6m46s
2020-05-07 13:52:36.686909: Epoch: 1, Batch: 339, Loss: 0.6949, Elapsed: 12m12s
2020-05-07 13:57:13.845941: Epoch: 1, Batch: 604, Loss: 0.6894, Elapsed: 4m57s
2020-05-07 14:00:00.998723: Epoch: 1, Batch: 605, Loss: 0.6641, Elapsed: 2m47s
2020-05-07 14:04:11.155500: Epoch: 1, Batch: 606, Loss: 0.6749, Elapsed: 4m10s
2020-05-07 14:04:16.500470: Epoch: 1, Batch: 598, Loss: 0.6693, Elapsed: 14m5s
2020-05-07 14:04:52.285798: Epoch: 1, Batch: 340, Loss: 0.7003, Elapsed: 12m15s
2020-05-07 14:08:07.238033: Epoch: 1, Batch: 607, Loss: 0.6967, Elapsed: 3m56s
2020-05-07 14:09:54.091705: Epoch: 1, Batch: 599, Loss: 0.6639, Elapsed: 5m37s
2020-05-07 14:17:03.100610: Epoch: 1, Batch: 600, Loss: 0.6673, Elapsed: 7m8s
Starting testing the validation set with 200 subgraphs!
2020-05-07 14:17:07.422745: Epoch: 1, Batch: 341, Loss: 0.7102, Elapsed: 12m15s
2020-05-07 14:18:03.612428: Epoch: 1, Batch: 608, Loss: 0.6998, Elapsed: 9m56s
2020-05-07 14:22:48.149038: Epoch: 1, Batch: 342, Loss: 0.7214, Elapsed: 5m40s
2020-05-07 14:25:58.193906: Epoch: 1, Batch: 609, Loss: 0.7132, Elapsed: 7m54s
2020-05-07 14:33:28.971107: Epoch: 1, Batch: 610, Loss: 0.6861, Elapsed: 7m30s
2020-05-07 14:36:52.174891: Epoch: 1, Batch: 343, Loss: 0.7073, Elapsed: 14m4s
2020-05-07 14:39:11.847517: Epoch: 1, Batch: 611, Loss: 0.6782, Elapsed: 5m42s
2020-05-07 14:45:17.574398: Epoch: 1, Batch: 612, Loss: 0.6998, Elapsed: 6m5s
2020-05-07 14:46:24.830257: Epoch: 1, Batch: 344, Loss: 0.7029, Elapsed: 9m32s
2020-05-07 14:50:49.064401: Epoch: 1, Batch: 613, Loss: 0.6904, Elapsed: 5m31s
2020-05-07 14:55:41.623395: Epoch: 1, Batch: 614, Loss: 0.6846, Elapsed: 4m52s
2020-05-07 14:57:53.466529: Epoch: 1, Batch: 345, Loss: 0.6835, Elapsed: 11m28s
2020-05-07 14:59:09.094826: Validation Test:  Loss: 0.6698,  Acc: 59.1576, AUC: 0.6236, Precision: 0.6365 -- Elapsed: 42m5s
2020-05-07 15:01:06.995061: Epoch: 1, Batch: 615, Loss: 0.6829, Elapsed: 5m25s
2020-05-07 15:06:17.684390: Epoch: 1, Batch: 601, Loss: 0.6695, Elapsed: 7m8s
2020-05-07 15:06:59.390628: Epoch: 1, Batch: 346, Loss: 0.6937, Elapsed: 9m5s
2020-05-07 15:08:18.344256: Epoch: 1, Batch: 616, Loss: 0.6779, Elapsed: 7m11s
2020-05-07 15:14:17.655701: Epoch: 1, Batch: 617, Loss: 0.7009, Elapsed: 5m59s
2020-05-07 15:15:54.468789: Epoch: 1, Batch: 347, Loss: 0.7116, Elapsed: 8m55s
2020-05-07 15:16:12.971261: Epoch: 1, Batch: 602, Loss: 0.6790, Elapsed: 9m55s
2020-05-07 15:24:26.625641: Epoch: 1, Batch: 618, Loss: 0.6761, Elapsed: 10m8s
2020-05-07 15:25:22.186509: Epoch: 1, Batch: 603, Loss: 0.6711, Elapsed: 9m9s
2020-05-07 15:32:18.951077: Epoch: 1, Batch: 604, Loss: 0.6752, Elapsed: 6m56s
2020-05-07 15:32:25.970754: Epoch: 1, Batch: 619, Loss: 0.6915, Elapsed: 7m59s
2020-05-07 15:32:45.902518: Epoch: 1, Batch: 348, Loss: 0.6926, Elapsed: 16m51s
2020-05-07 15:40:38.612422: Epoch: 1, Batch: 605, Loss: 0.6874, Elapsed: 8m19s
2020-05-07 15:42:10.520650: Epoch: 1, Batch: 620, Loss: 0.6807, Elapsed: 9m44s
2020-05-07 15:46:05.025154: Epoch: 1, Batch: 349, Loss: 0.6994, Elapsed: 13m19s
2020-05-07 15:46:18.789772: Epoch: 1, Batch: 621, Loss: 0.6908, Elapsed: 4m8s
2020-05-07 15:48:26.256997: Epoch: 1, Batch: 606, Loss: 0.6659, Elapsed: 7m47s
2020-05-07 15:53:40.096554: Epoch: 1, Batch: 622, Loss: 0.6942, Elapsed: 7m21s
2020-05-07 15:54:20.356728: Epoch: 1, Batch: 350, Loss: 0.6996, Elapsed: 8m15s
Starting testing the validation set with 200 subgraphs!
2020-05-07 15:56:03.507256: Epoch: 1, Batch: 607, Loss: 0.6673, Elapsed: 7m37s
2020-05-07 15:57:36.777721: Epoch: 1, Batch: 623, Loss: 0.6863, Elapsed: 3m56s
2020-05-07 16:02:39.030790: Epoch: 1, Batch: 608, Loss: 0.6726, Elapsed: 6m35s
2020-05-07 16:03:44.322278: Epoch: 1, Batch: 624, Loss: 0.6825, Elapsed: 6m7s
2020-05-07 16:07:03.014356: Epoch: 1, Batch: 625, Loss: 0.6702, Elapsed: 3m18s
2020-05-07 16:10:39.535356: Epoch: 1, Batch: 609, Loss: 0.6622, Elapsed: 8m0s
2020-05-07 16:12:33.960083: Epoch: 1, Batch: 626, Loss: 0.6756, Elapsed: 5m30s
2020-05-07 16:15:44.045700: Epoch: 1, Batch: 610, Loss: 0.6859, Elapsed: 5m4s
2020-05-07 16:19:32.614437: Epoch: 1, Batch: 627, Loss: 0.6992, Elapsed: 6m58s
2020-05-07 16:22:37.662563: Epoch: 1, Batch: 611, Loss: 0.6578, Elapsed: 6m53s
2020-05-07 16:28:01.488734: Epoch: 1, Batch: 628, Loss: 0.7084, Elapsed: 8m28s
2020-05-07 16:29:43.764376: Epoch: 1, Batch: 612, Loss: 0.6431, Elapsed: 7m5s
2020-05-07 16:33:51.461815: Epoch: 1, Batch: 629, Loss: 0.6877, Elapsed: 5m49s
2020-05-07 16:36:36.094326: Epoch: 1, Batch: 613, Loss: 0.6713, Elapsed: 6m52s
2020-05-07 16:40:22.415588: Epoch: 1, Batch: 630, Loss: 0.6927, Elapsed: 6m30s
2020-05-07 16:44:38.308233: Epoch: 1, Batch: 614, Loss: 0.6803, Elapsed: 8m2s
2020-05-07 16:48:16.931222: Validation Test:  Loss: 0.6918,  Acc: 55.7573, AUC: 0.5731, Precision: 0.6095 -- Elapsed: 53m56s
2020-05-07 16:49:57.955511: Epoch: 1, Batch: 631, Loss: 0.7199, Elapsed: 9m35s
2020-05-07 16:51:18.155876: Epoch: 1, Batch: 615, Loss: 0.6710, Elapsed: 6m39s
2020-05-07 16:58:11.834067: Epoch: 1, Batch: 616, Loss: 0.6708, Elapsed: 6m53s
2020-05-07 16:58:08.289448: Epoch: 1, Batch: 351, Loss: 0.6810, Elapsed: 9m51s
2020-05-07 16:58:45.429755: Epoch: 1, Batch: 632, Loss: 0.6939, Elapsed: 8m47s
2020-05-07 17:03:51.931973: Epoch: 1, Batch: 633, Loss: 0.6772, Elapsed: 5m6s
2020-05-07 17:03:59.973364: Epoch: 1, Batch: 617, Loss: 0.6641, Elapsed: 5m48s
2020-05-07 17:06:29.089061: Epoch: 1, Batch: 352, Loss: 0.6993, Elapsed: 8m20s
2020-05-07 17:09:31.883543: Epoch: 1, Batch: 634, Loss: 0.6874, Elapsed: 5m39s
2020-05-07 17:10:22.890134: Epoch: 1, Batch: 618, Loss: 0.6926, Elapsed: 6m22s
2020-05-07 17:14:56.645635: Epoch: 1, Batch: 635, Loss: 0.6905, Elapsed: 5m24s
2020-05-07 17:16:09.354943: Epoch: 1, Batch: 619, Loss: 0.6663, Elapsed: 5m46s
2020-05-07 17:20:46.640196: Epoch: 1, Batch: 353, Loss: 0.7008, Elapsed: 14m17s
2020-05-07 17:21:31.735108: Epoch: 1, Batch: 620, Loss: 0.6673, Elapsed: 5m22s
2020-05-07 17:25:24.671237: Epoch: 1, Batch: 636, Loss: 0.6771, Elapsed: 10m28s
2020-05-07 17:26:31.006019: Epoch: 1, Batch: 621, Loss: 0.6536, Elapsed: 4m59s
2020-05-07 17:33:21.426524: Epoch: 1, Batch: 622, Loss: 0.6691, Elapsed: 6m50s
2020-05-07 17:35:38.175605: Epoch: 1, Batch: 354, Loss: 0.6837, Elapsed: 14m51s
2020-05-07 17:36:38.797721: Epoch: 1, Batch: 637, Loss: 0.7097, Elapsed: 11m14s
2020-05-07 17:38:14.386573: Epoch: 1, Batch: 623, Loss: 0.6761, Elapsed: 4m52s
2020-05-07 17:44:21.091966: Epoch: 1, Batch: 624, Loss: 0.6469, Elapsed: 6m6s
2020-05-07 17:45:03.832020: Epoch: 1, Batch: 638, Loss: 0.6794, Elapsed: 8m25s
2020-05-07 17:50:25.880478: Epoch: 1, Batch: 625, Loss: 0.6544, Elapsed: 6m4s
2020-05-07 17:53:33.425297: Epoch: 1, Batch: 639, Loss: 0.6765, Elapsed: 8m29s
2020-05-07 17:55:42.555035: Epoch: 1, Batch: 355, Loss: 0.6805, Elapsed: 20m4s
2020-05-07 17:56:05.856942: Epoch: 1, Batch: 626, Loss: 0.6680, Elapsed: 5m39s
2020-05-07 18:00:02.162332: Epoch: 1, Batch: 640, Loss: 0.7006, Elapsed: 6m28s
2020-05-07 18:01:14.546683: Epoch: 1, Batch: 627, Loss: 0.6719, Elapsed: 5m8s
2020-05-07 18:05:21.131199: Epoch: 1, Batch: 356, Loss: 0.7014, Elapsed: 9m38s
2020-05-07 18:05:35.463550: Epoch: 1, Batch: 641, Loss: 0.7000, Elapsed: 5m33s
2020-05-07 18:08:54.357109: Epoch: 1, Batch: 642, Loss: 0.6459, Elapsed: 3m18s
2020-05-07 18:10:04.376075: Epoch: 1, Batch: 628, Loss: 0.6781, Elapsed: 8m49s
2020-05-07 18:13:25.524165: Epoch: 1, Batch: 643, Loss: 0.6892, Elapsed: 4m31s
2020-05-07 18:17:32.341057: Epoch: 1, Batch: 629, Loss: 0.6857, Elapsed: 7m27s
2020-05-07 18:19:12.122923: Epoch: 1, Batch: 357, Loss: 0.6894, Elapsed: 13m50s
2020-05-07 18:20:22.745390: Epoch: 1, Batch: 644, Loss: 0.6967, Elapsed: 6m57s
2020-05-07 18:28:47.479034: Epoch: 1, Batch: 645, Loss: 0.6776, Elapsed: 8m24s
2020-05-07 18:32:59.004078: Epoch: 1, Batch: 630, Loss: 0.6928, Elapsed: 15m26s
2020-05-07 18:34:42.128366: Epoch: 1, Batch: 358, Loss: 0.6876, Elapsed: 15m29s
2020-05-07 18:35:23.393381: Epoch: 1, Batch: 646, Loss: 0.6727, Elapsed: 6m35s
2020-05-07 18:38:46.048269: Epoch: 1, Batch: 631, Loss: 0.6635, Elapsed: 5m47s
2020-05-07 18:40:09.287855: Epoch: 1, Batch: 647, Loss: 0.7266, Elapsed: 4m45s
2020-05-07 18:43:05.772252: Epoch: 1, Batch: 632, Loss: 0.6522, Elapsed: 4m19s
2020-05-07 18:49:07.647481: Epoch: 1, Batch: 359, Loss: 0.6974, Elapsed: 14m25s
2020-05-07 18:50:27.002100: Epoch: 1, Batch: 633, Loss: 0.6857, Elapsed: 7m21s
2020-05-07 18:50:42.746983: Epoch: 1, Batch: 648, Loss: 0.7028, Elapsed: 10m33s
2020-05-07 18:55:53.218407: Epoch: 1, Batch: 634, Loss: 0.6705, Elapsed: 5m26s
2020-05-07 18:58:40.617918: Epoch: 1, Batch: 360, Loss: 0.7066, Elapsed: 9m32s
2020-05-07 18:59:09.754662: Epoch: 1, Batch: 649, Loss: 0.7050, Elapsed: 8m26s
2020-05-07 19:01:26.808025: Epoch: 1, Batch: 635, Loss: 0.6633, Elapsed: 5m33s
2020-05-07 19:07:53.406898: Epoch: 1, Batch: 636, Loss: 0.6656, Elapsed: 6m26s
2020-05-07 19:09:56.915272: Epoch: 1, Batch: 650, Loss: 0.6929, Elapsed: 10m47s
Starting testing the validation set with 200 subgraphs!
2020-05-07 19:14:46.505005: Epoch: 1, Batch: 637, Loss: 0.6550, Elapsed: 6m53s
2020-05-07 19:14:57.078063: Epoch: 1, Batch: 361, Loss: 0.6903, Elapsed: 16m16s
2020-05-07 19:23:06.358500: Epoch: 1, Batch: 638, Loss: 0.6578, Elapsed: 8m19s
2020-05-07 19:24:34.702504: Epoch: 1, Batch: 362, Loss: 0.6909, Elapsed: 9m37s
2020-05-07 19:30:25.471683: Epoch: 1, Batch: 639, Loss: 0.7161, Elapsed: 7m19s
2020-05-07 19:31:10.697127: Epoch: 1, Batch: 363, Loss: 0.6939, Elapsed: 6m35s
2020-05-07 19:33:16.401889: Epoch: 1, Batch: 640, Loss: 0.6454, Elapsed: 2m50s
2020-05-07 19:41:12.088000: Epoch: 1, Batch: 641, Loss: 0.6852, Elapsed: 7m55s
2020-05-07 19:43:49.384421: Epoch: 1, Batch: 364, Loss: 0.7012, Elapsed: 12m38s
2020-05-07 19:46:22.122540: Epoch: 1, Batch: 642, Loss: 0.6495, Elapsed: 5m10s
2020-05-07 19:51:28.136654: Epoch: 1, Batch: 643, Loss: 0.6631, Elapsed: 5m5s
2020-05-07 19:52:11.835497: Validation Test:  Loss: 0.6906,  Acc: 55.8101, AUC: 0.5704, Precision: 0.6063 -- Elapsed: 42m14s
2020-05-07 19:58:37.942440: Epoch: 1, Batch: 644, Loss: 0.6805, Elapsed: 7m9s
2020-05-07 19:58:41.138584: Epoch: 1, Batch: 651, Loss: 0.6672, Elapsed: 6m29s
2020-05-07 20:02:30.313774: Epoch: 1, Batch: 365, Loss: 0.6998, Elapsed: 18m40s
2020-05-07 20:04:51.233132: Epoch: 1, Batch: 645, Loss: 0.6631, Elapsed: 6m13s
2020-05-07 20:05:43.339729: Epoch: 1, Batch: 652, Loss: 0.6938, Elapsed: 7m2s
2020-05-07 20:13:18.149442: Epoch: 1, Batch: 646, Loss: 0.6883, Elapsed: 8m26s
2020-05-07 20:14:05.068231: Epoch: 1, Batch: 653, Loss: 0.6971, Elapsed: 8m21s
2020-05-07 20:15:54.652744: Epoch: 1, Batch: 366, Loss: 0.6792, Elapsed: 13m24s
2020-05-07 20:17:23.322010: Epoch: 1, Batch: 647, Loss: 0.6773, Elapsed: 4m5s
2020-05-07 20:20:00.276325: Epoch: 1, Batch: 654, Loss: 0.6833, Elapsed: 5m55s
2020-05-07 20:23:29.861224: Epoch: 1, Batch: 367, Loss: 0.7006, Elapsed: 7m35s
2020-05-07 20:24:03.445772: Epoch: 1, Batch: 648, Loss: 0.6549, Elapsed: 6m40s
2020-05-07 20:27:44.172195: Epoch: 1, Batch: 655, Loss: 0.6821, Elapsed: 7m43s
2020-05-07 20:32:43.326301: Epoch: 1, Batch: 649, Loss: 0.6792, Elapsed: 8m39s
2020-05-07 20:39:44.114909: Epoch: 1, Batch: 368, Loss: 0.6822, Elapsed: 16m14s
2020-05-07 20:40:32.747414: Epoch: 1, Batch: 656, Loss: 0.7080, Elapsed: 12m48s
2020-05-07 20:40:26.013672: Epoch: 1, Batch: 650, Loss: 0.6635, Elapsed: 7m42s
Starting testing the validation set with 200 subgraphs!
2020-05-07 20:45:46.586092: Epoch: 1, Batch: 657, Loss: 0.6588, Elapsed: 5m13s
2020-05-07 20:48:27.165458: Epoch: 1, Batch: 369, Loss: 0.6949, Elapsed: 8m43s
2020-05-07 20:53:40.037620: Epoch: 1, Batch: 658, Loss: 0.6857, Elapsed: 7m53s
2020-05-07 21:00:49.243552: Epoch: 1, Batch: 659, Loss: 0.6766, Elapsed: 7m9s
2020-05-07 21:04:29.238958: Epoch: 1, Batch: 370, Loss: 0.6957, Elapsed: 16m2s
2020-05-07 21:05:26.512350: Epoch: 1, Batch: 660, Loss: 0.6586, Elapsed: 4m37s
2020-05-07 21:10:33.143810: Epoch: 1, Batch: 661, Loss: 0.6723, Elapsed: 5m6s
2020-05-07 21:14:42.091775: Epoch: 1, Batch: 662, Loss: 0.6623, Elapsed: 4m8s
2020-05-07 21:15:06.728127: Epoch: 1, Batch: 371, Loss: 0.6942, Elapsed: 10m37s
2020-05-07 21:19:37.392628: Epoch: 1, Batch: 663, Loss: 0.6827, Elapsed: 4m55s
2020-05-07 21:22:36.948242: Epoch: 1, Batch: 664, Loss: 0.6642, Elapsed: 2m59s
2020-05-07 21:23:02.438581: Validation Test:  Loss: 0.6726,  Acc: 58.3258, AUC: 0.6087, Precision: 0.6239 -- Elapsed: 42m36s
2020-05-07 21:24:18.039319: Epoch: 1, Batch: 372, Loss: 0.6912, Elapsed: 9m11s
2020-05-07 21:28:46.210322: Epoch: 1, Batch: 665, Loss: 0.6942, Elapsed: 6m9s
2020-05-07 21:33:12.690872: Epoch: 1, Batch: 651, Loss: 0.6839, Elapsed: 10m10s
2020-05-07 21:37:04.521112: Epoch: 1, Batch: 666, Loss: 0.7280, Elapsed: 8m18s
2020-05-07 21:41:00.160308: Epoch: 1, Batch: 652, Loss: 0.6753, Elapsed: 7m47s
2020-05-07 21:41:39.468542: Epoch: 1, Batch: 373, Loss: 0.6883, Elapsed: 17m21s
2020-05-07 21:43:42.925261: Epoch: 1, Batch: 667, Loss: 0.6696, Elapsed: 6m38s
2020-05-07 21:46:53.188258: Epoch: 1, Batch: 653, Loss: 0.6667, Elapsed: 5m53s
2020-05-07 21:47:34.321675: Epoch: 1, Batch: 668, Loss: 0.6345, Elapsed: 3m51s
2020-05-07 21:51:27.844638: Epoch: 1, Batch: 654, Loss: 0.6451, Elapsed: 4m34s
2020-05-07 21:57:11.335536: Epoch: 1, Batch: 374, Loss: 0.6956, Elapsed: 15m31s
2020-05-07 22:00:26.358678: Epoch: 1, Batch: 655, Loss: 0.7640, Elapsed: 8m58s
2020-05-07 22:07:33.557734: Epoch: 1, Batch: 656, Loss: 0.6493, Elapsed: 7m7s
2020-05-07 22:08:23.040263: Epoch: 1, Batch: 375, Loss: 0.6845, Elapsed: 11m11s
2020-05-07 22:12:11.942554: Epoch: 1, Batch: 657, Loss: 0.6599, Elapsed: 4m38s
2020-05-07 22:15:38.860900: Epoch: 1, Batch: 658, Loss: 0.6466, Elapsed: 3m26s
2020-05-07 22:18:00.272505: Epoch: 1, Batch: 669, Loss: 0.8241, Elapsed: 30m25s
2020-05-07 22:22:01.390011: Epoch: 1, Batch: 376, Loss: 0.6905, Elapsed: 13m38s
2020-05-07 22:23:41.283567: Epoch: 1, Batch: 670, Loss: 0.6775, Elapsed: 5m40s
2020-05-07 22:24:20.832532: Epoch: 1, Batch: 659, Loss: 0.6652, Elapsed: 8m41s
2020-05-07 22:30:07.416308: Epoch: 1, Batch: 660, Loss: 0.7014, Elapsed: 5m46s
2020-05-07 22:33:24.422731: Epoch: 1, Batch: 671, Loss: 0.6956, Elapsed: 9m43s
2020-05-07 22:33:52.788923: Epoch: 1, Batch: 377, Loss: 0.7025, Elapsed: 11m51s
2020-05-07 22:37:31.368782: Epoch: 1, Batch: 661, Loss: 0.6709, Elapsed: 7m23s
2020-05-07 22:42:24.867913: Epoch: 1, Batch: 672, Loss: 0.6964, Elapsed: 9m0s
2020-05-07 22:43:52.497610: Epoch: 1, Batch: 662, Loss: 0.6708, Elapsed: 6m21s
2020-05-07 22:46:40.990322: Epoch: 1, Batch: 378, Loss: 0.7037, Elapsed: 12m48s
2020-05-07 22:51:16.188717: Epoch: 1, Batch: 673, Loss: 0.7154, Elapsed: 8m51s
2020-05-07 22:55:11.879915: Epoch: 1, Batch: 663, Loss: 0.6731, Elapsed: 11m19s
2020-05-07 22:59:21.800416: Epoch: 1, Batch: 664, Loss: 0.6575, Elapsed: 4m9s
2020-05-07 22:59:24.113469: Epoch: 1, Batch: 674, Loss: 0.7151, Elapsed: 8m7s
2020-05-07 23:03:22.696442: Epoch: 1, Batch: 665, Loss: 0.6655, Elapsed: 4m0s
2020-05-07 23:03:48.135384: Epoch: 1, Batch: 379, Loss: 0.6989, Elapsed: 17m7s
2020-05-07 23:07:06.945650: Epoch: 1, Batch: 675, Loss: 0.7097, Elapsed: 7m42s
2020-05-07 23:11:00.867023: Epoch: 1, Batch: 666, Loss: 0.6696, Elapsed: 7m38s
2020-05-07 23:12:39.445518: Epoch: 1, Batch: 676, Loss: 0.7405, Elapsed: 5m32s
2020-05-07 23:18:03.880857: Epoch: 1, Batch: 667, Loss: 0.6635, Elapsed: 7m2s
2020-05-07 23:18:09.794110: Epoch: 1, Batch: 380, Loss: 0.6905, Elapsed: 14m21s
2020-05-07 23:24:28.037032: Epoch: 1, Batch: 677, Loss: 0.7605, Elapsed: 11m48s
2020-05-07 23:27:09.703614: Epoch: 1, Batch: 668, Loss: 0.6539, Elapsed: 9m5s
2020-05-07 23:28:09.486851: Epoch: 1, Batch: 381, Loss: 0.7033, Elapsed: 9m59s
2020-05-07 23:29:03.653250: Epoch: 1, Batch: 678, Loss: 0.7182, Elapsed: 4m35s
2020-05-07 23:31:46.376859: Epoch: 1, Batch: 669, Loss: 0.6818, Elapsed: 4m36s
2020-05-07 23:41:46.401804: Epoch: 1, Batch: 679, Loss: 0.7283, Elapsed: 12m42s
2020-05-07 23:42:14.370665: Epoch: 1, Batch: 670, Loss: 0.7352, Elapsed: 10m27s
2020-05-07 23:43:01.798300: Epoch: 1, Batch: 382, Loss: 0.7039, Elapsed: 14m52s
2020-05-07 23:49:29.530808: Epoch: 1, Batch: 680, Loss: 0.7453, Elapsed: 7m43s
2020-05-07 23:50:23.570036: Epoch: 1, Batch: 671, Loss: 0.6741, Elapsed: 8m9s
2020-05-07 23:56:38.458306: Epoch: 1, Batch: 383, Loss: 0.6951, Elapsed: 13m36s
2020-05-07 23:57:00.804162: Epoch: 1, Batch: 672, Loss: 0.6988, Elapsed: 6m37s
2020-05-08 00:01:09.066062: Epoch: 1, Batch: 673, Loss: 0.6892, Elapsed: 4m8s
2020-05-08 00:04:04.163298: Epoch: 1, Batch: 681, Loss: 0.7020, Elapsed: 14m34s
2020-05-08 00:09:28.428896: Epoch: 1, Batch: 674, Loss: 0.6913, Elapsed: 8m19s
2020-05-08 00:09:37.849067: Epoch: 1, Batch: 682, Loss: 0.7750, Elapsed: 5m33s
2020-05-08 00:13:44.805688: Epoch: 1, Batch: 675, Loss: 0.6695, Elapsed: 4m16s
2020-05-08 00:13:49.143617: Epoch: 1, Batch: 384, Loss: 0.7078, Elapsed: 17m10s
2020-05-08 00:16:14.015659: Epoch: 1, Batch: 683, Loss: 0.7777, Elapsed: 6m36s
2020-05-08 00:22:16.590134: Epoch: 1, Batch: 676, Loss: 0.6743, Elapsed: 8m31s
2020-05-08 00:23:12.901006: Epoch: 1, Batch: 385, Loss: 0.7004, Elapsed: 9m23s
2020-05-08 00:27:51.734182: Epoch: 1, Batch: 684, Loss: 0.7502, Elapsed: 11m37s
2020-05-08 00:27:56.171229: Epoch: 1, Batch: 677, Loss: 0.6508, Elapsed: 5m39s
2020-05-08 00:33:13.331072: Epoch: 1, Batch: 685, Loss: 0.7662, Elapsed: 5m21s
2020-05-08 00:34:05.227166: Epoch: 1, Batch: 678, Loss: 0.6633, Elapsed: 6m9s
2020-05-08 00:34:20.258332: Epoch: 1, Batch: 386, Loss: 0.6895, Elapsed: 11m7s
2020-05-08 00:41:42.672841: Epoch: 1, Batch: 686, Loss: 0.7535, Elapsed: 8m29s
2020-05-08 00:41:55.517315: Epoch: 1, Batch: 679, Loss: 0.6739, Elapsed: 7m50s
2020-05-08 00:43:11.001594: Epoch: 1, Batch: 387, Loss: 0.7005, Elapsed: 8m50s
2020-05-08 00:47:07.375880: Epoch: 1, Batch: 687, Loss: 0.7686, Elapsed: 5m24s
2020-05-08 00:49:46.797582: Epoch: 1, Batch: 680, Loss: 0.6761, Elapsed: 7m51s
2020-05-08 00:52:20.150684: Epoch: 1, Batch: 388, Loss: 0.6955, Elapsed: 9m9s
2020-05-08 00:56:21.046248: Epoch: 1, Batch: 688, Loss: 0.7613, Elapsed: 9m13s
2020-05-08 00:56:54.647301: Epoch: 1, Batch: 681, Loss: 0.6919, Elapsed: 7m7s
2020-05-08 01:01:51.333004: Epoch: 1, Batch: 389, Loss: 0.6907, Elapsed: 9m31s
2020-05-08 01:03:28.871635: Epoch: 1, Batch: 689, Loss: 0.7749, Elapsed: 7m7s
2020-05-08 01:07:15.628819: Epoch: 1, Batch: 682, Loss: 0.6572, Elapsed: 10m20s
2020-05-08 01:10:54.444938: Epoch: 1, Batch: 690, Loss: 0.8161, Elapsed: 7m25s
2020-05-08 01:14:19.075804: Epoch: 1, Batch: 683, Loss: 0.6856, Elapsed: 7m3s
2020-05-08 01:14:15.555385: Epoch: 1, Batch: 390, Loss: 0.7015, Elapsed: 12m24s
2020-05-08 01:19:30.160939: Epoch: 1, Batch: 684, Loss: 0.6763, Elapsed: 5m11s
2020-05-08 01:21:09.291410: Epoch: 1, Batch: 691, Loss: 0.7733, Elapsed: 10m14s
2020-05-08 01:25:44.066004: Epoch: 1, Batch: 685, Loss: 0.6864, Elapsed: 6m13s
2020-05-08 01:27:35.792104: Epoch: 1, Batch: 391, Loss: 0.7073, Elapsed: 13m20s
2020-05-08 01:27:45.243414: Epoch: 1, Batch: 692, Loss: 0.7842, Elapsed: 6m35s
2020-05-08 01:33:18.651214: Epoch: 1, Batch: 693, Loss: 0.8040, Elapsed: 5m33s
2020-05-08 01:34:41.694893: Epoch: 1, Batch: 686, Loss: 0.6918, Elapsed: 8m57s
2020-05-08 01:38:25.574861: Epoch: 1, Batch: 392, Loss: 0.6849, Elapsed: 10m49s
2020-05-08 01:39:05.347109: Epoch: 1, Batch: 694, Loss: 0.7426, Elapsed: 5m46s
2020-05-08 01:41:31.791342: Epoch: 1, Batch: 687, Loss: 0.6506, Elapsed: 6m50s
2020-05-08 01:43:49.306929: Epoch: 1, Batch: 695, Loss: 0.7406, Elapsed: 4m43s
2020-05-08 01:49:00.836374: Epoch: 1, Batch: 688, Loss: 0.6464, Elapsed: 7m29s
2020-05-08 01:48:59.499036: Epoch: 1, Batch: 696, Loss: 0.7583, Elapsed: 5m10s
2020-05-08 01:50:38.143717: Epoch: 1, Batch: 393, Loss: 0.7015, Elapsed: 12m12s
2020-05-08 01:57:44.053546: Epoch: 1, Batch: 689, Loss: 0.6743, Elapsed: 8m43s
2020-05-08 01:58:02.985085: Epoch: 1, Batch: 697, Loss: 0.7325, Elapsed: 9m3s
2020-05-08 02:00:50.601153: Epoch: 1, Batch: 690, Loss: 0.6734, Elapsed: 3m6s
2020-05-08 02:03:20.251151: Epoch: 1, Batch: 698, Loss: 0.7983, Elapsed: 5m17s
2020-05-08 02:03:59.292590: Epoch: 1, Batch: 394, Loss: 0.7000, Elapsed: 13m21s
2020-05-08 02:05:50.924699: Epoch: 1, Batch: 691, Loss: 0.6717, Elapsed: 5m0s
2020-05-08 02:09:12.507586: Epoch: 1, Batch: 699, Loss: 0.7440, Elapsed: 5m52s
2020-05-08 02:14:43.847554: Epoch: 1, Batch: 395, Loss: 0.7005, Elapsed: 10m44s
2020-05-08 02:14:52.277141: Epoch: 1, Batch: 692, Loss: 0.7029, Elapsed: 9m1s
2020-05-08 02:15:21.291073: Epoch: 1, Batch: 700, Loss: 0.7484, Elapsed: 6m8s
Starting testing the validation set with 200 subgraphs!
2020-05-08 02:25:08.471780: Epoch: 1, Batch: 693, Loss: 0.6741, Elapsed: 10m16s
2020-05-08 02:28:22.746450: Epoch: 1, Batch: 396, Loss: 0.6820, Elapsed: 13m38s
2020-05-08 02:31:03.148990: Epoch: 1, Batch: 694, Loss: 0.6788, Elapsed: 5m54s
2020-05-08 02:37:54.548472: Epoch: 1, Batch: 695, Loss: 0.6547, Elapsed: 6m51s
2020-05-08 02:38:11.476892: Epoch: 1, Batch: 397, Loss: 0.7071, Elapsed: 9m48s
2020-05-08 02:43:42.614705: Epoch: 1, Batch: 696, Loss: 0.6443, Elapsed: 5m48s
2020-05-08 02:48:21.239902: Epoch: 1, Batch: 697, Loss: 0.6780, Elapsed: 4m38s
2020-05-08 02:48:39.947998: Epoch: 1, Batch: 398, Loss: 0.7026, Elapsed: 10m28s
2020-05-08 02:56:15.934081: Epoch: 1, Batch: 698, Loss: 0.6642, Elapsed: 7m54s
2020-05-08 02:57:24.211559: Validation Test:  Loss: 0.7606,  Acc: 49.0378, AUC: 0.4900, Precision: 0.5605 -- Elapsed: 42m2s
2020-05-08 02:57:51.396297: Epoch: 1, Batch: 399, Loss: 0.7082, Elapsed: 9m11s
2020-05-08 03:04:26.348887: Epoch: 1, Batch: 699, Loss: 0.6754, Elapsed: 8m10s
2020-05-08 03:04:38.287376: Epoch: 1, Batch: 701, Loss: 0.7373, Elapsed: 7m14s
2020-05-08 03:09:33.707811: Epoch: 1, Batch: 702, Loss: 0.7343, Elapsed: 4m55s
2020-05-08 03:09:41.330495: Epoch: 1, Batch: 400, Loss: 0.6935, Elapsed: 11m49s
Starting testing the validation set with 200 subgraphs!
2020-05-08 03:09:49.786152: Epoch: 1, Batch: 700, Loss: 0.6808, Elapsed: 5m23s
Starting testing the validation set with 200 subgraphs!
2020-05-08 03:16:58.150144: Epoch: 1, Batch: 703, Loss: 0.7971, Elapsed: 7m24s
2020-05-08 03:21:18.575815: Epoch: 1, Batch: 704, Loss: 0.7569, Elapsed: 4m20s
2020-05-08 03:26:00.649690: Epoch: 1, Batch: 705, Loss: 0.7748, Elapsed: 4m42s
2020-05-08 03:30:46.695196: Epoch: 1, Batch: 706, Loss: 0.7099, Elapsed: 4m46s
2020-05-08 03:37:41.596105: Epoch: 1, Batch: 707, Loss: 0.7324, Elapsed: 6m54s
2020-05-08 03:41:52.409352: Epoch: 1, Batch: 708, Loss: 0.7803, Elapsed: 4m10s
2020-05-08 03:45:15.896982: Epoch: 1, Batch: 709, Loss: 0.7439, Elapsed: 3m23s
2020-05-08 03:49:23.836742: Validation Test:  Loss: 0.6693,  Acc: 58.9111, AUC: 0.6217, Precision: 0.6315 -- Elapsed: 39m34s
2020-05-08 03:54:23.710532: Epoch: 1, Batch: 710, Loss: 0.7597, Elapsed: 9m7s
2020-05-08 03:56:58.932893: Epoch: 1, Batch: 701, Loss: 0.6641, Elapsed: 7m35s
2020-05-08 03:59:39.653579: Validation Test:  Loss: 0.7035,  Acc: 48.9354, AUC: 0.4953, Precision: 0.5454 -- Elapsed: 49m58s
2020-05-08 04:00:54.743345: Epoch: 1, Batch: 711, Loss: 0.7453, Elapsed: 6m31s
2020-05-08 04:01:24.564657: Epoch: 1, Batch: 702, Loss: 0.6635, Elapsed: 4m25s
2020-05-08 04:09:12.102410: Epoch: 1, Batch: 703, Loss: 0.6581, Elapsed: 7m47s
2020-05-08 04:11:53.880842: Epoch: 1, Batch: 712, Loss: 0.7637, Elapsed: 10m59s
2020-05-08 04:12:23.266885: Epoch: 1, Batch: 401, Loss: 0.7034, Elapsed: 12m43s
2020-05-08 04:14:55.374809: Epoch: 1, Batch: 704, Loss: 0.6549, Elapsed: 5m43s
2020-05-08 04:20:58.977237: Epoch: 1, Batch: 705, Loss: 0.6625, Elapsed: 6m3s
2020-05-08 04:23:08.441667: Epoch: 1, Batch: 402, Loss: 0.6840, Elapsed: 10m45s
2020-05-08 04:23:06.381778: Epoch: 1, Batch: 713, Loss: 0.7242, Elapsed: 11m12s
2020-05-08 04:27:01.524474: Epoch: 1, Batch: 706, Loss: 0.6692, Elapsed: 6m2s
2020-05-08 04:29:34.950807: Epoch: 1, Batch: 714, Loss: 0.7636, Elapsed: 6m28s
2020-05-08 04:35:43.252418: Epoch: 1, Batch: 715, Loss: 0.7783, Elapsed: 6m8s
2020-05-08 04:37:29.277399: Epoch: 1, Batch: 707, Loss: 0.6816, Elapsed: 10m27s
2020-05-08 04:40:48.490352: Epoch: 1, Batch: 708, Loss: 0.6693, Elapsed: 3m19s
2020-05-08 04:41:51.775652: Epoch: 1, Batch: 403, Loss: 0.7134, Elapsed: 18m43s
2020-05-08 04:43:34.425669: Epoch: 1, Batch: 709, Loss: 0.6158, Elapsed: 2m45s
2020-05-08 04:45:17.516093: Epoch: 1, Batch: 716, Loss: 0.7509, Elapsed: 9m34s
2020-05-08 04:49:46.457442: Epoch: 1, Batch: 717, Loss: 0.7316, Elapsed: 4m28s
2020-05-08 04:51:11.612664: Epoch: 1, Batch: 404, Loss: 0.6895, Elapsed: 9m19s
2020-05-08 04:51:47.017392: Epoch: 1, Batch: 710, Loss: 0.6660, Elapsed: 8m12s
2020-05-08 04:54:46.178249: Epoch: 1, Batch: 718, Loss: 0.7309, Elapsed: 4m59s
2020-05-08 05:01:21.324113: Epoch: 1, Batch: 719, Loss: 0.7571, Elapsed: 6m35s
2020-05-08 05:02:26.842193: Epoch: 1, Batch: 711, Loss: 0.6794, Elapsed: 10m39s
2020-05-08 05:08:09.148803: Epoch: 1, Batch: 712, Loss: 0.6702, Elapsed: 5m42s
2020-05-08 05:09:42.278358: Epoch: 1, Batch: 405, Loss: 0.6934, Elapsed: 18m30s
2020-05-08 05:12:17.354881: Epoch: 1, Batch: 720, Loss: 0.7759, Elapsed: 10m56s
2020-05-08 05:15:56.739041: Epoch: 1, Batch: 713, Loss: 0.6609, Elapsed: 7m47s
2020-05-08 05:16:13.876069: Epoch: 1, Batch: 721, Loss: 0.7790, Elapsed: 3m56s
2020-05-08 05:22:43.915186: Epoch: 1, Batch: 722, Loss: 0.7540, Elapsed: 6m30s
2020-05-08 05:23:46.242839: Epoch: 1, Batch: 714, Loss: 0.6904, Elapsed: 7m49s
2020-05-08 05:25:26.105942: Epoch: 1, Batch: 406, Loss: 0.6998, Elapsed: 15m43s
2020-05-08 05:27:48.634417: Epoch: 1, Batch: 723, Loss: 0.7462, Elapsed: 5m4s
2020-05-08 05:30:24.753519: Epoch: 1, Batch: 715, Loss: 0.6718, Elapsed: 6m38s
2020-05-08 05:34:48.740950: Epoch: 1, Batch: 724, Loss: 0.7465, Elapsed: 7m0s
2020-05-08 05:35:32.417413: Epoch: 1, Batch: 716, Loss: 0.6642, Elapsed: 5m7s
2020-05-08 05:40:17.481092: Epoch: 1, Batch: 725, Loss: 0.7433, Elapsed: 5m28s
2020-05-08 05:41:49.268883: Epoch: 1, Batch: 717, Loss: 0.6560, Elapsed: 6m16s
2020-05-08 05:45:43.274390: Epoch: 1, Batch: 407, Loss: 0.6969, Elapsed: 20m17s
2020-05-08 05:47:03.685390: Epoch: 1, Batch: 726, Loss: 0.7732, Elapsed: 6m46s
2020-05-08 05:52:32.549974: Epoch: 1, Batch: 718, Loss: 0.6465, Elapsed: 10m43s
2020-05-08 05:54:17.384598: Epoch: 1, Batch: 727, Loss: 0.7439, Elapsed: 7m13s
2020-05-08 05:58:54.265312: Epoch: 1, Batch: 408, Loss: 0.6982, Elapsed: 13m10s
2020-05-08 05:59:38.204368: Epoch: 1, Batch: 719, Loss: 0.6696, Elapsed: 7m5s
2020-05-08 06:04:23.647984: Epoch: 1, Batch: 728, Loss: 0.7257, Elapsed: 10m6s
2020-05-08 06:07:04.372548: Epoch: 1, Batch: 720, Loss: 0.6706, Elapsed: 7m26s
2020-05-08 06:07:34.565770: Epoch: 1, Batch: 409, Loss: 0.7066, Elapsed: 8m40s
2020-05-08 06:13:55.265364: Epoch: 1, Batch: 729, Loss: 0.7427, Elapsed: 9m31s
2020-05-08 06:15:57.918239: Epoch: 1, Batch: 721, Loss: 0.6797, Elapsed: 8m53s
2020-05-08 06:20:04.509994: Epoch: 1, Batch: 730, Loss: 0.7514, Elapsed: 6m9s
2020-05-08 06:22:28.394819: Epoch: 1, Batch: 410, Loss: 0.7110, Elapsed: 14m53s
2020-05-08 06:26:33.079407: Epoch: 1, Batch: 722, Loss: 0.6882, Elapsed: 10m35s
2020-05-08 06:26:56.829905: Epoch: 1, Batch: 731, Loss: 0.7541, Elapsed: 6m52s
2020-05-08 06:31:33.061960: Epoch: 1, Batch: 411, Loss: 0.7080, Elapsed: 9m4s
2020-05-08 06:33:28.645590: Epoch: 1, Batch: 723, Loss: 0.6788, Elapsed: 6m55s
2020-05-08 06:34:45.077855: Epoch: 1, Batch: 732, Loss: 0.7415, Elapsed: 7m48s
2020-05-08 06:40:43.602766: Epoch: 1, Batch: 733, Loss: 0.7043, Elapsed: 5m58s
2020-05-08 06:40:57.199706: Epoch: 1, Batch: 412, Loss: 0.7034, Elapsed: 9m24s
2020-05-08 06:45:14.193134: Epoch: 1, Batch: 724, Loss: 0.6704, Elapsed: 11m45s
2020-05-08 06:48:05.510112: Epoch: 1, Batch: 734, Loss: 0.7373, Elapsed: 7m21s
2020-05-08 06:52:04.762076: Epoch: 1, Batch: 725, Loss: 0.6452, Elapsed: 6m50s
2020-05-08 06:55:35.354635: Epoch: 1, Batch: 735, Loss: 0.7452, Elapsed: 7m29s
2020-05-08 06:58:39.776942: Epoch: 1, Batch: 726, Loss: 0.6712, Elapsed: 6m34s
2020-05-08 06:59:49.402610: Epoch: 1, Batch: 413, Loss: 0.7072, Elapsed: 18m52s
2020-05-08 07:04:03.791482: Epoch: 1, Batch: 727, Loss: 0.6810, Elapsed: 5m23s
2020-05-08 07:04:08.933882: Epoch: 1, Batch: 736, Loss: 0.7416, Elapsed: 8m33s
2020-05-08 07:08:20.407105: Epoch: 1, Batch: 737, Loss: 0.7675, Elapsed: 4m11s
2020-05-08 07:11:35.164810: Epoch: 1, Batch: 728, Loss: 0.6714, Elapsed: 7m31s
2020-05-08 07:13:47.845401: Epoch: 1, Batch: 414, Loss: 0.7118, Elapsed: 13m58s
2020-05-08 07:15:24.327035: Epoch: 1, Batch: 738, Loss: 0.7182, Elapsed: 7m3s
2020-05-08 07:16:23.967417: Epoch: 1, Batch: 729, Loss: 0.6467, Elapsed: 4m48s
2020-05-08 07:21:20.381960: Epoch: 1, Batch: 730, Loss: 0.6528, Elapsed: 4m56s
2020-05-08 07:22:48.759077: Epoch: 1, Batch: 415, Loss: 0.7217, Elapsed: 9m0s
2020-05-08 07:24:16.106908: Epoch: 1, Batch: 739, Loss: 0.7375, Elapsed: 8m51s
2020-05-08 07:31:19.352925: Epoch: 1, Batch: 731, Loss: 0.6594, Elapsed: 9m58s
2020-05-08 07:32:01.734054: Epoch: 1, Batch: 740, Loss: 0.7210, Elapsed: 7m45s
2020-05-08 07:37:25.029705: Epoch: 1, Batch: 741, Loss: 0.7344, Elapsed: 5m23s
2020-05-08 07:38:18.866455: Epoch: 1, Batch: 732, Loss: 0.6706, Elapsed: 6m59s
2020-05-08 07:41:03.177567: Epoch: 1, Batch: 416, Loss: 0.6979, Elapsed: 18m14s
2020-05-08 07:46:49.596354: Epoch: 1, Batch: 733, Loss: 0.6908, Elapsed: 8m30s
2020-05-08 07:48:22.364722: Epoch: 1, Batch: 742, Loss: 0.8189, Elapsed: 10m57s
2020-05-08 07:50:48.553667: Epoch: 1, Batch: 417, Loss: 0.7120, Elapsed: 9m45s
2020-05-08 07:55:14.301422: Epoch: 1, Batch: 734, Loss: 0.6757, Elapsed: 8m24s
2020-05-08 07:55:23.525878: Epoch: 1, Batch: 743, Loss: 0.7323, Elapsed: 7m1s
2020-05-08 07:55:50.270242: Epoch: 1, Batch: 418, Loss: 0.7188, Elapsed: 5m1s
2020-05-08 08:01:51.996401: Epoch: 1, Batch: 419, Loss: 0.7019, Elapsed: 6m1s
2020-05-08 08:02:55.862061: Epoch: 1, Batch: 735, Loss: 0.6639, Elapsed: 7m41s
2020-05-08 08:04:45.752745: Epoch: 1, Batch: 744, Loss: 0.7483, Elapsed: 9m22s
2020-05-08 08:08:21.745974: Epoch: 1, Batch: 736, Loss: 0.6768, Elapsed: 5m25s
2020-05-08 08:10:09.289902: Epoch: 1, Batch: 745, Loss: 0.7382, Elapsed: 5m23s
2020-05-08 08:16:06.555581: Epoch: 1, Batch: 737, Loss: 0.6607, Elapsed: 7m44s
2020-05-08 08:17:40.277742: Epoch: 1, Batch: 746, Loss: 0.7133, Elapsed: 7m30s
2020-05-08 08:23:10.832649: Epoch: 1, Batch: 747, Loss: 0.7101, Elapsed: 5m30s
2020-05-08 08:24:14.322105: Epoch: 1, Batch: 738, Loss: 0.6562, Elapsed: 8m7s
2020-05-08 08:29:21.594325: Epoch: 1, Batch: 420, Loss: 0.7075, Elapsed: 27m29s
2020-05-08 08:29:27.678796: Epoch: 1, Batch: 748, Loss: 0.7217, Elapsed: 6m16s
2020-05-08 08:32:32.259342: Epoch: 1, Batch: 739, Loss: 0.6629, Elapsed: 8m17s
2020-05-08 08:38:53.925362: Epoch: 1, Batch: 740, Loss: 0.6778, Elapsed: 6m21s
2020-05-08 08:39:31.838472: Epoch: 1, Batch: 749, Loss: 0.7467, Elapsed: 10m4s
2020-05-08 08:44:51.586555: Epoch: 1, Batch: 421, Loss: 0.6935, Elapsed: 15m29s
2020-05-08 08:45:59.055765: Epoch: 1, Batch: 741, Loss: 0.6602, Elapsed: 7m5s
2020-05-08 08:48:16.491310: Epoch: 1, Batch: 750, Loss: 0.7347, Elapsed: 8m44s
Starting testing the validation set with 200 subgraphs!
2020-05-08 08:50:20.918719: Epoch: 1, Batch: 742, Loss: 0.6543, Elapsed: 4m21s
2020-05-08 08:53:54.008066: Epoch: 1, Batch: 422, Loss: 0.7151, Elapsed: 9m2s
2020-05-08 09:01:16.233094: Epoch: 1, Batch: 743, Loss: 0.6545, Elapsed: 10m55s
2020-05-08 09:05:36.928281: Epoch: 1, Batch: 423, Loss: 0.6925, Elapsed: 11m42s
2020-05-08 09:08:40.601756: Epoch: 1, Batch: 744, Loss: 0.6811, Elapsed: 7m24s
2020-05-08 09:14:59.510402: Epoch: 1, Batch: 745, Loss: 0.6576, Elapsed: 6m18s
2020-05-08 09:18:34.395712: Epoch: 1, Batch: 424, Loss: 0.7133, Elapsed: 12m57s
2020-05-08 09:22:42.070921: Epoch: 1, Batch: 746, Loss: 0.6843, Elapsed: 7m42s
2020-05-08 09:30:15.102315: Validation Test:  Loss: 0.7357,  Acc: 52.8409, AUC: 0.5280, Precision: 0.5845 -- Elapsed: 41m58s
2020-05-08 09:30:18.278444: Epoch: 1, Batch: 747, Loss: 0.6750, Elapsed: 7m36s
2020-05-08 09:36:48.964380: Epoch: 1, Batch: 748, Loss: 0.6725, Elapsed: 6m30s
2020-05-08 09:37:57.102797: Epoch: 1, Batch: 425, Loss: 0.7018, Elapsed: 19m22s
2020-05-08 09:38:51.918875: Epoch: 1, Batch: 751, Loss: 0.7454, Elapsed: 8m36s
2020-05-08 09:40:27.367878: Epoch: 1, Batch: 749, Loss: 0.6385, Elapsed: 3m38s
2020-05-08 09:44:55.135340: Epoch: 1, Batch: 752, Loss: 0.7508, Elapsed: 6m3s
2020-05-08 09:47:50.283975: Epoch: 1, Batch: 750, Loss: 0.6567, Elapsed: 7m22s
Starting testing the validation set with 200 subgraphs!
2020-05-08 09:50:48.938058: Epoch: 1, Batch: 426, Loss: 0.6995, Elapsed: 12m51s
2020-05-08 09:52:06.675178: Epoch: 1, Batch: 753, Loss: 0.7327, Elapsed: 7m11s
2020-05-08 09:57:43.956862: Epoch: 1, Batch: 754, Loss: 0.7058, Elapsed: 5m37s
2020-05-08 10:03:53.254990: Epoch: 1, Batch: 755, Loss: 0.7099, Elapsed: 6m9s
2020-05-08 10:06:25.980984: Epoch: 1, Batch: 427, Loss: 0.6868, Elapsed: 15m37s
2020-05-08 10:07:35.953814: Epoch: 1, Batch: 756, Loss: 0.7580, Elapsed: 3m42s
2020-05-08 10:12:42.141755: Epoch: 1, Batch: 428, Loss: 0.6971, Elapsed: 6m16s
2020-05-08 10:16:24.754422: Epoch: 1, Batch: 757, Loss: 0.7407, Elapsed: 8m48s
2020-05-08 10:17:12.787172: Epoch: 1, Batch: 429, Loss: 0.7088, Elapsed: 4m30s
2020-05-08 10:18:44.068277: Epoch: 1, Batch: 758, Loss: 0.7400, Elapsed: 2m19s
2020-05-08 10:25:38.014545: Epoch: 1, Batch: 430, Loss: 0.7082, Elapsed: 8m25s
2020-05-08 10:26:32.585140: Epoch: 1, Batch: 759, Loss: 0.7212, Elapsed: 7m48s
2020-05-08 10:30:10.296350: Validation Test:  Loss: 0.6689,  Acc: 58.8868, AUC: 0.6163, Precision: 0.6310 -- Elapsed: 42m19s
2020-05-08 10:35:50.899115: Epoch: 1, Batch: 751, Loss: 0.6547, Elapsed: 5m40s
2020-05-08 10:36:08.072772: Epoch: 1, Batch: 431, Loss: 0.7131, Elapsed: 10m30s
2020-05-08 10:37:53.297457: Epoch: 1, Batch: 760, Loss: 0.7200, Elapsed: 11m20s
2020-05-08 10:47:41.336440: Epoch: 1, Batch: 432, Loss: 0.7088, Elapsed: 11m33s
2020-05-08 10:48:26.070618: Epoch: 1, Batch: 752, Loss: 0.6891, Elapsed: 12m35s
2020-05-08 10:48:26.078974: Epoch: 1, Batch: 761, Loss: 0.7344, Elapsed: 10m32s
2020-05-08 10:54:52.314865: Epoch: 1, Batch: 753, Loss: 0.6576, Elapsed: 6m26s
2020-05-08 10:57:05.995974: Epoch: 1, Batch: 762, Loss: 0.7253, Elapsed: 8m39s
2020-05-08 10:57:45.283339: Epoch: 1, Batch: 433, Loss: 0.6958, Elapsed: 10m3s
2020-05-08 11:01:46.928196: Epoch: 1, Batch: 754, Loss: 0.6515, Elapsed: 6m54s
2020-05-08 11:05:47.749596: Epoch: 1, Batch: 763, Loss: 0.7103, Elapsed: 8m41s
2020-05-08 11:08:04.478822: Epoch: 1, Batch: 434, Loss: 0.7170, Elapsed: 10m19s
2020-05-08 11:08:59.100737: Epoch: 1, Batch: 755, Loss: 0.6940, Elapsed: 7m12s
2020-05-08 11:15:41.149819: Epoch: 1, Batch: 764, Loss: 0.7133, Elapsed: 9m53s
2020-05-08 11:17:20.755170: Epoch: 1, Batch: 756, Loss: 0.6536, Elapsed: 8m21s
2020-05-08 11:19:47.774334: Epoch: 1, Batch: 765, Loss: 0.6922, Elapsed: 4m6s
2020-05-08 11:24:09.686774: Epoch: 1, Batch: 757, Loss: 0.6604, Elapsed: 6m48s
2020-05-08 11:25:55.509924: Epoch: 1, Batch: 435, Loss: 0.7073, Elapsed: 17m51s
2020-05-08 11:28:20.301064: Epoch: 1, Batch: 766, Loss: 0.7163, Elapsed: 8m32s
2020-05-08 11:32:45.120918: Epoch: 1, Batch: 758, Loss: 0.6818, Elapsed: 8m35s
2020-05-08 11:36:19.586298: Epoch: 1, Batch: 767, Loss: 0.7276, Elapsed: 7m59s
2020-05-08 11:37:36.886035: Epoch: 1, Batch: 436, Loss: 0.6907, Elapsed: 11m41s
2020-05-08 11:38:59.579847: Epoch: 1, Batch: 759, Loss: 0.6731, Elapsed: 6m14s
2020-05-08 11:42:40.486155: Epoch: 1, Batch: 768, Loss: 0.7510, Elapsed: 6m20s
2020-05-08 11:46:40.164242: Epoch: 1, Batch: 760, Loss: 0.6795, Elapsed: 7m40s
2020-05-08 11:46:36.968572: Epoch: 1, Batch: 437, Loss: 0.6981, Elapsed: 9m0s
2020-05-08 11:46:58.841311: Epoch: 1, Batch: 769, Loss: 0.7600, Elapsed: 4m18s
2020-05-08 11:54:31.237799: Epoch: 1, Batch: 761, Loss: 0.6780, Elapsed: 7m51s
2020-05-08 11:54:38.126456: Epoch: 1, Batch: 770, Loss: 0.7195, Elapsed: 7m39s
2020-05-08 12:03:20.375442: Epoch: 1, Batch: 438, Loss: 0.6838, Elapsed: 16m43s
2020-05-08 12:03:44.527708: Epoch: 1, Batch: 771, Loss: 0.7202, Elapsed: 9m6s
2020-05-08 12:06:20.816522: Epoch: 1, Batch: 762, Loss: 0.6663, Elapsed: 11m49s
2020-05-08 12:08:02.253877: Epoch: 1, Batch: 772, Loss: 0.7155, Elapsed: 4m17s
2020-05-08 12:11:30.587850: Epoch: 1, Batch: 439, Loss: 0.7089, Elapsed: 8m10s
2020-05-08 12:13:55.021815: Epoch: 1, Batch: 763, Loss: 0.6970, Elapsed: 7m34s
2020-05-08 12:18:19.055154: Epoch: 1, Batch: 773, Loss: 0.7326, Elapsed: 10m16s
2020-05-08 12:18:42.791910: Epoch: 1, Batch: 764, Loss: 0.6475, Elapsed: 4m47s
2020-05-08 12:25:21.453088: Epoch: 1, Batch: 440, Loss: 0.7075, Elapsed: 13m50s
2020-05-08 12:25:56.214566: Epoch: 1, Batch: 774, Loss: 0.7135, Elapsed: 7m37s
2020-05-08 12:29:24.133556: Epoch: 1, Batch: 765, Loss: 0.6697, Elapsed: 10m41s
2020-05-08 12:31:33.752252: Epoch: 1, Batch: 775, Loss: 0.7043, Elapsed: 5m37s
2020-05-08 12:33:00.302154: Epoch: 1, Batch: 441, Loss: 0.6946, Elapsed: 7m38s
2020-05-08 12:38:28.115278: Epoch: 1, Batch: 766, Loss: 0.6790, Elapsed: 9m3s
2020-05-08 12:41:54.626395: Epoch: 1, Batch: 776, Loss: 0.7515, Elapsed: 10m20s
2020-05-08 12:44:06.257700: Epoch: 1, Batch: 442, Loss: 0.6892, Elapsed: 11m5s
2020-05-08 12:47:14.113835: Epoch: 1, Batch: 767, Loss: 0.6915, Elapsed: 8m45s
2020-05-08 12:53:55.923094: Epoch: 1, Batch: 777, Loss: 0.7117, Elapsed: 12m1s
2020-05-08 12:54:35.123295: Epoch: 1, Batch: 768, Loss: 0.6592, Elapsed: 7m20s
2020-05-08 12:59:31.515138: Epoch: 1, Batch: 443, Loss: 0.6869, Elapsed: 15m25s
2020-05-08 13:01:28.726048: Epoch: 1, Batch: 769, Loss: 0.6499, Elapsed: 6m53s
2020-05-08 13:01:45.190097: Epoch: 1, Batch: 778, Loss: 0.7250, Elapsed: 7m49s
2020-05-08 13:06:53.972171: Epoch: 1, Batch: 770, Loss: 0.6880, Elapsed: 5m25s
2020-05-08 13:10:55.671560: Epoch: 1, Batch: 779, Loss: 0.7162, Elapsed: 9m10s
2020-05-08 13:11:39.231758: Epoch: 1, Batch: 444, Loss: 0.6751, Elapsed: 12m7s
2020-05-08 13:13:52.527045: Epoch: 1, Batch: 771, Loss: 0.6673, Elapsed: 6m58s
2020-05-08 13:17:01.940188: Epoch: 1, Batch: 780, Loss: 0.7362, Elapsed: 6m6s
2020-05-08 13:20:58.068478: Epoch: 1, Batch: 772, Loss: 0.6811, Elapsed: 7m5s
2020-05-08 13:22:55.777140: Epoch: 1, Batch: 445, Loss: 0.7101, Elapsed: 11m16s
2020-05-08 13:24:44.742183: Epoch: 1, Batch: 773, Loss: 0.6211, Elapsed: 3m46s
2020-05-08 13:25:56.891190: Epoch: 1, Batch: 781, Loss: 0.7273, Elapsed: 8m54s
2020-05-08 13:31:25.799870: Epoch: 1, Batch: 774, Loss: 0.6600, Elapsed: 6m41s
2020-05-08 13:31:49.521291: Epoch: 1, Batch: 782, Loss: 0.7243, Elapsed: 5m52s
2020-05-08 13:36:08.801342: Epoch: 1, Batch: 446, Loss: 0.6906, Elapsed: 13m13s
2020-05-08 13:38:05.358323: Epoch: 1, Batch: 783, Loss: 0.7203, Elapsed: 6m15s
2020-05-08 13:38:16.006167: Epoch: 1, Batch: 775, Loss: 0.6651, Elapsed: 6m50s
2020-05-08 13:42:21.866350: Epoch: 1, Batch: 784, Loss: 0.7222, Elapsed: 4m16s
2020-05-08 13:46:57.213661: Epoch: 1, Batch: 447, Loss: 0.7039, Elapsed: 10m48s
2020-05-08 13:48:17.203266: Epoch: 1, Batch: 776, Loss: 0.6905, Elapsed: 10m1s
2020-05-08 13:52:56.426377: Epoch: 1, Batch: 785, Loss: 0.7234, Elapsed: 10m34s
2020-05-08 13:56:58.056345: Epoch: 1, Batch: 777, Loss: 0.6701, Elapsed: 8m40s
2020-05-08 14:00:52.488827: Epoch: 1, Batch: 786, Loss: 0.7201, Elapsed: 7m56s
2020-05-08 14:01:11.005087: Epoch: 1, Batch: 448, Loss: 0.7013, Elapsed: 14m13s
2020-05-08 14:04:51.946010: Epoch: 1, Batch: 778, Loss: 0.6848, Elapsed: 7m53s
2020-05-08 14:07:00.836239: Epoch: 1, Batch: 787, Loss: 0.7175, Elapsed: 6m8s
2020-05-08 14:10:38.513716: Epoch: 1, Batch: 779, Loss: 0.6537, Elapsed: 5m46s
2020-05-08 14:14:06.459131: Epoch: 1, Batch: 788, Loss: 0.7438, Elapsed: 7m5s
2020-05-08 14:14:37.488788: Epoch: 1, Batch: 449, Loss: 0.6879, Elapsed: 13m26s
2020-05-08 14:16:45.263020: Epoch: 1, Batch: 780, Loss: 0.6633, Elapsed: 6m6s
2020-05-08 14:19:17.080333: Epoch: 1, Batch: 789, Loss: 0.7346, Elapsed: 5m10s
2020-05-08 14:22:14.985293: Epoch: 1, Batch: 781, Loss: 0.6575, Elapsed: 5m29s
2020-05-08 14:25:01.220684: Epoch: 1, Batch: 450, Loss: 0.6920, Elapsed: 10m23s
Starting testing the validation set with 200 subgraphs!
2020-05-08 14:27:42.928939: Epoch: 1, Batch: 790, Loss: 0.7291, Elapsed: 8m25s
2020-05-08 14:27:54.198218: Epoch: 1, Batch: 782, Loss: 0.6641, Elapsed: 5m39s
2020-05-08 14:32:10.718705: Epoch: 1, Batch: 783, Loss: 0.6668, Elapsed: 4m16s
2020-05-08 14:32:55.081700: Epoch: 1, Batch: 791, Loss: 0.7231, Elapsed: 5m12s
2020-05-08 14:39:00.379162: Epoch: 1, Batch: 784, Loss: 0.6801, Elapsed: 6m49s
2020-05-08 14:39:38.646218: Epoch: 1, Batch: 792, Loss: 0.7225, Elapsed: 6m43s
2020-05-08 14:46:34.235267: Epoch: 1, Batch: 793, Loss: 0.7550, Elapsed: 6m55s
2020-05-08 14:47:06.226195: Epoch: 1, Batch: 785, Loss: 0.6685, Elapsed: 8m5s
2020-05-08 14:52:17.106086: Epoch: 1, Batch: 794, Loss: 0.7340, Elapsed: 5m42s
2020-05-08 14:54:02.925069: Epoch: 1, Batch: 786, Loss: 0.6859, Elapsed: 6m56s
2020-05-08 14:57:05.581608: Epoch: 1, Batch: 795, Loss: 0.7164, Elapsed: 4m48s
2020-05-08 15:02:09.928499: Epoch: 1, Batch: 787, Loss: 0.6742, Elapsed: 8m6s
2020-05-08 15:03:45.073535: Epoch: 1, Batch: 796, Loss: 0.7381, Elapsed: 6m39s
2020-05-08 15:07:38.455586: Epoch: 1, Batch: 788, Loss: 0.6780, Elapsed: 5m28s
2020-05-08 15:08:18.088003: Epoch: 1, Batch: 797, Loss: 0.7119, Elapsed: 4m32s
2020-05-08 15:12:07.678363: Epoch: 1, Batch: 789, Loss: 0.6571, Elapsed: 4m29s
2020-05-08 15:12:24.992884: Epoch: 1, Batch: 798, Loss: 0.7244, Elapsed: 4m6s
2020-05-08 15:18:25.900564: Epoch: 1, Batch: 790, Loss: 0.6736, Elapsed: 6m18s
2020-05-08 15:18:57.077002: Validation Test:  Loss: 0.6946,  Acc: 55.3100, AUC: 0.5686, Precision: 0.6033 -- Elapsed: 53m55s
2020-05-08 15:20:18.053258: Epoch: 1, Batch: 799, Loss: 0.7153, Elapsed: 7m53s
2020-05-08 15:25:00.887966: Epoch: 1, Batch: 451, Loss: 0.7352, Elapsed: 6m3s
2020-05-08 15:26:40.434622: Epoch: 1, Batch: 791, Loss: 0.6892, Elapsed: 8m14s
2020-05-08 15:27:20.672728: Epoch: 1, Batch: 800, Loss: 0.7145, Elapsed: 7m2s
Starting testing the validation set with 200 subgraphs!
2020-05-08 15:30:03.935640: Epoch: 1, Batch: 452, Loss: 0.6997, Elapsed: 5m3s
2020-05-08 15:33:36.645853: Epoch: 1, Batch: 792, Loss: 0.6942, Elapsed: 6m56s
2020-05-08 15:38:21.531651: Epoch: 1, Batch: 793, Loss: 0.6711, Elapsed: 4m44s
2020-05-08 15:45:24.120074: Epoch: 1, Batch: 794, Loss: 0.6958, Elapsed: 7m2s
2020-05-08 15:46:24.379402: Epoch: 1, Batch: 453, Loss: 0.6920, Elapsed: 16m20s
2020-05-08 15:50:19.530068: Epoch: 1, Batch: 795, Loss: 0.6655, Elapsed: 4m55s
2020-05-08 15:58:44.124517: Epoch: 1, Batch: 796, Loss: 0.6881, Elapsed: 8m24s
2020-05-08 15:58:37.276446: Epoch: 1, Batch: 454, Loss: 0.6971, Elapsed: 12m12s
2020-05-08 16:06:09.919998: Epoch: 1, Batch: 455, Loss: 0.7248, Elapsed: 7m32s
2020-05-08 16:06:18.324979: Epoch: 1, Batch: 797, Loss: 0.6764, Elapsed: 7m34s
2020-05-08 16:09:26.912404: Validation Test:  Loss: 0.7217,  Acc: 52.1572, AUC: 0.5268, Precision: 0.5869 -- Elapsed: 42m6s
2020-05-08 16:13:11.554989: Epoch: 1, Batch: 798, Loss: 0.6619, Elapsed: 6m53s
2020-05-08 16:15:11.162867: Epoch: 1, Batch: 456, Loss: 0.6967, Elapsed: 9m1s
2020-05-08 16:20:49.868631: Epoch: 1, Batch: 801, Loss: 0.7104, Elapsed: 11m22s
2020-05-08 16:20:57.506499: Epoch: 1, Batch: 799, Loss: 0.6875, Elapsed: 7m45s
2020-05-08 16:27:59.739177: Epoch: 1, Batch: 457, Loss: 0.7060, Elapsed: 12m48s
2020-05-08 16:28:10.967291: Epoch: 1, Batch: 802, Loss: 0.7257, Elapsed: 7m21s
2020-05-08 16:30:35.782735: Epoch: 1, Batch: 800, Loss: 0.6681, Elapsed: 9m38s
Starting testing the validation set with 200 subgraphs!
2020-05-08 16:33:05.664656: Epoch: 1, Batch: 803, Loss: 0.6868, Elapsed: 4m54s
2020-05-08 16:40:34.087226: Epoch: 1, Batch: 458, Loss: 0.6915, Elapsed: 12m34s
2020-05-08 16:41:24.277491: Epoch: 1, Batch: 804, Loss: 0.7500, Elapsed: 8m18s
2020-05-08 16:47:50.835601: Epoch: 1, Batch: 805, Loss: 0.7139, Elapsed: 6m26s
2020-05-08 16:49:43.577206: Epoch: 1, Batch: 459, Loss: 0.7034, Elapsed: 9m9s
2020-05-08 16:51:48.292856: Epoch: 1, Batch: 806, Loss: 0.7123, Elapsed: 3m57s
2020-05-08 16:58:50.121958: Epoch: 1, Batch: 807, Loss: 0.7043, Elapsed: 7m1s
2020-05-08 17:00:52.082069: Epoch: 1, Batch: 460, Loss: 0.6922, Elapsed: 11m8s
2020-05-08 17:03:46.430571: Epoch: 1, Batch: 808, Loss: 0.7216, Elapsed: 4m56s
2020-05-08 17:07:12.158718: Epoch: 1, Batch: 809, Loss: 0.7159, Elapsed: 3m25s
2020-05-08 17:12:51.085097: Validation Test:  Loss: 0.6751,  Acc: 57.8287, AUC: 0.6027, Precision: 0.6275 -- Elapsed: 42m15s
2020-05-08 17:12:59.609234: Epoch: 1, Batch: 461, Loss: 0.6915, Elapsed: 12m7s
2020-05-08 17:15:31.042423: Epoch: 1, Batch: 810, Loss: 0.6991, Elapsed: 8m18s
2020-05-08 17:19:11.320723: Epoch: 1, Batch: 801, Loss: 0.6608, Elapsed: 6m20s
2020-05-08 17:22:12.796622: Epoch: 1, Batch: 462, Loss: 0.7006, Elapsed: 9m13s
2020-05-08 17:23:37.448611: Epoch: 1, Batch: 811, Loss: 0.7046, Elapsed: 8m6s
2020-05-08 17:26:11.390457: Epoch: 1, Batch: 802, Loss: 0.6653, Elapsed: 7m0s
2020-05-08 17:27:46.945940: Epoch: 1, Batch: 812, Loss: 0.7509, Elapsed: 4m9s
2020-05-08 17:34:05.755851: Epoch: 1, Batch: 813, Loss: 0.7289, Elapsed: 6m18s
2020-05-08 17:34:03.803758: Epoch: 1, Batch: 803, Loss: 0.6744, Elapsed: 7m52s
2020-05-08 17:41:11.352233: Epoch: 1, Batch: 463, Loss: 0.7068, Elapsed: 18m58s
2020-05-08 17:42:13.866735: Epoch: 1, Batch: 804, Loss: 0.6662, Elapsed: 8m10s
2020-05-08 17:45:23.972256: Epoch: 1, Batch: 814, Loss: 0.7189, Elapsed: 11m18s
2020-05-08 17:49:36.515853: Epoch: 1, Batch: 815, Loss: 0.6896, Elapsed: 4m12s
2020-05-08 17:49:51.369669: Epoch: 1, Batch: 805, Loss: 0.6768, Elapsed: 7m37s
2020-05-08 17:52:58.243750: Epoch: 1, Batch: 816, Loss: 0.7177, Elapsed: 3m21s
2020-05-08 17:56:49.196848: Epoch: 1, Batch: 464, Loss: 0.6942, Elapsed: 15m37s
2020-05-08 17:57:36.318617: Epoch: 1, Batch: 806, Loss: 0.6729, Elapsed: 7m44s
2020-05-08 17:58:00.065879: Epoch: 1, Batch: 817, Loss: 0.7572, Elapsed: 5m1s
2020-05-08 18:03:38.117594: Epoch: 1, Batch: 807, Loss: 0.6723, Elapsed: 6m1s
2020-05-08 18:07:39.253616: Epoch: 1, Batch: 818, Loss: 0.7220, Elapsed: 9m39s
2020-05-08 18:08:51.230754: Epoch: 1, Batch: 808, Loss: 0.6688, Elapsed: 5m13s
2020-05-08 18:12:30.172859: Epoch: 1, Batch: 819, Loss: 0.7035, Elapsed: 4m50s
2020-05-08 18:13:43.440297: Epoch: 1, Batch: 465, Loss: 0.6887, Elapsed: 16m54s
2020-05-08 18:16:59.991889: Epoch: 1, Batch: 809, Loss: 0.6745, Elapsed: 8m8s
2020-05-08 18:18:04.797583: Epoch: 1, Batch: 820, Loss: 0.7084, Elapsed: 5m34s
2020-05-08 18:23:19.721093: Epoch: 1, Batch: 821, Loss: 0.7279, Elapsed: 5m14s
2020-05-08 18:25:15.575686: Epoch: 1, Batch: 810, Loss: 0.6881, Elapsed: 8m15s
2020-05-08 18:25:58.531842: Epoch: 1, Batch: 466, Loss: 0.6826, Elapsed: 12m15s
2020-05-08 18:29:01.713556: Epoch: 1, Batch: 822, Loss: 0.7205, Elapsed: 5m41s
2020-05-08 18:29:20.580153: Epoch: 1, Batch: 811, Loss: 0.6549, Elapsed: 4m4s
2020-05-08 18:37:48.019699: Epoch: 1, Batch: 823, Loss: 0.7106, Elapsed: 8m46s
2020-05-08 18:38:33.799841: Epoch: 1, Batch: 467, Loss: 0.6803, Elapsed: 12m35s
2020-05-08 18:40:18.497615: Epoch: 1, Batch: 812, Loss: 0.6821, Elapsed: 10m57s
2020-05-08 18:45:55.157750: Epoch: 1, Batch: 824, Loss: 0.7125, Elapsed: 8m7s
2020-05-08 18:50:13.179821: Epoch: 1, Batch: 813, Loss: 0.6781, Elapsed: 9m54s
2020-05-08 18:51:01.338097: Epoch: 1, Batch: 468, Loss: 0.7036, Elapsed: 12m27s
2020-05-08 18:52:10.911115: Epoch: 1, Batch: 825, Loss: 0.7273, Elapsed: 6m15s
2020-05-08 18:57:19.211584: Epoch: 1, Batch: 826, Loss: 0.7104, Elapsed: 5m8s
2020-05-08 18:58:02.685451: Epoch: 1, Batch: 814, Loss: 0.6771, Elapsed: 7m49s
2020-05-08 19:02:07.281977: Epoch: 1, Batch: 827, Loss: 0.6981, Elapsed: 4m48s
2020-05-08 19:03:30.171223: Epoch: 1, Batch: 469, Loss: 0.6988, Elapsed: 12m28s
2020-05-08 19:06:18.160312: Epoch: 1, Batch: 815, Loss: 0.6878, Elapsed: 8m15s
2020-05-08 19:12:39.692880: Epoch: 1, Batch: 828, Loss: 0.7194, Elapsed: 10m32s
2020-05-08 19:13:03.820536: Epoch: 1, Batch: 816, Loss: 0.6644, Elapsed: 6m45s
2020-05-08 19:19:39.669454: Epoch: 1, Batch: 470, Loss: 0.6959, Elapsed: 16m9s
2020-05-08 19:20:48.479083: Epoch: 1, Batch: 829, Loss: 0.7154, Elapsed: 8m8s
2020-05-08 19:22:43.605945: Epoch: 1, Batch: 817, Loss: 0.6779, Elapsed: 9m39s
2020-05-08 19:28:18.931470: Epoch: 1, Batch: 830, Loss: 0.7055, Elapsed: 7m30s
2020-05-08 19:30:41.340865: Epoch: 1, Batch: 818, Loss: 0.6695, Elapsed: 7m57s
2020-05-08 19:30:52.407568: Epoch: 1, Batch: 471, Loss: 0.6978, Elapsed: 11m12s
2020-05-08 19:33:05.424745: Epoch: 1, Batch: 831, Loss: 0.7153, Elapsed: 4m46s
2020-05-08 19:35:02.183362: Epoch: 1, Batch: 819, Loss: 0.6416, Elapsed: 4m20s
2020-05-08 19:37:31.915986: Epoch: 1, Batch: 832, Loss: 0.7245, Elapsed: 4m26s
2020-05-08 19:40:23.403818: Epoch: 1, Batch: 820, Loss: 0.6717, Elapsed: 5m21s
2020-05-08 19:40:22.775700: Epoch: 1, Batch: 472, Loss: 0.7002, Elapsed: 9m30s
2020-05-08 19:46:35.383963: Epoch: 1, Batch: 821, Loss: 0.6689, Elapsed: 6m11s
2020-05-08 19:51:02.308824: Epoch: 1, Batch: 833, Loss: 0.7272, Elapsed: 13m30s
2020-05-08 19:54:01.504151: Epoch: 1, Batch: 822, Loss: 0.6974, Elapsed: 7m26s
2020-05-08 19:56:19.233626: Epoch: 1, Batch: 473, Loss: 0.7117, Elapsed: 15m56s
2020-05-08 19:59:33.314830: Epoch: 1, Batch: 834, Loss: 0.7408, Elapsed: 8m30s
2020-05-08 20:00:59.866496: Epoch: 1, Batch: 823, Loss: 0.6723, Elapsed: 6m58s
2020-05-08 20:04:26.240268: Epoch: 1, Batch: 474, Loss: 0.6921, Elapsed: 8m6s
2020-05-08 20:07:08.322858: Epoch: 1, Batch: 835, Loss: 0.7097, Elapsed: 7m34s
2020-05-08 20:10:52.332952: Epoch: 1, Batch: 824, Loss: 0.6778, Elapsed: 9m52s
2020-05-08 20:12:13.332239: Epoch: 1, Batch: 475, Loss: 0.7001, Elapsed: 7m47s
2020-05-08 20:14:10.584223: Epoch: 1, Batch: 836, Loss: 0.6967, Elapsed: 7m2s
2020-05-08 20:16:16.410772: Epoch: 1, Batch: 825, Loss: 0.6553, Elapsed: 5m24s
2020-05-08 20:22:04.698260: Epoch: 1, Batch: 476, Loss: 0.6938, Elapsed: 9m51s
2020-05-08 20:26:50.261516: Epoch: 1, Batch: 837, Loss: 0.7111, Elapsed: 12m39s
2020-05-08 20:27:44.531228: Epoch: 1, Batch: 826, Loss: 0.6818, Elapsed: 11m28s
2020-05-08 20:34:39.223381: Epoch: 1, Batch: 838, Loss: 0.6998, Elapsed: 7m48s
2020-05-08 20:35:52.293381: Epoch: 1, Batch: 477, Loss: 0.6840, Elapsed: 13m47s
2020-05-08 20:36:06.715032: Epoch: 1, Batch: 827, Loss: 0.6788, Elapsed: 8m22s
2020-05-08 20:40:35.231598: Epoch: 1, Batch: 828, Loss: 0.6546, Elapsed: 4m28s
2020-05-08 20:42:09.617283: Epoch: 1, Batch: 839, Loss: 0.7134, Elapsed: 7m30s
2020-05-08 20:47:01.899647: Epoch: 1, Batch: 829, Loss: 0.6692, Elapsed: 6m26s
2020-05-08 20:50:03.514145: Epoch: 1, Batch: 840, Loss: 0.7025, Elapsed: 7m53s
2020-05-08 20:53:09.827833: Epoch: 1, Batch: 478, Loss: 0.6775, Elapsed: 17m17s
2020-05-08 20:55:41.487197: Epoch: 1, Batch: 830, Loss: 0.6738, Elapsed: 8m39s
2020-05-08 20:55:49.004041: Epoch: 1, Batch: 841, Loss: 0.7185, Elapsed: 5m45s
2020-05-08 21:02:48.055199: Epoch: 1, Batch: 842, Loss: 0.7270, Elapsed: 6m59s
2020-05-08 21:06:01.087096: Epoch: 1, Batch: 831, Loss: 0.6639, Elapsed: 10m19s
2020-05-08 21:07:18.860980: Epoch: 1, Batch: 479, Loss: 0.6876, Elapsed: 14m9s
2020-05-08 21:11:17.762167: Epoch: 1, Batch: 843, Loss: 0.7210, Elapsed: 8m29s
2020-05-08 21:14:54.454964: Epoch: 1, Batch: 480, Loss: 0.7038, Elapsed: 7m35s
2020-05-08 21:15:55.920014: Epoch: 1, Batch: 832, Loss: 0.6929, Elapsed: 9m54s
2020-05-08 21:15:57.589913: Epoch: 1, Batch: 844, Loss: 0.6944, Elapsed: 4m39s
2020-05-08 21:23:20.008014: Epoch: 1, Batch: 833, Loss: 0.6615, Elapsed: 7m24s
2020-05-08 21:23:28.343469: Epoch: 1, Batch: 845, Loss: 0.7209, Elapsed: 7m30s
2020-05-08 21:26:13.775001: Epoch: 1, Batch: 481, Loss: 0.6963, Elapsed: 11m19s
2020-05-08 21:28:58.142448: Epoch: 1, Batch: 834, Loss: 0.6628, Elapsed: 5m38s
2020-05-08 21:33:31.138678: Epoch: 1, Batch: 835, Loss: 0.6791, Elapsed: 4m32s
2020-05-08 21:34:33.206055: Epoch: 1, Batch: 846, Loss: 0.7157, Elapsed: 11m4s
2020-05-08 21:36:43.783253: Epoch: 1, Batch: 482, Loss: 0.7116, Elapsed: 10m30s
2020-05-08 21:38:07.138811: Epoch: 1, Batch: 836, Loss: 0.6613, Elapsed: 4m35s
2020-05-08 21:41:15.026249: Epoch: 1, Batch: 847, Loss: 0.7143, Elapsed: 6m41s
2020-05-08 21:47:53.681234: Epoch: 1, Batch: 837, Loss: 0.6874, Elapsed: 9m46s
2020-05-08 21:51:03.235561: Epoch: 1, Batch: 848, Loss: 0.7154, Elapsed: 9m48s
2020-05-08 21:51:50.631428: Epoch: 1, Batch: 483, Loss: 0.6975, Elapsed: 15m6s
2020-05-08 21:53:50.408688: Epoch: 1, Batch: 838, Loss: 0.6666, Elapsed: 5m56s
2020-05-08 21:58:00.605705: Epoch: 1, Batch: 849, Loss: 0.7096, Elapsed: 6m57s
2020-05-08 22:03:37.043402: Epoch: 1, Batch: 850, Loss: 0.7019, Elapsed: 5m36s
Starting testing the validation set with 200 subgraphs!
2020-05-08 22:05:08.005567: Epoch: 1, Batch: 839, Loss: 0.6708, Elapsed: 11m17s
2020-05-08 22:11:11.927415: Epoch: 1, Batch: 840, Loss: 0.6740, Elapsed: 6m3s
2020-05-08 22:12:00.323251: Epoch: 1, Batch: 484, Loss: 0.6823, Elapsed: 20m9s
2020-05-08 22:19:35.396377: Epoch: 1, Batch: 841, Loss: 0.6702, Elapsed: 8m23s
2020-05-08 22:20:22.020206: Epoch: 1, Batch: 485, Loss: 0.7020, Elapsed: 8m21s
2020-05-08 22:29:27.020355: Epoch: 1, Batch: 486, Loss: 0.6867, Elapsed: 9m4s
2020-05-08 22:31:45.716005: Epoch: 1, Batch: 842, Loss: 0.7359, Elapsed: 12m10s
2020-05-08 22:39:32.393696: Epoch: 1, Batch: 487, Loss: 0.7121, Elapsed: 10m5s
2020-05-08 22:39:58.860751: Epoch: 1, Batch: 843, Loss: 0.7079, Elapsed: 8m13s
2020-05-08 22:45:19.296025: Epoch: 1, Batch: 844, Loss: 0.6574, Elapsed: 5m20s
2020-05-08 22:45:40.920995: Epoch: 1, Batch: 488, Loss: 0.7288, Elapsed: 6m8s
2020-05-08 22:45:45.374503: Validation Test:  Loss: 0.7118,  Acc: 53.9750, AUC: 0.5501, Precision: 0.6029 -- Elapsed: 42m8s
2020-05-08 22:52:44.082726: Epoch: 1, Batch: 845, Loss: 0.6780, Elapsed: 7m24s
2020-05-08 22:53:51.864055: Epoch: 1, Batch: 489, Loss: 0.7256, Elapsed: 8m10s
2020-05-08 22:54:25.806119: Epoch: 1, Batch: 851, Loss: 0.7137, Elapsed: 8m40s
2020-05-08 22:58:45.761462: Epoch: 1, Batch: 846, Loss: 0.6838, Elapsed: 6m1s
2020-05-08 23:00:39.815138: Epoch: 1, Batch: 852, Loss: 0.7065, Elapsed: 6m13s
2020-05-08 23:06:48.679381: Epoch: 1, Batch: 847, Loss: 0.6750, Elapsed: 8m2s
2020-05-08 23:07:19.896935: Epoch: 1, Batch: 490, Loss: 0.6975, Elapsed: 13m28s
2020-05-08 23:07:36.433994: Epoch: 1, Batch: 853, Loss: 0.7156, Elapsed: 6m56s
2020-05-08 23:13:01.980307: Epoch: 1, Batch: 854, Loss: 0.6887, Elapsed: 5m25s
2020-05-08 23:17:37.581605: Epoch: 1, Batch: 855, Loss: 0.7246, Elapsed: 4m35s
2020-05-08 23:17:46.974335: Epoch: 1, Batch: 848, Loss: 0.6746, Elapsed: 10m58s
2020-05-08 23:20:07.775841: Epoch: 1, Batch: 491, Loss: 0.6992, Elapsed: 12m47s
2020-05-08 23:25:36.428285: Epoch: 1, Batch: 849, Loss: 0.6706, Elapsed: 7m49s
2020-05-08 23:27:35.975448: Epoch: 1, Batch: 856, Loss: 0.6958, Elapsed: 9m58s
2020-05-08 23:32:04.944861: Epoch: 1, Batch: 492, Loss: 0.7084, Elapsed: 11m57s
2020-05-08 23:32:37.407232: Epoch: 1, Batch: 857, Loss: 0.6942, Elapsed: 5m1s
2020-05-08 23:34:05.489665: Epoch: 1, Batch: 850, Loss: 0.6660, Elapsed: 8m29s
Starting testing the validation set with 200 subgraphs!
2020-05-08 23:40:23.343129: Epoch: 1, Batch: 858, Loss: 0.7218, Elapsed: 7m45s
2020-05-08 23:41:13.813919: Epoch: 1, Batch: 493, Loss: 0.7145, Elapsed: 9m8s
2020-05-08 23:47:04.234239: Epoch: 1, Batch: 859, Loss: 0.7215, Elapsed: 6m40s
2020-05-08 23:49:42.787898: Epoch: 1, Batch: 494, Loss: 0.7126, Elapsed: 8m28s
2020-05-08 23:56:29.236335: Epoch: 1, Batch: 860, Loss: 0.7298, Elapsed: 9m24s
2020-05-09 00:00:42.442874: Epoch: 1, Batch: 495, Loss: 0.6942, Elapsed: 10m59s
2020-05-09 00:01:41.026910: Epoch: 1, Batch: 861, Loss: 0.7082, Elapsed: 5m11s
2020-05-09 00:07:21.105937: Epoch: 1, Batch: 862, Loss: 0.6968, Elapsed: 5m40s
2020-05-09 00:14:04.848114: Epoch: 1, Batch: 863, Loss: 0.7128, Elapsed: 6m43s
2020-05-09 00:14:01.755338: Epoch: 1, Batch: 496, Loss: 0.6910, Elapsed: 13m19s
2020-05-09 00:16:33.232456: Validation Test:  Loss: 0.6714,  Acc: 56.8366, AUC: 0.6087, Precision: 0.6257 -- Elapsed: 42m27s
2020-05-09 00:22:04.453205: Epoch: 1, Batch: 864, Loss: 0.7041, Elapsed: 7m59s
2020-05-09 00:24:02.669856: Epoch: 1, Batch: 851, Loss: 0.6702, Elapsed: 7m29s
2020-05-09 00:29:18.753359: Epoch: 1, Batch: 865, Loss: 0.7055, Elapsed: 7m14s
2020-05-09 00:29:38.066427: Epoch: 1, Batch: 852, Loss: 0.6735, Elapsed: 5m35s
2020-05-09 00:34:28.816469: Epoch: 1, Batch: 497, Loss: 0.6766, Elapsed: 20m27s
2020-05-09 00:34:33.754704: Epoch: 1, Batch: 866, Loss: 0.7133, Elapsed: 5m14s
2020-05-09 00:38:31.669964: Epoch: 1, Batch: 853, Loss: 0.6742, Elapsed: 8m53s
2020-05-09 00:41:08.363233: Epoch: 1, Batch: 867, Loss: 0.7189, Elapsed: 6m34s
2020-05-09 00:47:21.980459: Epoch: 1, Batch: 854, Loss: 0.6820, Elapsed: 8m50s
2020-05-09 00:49:12.194820: Epoch: 1, Batch: 868, Loss: 0.7333, Elapsed: 8m3s
2020-05-09 00:49:48.699482: Epoch: 1, Batch: 498, Loss: 0.6779, Elapsed: 15m19s
2020-05-09 00:53:36.276308: Epoch: 1, Batch: 869, Loss: 0.7258, Elapsed: 4m24s
2020-05-09 00:54:13.497733: Epoch: 1, Batch: 855, Loss: 0.6685, Elapsed: 6m51s
2020-05-09 01:00:00.948462: Epoch: 1, Batch: 856, Loss: 0.6601, Elapsed: 5m47s
2020-05-09 01:01:29.806294: Epoch: 1, Batch: 499, Loss: 0.6834, Elapsed: 11m41s
2020-05-09 01:03:28.975067: Epoch: 1, Batch: 870, Loss: 0.7111, Elapsed: 9m52s
2020-05-09 01:07:22.570384: Epoch: 1, Batch: 857, Loss: 0.6810, Elapsed: 7m21s
2020-05-09 01:11:02.861716: Epoch: 1, Batch: 871, Loss: 0.7071, Elapsed: 7m33s
2020-05-09 01:12:39.927159: Epoch: 1, Batch: 858, Loss: 0.6557, Elapsed: 5m17s
2020-05-09 01:17:30.614952: Epoch: 1, Batch: 500, Loss: 0.6863, Elapsed: 16m0s
Starting testing the validation set with 200 subgraphs!
2020-05-09 01:19:55.993063: Epoch: 1, Batch: 859, Loss: 0.6760, Elapsed: 7m16s
2020-05-09 01:21:52.470886: Epoch: 1, Batch: 872, Loss: 0.6628, Elapsed: 10m49s
2020-05-09 01:26:41.987831: Epoch: 1, Batch: 860, Loss: 0.6653, Elapsed: 6m45s
2020-05-09 01:29:36.829795: Epoch: 1, Batch: 873, Loss: 0.7023, Elapsed: 7m44s
2020-05-09 01:33:14.660983: Epoch: 1, Batch: 874, Loss: 0.6768, Elapsed: 3m37s
2020-05-09 01:34:12.216481: Epoch: 1, Batch: 861, Loss: 0.6667, Elapsed: 7m30s
2020-05-09 01:37:42.756565: Epoch: 1, Batch: 875, Loss: 0.7298, Elapsed: 4m28s
2020-05-09 01:38:37.939537: Epoch: 1, Batch: 862, Loss: 0.6625, Elapsed: 4m25s
2020-05-09 01:43:58.814956: Epoch: 1, Batch: 863, Loss: 0.6767, Elapsed: 5m20s
2020-05-09 01:46:44.830816: Epoch: 1, Batch: 876, Loss: 0.7186, Elapsed: 9m2s
2020-05-09 01:51:24.170578: Epoch: 1, Batch: 877, Loss: 0.7157, Elapsed: 4m39s
2020-05-09 01:51:36.581306: Epoch: 1, Batch: 864, Loss: 0.6735, Elapsed: 7m37s
2020-05-09 01:55:27.323798: Epoch: 1, Batch: 865, Loss: 0.6318, Elapsed: 3m50s
2020-05-09 01:57:46.134742: Epoch: 1, Batch: 878, Loss: 0.7042, Elapsed: 6m21s
2020-05-09 02:05:46.741636: Epoch: 1, Batch: 866, Loss: 0.7083, Elapsed: 10m19s
2020-05-09 02:06:44.934129: Epoch: 1, Batch: 879, Loss: 0.6928, Elapsed: 8m58s
2020-05-09 02:11:32.639600: Validation Test:  Loss: 0.6951,  Acc: 56.9036, AUC: 0.5792, Precision: 0.6155 -- Elapsed: 54m2s
2020-05-09 02:11:44.293278: Epoch: 1, Batch: 867, Loss: 0.6562, Elapsed: 5m57s
2020-05-09 02:13:40.724462: Epoch: 1, Batch: 880, Loss: 0.6963, Elapsed: 6m55s
2020-05-09 02:17:35.547989: Epoch: 1, Batch: 868, Loss: 0.6562, Elapsed: 5m51s
2020-05-09 02:20:47.003003: Epoch: 1, Batch: 501, Loss: 0.7067, Elapsed: 9m14s
2020-05-09 02:23:16.331154: Epoch: 1, Batch: 881, Loss: 0.7036, Elapsed: 9m35s
2020-05-09 02:24:12.600231: Epoch: 1, Batch: 869, Loss: 0.6492, Elapsed: 6m37s
2020-05-09 02:29:39.351160: Epoch: 1, Batch: 882, Loss: 0.7175, Elapsed: 6m22s
2020-05-09 02:30:56.756355: Epoch: 1, Batch: 502, Loss: 0.6948, Elapsed: 10m9s
2020-05-09 02:32:11.654447: Epoch: 1, Batch: 870, Loss: 0.6566, Elapsed: 7m59s
2020-05-09 02:35:23.594772: Epoch: 1, Batch: 883, Loss: 0.7374, Elapsed: 5m44s
2020-05-09 02:39:32.560303: Epoch: 1, Batch: 871, Loss: 0.6558, Elapsed: 7m20s
2020-05-09 02:43:19.772657: Epoch: 1, Batch: 503, Loss: 0.6949, Elapsed: 12m23s
2020-05-09 02:43:56.719803: Epoch: 1, Batch: 884, Loss: 0.7106, Elapsed: 8m33s
2020-05-09 02:48:09.068494: Epoch: 1, Batch: 872, Loss: 0.6611, Elapsed: 8m36s
2020-05-09 02:52:26.522565: Epoch: 1, Batch: 885, Loss: 0.7065, Elapsed: 8m29s
2020-05-09 02:56:42.218462: Epoch: 1, Batch: 504, Loss: 0.6968, Elapsed: 13m22s
2020-05-09 02:57:45.201492: Epoch: 1, Batch: 873, Loss: 0.6875, Elapsed: 9m36s
2020-05-09 02:58:32.030265: Epoch: 1, Batch: 886, Loss: 0.6869, Elapsed: 6m5s
2020-05-09 03:05:11.717786: Epoch: 1, Batch: 874, Loss: 0.6691, Elapsed: 7m26s
2020-05-09 03:07:13.667305: Epoch: 1, Batch: 887, Loss: 0.7019, Elapsed: 8m41s
2020-05-09 03:12:00.091677: Epoch: 1, Batch: 888, Loss: 0.7245, Elapsed: 4m46s
2020-05-09 03:12:17.084840: Epoch: 1, Batch: 875, Loss: 0.6857, Elapsed: 7m5s
2020-05-09 03:13:33.649280: Epoch: 1, Batch: 505, Loss: 0.6769, Elapsed: 16m51s
2020-05-09 03:17:33.819806: Epoch: 1, Batch: 876, Loss: 0.6759, Elapsed: 5m16s
2020-05-09 03:18:02.427316: Epoch: 1, Batch: 889, Loss: 0.7061, Elapsed: 6m2s
2020-05-09 03:24:45.996321: Epoch: 1, Batch: 890, Loss: 0.7086, Elapsed: 6m43s
2020-05-09 03:25:28.691263: Epoch: 1, Batch: 506, Loss: 0.6857, Elapsed: 11m55s
2020-05-09 03:25:50.649139: Epoch: 1, Batch: 877, Loss: 0.6692, Elapsed: 8m16s
2020-05-09 03:33:40.835722: Epoch: 1, Batch: 878, Loss: 0.6876, Elapsed: 7m50s
2020-05-09 03:34:55.024063: Epoch: 1, Batch: 891, Loss: 0.7067, Elapsed: 10m9s
2020-05-09 03:35:23.039512: Epoch: 1, Batch: 507, Loss: 0.6855, Elapsed: 9m54s
2020-05-09 03:41:41.010338: Epoch: 1, Batch: 892, Loss: 0.7099, Elapsed: 6m45s
2020-05-09 03:44:37.480308: Epoch: 1, Batch: 879, Loss: 0.6851, Elapsed: 10m56s
2020-05-09 03:48:58.563856: Epoch: 1, Batch: 893, Loss: 0.7377, Elapsed: 7m17s
2020-05-09 03:50:51.960668: Epoch: 1, Batch: 880, Loss: 0.6439, Elapsed: 6m14s
2020-05-09 03:56:17.466988: Epoch: 1, Batch: 881, Loss: 0.6637, Elapsed: 5m25s
2020-05-09 03:57:11.556583: Epoch: 1, Batch: 894, Loss: 0.7218, Elapsed: 8m12s
2020-05-09 03:58:41.737553: Epoch: 1, Batch: 508, Loss: 0.6789, Elapsed: 23m18s
2020-05-09 04:01:23.254978: Epoch: 1, Batch: 895, Loss: 0.7177, Elapsed: 4m11s
2020-05-09 04:04:00.851384: Epoch: 1, Batch: 882, Loss: 0.6464, Elapsed: 7m43s
2020-05-09 04:08:11.324590: Epoch: 1, Batch: 509, Loss: 0.7060, Elapsed: 9m29s
2020-05-09 04:08:34.751925: Epoch: 1, Batch: 896, Loss: 0.7197, Elapsed: 7m11s
2020-05-09 04:10:26.138822: Epoch: 1, Batch: 883, Loss: 0.6675, Elapsed: 6m25s
2020-05-09 04:14:12.729358: Epoch: 1, Batch: 884, Loss: 0.6385, Elapsed: 3m46s
2020-05-09 04:16:50.527937: Epoch: 1, Batch: 897, Loss: 0.7179, Elapsed: 8m15s
2020-05-09 04:19:48.387405: Epoch: 1, Batch: 510, Loss: 0.6993, Elapsed: 11m37s
2020-05-09 04:22:02.371487: Epoch: 1, Batch: 885, Loss: 0.6509, Elapsed: 7m49s
2020-05-09 04:23:53.065552: Epoch: 1, Batch: 898, Loss: 0.7101, Elapsed: 7m2s
2020-05-09 04:26:53.808340: Epoch: 1, Batch: 511, Loss: 0.7038, Elapsed: 7m5s
2020-05-09 04:27:34.735894: Epoch: 1, Batch: 886, Loss: 0.6650, Elapsed: 5m32s
2020-05-09 04:30:23.375093: Epoch: 1, Batch: 899, Loss: 0.7112, Elapsed: 6m30s
2020-05-09 04:34:43.766983: Epoch: 1, Batch: 512, Loss: 0.7103, Elapsed: 7m49s
2020-05-09 04:37:39.169676: Epoch: 1, Batch: 900, Loss: 0.6981, Elapsed: 7m15s
Starting testing the validation set with 200 subgraphs!
2020-05-09 04:37:34.475029: Epoch: 1, Batch: 887, Loss: 0.6775, Elapsed: 9m59s
2020-05-09 04:42:25.274773: Epoch: 1, Batch: 888, Loss: 0.6667, Elapsed: 4m50s
2020-05-09 04:48:22.251494: Epoch: 1, Batch: 889, Loss: 0.6800, Elapsed: 5m56s
2020-05-09 04:48:22.124958: Epoch: 1, Batch: 513, Loss: 0.6950, Elapsed: 13m38s
2020-05-09 04:54:25.078995: Epoch: 1, Batch: 890, Loss: 0.6561, Elapsed: 6m2s
2020-05-09 04:59:36.774371: Epoch: 1, Batch: 891, Loss: 0.6553, Elapsed: 5m11s
2020-05-09 05:01:29.369112: Epoch: 1, Batch: 514, Loss: 0.7008, Elapsed: 13m7s
2020-05-09 05:08:24.366771: Epoch: 1, Batch: 892, Loss: 0.6886, Elapsed: 8m47s
2020-05-09 05:12:14.085060: Epoch: 1, Batch: 893, Loss: 0.6539, Elapsed: 3m49s
2020-05-09 05:16:06.217852: Epoch: 1, Batch: 515, Loss: 0.7088, Elapsed: 14m36s
2020-05-09 05:17:05.531030: Epoch: 1, Batch: 894, Loss: 0.6540, Elapsed: 4m51s
2020-05-09 05:19:49.543857: Validation Test:  Loss: 0.7091,  Acc: 54.2428, AUC: 0.5528, Precision: 0.6055 -- Elapsed: 42m10s
2020-05-09 05:25:54.887039: Epoch: 1, Batch: 895, Loss: 0.6967, Elapsed: 8m49s
2020-05-09 05:26:00.851324: Epoch: 1, Batch: 516, Loss: 0.7070, Elapsed: 9m54s
2020-05-09 05:27:54.043044: Epoch: 1, Batch: 901, Loss: 0.6987, Elapsed: 8m4s
2020-05-09 05:33:06.357043: Epoch: 1, Batch: 902, Loss: 0.7088, Elapsed: 5m12s
2020-05-09 05:36:26.376797: Epoch: 1, Batch: 896, Loss: 0.6705, Elapsed: 10m31s
2020-05-09 05:41:58.034468: Epoch: 1, Batch: 517, Loss: 0.6960, Elapsed: 15m57s
2020-05-09 05:42:42.653325: Epoch: 1, Batch: 903, Loss: 0.6994, Elapsed: 9m36s
2020-05-09 05:42:58.031439: Epoch: 1, Batch: 897, Loss: 0.6620, Elapsed: 6m31s
2020-05-09 05:52:28.310933: Epoch: 1, Batch: 518, Loss: 0.6949, Elapsed: 10m30s
2020-05-09 05:53:09.130232: Epoch: 1, Batch: 904, Loss: 0.7103, Elapsed: 10m26s
2020-05-09 05:53:29.246193: Epoch: 1, Batch: 898, Loss: 0.6804, Elapsed: 10m31s
2020-05-09 06:00:09.520027: Epoch: 1, Batch: 905, Loss: 0.7180, Elapsed: 7m0s
2020-05-09 06:02:01.063637: Epoch: 1, Batch: 899, Loss: 0.6781, Elapsed: 8m31s
2020-05-09 06:07:57.488438: Epoch: 1, Batch: 519, Loss: 0.6922, Elapsed: 15m29s
2020-05-09 06:08:16.055948: Epoch: 1, Batch: 900, Loss: 0.6541, Elapsed: 6m14s
Starting testing the validation set with 200 subgraphs!
2020-05-09 06:08:55.981895: Epoch: 1, Batch: 906, Loss: 0.6928, Elapsed: 8m46s
2020-05-09 06:12:20.737729: Epoch: 1, Batch: 907, Loss: 0.6893, Elapsed: 3m24s
2020-05-09 06:19:37.212098: Epoch: 1, Batch: 908, Loss: 0.6982, Elapsed: 7m16s
2020-05-09 06:22:04.148603: Epoch: 1, Batch: 520, Loss: 0.6887, Elapsed: 14m6s
2020-05-09 06:26:13.196547: Epoch: 1, Batch: 909, Loss: 0.7288, Elapsed: 6m35s
2020-05-09 06:31:07.906410: Epoch: 1, Batch: 910, Loss: 0.7097, Elapsed: 4m54s
2020-05-09 06:38:04.235461: Epoch: 1, Batch: 521, Loss: 0.6997, Elapsed: 16m0s
2020-05-09 06:38:13.632610: Epoch: 1, Batch: 911, Loss: 0.7291, Elapsed: 7m5s
2020-05-09 06:44:21.404424: Epoch: 1, Batch: 912, Loss: 0.7165, Elapsed: 6m7s
2020-05-09 06:47:35.274030: Epoch: 1, Batch: 522, Loss: 0.7004, Elapsed: 9m31s
2020-05-09 06:50:27.535742: Validation Test:  Loss: 0.6701,  Acc: 57.6705, AUC: 0.6126, Precision: 0.6285 -- Elapsed: 42m11s
2020-05-09 06:51:46.930606: Epoch: 1, Batch: 913, Loss: 0.7186, Elapsed: 7m25s
2020-05-09 06:55:11.840284: Epoch: 1, Batch: 914, Loss: 0.7342, Elapsed: 3m24s
2020-05-09 07:00:19.558544: Epoch: 1, Batch: 901, Loss: 0.7226, Elapsed: 9m52s
2020-05-09 07:03:31.878772: Epoch: 1, Batch: 523, Loss: 0.6822, Elapsed: 15m56s
2020-05-09 07:07:08.133342: Epoch: 1, Batch: 902, Loss: 0.6726, Elapsed: 6m48s
2020-05-09 07:08:48.518228: Epoch: 1, Batch: 915, Loss: 0.7224, Elapsed: 13m36s
2020-05-09 07:14:19.215024: Epoch: 1, Batch: 524, Loss: 0.6922, Elapsed: 10m47s
2020-05-09 07:14:35.961557: Epoch: 1, Batch: 916, Loss: 0.7060, Elapsed: 5m47s
2020-05-09 07:15:20.324317: Epoch: 1, Batch: 903, Loss: 0.6483, Elapsed: 8m12s
2020-05-09 07:24:01.490142: Epoch: 1, Batch: 904, Loss: 0.6597, Elapsed: 8m41s
2020-05-09 07:24:29.751764: Epoch: 1, Batch: 917, Loss: 0.7021, Elapsed: 9m53s
2020-05-09 07:31:29.069077: Epoch: 1, Batch: 525, Loss: 0.6907, Elapsed: 17m9s
2020-05-09 07:32:02.013623: Epoch: 1, Batch: 918, Loss: 0.7132, Elapsed: 7m32s
2020-05-09 07:35:58.568132: Epoch: 1, Batch: 905, Loss: 0.6746, Elapsed: 11m57s
2020-05-09 07:40:00.957426: Epoch: 1, Batch: 919, Loss: 0.6942, Elapsed: 7m58s
2020-05-09 07:45:09.116712: Epoch: 1, Batch: 920, Loss: 0.7167, Elapsed: 5m8s
2020-05-09 07:45:55.790960: Epoch: 1, Batch: 906, Loss: 0.6851, Elapsed: 9m57s
2020-05-09 07:46:22.056871: Epoch: 1, Batch: 526, Loss: 0.7001, Elapsed: 14m52s
2020-05-09 07:51:18.828139: Epoch: 1, Batch: 921, Loss: 0.6882, Elapsed: 6m9s
2020-05-09 07:55:03.185482: Epoch: 1, Batch: 907, Loss: 0.6740, Elapsed: 9m7s
2020-05-09 07:57:00.160657: Epoch: 1, Batch: 922, Loss: 0.7067, Elapsed: 5m41s
2020-05-09 08:00:57.494581: Epoch: 1, Batch: 908, Loss: 0.6701, Elapsed: 5m54s
2020-05-09 08:00:58.663177: Epoch: 1, Batch: 527, Loss: 0.6999, Elapsed: 14m36s
2020-05-09 08:05:54.638356: Epoch: 1, Batch: 923, Loss: 0.7037, Elapsed: 8m54s
2020-05-09 08:11:04.780556: Epoch: 1, Batch: 909, Loss: 0.6643, Elapsed: 10m7s
2020-05-09 08:15:13.523449: Epoch: 1, Batch: 528, Loss: 0.7048, Elapsed: 14m14s
2020-05-09 08:16:08.333432: Epoch: 1, Batch: 924, Loss: 0.7095, Elapsed: 10m13s
2020-05-09 08:16:49.191220: Epoch: 1, Batch: 910, Loss: 0.6828, Elapsed: 5m44s
2020-05-09 08:22:35.939649: Epoch: 1, Batch: 529, Loss: 0.7043, Elapsed: 7m22s
2020-05-09 08:23:02.761279: Epoch: 1, Batch: 911, Loss: 0.6553, Elapsed: 6m13s
2020-05-09 08:24:54.800583: Epoch: 1, Batch: 925, Loss: 0.7083, Elapsed: 8m46s
2020-05-09 08:30:17.522836: Epoch: 1, Batch: 912, Loss: 0.6874, Elapsed: 7m14s
2020-05-09 08:31:02.066914: Epoch: 1, Batch: 926, Loss: 0.6895, Elapsed: 6m7s
2020-05-09 08:35:56.169943: Epoch: 1, Batch: 927, Loss: 0.6874, Elapsed: 4m54s
2020-05-09 08:36:30.900004: Epoch: 1, Batch: 530, Loss: 0.6881, Elapsed: 13m54s
2020-05-09 08:36:57.309004: Epoch: 1, Batch: 913, Loss: 0.6777, Elapsed: 6m39s
2020-05-09 08:42:48.542618: Epoch: 1, Batch: 914, Loss: 0.6538, Elapsed: 5m51s
2020-05-09 08:45:08.303672: Epoch: 1, Batch: 928, Loss: 0.7033, Elapsed: 9m12s
2020-05-09 08:47:22.351604: Epoch: 1, Batch: 915, Loss: 0.6432, Elapsed: 4m33s
2020-05-09 08:50:14.316310: Epoch: 1, Batch: 531, Loss: 0.7035, Elapsed: 13m43s
2020-05-09 08:55:37.284548: Epoch: 1, Batch: 916, Loss: 0.6590, Elapsed: 8m14s
2020-05-09 08:56:22.459327: Epoch: 1, Batch: 929, Loss: 0.7110, Elapsed: 11m14s
2020-05-09 08:58:19.298314: Epoch: 1, Batch: 532, Loss: 0.7253, Elapsed: 8m4s
2020-05-09 08:59:11.809698: Epoch: 1, Batch: 930, Loss: 0.6757, Elapsed: 2m49s
2020-05-09 09:03:55.813411: Epoch: 1, Batch: 533, Loss: 0.7003, Elapsed: 5m36s
2020-05-09 09:04:43.679105: Epoch: 1, Batch: 917, Loss: 0.6862, Elapsed: 9m6s
2020-05-09 09:05:06.162959: Epoch: 1, Batch: 931, Loss: 0.7015, Elapsed: 5m54s
2020-05-09 09:11:01.417345: Epoch: 1, Batch: 918, Loss: 0.6595, Elapsed: 6m17s
2020-05-09 09:11:07.537340: Epoch: 1, Batch: 932, Loss: 0.7099, Elapsed: 6m1s
2020-05-09 09:15:57.709601: Epoch: 1, Batch: 534, Loss: 0.6931, Elapsed: 12m1s
2020-05-09 09:17:05.466050: Epoch: 1, Batch: 919, Loss: 0.6585, Elapsed: 6m4s
2020-05-09 09:17:54.717640: Epoch: 1, Batch: 933, Loss: 0.6970, Elapsed: 6m47s
2020-05-09 09:23:11.327705: Epoch: 1, Batch: 920, Loss: 0.6594, Elapsed: 6m5s
2020-05-09 09:26:39.858953: Epoch: 1, Batch: 934, Loss: 0.6922, Elapsed: 8m45s
2020-05-09 09:30:11.085654: Epoch: 1, Batch: 535, Loss: 0.7021, Elapsed: 14m13s
2020-05-09 09:30:19.762568: Epoch: 1, Batch: 921, Loss: 0.6614, Elapsed: 7m8s
2020-05-09 09:32:31.968903: Epoch: 1, Batch: 935, Loss: 0.6945, Elapsed: 5m52s
2020-05-09 09:36:30.156700: Epoch: 1, Batch: 936, Loss: 0.6952, Elapsed: 3m58s
2020-05-09 09:38:19.772998: Epoch: 1, Batch: 922, Loss: 0.6710, Elapsed: 7m59s
2020-05-09 09:44:25.914697: Epoch: 1, Batch: 937, Loss: 0.6995, Elapsed: 7m55s
2020-05-09 09:49:24.427135: Epoch: 1, Batch: 923, Loss: 0.6592, Elapsed: 11m4s
2020-05-09 09:51:25.261171: Epoch: 1, Batch: 536, Loss: 0.6921, Elapsed: 21m14s
2020-05-09 09:51:31.686385: Epoch: 1, Batch: 938, Loss: 0.7164, Elapsed: 7m5s
2020-05-09 09:56:32.016357: Epoch: 1, Batch: 924, Loss: 0.6774, Elapsed: 7m7s
2020-05-09 10:01:27.665648: Epoch: 1, Batch: 939, Loss: 0.7189, Elapsed: 9m55s
2020-05-09 10:02:48.613103: Epoch: 1, Batch: 925, Loss: 0.6757, Elapsed: 6m16s
2020-05-09 10:05:23.630052: Epoch: 1, Batch: 537, Loss: 0.6761, Elapsed: 13m58s
2020-05-09 10:06:50.318658: Epoch: 1, Batch: 940, Loss: 0.7184, Elapsed: 5m22s
2020-05-09 10:11:05.015289: Epoch: 1, Batch: 926, Loss: 0.6497, Elapsed: 8m16s
2020-05-09 10:12:10.515533: Epoch: 1, Batch: 941, Loss: 0.7043, Elapsed: 5m20s
2020-05-09 10:16:13.072402: Epoch: 1, Batch: 538, Loss: 0.6922, Elapsed: 10m49s
2020-05-09 10:16:36.582421: Epoch: 1, Batch: 927, Loss: 0.6629, Elapsed: 5m31s
2020-05-09 10:20:04.761591: Epoch: 1, Batch: 942, Loss: 0.7209, Elapsed: 7m54s
2020-05-09 10:25:30.394583: Epoch: 1, Batch: 928, Loss: 0.6671, Elapsed: 8m53s
2020-05-09 10:27:47.043959: Epoch: 1, Batch: 943, Loss: 0.6830, Elapsed: 7m42s
2020-05-09 10:33:08.762456: Epoch: 1, Batch: 539, Loss: 0.6855, Elapsed: 16m55s
2020-05-09 10:36:07.336085: Epoch: 1, Batch: 944, Loss: 0.7183, Elapsed: 8m20s
2020-05-09 10:37:12.111041: Epoch: 1, Batch: 929, Loss: 0.6651, Elapsed: 11m41s
2020-05-09 10:42:19.364381: Epoch: 1, Batch: 930, Loss: 0.6717, Elapsed: 5m7s
2020-05-09 10:44:07.601011: Epoch: 1, Batch: 945, Loss: 0.7007, Elapsed: 8m0s
2020-05-09 10:46:43.623709: Epoch: 1, Batch: 931, Loss: 0.6726, Elapsed: 4m24s
2020-05-09 10:50:32.610319: Epoch: 1, Batch: 946, Loss: 0.7065, Elapsed: 6m24s
2020-05-09 10:52:42.976770: Epoch: 1, Batch: 540, Loss: 0.6975, Elapsed: 19m34s
2020-05-09 10:56:19.055222: Epoch: 1, Batch: 932, Loss: 0.6773, Elapsed: 9m35s
2020-05-09 10:56:57.607996: Epoch: 1, Batch: 947, Loss: 0.7138, Elapsed: 6m24s
2020-05-09 11:02:20.062826: Epoch: 1, Batch: 933, Loss: 0.6830, Elapsed: 6m0s
2020-05-09 11:04:06.156443: Epoch: 1, Batch: 948, Loss: 0.7024, Elapsed: 7m8s
2020-05-09 11:07:17.530564: Epoch: 1, Batch: 949, Loss: 0.6934, Elapsed: 3m11s
2020-05-09 11:08:12.186623: Epoch: 1, Batch: 934, Loss: 0.6802, Elapsed: 5m52s
2020-05-09 11:15:47.651805: Epoch: 1, Batch: 935, Loss: 0.6793, Elapsed: 7m35s
2020-05-09 11:16:04.497343: Epoch: 1, Batch: 541, Loss: 0.6905, Elapsed: 23m21s
2020-05-09 11:16:05.184485: Epoch: 1, Batch: 950, Loss: 0.7112, Elapsed: 8m47s
Starting testing the validation set with 200 subgraphs!
2020-05-09 11:22:18.727801: Epoch: 1, Batch: 936, Loss: 0.6794, Elapsed: 6m31s
2020-05-09 11:25:51.104826: Epoch: 1, Batch: 542, Loss: 0.7141, Elapsed: 9m46s
2020-05-09 11:28:44.328423: Epoch: 1, Batch: 937, Loss: 0.6599, Elapsed: 6m25s
2020-05-09 11:32:30.280327: Epoch: 1, Batch: 938, Loss: 0.6783, Elapsed: 3m45s
2020-05-09 11:37:53.746376: Epoch: 1, Batch: 543, Loss: 0.6959, Elapsed: 12m2s
2020-05-09 11:39:24.581764: Epoch: 1, Batch: 939, Loss: 0.6763, Elapsed: 6m54s
2020-05-09 11:45:58.462325: Epoch: 1, Batch: 940, Loss: 0.6857, Elapsed: 6m33s
2020-05-09 11:48:20.237176: Epoch: 1, Batch: 544, Loss: 0.6879, Elapsed: 10m26s
2020-05-09 11:48:44.882052: Epoch: 1, Batch: 941, Loss: 0.6480, Elapsed: 2m46s
2020-05-09 11:54:43.632673: Epoch: 1, Batch: 942, Loss: 0.6762, Elapsed: 5m58s
2020-05-09 11:58:12.884581: Epoch: 1, Batch: 545, Loss: 0.6938, Elapsed: 9m52s
2020-05-09 11:58:21.242132: Validation Test:  Loss: 0.7086,  Acc: 52.7506, AUC: 0.5341, Precision: 0.5986 -- Elapsed: 42m16s
2020-05-09 12:01:18.924409: Epoch: 1, Batch: 943, Loss: 0.6675, Elapsed: 6m35s
2020-05-09 12:07:59.187993: Epoch: 1, Batch: 951, Loss: 0.6908, Elapsed: 9m37s
2020-05-09 12:13:20.205755: Epoch: 1, Batch: 944, Loss: 0.7036, Elapsed: 12m1s
2020-05-09 12:13:52.514152: Epoch: 1, Batch: 546, Loss: 0.6912, Elapsed: 15m39s
2020-05-09 12:13:53.222065: Epoch: 1, Batch: 952, Loss: 0.7073, Elapsed: 5m54s
2020-05-09 12:19:40.822125: Epoch: 1, Batch: 945, Loss: 0.6733, Elapsed: 6m20s
2020-05-09 12:20:35.422598: Epoch: 1, Batch: 953, Loss: 0.7122, Elapsed: 6m42s
2020-05-09 12:26:05.456834: Epoch: 1, Batch: 946, Loss: 0.6536, Elapsed: 6m24s
2020-05-09 12:26:10.751296: Epoch: 1, Batch: 547, Loss: 0.7025, Elapsed: 12m18s
2020-05-09 12:28:52.263329: Epoch: 1, Batch: 954, Loss: 0.6998, Elapsed: 8m16s
2020-05-09 12:32:22.378759: Epoch: 1, Batch: 947, Loss: 0.6354, Elapsed: 6m16s
2020-05-09 12:34:04.246722: Epoch: 1, Batch: 955, Loss: 0.6834, Elapsed: 5m11s
2020-05-09 12:37:54.875511: Epoch: 1, Batch: 548, Loss: 0.6783, Elapsed: 11m44s
2020-05-09 12:40:50.731880: Epoch: 1, Batch: 948, Loss: 0.7095, Elapsed: 8m28s
2020-05-09 12:41:25.363237: Epoch: 1, Batch: 956, Loss: 0.6993, Elapsed: 7m21s
2020-05-09 12:49:55.066632: Epoch: 1, Batch: 957, Loss: 0.7056, Elapsed: 8m29s
2020-05-09 12:52:15.684507: Epoch: 1, Batch: 949, Loss: 0.6775, Elapsed: 11m24s
2020-05-09 12:53:55.721122: Epoch: 1, Batch: 549, Loss: 0.6928, Elapsed: 16m0s
2020-05-09 13:00:06.275077: Epoch: 1, Batch: 950, Loss: 0.6466, Elapsed: 7m50s
Starting testing the validation set with 200 subgraphs!
2020-05-09 13:00:40.707020: Epoch: 1, Batch: 958, Loss: 0.7295, Elapsed: 10m45s
2020-05-09 13:07:37.452920: Epoch: 1, Batch: 959, Loss: 0.7202, Elapsed: 6m56s
2020-05-09 13:09:11.618604: Epoch: 1, Batch: 550, Loss: 0.6989, Elapsed: 15m15s
Starting testing the validation set with 200 subgraphs!
2020-05-09 13:14:02.836795: Epoch: 1, Batch: 960, Loss: 0.7075, Elapsed: 6m25s
2020-05-09 13:22:53.453540: Epoch: 1, Batch: 961, Loss: 0.7102, Elapsed: 8m50s
2020-05-09 13:28:15.023981: Epoch: 1, Batch: 962, Loss: 0.6853, Elapsed: 5m21s
2020-05-09 13:34:10.809251: Epoch: 1, Batch: 963, Loss: 0.7046, Elapsed: 5m55s
2020-05-09 13:40:10.369557: Validation Test:  Loss: 0.6711,  Acc: 56.8042, AUC: 0.6123, Precision: 0.6274 -- Elapsed: 40m4s
2020-05-09 13:44:00.963606: Epoch: 1, Batch: 964, Loss: 0.6961, Elapsed: 9m50s
2020-05-09 13:45:02.678545: Epoch: 1, Batch: 951, Loss: 0.6671, Elapsed: 4m52s
2020-05-09 13:49:51.737020: Epoch: 1, Batch: 965, Loss: 0.6990, Elapsed: 5m50s
2020-05-09 13:51:09.594746: Epoch: 1, Batch: 952, Loss: 0.6814, Elapsed: 6m6s
2020-05-09 13:56:07.948450: Epoch: 1, Batch: 953, Loss: 0.6607, Elapsed: 4m58s
2020-05-09 13:57:19.175317: Epoch: 1, Batch: 966, Loss: 0.7093, Elapsed: 7m27s
2020-05-09 14:00:07.289674: Validation Test:  Loss: 0.6956,  Acc: 57.5346, AUC: 0.5936, Precision: 0.6275 -- Elapsed: 50m55s
2020-05-09 14:01:08.026444: Epoch: 1, Batch: 954, Loss: 0.7054, Elapsed: 5m0s
2020-05-09 14:04:14.814540: Epoch: 1, Batch: 967, Loss: 0.7014, Elapsed: 6m55s
2020-05-09 14:11:42.962758: Epoch: 1, Batch: 955, Loss: 0.6717, Elapsed: 10m34s
2020-05-09 14:13:32.344906: Epoch: 1, Batch: 968, Loss: 0.6960, Elapsed: 9m17s
2020-05-09 14:14:02.319683: Epoch: 1, Batch: 551, Loss: 0.6816, Elapsed: 13m55s
2020-05-09 14:18:31.976792: Epoch: 1, Batch: 956, Loss: 0.6530, Elapsed: 6m48s
2020-05-09 14:18:45.015960: Epoch: 1, Batch: 969, Loss: 0.7083, Elapsed: 5m12s
2020-05-09 14:25:41.044268: Epoch: 1, Batch: 957, Loss: 0.6667, Elapsed: 7m9s
2020-05-09 14:27:48.786183: Epoch: 1, Batch: 970, Loss: 0.6899, Elapsed: 9m3s
2020-05-09 14:33:37.134764: Epoch: 1, Batch: 958, Loss: 0.6888, Elapsed: 7m56s
2020-05-09 14:34:07.236390: Epoch: 1, Batch: 552, Loss: 0.6704, Elapsed: 20m4s
2020-05-09 14:34:59.514815: Epoch: 1, Batch: 971, Loss: 0.6991, Elapsed: 7m10s
2020-05-09 14:41:38.918568: Epoch: 1, Batch: 959, Loss: 0.6742, Elapsed: 8m1s
2020-05-09 14:42:11.117320: Epoch: 1, Batch: 972, Loss: 0.7342, Elapsed: 7m11s
2020-05-09 14:46:43.169408: Epoch: 1, Batch: 553, Loss: 0.7066, Elapsed: 12m35s
2020-05-09 14:48:40.734823: Epoch: 1, Batch: 973, Loss: 0.6956, Elapsed: 6m29s
2020-05-09 14:54:40.880593: Epoch: 1, Batch: 960, Loss: 0.7051, Elapsed: 13m1s
2020-05-09 14:57:43.309686: Epoch: 1, Batch: 974, Loss: 0.7182, Elapsed: 9m2s
2020-05-09 15:03:09.615263: Epoch: 1, Batch: 961, Loss: 0.6813, Elapsed: 8m28s
2020-05-09 15:03:55.237085: Epoch: 1, Batch: 554, Loss: 0.6820, Elapsed: 17m12s
2020-05-09 15:03:55.379027: Epoch: 1, Batch: 975, Loss: 0.7169, Elapsed: 6m12s
2020-05-09 15:11:08.153850: Epoch: 1, Batch: 976, Loss: 0.7210, Elapsed: 7m12s
2020-05-09 15:11:53.082351: Epoch: 1, Batch: 962, Loss: 0.6650, Elapsed: 8m43s
2020-05-09 15:15:26.352859: Epoch: 1, Batch: 555, Loss: 0.7003, Elapsed: 11m31s
2020-05-09 15:18:20.554414: Epoch: 1, Batch: 963, Loss: 0.6405, Elapsed: 6m27s
2020-05-09 15:22:44.010896: Epoch: 1, Batch: 977, Loss: 0.7224, Elapsed: 11m35s
2020-05-09 15:26:11.549088: Epoch: 1, Batch: 964, Loss: 0.6696, Elapsed: 7m50s
2020-05-09 15:28:57.849068: Epoch: 1, Batch: 978, Loss: 0.6875, Elapsed: 6m13s
2020-05-09 15:29:54.698544: Epoch: 1, Batch: 556, Loss: 0.6974, Elapsed: 14m28s
2020-05-09 15:32:19.358229: Epoch: 1, Batch: 965, Loss: 0.6670, Elapsed: 6m7s
2020-05-09 15:36:14.745560: Epoch: 1, Batch: 979, Loss: 0.6951, Elapsed: 7m16s
2020-05-09 15:38:31.607702: Epoch: 1, Batch: 966, Loss: 0.6359, Elapsed: 6m12s
2020-05-09 15:43:21.627512: Epoch: 1, Batch: 557, Loss: 0.6974, Elapsed: 13m26s
2020-05-09 15:43:46.826294: Epoch: 1, Batch: 967, Loss: 0.6531, Elapsed: 5m15s
2020-05-09 15:43:57.823003: Epoch: 1, Batch: 980, Loss: 0.6986, Elapsed: 7m43s
2020-05-09 15:49:06.765579: Epoch: 1, Batch: 968, Loss: 0.6558, Elapsed: 5m19s
2020-05-09 15:57:08.163359: Epoch: 1, Batch: 981, Loss: 0.6726, Elapsed: 13m10s
2020-05-09 15:57:25.798134: Epoch: 1, Batch: 969, Loss: 0.6488, Elapsed: 8m19s
2020-05-09 15:59:34.355622: Epoch: 1, Batch: 558, Loss: 0.7056, Elapsed: 16m12s
2020-05-09 16:02:41.863390: Epoch: 1, Batch: 970, Loss: 0.6574, Elapsed: 5m16s
2020-05-09 16:03:13.174915: Epoch: 1, Batch: 982, Loss: 0.6912, Elapsed: 6m4s
2020-05-09 16:11:59.260052: Epoch: 1, Batch: 983, Loss: 0.7106, Elapsed: 8m46s
2020-05-09 16:12:35.976285: Epoch: 1, Batch: 971, Loss: 0.6763, Elapsed: 9m54s
2020-05-09 16:16:18.006902: Epoch: 1, Batch: 559, Loss: 0.6811, Elapsed: 16m43s
2020-05-09 16:21:08.571515: Epoch: 1, Batch: 984, Loss: 0.7092, Elapsed: 9m9s
2020-05-09 16:21:21.078594: Epoch: 1, Batch: 972, Loss: 0.6615, Elapsed: 8m45s
2020-05-09 16:27:11.100102: Epoch: 1, Batch: 560, Loss: 0.6981, Elapsed: 10m53s
2020-05-09 16:27:18.810198: Epoch: 1, Batch: 985, Loss: 0.7179, Elapsed: 6m10s
2020-05-09 16:27:45.588205: Epoch: 1, Batch: 973, Loss: 0.6591, Elapsed: 6m24s
2020-05-09 16:34:03.283089: Epoch: 1, Batch: 974, Loss: 0.6687, Elapsed: 6m17s
2020-05-09 16:36:25.988911: Epoch: 1, Batch: 986, Loss: 0.7195, Elapsed: 9m7s
2020-05-09 16:38:03.170906: Epoch: 1, Batch: 561, Loss: 0.7130, Elapsed: 10m52s
2020-05-09 16:38:59.344512: Epoch: 1, Batch: 975, Loss: 0.6654, Elapsed: 4m56s
2020-05-09 16:43:13.713977: Epoch: 1, Batch: 987, Loss: 0.7315, Elapsed: 6m47s
2020-05-09 16:47:30.087515: Epoch: 1, Batch: 976, Loss: 0.6568, Elapsed: 8m30s
2020-05-09 16:49:35.486049: Epoch: 1, Batch: 988, Loss: 0.6799, Elapsed: 6m21s
2020-05-09 16:52:22.052091: Epoch: 1, Batch: 562, Loss: 0.6984, Elapsed: 14m18s
2020-05-09 16:55:28.026503: Epoch: 1, Batch: 989, Loss: 0.6917, Elapsed: 5m52s
2020-05-09 16:55:29.621698: Epoch: 1, Batch: 977, Loss: 0.6783, Elapsed: 7m59s
2020-05-09 17:01:01.589811: Epoch: 1, Batch: 978, Loss: 0.6742, Elapsed: 5m31s
2020-05-09 17:01:44.004896: Epoch: 1, Batch: 563, Loss: 0.7194, Elapsed: 9m21s
2020-05-09 17:03:14.952425: Epoch: 1, Batch: 990, Loss: 0.7064, Elapsed: 7m46s
2020-05-09 17:08:04.076770: Epoch: 1, Batch: 979, Loss: 0.6521, Elapsed: 7m2s
2020-05-09 17:13:02.412907: Epoch: 1, Batch: 991, Loss: 0.7193, Elapsed: 9m47s
2020-05-09 17:13:15.677664: Epoch: 1, Batch: 980, Loss: 0.6554, Elapsed: 5m11s
2020-05-09 17:16:33.317875: Epoch: 1, Batch: 564, Loss: 0.7028, Elapsed: 14m49s
2020-05-09 17:20:07.972804: Epoch: 1, Batch: 992, Loss: 0.7008, Elapsed: 7m5s
2020-05-09 17:21:15.065193: Epoch: 1, Batch: 981, Loss: 0.6718, Elapsed: 7m59s
2020-05-09 17:25:21.656133: Epoch: 1, Batch: 993, Loss: 0.7298, Elapsed: 5m13s
2020-05-09 17:26:42.625806: Epoch: 1, Batch: 982, Loss: 0.6325, Elapsed: 5m27s
2020-05-09 17:28:15.785240: Epoch: 1, Batch: 565, Loss: 0.6908, Elapsed: 11m42s
2020-05-09 17:33:07.675397: Epoch: 1, Batch: 983, Loss: 0.6787, Elapsed: 6m25s
2020-05-09 17:33:46.090727: Epoch: 1, Batch: 994, Loss: 0.7001, Elapsed: 8m24s
2020-05-09 17:37:28.454846: Epoch: 1, Batch: 566, Loss: 0.6843, Elapsed: 9m12s
2020-05-09 17:41:21.300034: Epoch: 1, Batch: 995, Loss: 0.7051, Elapsed: 7m35s
2020-05-09 17:42:46.920156: Epoch: 1, Batch: 984, Loss: 0.6825, Elapsed: 9m39s
2020-05-09 17:46:21.253135: Epoch: 1, Batch: 996, Loss: 0.6999, Elapsed: 4m59s
2020-05-09 17:50:00.156278: Epoch: 1, Batch: 985, Loss: 0.6413, Elapsed: 7m13s
2020-05-09 17:50:08.655415: Epoch: 1, Batch: 567, Loss: 0.7094, Elapsed: 12m40s
2020-05-09 17:51:19.939320: Epoch: 1, Batch: 997, Loss: 0.7040, Elapsed: 4m58s
2020-05-09 17:54:46.441340: Epoch: 1, Batch: 986, Loss: 0.6455, Elapsed: 4m46s
2020-05-09 17:56:52.205532: Epoch: 1, Batch: 568, Loss: 0.7551, Elapsed: 6m43s
2020-05-09 17:57:00.328705: Epoch: 1, Batch: 998, Loss: 0.7203, Elapsed: 5m40s
2020-05-09 17:59:28.010660: Epoch: 1, Batch: 987, Loss: 0.6306, Elapsed: 4m41s
2020-05-09 18:01:53.326095: Epoch: 1, Batch: 999, Loss: 0.7136, Elapsed: 4m52s
2020-05-09 18:05:57.945946: Epoch: 1, Batch: 988, Loss: 0.6481, Elapsed: 6m29s
2020-05-09 18:08:42.710103: Epoch: 1, Batch: 1000, Loss: 0.6898, Elapsed: 6m49s
Starting testing the validation set with 200 subgraphs!
2020-05-09 18:12:20.524352: Epoch: 1, Batch: 569, Loss: 0.6889, Elapsed: 15m28s
2020-05-09 18:13:32.442668: Epoch: 1, Batch: 989, Loss: 0.6705, Elapsed: 7m34s
2020-05-09 18:19:04.080092: Epoch: 1, Batch: 990, Loss: 0.6549, Elapsed: 5m31s
2020-05-09 18:24:16.144144: Epoch: 1, Batch: 570, Loss: 0.6848, Elapsed: 11m55s
2020-05-09 18:28:51.972318: Epoch: 1, Batch: 991, Loss: 0.6973, Elapsed: 9m47s
2020-05-09 18:33:35.614541: Epoch: 1, Batch: 571, Loss: 0.7173, Elapsed: 9m19s
2020-05-09 18:40:37.902198: Epoch: 1, Batch: 992, Loss: 0.7019, Elapsed: 11m45s
2020-05-09 18:40:51.389011: Epoch: 1, Batch: 572, Loss: 0.7030, Elapsed: 7m15s
2020-05-09 18:47:21.246514: Epoch: 1, Batch: 993, Loss: 0.6735, Elapsed: 6m43s
2020-05-09 18:50:06.468587: Epoch: 1, Batch: 573, Loss: 0.7174, Elapsed: 9m15s
2020-05-09 18:50:48.466569: Validation Test:  Loss: 0.7093,  Acc: 52.3215, AUC: 0.5306, Precision: 0.5953 -- Elapsed: 42m5s
2020-05-09 18:53:42.890693: Epoch: 1, Batch: 994, Loss: 0.6506, Elapsed: 6m21s
2020-05-09 18:58:39.654282: Epoch: 1, Batch: 1001, Loss: 0.7062, Elapsed: 7m51s
2020-05-09 18:58:51.493403: Epoch: 1, Batch: 995, Loss: 0.6462, Elapsed: 5m8s
2020-05-09 19:00:47.545861: Epoch: 1, Batch: 574, Loss: 0.7374, Elapsed: 10m41s
2020-05-09 19:04:16.241613: Epoch: 1, Batch: 996, Loss: 0.6588, Elapsed: 5m24s
2020-05-09 19:05:37.798650: Epoch: 1, Batch: 1002, Loss: 0.7224, Elapsed: 6m58s
2020-05-09 19:09:17.758738: Epoch: 1, Batch: 575, Loss: 0.7239, Elapsed: 8m30s
2020-05-09 19:11:56.078689: Epoch: 1, Batch: 1003, Loss: 0.7085, Elapsed: 6m18s
2020-05-09 19:14:49.216628: Epoch: 1, Batch: 997, Loss: 0.6601, Elapsed: 10m32s
2020-05-09 19:20:12.273727: Epoch: 1, Batch: 1004, Loss: 0.7139, Elapsed: 8m16s
2020-05-09 19:25:07.666099: Epoch: 1, Batch: 1005, Loss: 0.6926, Elapsed: 4m55s
2020-05-09 19:26:22.846417: Epoch: 1, Batch: 998, Loss: 0.7206, Elapsed: 11m33s
2020-05-09 19:31:09.865160: Epoch: 1, Batch: 999, Loss: 0.6453, Elapsed: 4m46s
2020-05-09 19:31:33.068706: Epoch: 1, Batch: 576, Loss: 0.6742, Elapsed: 22m15s
2020-05-09 19:34:57.129672: Epoch: 1, Batch: 1006, Loss: 0.7169, Elapsed: 9m49s
2020-05-09 19:38:24.758049: Epoch: 1, Batch: 1000, Loss: 0.6775, Elapsed: 7m14s
Starting testing the validation set with 200 subgraphs!
2020-05-09 19:39:40.932431: Epoch: 1, Batch: 1007, Loss: 0.7034, Elapsed: 4m43s
2020-05-09 19:45:36.273449: Epoch: 1, Batch: 1008, Loss: 0.7069, Elapsed: 5m55s
2020-05-09 19:45:48.189095: Epoch: 1, Batch: 577, Loss: 0.7138, Elapsed: 14m15s
2020-05-09 19:53:43.356160: Epoch: 1, Batch: 1009, Loss: 0.7121, Elapsed: 8m7s
2020-05-09 19:59:08.682133: Epoch: 1, Batch: 578, Loss: 0.6891, Elapsed: 13m20s
2020-05-09 20:02:50.661865: Epoch: 1, Batch: 1010, Loss: 0.7292, Elapsed: 9m7s
2020-05-09 20:09:44.701594: Epoch: 1, Batch: 1011, Loss: 0.7125, Elapsed: 6m54s
2020-05-09 20:13:14.476223: Epoch: 1, Batch: 579, Loss: 0.6986, Elapsed: 14m5s
2020-05-09 20:13:16.918642: Epoch: 1, Batch: 1012, Loss: 0.7090, Elapsed: 3m32s
2020-05-09 20:20:14.900881: Epoch: 1, Batch: 1013, Loss: 0.7121, Elapsed: 6m57s
2020-05-09 20:20:41.559307: Validation Test:  Loss: 0.6710,  Acc: 57.7232, AUC: 0.6077, Precision: 0.6308 -- Elapsed: 42m16s
2020-05-09 20:23:23.131323: Epoch: 1, Batch: 1014, Loss: 0.7248, Elapsed: 3m8s
2020-05-09 20:24:13.501405: Epoch: 1, Batch: 580, Loss: 0.7082, Elapsed: 10m59s
2020-05-09 20:28:33.601415: Epoch: 1, Batch: 1001, Loss: 0.6762, Elapsed: 7m52s
2020-05-09 20:29:59.362578: Epoch: 1, Batch: 1015, Loss: 0.6850, Elapsed: 6m36s
2020-05-09 20:35:29.949207: Epoch: 1, Batch: 1016, Loss: 0.7250, Elapsed: 5m30s
2020-05-09 20:35:38.741688: Epoch: 1, Batch: 1002, Loss: 0.6783, Elapsed: 7m5s
2020-05-09 20:39:02.261285: Epoch: 1, Batch: 581, Loss: 0.7069, Elapsed: 14m48s
2020-05-09 20:40:37.961153: Epoch: 1, Batch: 1003, Loss: 0.6603, Elapsed: 4m59s
2020-05-09 20:43:16.691274: Epoch: 1, Batch: 1017, Loss: 0.7347, Elapsed: 7m46s
2020-05-09 20:47:22.188121: Epoch: 1, Batch: 582, Loss: 0.7833, Elapsed: 8m19s
2020-05-09 20:47:39.439587: Epoch: 1, Batch: 1004, Loss: 0.6535, Elapsed: 7m1s
2020-05-09 20:49:08.101400: Epoch: 1, Batch: 1018, Loss: 0.7247, Elapsed: 5m51s
2020-05-09 20:55:59.912180: Epoch: 1, Batch: 1019, Loss: 0.7171, Elapsed: 6m51s
2020-05-09 20:59:37.414965: Epoch: 1, Batch: 1005, Loss: 0.6879, Elapsed: 11m57s
2020-05-09 21:02:26.706767: Epoch: 1, Batch: 1020, Loss: 0.7300, Elapsed: 6m26s
2020-05-09 21:04:43.279822: Epoch: 1, Batch: 1006, Loss: 0.6671, Elapsed: 5m5s
2020-05-09 21:07:05.745160: Epoch: 1, Batch: 583, Loss: 0.6656, Elapsed: 19m43s
2020-05-09 21:11:22.641373: Epoch: 1, Batch: 1021, Loss: 0.7113, Elapsed: 8m55s
2020-05-09 21:12:49.281241: Epoch: 1, Batch: 1007, Loss: 0.6610, Elapsed: 8m5s
2020-05-09 21:19:46.840501: Epoch: 1, Batch: 584, Loss: 0.7095, Elapsed: 12m41s
2020-05-09 21:20:35.116657: Epoch: 1, Batch: 1022, Loss: 0.7116, Elapsed: 9m12s
2020-05-09 21:21:50.342626: Epoch: 1, Batch: 1008, Loss: 0.6736, Elapsed: 9m1s
2020-05-09 21:28:13.605243: Epoch: 1, Batch: 1009, Loss: 0.6768, Elapsed: 6m23s
2020-05-09 21:31:43.163498: Epoch: 1, Batch: 1023, Loss: 0.7171, Elapsed: 11m8s
2020-05-09 21:33:53.425545: Epoch: 1, Batch: 1010, Loss: 0.6470, Elapsed: 5m39s
2020-05-09 21:34:34.624710: Epoch: 1, Batch: 585, Loss: 0.6966, Elapsed: 14m47s
2020-05-09 21:36:08.566055: Epoch: 1, Batch: 1024, Loss: 0.7130, Elapsed: 4m25s
2020-05-09 21:41:52.370000: Epoch: 1, Batch: 1011, Loss: 0.6445, Elapsed: 7m58s
2020-05-09 21:42:08.940604: Epoch: 1, Batch: 1025, Loss: 0.7124, Elapsed: 6m0s
2020-05-09 21:45:54.995727: Epoch: 1, Batch: 1026, Loss: 0.7418, Elapsed: 3m46s
2020-05-09 21:48:23.078045: Epoch: 1, Batch: 586, Loss: 0.7259, Elapsed: 13m48s
2020-05-09 21:49:24.381873: Epoch: 1, Batch: 1012, Loss: 0.6708, Elapsed: 7m31s
2020-05-09 21:52:03.323071: Epoch: 1, Batch: 1027, Loss: 0.7378, Elapsed: 6m8s
2020-05-09 21:55:48.867938: Epoch: 1, Batch: 1013, Loss: 0.6560, Elapsed: 6m24s
2020-05-09 22:01:04.544512: Epoch: 1, Batch: 1028, Loss: 0.7294, Elapsed: 9m1s
2020-05-09 22:02:05.899639: Epoch: 1, Batch: 1014, Loss: 0.6733, Elapsed: 6m17s
2020-05-09 22:07:44.727293: Epoch: 1, Batch: 1015, Loss: 0.6618, Elapsed: 5m38s
2020-05-09 22:08:36.540672: Epoch: 1, Batch: 587, Loss: 0.6827, Elapsed: 20m13s
2020-05-09 22:11:07.058139: Epoch: 1, Batch: 1029, Loss: 0.7211, Elapsed: 10m2s
2020-05-09 22:12:21.795693: Epoch: 1, Batch: 1016, Loss: 0.6401, Elapsed: 4m37s
2020-05-09 22:18:03.158567: Epoch: 1, Batch: 1017, Loss: 0.6616, Elapsed: 5m41s
2020-05-09 22:18:18.311976: Epoch: 1, Batch: 1030, Loss: 0.6971, Elapsed: 7m11s
2020-05-09 22:22:44.897782: Epoch: 1, Batch: 1018, Loss: 0.6478, Elapsed: 4m41s
2020-05-09 22:26:02.671704: Epoch: 1, Batch: 588, Loss: 0.6908, Elapsed: 17m26s
2020-05-09 22:26:49.370020: Epoch: 1, Batch: 1031, Loss: 0.7474, Elapsed: 8m31s
2020-05-09 22:28:53.578294: Epoch: 1, Batch: 1019, Loss: 0.6502, Elapsed: 6m8s
2020-05-09 22:32:17.422885: Epoch: 1, Batch: 1032, Loss: 0.7184, Elapsed: 5m28s
2020-05-09 22:34:40.402669: Epoch: 1, Batch: 1020, Loss: 0.6513, Elapsed: 5m46s
2020-05-09 22:38:52.440810: Epoch: 1, Batch: 1021, Loss: 0.6748, Elapsed: 4m12s
2020-05-09 22:41:35.524945: Epoch: 1, Batch: 1033, Loss: 0.7245, Elapsed: 9m18s
2020-05-09 22:41:57.391612: Epoch: 1, Batch: 589, Loss: 0.7282, Elapsed: 15m54s
2020-05-09 22:46:04.547015: Epoch: 1, Batch: 1022, Loss: 0.6856, Elapsed: 7m12s
2020-05-09 22:47:33.279672: Epoch: 1, Batch: 1034, Loss: 0.7129, Elapsed: 5m57s
2020-05-09 22:52:48.310157: Epoch: 1, Batch: 590, Loss: 0.7611, Elapsed: 10m50s
2020-05-09 22:53:27.376543: Epoch: 1, Batch: 1023, Loss: 0.6947, Elapsed: 7m22s
2020-05-09 23:00:04.346439: Epoch: 1, Batch: 1035, Loss: 0.7445, Elapsed: 12m31s
2020-05-09 23:01:24.346292: Epoch: 1, Batch: 1024, Loss: 0.6758, Elapsed: 7m56s
2020-05-09 23:05:41.490509: Epoch: 1, Batch: 591, Loss: 0.7466, Elapsed: 12m53s
2020-05-09 23:07:07.321305: Epoch: 1, Batch: 1036, Loss: 0.7350, Elapsed: 7m2s
2020-05-09 23:08:12.989665: Epoch: 1, Batch: 1025, Loss: 0.6508, Elapsed: 6m48s
2020-05-09 23:14:48.209393: Epoch: 1, Batch: 1037, Loss: 0.7288, Elapsed: 7m40s
2020-05-09 23:18:42.145374: Epoch: 1, Batch: 1026, Loss: 0.6751, Elapsed: 10m29s
2020-05-09 23:23:49.472556: Epoch: 1, Batch: 1027, Loss: 0.6562, Elapsed: 5m7s
2020-05-09 23:24:25.606578: Epoch: 1, Batch: 1038, Loss: 0.7441, Elapsed: 9m37s
2020-05-09 23:26:50.787383: Epoch: 1, Batch: 592, Loss: 0.7168, Elapsed: 21m9s
2020-05-09 23:32:31.092513: Epoch: 1, Batch: 1039, Loss: 0.7336, Elapsed: 8m5s
2020-05-09 23:34:24.897374: Epoch: 1, Batch: 1028, Loss: 0.6972, Elapsed: 10m35s
2020-05-09 23:35:57.043776: Epoch: 1, Batch: 1040, Loss: 0.7251, Elapsed: 3m25s
2020-05-09 23:42:11.192779: Epoch: 1, Batch: 593, Loss: 0.7081, Elapsed: 15m20s
2020-05-09 23:43:26.465323: Epoch: 1, Batch: 1041, Loss: 0.7778, Elapsed: 7m29s
2020-05-09 23:43:26.854639: Epoch: 1, Batch: 1029, Loss: 0.6804, Elapsed: 9m1s
2020-05-09 23:51:43.530025: Epoch: 1, Batch: 1042, Loss: 0.7278, Elapsed: 8m17s
2020-05-09 23:56:50.483647: Epoch: 1, Batch: 594, Loss: 0.7058, Elapsed: 14m39s
2020-05-09 23:57:13.692592: Epoch: 1, Batch: 1030, Loss: 0.6886, Elapsed: 13m46s
2020-05-09 23:58:35.385003: Epoch: 1, Batch: 1043, Loss: 0.7265, Elapsed: 6m51s
2020-05-10 00:04:20.657185: Epoch: 1, Batch: 1044, Loss: 0.7521, Elapsed: 5m45s
2020-05-10 00:05:06.710674: Epoch: 1, Batch: 1031, Loss: 0.6561, Elapsed: 7m52s
2020-05-10 00:10:25.747188: Epoch: 1, Batch: 1032, Loss: 0.6543, Elapsed: 5m19s
2020-05-10 00:11:35.945419: Epoch: 1, Batch: 1045, Loss: 0.7121, Elapsed: 7m15s
2020-05-10 00:11:36.106154: Epoch: 1, Batch: 595, Loss: 0.7154, Elapsed: 14m45s
2020-05-10 00:17:01.465389: Epoch: 1, Batch: 1046, Loss: 0.7437, Elapsed: 5m25s
2020-05-10 00:17:12.975761: Epoch: 1, Batch: 1033, Loss: 0.6478, Elapsed: 6m47s
2020-05-10 00:21:13.552690: Epoch: 1, Batch: 596, Loss: 0.7485, Elapsed: 9m37s
2020-05-10 00:24:45.172232: Epoch: 1, Batch: 1047, Loss: 0.7560, Elapsed: 7m43s
2020-05-10 00:25:33.762563: Epoch: 1, Batch: 1034, Loss: 0.6931, Elapsed: 8m20s
2020-05-10 00:31:41.019173: Epoch: 1, Batch: 1048, Loss: 0.7063, Elapsed: 6m55s
2020-05-10 00:34:47.035963: Epoch: 1, Batch: 1035, Loss: 0.6858, Elapsed: 9m13s
2020-05-10 00:37:54.813002: Epoch: 1, Batch: 597, Loss: 0.6787, Elapsed: 16m41s
2020-05-10 00:38:24.107842: Epoch: 1, Batch: 1049, Loss: 0.7162, Elapsed: 6m43s
2020-05-10 00:42:51.381169: Epoch: 1, Batch: 1036, Loss: 0.6639, Elapsed: 8m4s
2020-05-10 00:44:41.939130: Epoch: 1, Batch: 1050, Loss: 0.7524, Elapsed: 6m17s
Starting testing the validation set with 200 subgraphs!
2020-05-10 00:45:49.084360: Epoch: 1, Batch: 1037, Loss: 0.6374, Elapsed: 2m57s
2020-05-10 00:46:09.532440: Epoch: 1, Batch: 598, Loss: 0.7263, Elapsed: 8m14s
2020-05-10 00:50:27.815084: Epoch: 1, Batch: 1038, Loss: 0.6574, Elapsed: 4m38s
2020-05-10 00:56:43.540443: Epoch: 1, Batch: 1039, Loss: 0.6615, Elapsed: 6m15s
2020-05-10 01:03:52.496531: Epoch: 1, Batch: 599, Loss: 0.6845, Elapsed: 17m42s
2020-05-10 01:04:18.372068: Epoch: 1, Batch: 1040, Loss: 0.6624, Elapsed: 7m34s
2020-05-10 01:12:52.190462: Epoch: 1, Batch: 1041, Loss: 0.6996, Elapsed: 8m33s
2020-05-10 01:15:34.123325: Epoch: 1, Batch: 600, Loss: 0.6812, Elapsed: 11m41s
Starting testing the validation set with 200 subgraphs!
2020-05-10 01:16:12.575517: Epoch: 1, Batch: 1042, Loss: 0.6552, Elapsed: 3m20s
2020-05-10 01:22:44.724511: Epoch: 1, Batch: 1043, Loss: 0.6699, Elapsed: 6m32s
2020-05-10 01:25:58.998028: Validation Test:  Loss: 0.7258,  Acc: 51.9249, AUC: 0.5278, Precision: 0.5840 -- Elapsed: 41m17s
2020-05-10 01:29:46.495524: Epoch: 1, Batch: 1044, Loss: 0.6861, Elapsed: 7m1s
2020-05-10 01:34:25.458005: Epoch: 1, Batch: 1051, Loss: 0.7469, Elapsed: 8m26s
2020-05-10 01:37:21.255701: Epoch: 1, Batch: 1045, Loss: 0.6761, Elapsed: 7m34s
2020-05-10 01:42:10.398123: Epoch: 1, Batch: 1046, Loss: 0.6611, Elapsed: 4m49s
2020-05-10 01:42:25.719331: Epoch: 1, Batch: 1052, Loss: 0.7280, Elapsed: 8m0s
2020-05-10 01:47:37.901724: Epoch: 1, Batch: 1053, Loss: 0.7476, Elapsed: 5m12s
2020-05-10 01:52:00.104505: Epoch: 1, Batch: 1054, Loss: 0.7034, Elapsed: 4m22s
2020-05-10 01:53:47.891330: Epoch: 1, Batch: 1047, Loss: 0.7304, Elapsed: 11m37s
2020-05-10 01:57:07.116863: Epoch: 1, Batch: 1055, Loss: 0.7360, Elapsed: 5m6s
2020-05-10 01:58:29.993045: Epoch: 1, Batch: 1048, Loss: 0.6826, Elapsed: 4m42s
2020-05-10 02:04:34.031433: Epoch: 1, Batch: 1056, Loss: 0.7179, Elapsed: 7m26s
2020-05-10 02:08:08.821648: Epoch: 1, Batch: 1049, Loss: 0.6847, Elapsed: 9m38s
2020-05-10 02:08:35.858684: Validation Test:  Loss: 0.7011,  Acc: 57.7648, AUC: 0.5992, Precision: 0.6323 -- Elapsed: 53m1s
2020-05-10 02:12:57.929698: Epoch: 1, Batch: 1057, Loss: 0.7364, Elapsed: 8m23s
2020-05-10 02:14:37.163654: Epoch: 1, Batch: 1050, Loss: 0.6935, Elapsed: 6m28s
Starting testing the validation set with 200 subgraphs!
2020-05-10 02:15:50.312923: Epoch: 1, Batch: 601, Loss: 0.7468, Elapsed: 7m14s
2020-05-10 02:18:51.933397: Epoch: 1, Batch: 1058, Loss: 0.7274, Elapsed: 5m53s
2020-05-10 02:25:48.583881: Epoch: 1, Batch: 1059, Loss: 0.7030, Elapsed: 6m56s
2020-05-10 02:27:15.813983: Epoch: 1, Batch: 602, Loss: 0.7010, Elapsed: 11m25s
2020-05-10 02:30:03.789240: Epoch: 1, Batch: 1060, Loss: 0.6959, Elapsed: 4m15s
2020-05-10 02:33:47.320793: Epoch: 1, Batch: 603, Loss: 0.7301, Elapsed: 6m31s
2020-05-10 02:35:19.127792: Epoch: 1, Batch: 1061, Loss: 0.7369, Elapsed: 5m15s
2020-05-10 02:42:56.169451: Epoch: 1, Batch: 1062, Loss: 0.7539, Elapsed: 7m37s
2020-05-10 02:47:47.148561: Epoch: 1, Batch: 1063, Loss: 0.7302, Elapsed: 4m50s
2020-05-10 02:56:15.762299: Epoch: 1, Batch: 604, Loss: 0.6783, Elapsed: 22m28s
2020-05-10 02:56:40.562342: Epoch: 1, Batch: 1064, Loss: 0.7498, Elapsed: 8m53s
2020-05-10 02:56:45.521133: Validation Test:  Loss: 0.6739,  Acc: 57.0527, AUC: 0.6069, Precision: 0.6208 -- Elapsed: 42m8s
2020-05-10 03:01:57.244415: Epoch: 1, Batch: 1065, Loss: 0.7248, Elapsed: 5m16s
2020-05-10 03:05:41.151997: Epoch: 1, Batch: 1051, Loss: 0.6875, Elapsed: 8m55s
2020-05-10 03:06:36.451163: Epoch: 1, Batch: 1066, Loss: 0.6783, Elapsed: 4m39s
2020-05-10 03:11:18.088512: Epoch: 1, Batch: 605, Loss: 0.6799, Elapsed: 15m2s
2020-05-10 03:13:57.456200: Epoch: 1, Batch: 1052, Loss: 0.6984, Elapsed: 8m16s
2020-05-10 03:14:18.667549: Epoch: 1, Batch: 1067, Loss: 0.7159, Elapsed: 7m42s
2020-05-10 03:18:44.163600: Epoch: 1, Batch: 1068, Loss: 0.7263, Elapsed: 4m25s
2020-05-10 03:20:36.353774: Epoch: 1, Batch: 606, Loss: 0.7214, Elapsed: 9m18s
2020-05-10 03:21:01.640307: Epoch: 1, Batch: 1053, Loss: 0.6659, Elapsed: 7m4s
2020-05-10 03:24:23.069932: Epoch: 1, Batch: 1069, Loss: 0.7305, Elapsed: 5m38s
2020-05-10 03:27:35.614367: Epoch: 1, Batch: 1054, Loss: 0.6660, Elapsed: 6m33s
2020-05-10 03:29:10.952619: Epoch: 1, Batch: 1070, Loss: 0.7061, Elapsed: 4m47s
2020-05-10 03:34:48.556842: Epoch: 1, Batch: 1071, Loss: 0.7143, Elapsed: 5m37s
2020-05-10 03:35:46.703793: Epoch: 1, Batch: 1055, Loss: 0.6733, Elapsed: 8m11s
2020-05-10 03:37:27.313062: Epoch: 1, Batch: 607, Loss: 0.6834, Elapsed: 16m50s
2020-05-10 03:40:49.957977: Epoch: 1, Batch: 1072, Loss: 0.6827, Elapsed: 6m1s
2020-05-10 03:44:58.963659: Epoch: 1, Batch: 1056, Loss: 0.6810, Elapsed: 9m12s
2020-05-10 03:45:56.134055: Epoch: 1, Batch: 608, Loss: 0.7433, Elapsed: 8m28s
2020-05-10 03:50:32.077093: Epoch: 1, Batch: 1073, Loss: 0.7180, Elapsed: 9m42s
2020-05-10 03:54:18.090417: Epoch: 1, Batch: 1057, Loss: 0.6904, Elapsed: 9m19s
2020-05-10 03:58:13.949246: Epoch: 1, Batch: 1074, Loss: 0.7333, Elapsed: 7m41s
2020-05-10 03:59:26.510236: Epoch: 1, Batch: 609, Loss: 0.6832, Elapsed: 13m30s
2020-05-10 04:00:42.872477: Epoch: 1, Batch: 1058, Loss: 0.6398, Elapsed: 6m24s
2020-05-10 04:04:07.853066: Epoch: 1, Batch: 1059, Loss: 0.6318, Elapsed: 3m24s
2020-05-10 04:05:50.623005: Epoch: 1, Batch: 1075, Loss: 0.7431, Elapsed: 7m36s
2020-05-10 04:10:33.427516: Epoch: 1, Batch: 1060, Loss: 0.6702, Elapsed: 6m25s
2020-05-10 04:14:04.006890: Epoch: 1, Batch: 610, Loss: 0.7024, Elapsed: 14m37s
2020-05-10 04:15:43.498506: Epoch: 1, Batch: 1061, Loss: 0.6543, Elapsed: 5m10s
2020-05-10 04:15:54.509351: Epoch: 1, Batch: 1076, Loss: 0.7244, Elapsed: 10m3s
2020-05-10 04:20:22.244147: Epoch: 1, Batch: 611, Loss: 0.7317, Elapsed: 6m18s
2020-05-10 04:21:26.555772: Epoch: 1, Batch: 1077, Loss: 0.6996, Elapsed: 5m32s
2020-05-10 04:23:26.097103: Epoch: 1, Batch: 1062, Loss: 0.6889, Elapsed: 7m42s
2020-05-10 04:30:09.150018: Epoch: 1, Batch: 1078, Loss: 0.7005, Elapsed: 8m42s
2020-05-10 04:31:38.595573: Epoch: 1, Batch: 1063, Loss: 0.6620, Elapsed: 8m12s
2020-05-10 04:33:35.649104: Epoch: 1, Batch: 612, Loss: 0.6877, Elapsed: 13m13s
2020-05-10 04:35:57.226704: Epoch: 1, Batch: 1064, Loss: 0.6576, Elapsed: 4m18s
2020-05-10 04:38:18.686844: Epoch: 1, Batch: 1079, Loss: 0.6958, Elapsed: 8m9s
2020-05-10 04:41:20.115832: Epoch: 1, Batch: 1065, Loss: 0.6564, Elapsed: 5m22s
2020-05-10 04:44:48.614231: Epoch: 1, Batch: 1080, Loss: 0.7214, Elapsed: 6m29s
2020-05-10 04:45:17.766470: Epoch: 1, Batch: 1066, Loss: 0.6813, Elapsed: 3m57s
2020-05-10 04:46:13.959307: Epoch: 1, Batch: 613, Loss: 0.7035, Elapsed: 12m38s
2020-05-10 04:53:06.276394: Epoch: 1, Batch: 1067, Loss: 0.6799, Elapsed: 7m48s
2020-05-10 04:55:04.315137: Epoch: 1, Batch: 1081, Loss: 0.7233, Elapsed: 10m15s
2020-05-10 04:56:49.208835: Epoch: 1, Batch: 1068, Loss: 0.6445, Elapsed: 3m42s
2020-05-10 04:57:25.642865: Epoch: 1, Batch: 614, Loss: 0.7018, Elapsed: 11m11s
2020-05-10 05:00:55.435330: Epoch: 1, Batch: 1082, Loss: 0.7059, Elapsed: 5m51s
2020-05-10 05:05:49.429939: Epoch: 1, Batch: 1083, Loss: 0.7220, Elapsed: 4m53s
2020-05-10 05:07:14.989692: Epoch: 1, Batch: 1069, Loss: 0.7158, Elapsed: 10m25s
2020-05-10 05:09:22.760826: Epoch: 1, Batch: 615, Loss: 0.6951, Elapsed: 11m57s
2020-05-10 05:11:35.922571: Epoch: 1, Batch: 1084, Loss: 0.7285, Elapsed: 5m46s
2020-05-10 05:13:03.359620: Epoch: 1, Batch: 1070, Loss: 0.6547, Elapsed: 5m48s
2020-05-10 05:16:46.218805: Epoch: 1, Batch: 1085, Loss: 0.7101, Elapsed: 5m10s
2020-05-10 05:20:15.887807: Epoch: 1, Batch: 616, Loss: 0.7011, Elapsed: 10m53s
2020-05-10 05:20:42.296768: Epoch: 1, Batch: 1071, Loss: 0.6711, Elapsed: 7m38s
2020-05-10 05:23:32.537788: Epoch: 1, Batch: 1086, Loss: 0.7566, Elapsed: 6m46s
2020-05-10 05:30:01.737601: Epoch: 1, Batch: 617, Loss: 0.6909, Elapsed: 9m45s
2020-05-10 05:30:59.166596: Epoch: 1, Batch: 1072, Loss: 0.6971, Elapsed: 10m16s
2020-05-10 05:37:43.939525: Epoch: 1, Batch: 1073, Loss: 0.6383, Elapsed: 6m44s
2020-05-10 05:39:43.926019: Epoch: 1, Batch: 1087, Loss: 0.7016, Elapsed: 16m11s
2020-05-10 05:45:45.307066: Epoch: 1, Batch: 618, Loss: 0.6912, Elapsed: 15m43s
2020-05-10 05:48:08.076346: Epoch: 1, Batch: 1074, Loss: 0.6976, Elapsed: 10m24s
2020-05-10 05:53:22.662169: Epoch: 1, Batch: 1088, Loss: 0.7357, Elapsed: 13m38s
2020-05-10 05:56:08.850355: Epoch: 1, Batch: 1075, Loss: 0.6756, Elapsed: 8m0s
2020-05-10 06:00:04.862629: Epoch: 1, Batch: 1089, Loss: 0.7137, Elapsed: 6m42s
2020-05-10 06:02:41.434858: Epoch: 1, Batch: 619, Loss: 0.6727, Elapsed: 16m56s
2020-05-10 06:03:15.907863: Epoch: 1, Batch: 1076, Loss: 0.6978, Elapsed: 7m7s
2020-05-10 06:09:56.758684: Epoch: 1, Batch: 1090, Loss: 0.7164, Elapsed: 9m51s
2020-05-10 06:15:16.304642: Epoch: 1, Batch: 1077, Loss: 0.6975, Elapsed: 12m0s
2020-05-10 06:19:32.904741: Epoch: 1, Batch: 1091, Loss: 0.7370, Elapsed: 9m36s
2020-05-10 06:20:06.250162: Epoch: 1, Batch: 620, Loss: 0.6893, Elapsed: 17m24s
2020-05-10 06:25:19.838900: Epoch: 1, Batch: 1078, Loss: 0.7356, Elapsed: 10m3s
2020-05-10 06:26:09.187719: Epoch: 1, Batch: 1092, Loss: 0.7112, Elapsed: 6m36s
2020-05-10 06:29:45.135597: Epoch: 1, Batch: 1079, Loss: 0.6652, Elapsed: 4m25s
2020-05-10 06:33:44.996918: Epoch: 1, Batch: 621, Loss: 0.6895, Elapsed: 13m38s
2020-05-10 06:36:54.580126: Epoch: 1, Batch: 1093, Loss: 0.7278, Elapsed: 10m45s
2020-05-10 06:42:16.726746: Epoch: 1, Batch: 1094, Loss: 0.7184, Elapsed: 5m22s
2020-05-10 06:42:53.035717: Epoch: 1, Batch: 1080, Loss: 0.7198, Elapsed: 13m7s
2020-05-10 06:46:49.110596: Epoch: 1, Batch: 1081, Loss: 0.6369, Elapsed: 3m56s
2020-05-10 06:47:37.999059: Epoch: 1, Batch: 622, Loss: 0.6846, Elapsed: 13m52s
2020-05-10 06:49:24.318862: Epoch: 1, Batch: 1095, Loss: 0.7163, Elapsed: 7m7s
2020-05-10 06:53:40.672720: Epoch: 1, Batch: 1082, Loss: 0.6815, Elapsed: 6m51s
2020-05-10 06:59:34.846701: Epoch: 1, Batch: 1096, Loss: 0.7054, Elapsed: 10m10s
2020-05-10 07:02:46.023605: Epoch: 1, Batch: 1083, Loss: 0.7197, Elapsed: 9m5s
2020-05-10 07:04:18.141032: Epoch: 1, Batch: 623, Loss: 0.6839, Elapsed: 16m40s
2020-05-10 07:05:11.982287: Epoch: 1, Batch: 1097, Loss: 0.6998, Elapsed: 5m37s
2020-05-10 07:09:54.377065: Epoch: 1, Batch: 1084, Loss: 0.6736, Elapsed: 7m8s
2020-05-10 07:12:53.545252: Epoch: 1, Batch: 624, Loss: 0.6999, Elapsed: 8m35s
2020-05-10 07:13:02.090986: Epoch: 1, Batch: 1098, Loss: 0.6897, Elapsed: 7m50s
2020-05-10 07:15:20.704932: Epoch: 1, Batch: 1085, Loss: 0.7113, Elapsed: 5m26s
2020-05-10 07:22:49.024081: Epoch: 1, Batch: 1086, Loss: 0.6704, Elapsed: 7m28s
2020-05-10 07:23:44.084881: Epoch: 1, Batch: 625, Loss: 0.6948, Elapsed: 10m50s
2020-05-10 07:23:59.031296: Epoch: 1, Batch: 1099, Loss: 0.7354, Elapsed: 10m56s
2020-05-10 07:31:12.282808: Epoch: 1, Batch: 1087, Loss: 0.6679, Elapsed: 8m23s
2020-05-10 07:33:19.946014: Epoch: 1, Batch: 1100, Loss: 0.7303, Elapsed: 9m20s
Starting testing the validation set with 200 subgraphs!
2020-05-10 07:36:25.001217: Epoch: 1, Batch: 626, Loss: 0.6904, Elapsed: 12m40s
2020-05-10 07:38:37.754086: Epoch: 1, Batch: 1088, Loss: 0.7049, Elapsed: 7m25s
2020-05-10 07:43:19.940874: Epoch: 1, Batch: 1089, Loss: 0.6911, Elapsed: 4m42s
2020-05-10 07:47:03.509474: Epoch: 1, Batch: 627, Loss: 0.7011, Elapsed: 10m38s
2020-05-10 07:47:51.657071: Epoch: 1, Batch: 1090, Loss: 0.6694, Elapsed: 4m31s
2020-05-10 07:52:33.289727: Epoch: 1, Batch: 1091, Loss: 0.6871, Elapsed: 4m41s
2020-05-10 07:59:22.755115: Epoch: 1, Batch: 1092, Loss: 0.7198, Elapsed: 6m49s
2020-05-10 08:03:39.562164: Epoch: 1, Batch: 628, Loss: 0.6798, Elapsed: 16m36s
2020-05-10 08:04:22.095442: Epoch: 1, Batch: 1093, Loss: 0.6768, Elapsed: 4m59s
2020-05-10 08:09:53.994801: Epoch: 1, Batch: 1094, Loss: 0.6856, Elapsed: 5m31s
2020-05-10 08:14:11.726900: Epoch: 1, Batch: 629, Loss: 0.6934, Elapsed: 10m32s
2020-05-10 08:15:26.350741: Validation Test:  Loss: 0.7123,  Acc: 52.2779, AUC: 0.5337, Precision: 0.5892 -- Elapsed: 42m6s
2020-05-10 08:19:44.982706: Epoch: 1, Batch: 1095, Loss: 0.6862, Elapsed: 9m50s
2020-05-10 08:21:17.382460: Epoch: 1, Batch: 1101, Loss: 0.6989, Elapsed: 5m51s
2020-05-10 08:23:19.749734: Epoch: 1, Batch: 630, Loss: 0.7004, Elapsed: 9m8s
2020-05-10 08:27:37.631445: Epoch: 1, Batch: 1102, Loss: 0.7357, Elapsed: 6m20s
2020-05-10 08:30:03.309613: Epoch: 1, Batch: 1096, Loss: 0.6700, Elapsed: 10m18s
2020-05-10 08:32:27.502357: Epoch: 1, Batch: 1103, Loss: 0.7007, Elapsed: 4m49s
2020-05-10 08:33:06.410300: Epoch: 1, Batch: 631, Loss: 0.6976, Elapsed: 9m46s
2020-05-10 08:34:51.631419: Epoch: 1, Batch: 1097, Loss: 0.6885, Elapsed: 4m48s
2020-05-10 08:42:54.025120: Epoch: 1, Batch: 1098, Loss: 0.6776, Elapsed: 8m2s
2020-05-10 08:43:08.527060: Epoch: 1, Batch: 632, Loss: 0.6943, Elapsed: 10m2s
2020-05-10 08:49:07.430766: Epoch: 1, Batch: 1104, Loss: 0.7372, Elapsed: 16m39s
2020-05-10 08:50:25.551976: Epoch: 1, Batch: 1099, Loss: 0.6803, Elapsed: 7m31s
2020-05-10 08:53:53.975505: Epoch: 1, Batch: 633, Loss: 0.7041, Elapsed: 10m45s
2020-05-10 08:55:54.284040: Epoch: 1, Batch: 1100, Loss: 0.6888, Elapsed: 5m28s
Starting testing the validation set with 200 subgraphs!
2020-05-10 08:57:58.225347: Epoch: 1, Batch: 1105, Loss: 0.7061, Elapsed: 8m50s
2020-05-10 09:02:25.648540: Epoch: 1, Batch: 1106, Loss: 0.7140, Elapsed: 4m27s
2020-05-10 09:06:30.577369: Epoch: 1, Batch: 634, Loss: 0.6811, Elapsed: 12m36s
2020-05-10 09:07:42.340717: Epoch: 1, Batch: 1107, Loss: 0.6944, Elapsed: 5m16s
2020-05-10 09:14:23.930488: Epoch: 1, Batch: 1108, Loss: 0.7303, Elapsed: 6m41s
2020-05-10 09:22:23.028485: Epoch: 1, Batch: 1109, Loss: 0.7377, Elapsed: 7m59s
2020-05-10 09:28:12.980403: Epoch: 1, Batch: 1110, Loss: 0.7032, Elapsed: 5m49s
2020-05-10 09:30:52.124095: Epoch: 1, Batch: 635, Loss: 0.6815, Elapsed: 24m21s
2020-05-10 09:34:14.510040: Epoch: 1, Batch: 1111, Loss: 0.6967, Elapsed: 6m1s
2020-05-10 09:38:05.809427: Validation Test:  Loss: 0.6836,  Acc: 58.6859, AUC: 0.6094, Precision: 0.6366 -- Elapsed: 42m11s
2020-05-10 09:41:10.897019: Epoch: 1, Batch: 636, Loss: 0.6878, Elapsed: 10m18s
2020-05-10 09:41:56.041044: Epoch: 1, Batch: 1112, Loss: 0.7030, Elapsed: 7m41s
2020-05-10 09:44:04.044303: Epoch: 1, Batch: 1101, Loss: 0.6771, Elapsed: 5m58s
2020-05-10 09:48:32.083082: Epoch: 1, Batch: 1113, Loss: 0.7489, Elapsed: 6m36s
2020-05-10 09:51:58.523183: Epoch: 1, Batch: 1102, Loss: 0.6830, Elapsed: 7m54s
2020-05-10 09:55:37.888590: Epoch: 1, Batch: 1114, Loss: 0.7181, Elapsed: 7m5s
2020-05-10 09:55:34.955569: Epoch: 1, Batch: 637, Loss: 0.6801, Elapsed: 14m24s
2020-05-10 09:58:53.560982: Epoch: 1, Batch: 1103, Loss: 0.6913, Elapsed: 6m55s
2020-05-10 09:59:37.811817: Epoch: 1, Batch: 1115, Loss: 0.6855, Elapsed: 3m59s
2020-05-10 10:05:09.174764: Epoch: 1, Batch: 1104, Loss: 0.6870, Elapsed: 6m15s
2020-05-10 10:05:34.733023: Epoch: 1, Batch: 1116, Loss: 0.7091, Elapsed: 5m56s
2020-05-10 10:09:10.623960: Epoch: 1, Batch: 1105, Loss: 0.6803, Elapsed: 4m1s
2020-05-10 10:11:44.016588: Epoch: 1, Batch: 1117, Loss: 0.7258, Elapsed: 6m9s
2020-05-10 10:15:03.114228: Epoch: 1, Batch: 1106, Loss: 0.6963, Elapsed: 5m52s
2020-05-10 10:15:57.968356: Epoch: 1, Batch: 1118, Loss: 0.7046, Elapsed: 4m13s
2020-05-10 10:17:13.988708: Epoch: 1, Batch: 638, Loss: 0.6729, Elapsed: 21m39s
2020-05-10 10:23:29.094506: Epoch: 1, Batch: 1107, Loss: 0.7113, Elapsed: 8m25s
2020-05-10 10:24:56.516998: Epoch: 1, Batch: 1119, Loss: 0.7278, Elapsed: 8m58s
2020-05-10 10:29:00.302822: Epoch: 1, Batch: 639, Loss: 0.6801, Elapsed: 11m46s
2020-05-10 10:32:45.767779: Epoch: 1, Batch: 1120, Loss: 0.7140, Elapsed: 7m49s
2020-05-10 10:33:01.186090: Epoch: 1, Batch: 1108, Loss: 0.6959, Elapsed: 9m32s
2020-05-10 10:38:46.729177: Epoch: 1, Batch: 1109, Loss: 0.6976, Elapsed: 5m45s
2020-05-10 10:40:50.645355: Epoch: 1, Batch: 1121, Loss: 0.7135, Elapsed: 8m4s
2020-05-10 10:46:48.906737: Epoch: 1, Batch: 1110, Loss: 0.7043, Elapsed: 8m2s
2020-05-10 10:46:55.622202: Epoch: 1, Batch: 640, Loss: 0.6808, Elapsed: 17m55s
2020-05-10 10:48:41.686666: Epoch: 1, Batch: 1122, Loss: 0.7108, Elapsed: 7m51s
2020-05-10 10:53:20.074897: Epoch: 1, Batch: 1111, Loss: 0.6708, Elapsed: 6m31s
2020-05-10 10:58:41.974752: Epoch: 1, Batch: 1112, Loss: 0.6923, Elapsed: 5m21s
2020-05-10 11:00:16.863587: Epoch: 1, Batch: 1123, Loss: 0.7359, Elapsed: 11m35s
2020-05-10 11:01:07.260009: Epoch: 1, Batch: 641, Loss: 0.6945, Elapsed: 14m11s
2020-05-10 11:03:41.404954: Epoch: 1, Batch: 1113, Loss: 0.7015, Elapsed: 4m59s
2020-05-10 11:08:19.975728: Epoch: 1, Batch: 1124, Loss: 0.7177, Elapsed: 8m3s
2020-05-10 11:11:22.191596: Epoch: 1, Batch: 1114, Loss: 0.6738, Elapsed: 7m40s
2020-05-10 11:13:27.846823: Epoch: 1, Batch: 1125, Loss: 0.7248, Elapsed: 5m7s
2020-05-10 11:17:42.859606: Epoch: 1, Batch: 1115, Loss: 0.6912, Elapsed: 6m20s
2020-05-10 11:19:51.497722: Epoch: 1, Batch: 1126, Loss: 0.6977, Elapsed: 6m23s
2020-05-10 11:21:33.253639: Epoch: 1, Batch: 642, Loss: 0.6762, Elapsed: 20m25s
2020-05-10 11:22:11.723157: Epoch: 1, Batch: 1116, Loss: 0.6785, Elapsed: 4m28s
2020-05-10 11:25:45.352535: Epoch: 1, Batch: 1127, Loss: 0.7163, Elapsed: 5m53s
2020-05-10 11:27:40.992606: Epoch: 1, Batch: 1117, Loss: 0.6970, Elapsed: 5m29s
2020-05-10 11:32:22.714759: Epoch: 1, Batch: 1118, Loss: 0.6907, Elapsed: 4m41s
2020-05-10 11:33:20.764626: Epoch: 1, Batch: 1128, Loss: 0.7077, Elapsed: 7m35s
2020-05-10 11:36:06.425345: Epoch: 1, Batch: 643, Loss: 0.6893, Elapsed: 14m33s
2020-05-10 11:38:51.797467: Epoch: 1, Batch: 1129, Loss: 0.7136, Elapsed: 5m31s
2020-05-10 11:39:33.227049: Epoch: 1, Batch: 1119, Loss: 0.6874, Elapsed: 7m10s
2020-05-10 11:44:41.680898: Epoch: 1, Batch: 1120, Loss: 0.6932, Elapsed: 5m8s
2020-05-10 11:50:47.919789: Epoch: 1, Batch: 1130, Loss: 0.7120, Elapsed: 11m56s
2020-05-10 11:52:50.380158: Epoch: 1, Batch: 644, Loss: 0.6872, Elapsed: 16m43s
2020-05-10 11:53:43.809066: Epoch: 1, Batch: 1121, Loss: 0.6880, Elapsed: 9m2s
2020-05-10 11:57:08.620636: Epoch: 1, Batch: 1131, Loss: 0.7000, Elapsed: 6m20s
2020-05-10 12:00:31.557233: Epoch: 1, Batch: 645, Loss: 0.6926, Elapsed: 7m41s
2020-05-10 12:00:48.365410: Epoch: 1, Batch: 1122, Loss: 0.6865, Elapsed: 7m4s
2020-05-10 12:04:47.256597: Epoch: 1, Batch: 1132, Loss: 0.7131, Elapsed: 7m38s
2020-05-10 12:11:04.274577: Epoch: 1, Batch: 646, Loss: 0.6905, Elapsed: 10m32s
2020-05-10 12:11:26.420858: Epoch: 1, Batch: 1123, Loss: 0.6896, Elapsed: 10m38s
2020-05-10 12:12:02.515739: Epoch: 1, Batch: 1133, Loss: 0.7301, Elapsed: 7m15s
2020-05-10 12:17:31.777623: Epoch: 1, Batch: 1134, Loss: 0.7006, Elapsed: 5m29s
2020-05-10 12:17:51.262927: Epoch: 1, Batch: 1124, Loss: 0.6739, Elapsed: 6m24s
2020-05-10 12:21:50.529837: Epoch: 1, Batch: 1135, Loss: 0.6981, Elapsed: 4m18s
2020-05-10 12:22:12.992024: Epoch: 1, Batch: 1125, Loss: 0.6928, Elapsed: 4m21s
2020-05-10 12:26:21.490478: Epoch: 1, Batch: 1136, Loss: 0.6954, Elapsed: 4m30s
2020-05-10 12:30:08.118945: Epoch: 1, Batch: 647, Loss: 0.6832, Elapsed: 19m3s
2020-05-10 12:30:21.682596: Epoch: 1, Batch: 1126, Loss: 0.6834, Elapsed: 8m8s
2020-05-10 12:35:48.953986: Epoch: 1, Batch: 1127, Loss: 0.6832, Elapsed: 5m27s
2020-05-10 12:44:06.803501: Epoch: 1, Batch: 1137, Loss: 0.7166, Elapsed: 17m45s
2020-05-10 12:47:53.204437: Epoch: 1, Batch: 1128, Loss: 0.7037, Elapsed: 12m4s
2020-05-10 12:50:04.112846: Epoch: 1, Batch: 1138, Loss: 0.7085, Elapsed: 5m57s
2020-05-10 12:51:20.082986: Epoch: 1, Batch: 648, Loss: 0.7107, Elapsed: 21m11s
2020-05-10 12:56:00.730235: Epoch: 1, Batch: 1129, Loss: 0.6786, Elapsed: 8m7s
2020-05-10 12:57:31.226418: Epoch: 1, Batch: 1139, Loss: 0.7250, Elapsed: 7m27s
2020-05-10 13:05:06.649844: Epoch: 1, Batch: 649, Loss: 0.6884, Elapsed: 13m46s
2020-05-10 13:05:28.078579: Epoch: 1, Batch: 1130, Loss: 0.6764, Elapsed: 9m27s
2020-05-10 13:08:52.099072: Epoch: 1, Batch: 1140, Loss: 0.6848, Elapsed: 11m20s
2020-05-10 13:15:20.535146: Epoch: 1, Batch: 1131, Loss: 0.6805, Elapsed: 9m52s
2020-05-10 13:15:49.544828: Epoch: 1, Batch: 1141, Loss: 0.6968, Elapsed: 6m57s
2020-05-10 13:18:40.938852: Epoch: 1, Batch: 650, Loss: 0.6909, Elapsed: 13m34s
Starting testing the validation set with 200 subgraphs!
2020-05-10 13:19:39.273402: Epoch: 1, Batch: 1142, Loss: 0.6951, Elapsed: 3m49s
2020-05-10 13:20:16.595169: Epoch: 1, Batch: 1132, Loss: 0.7008, Elapsed: 4m56s
2020-05-10 13:26:21.111718: Epoch: 1, Batch: 1143, Loss: 0.7024, Elapsed: 6m41s
2020-05-10 13:26:50.125959: Epoch: 1, Batch: 1133, Loss: 0.6801, Elapsed: 6m33s
2020-05-10 13:30:35.359048: Epoch: 1, Batch: 1144, Loss: 0.6927, Elapsed: 4m14s
2020-05-10 13:32:54.512742: Epoch: 1, Batch: 1134, Loss: 0.6920, Elapsed: 6m4s
2020-05-10 13:35:50.128369: Epoch: 1, Batch: 1145, Loss: 0.7118, Elapsed: 5m14s
2020-05-10 13:38:19.520103: Epoch: 1, Batch: 1135, Loss: 0.6935, Elapsed: 5m24s
2020-05-10 13:40:47.770885: Epoch: 1, Batch: 1146, Loss: 0.7204, Elapsed: 4m57s
2020-05-10 13:44:53.658523: Epoch: 1, Batch: 1147, Loss: 0.6782, Elapsed: 4m5s
2020-05-10 13:46:08.022627: Epoch: 1, Batch: 1136, Loss: 0.6952, Elapsed: 7m48s
2020-05-10 13:50:48.447579: Epoch: 1, Batch: 1148, Loss: 0.7177, Elapsed: 5m54s
2020-05-10 13:51:33.386464: Epoch: 1, Batch: 1137, Loss: 0.6923, Elapsed: 5m25s
2020-05-10 13:55:23.271227: Epoch: 1, Batch: 1149, Loss: 0.7261, Elapsed: 4m34s
2020-05-10 13:59:17.755368: Epoch: 1, Batch: 1138, Loss: 0.7026, Elapsed: 7m44s
2020-05-10 14:00:02.246451: Epoch: 1, Batch: 1150, Loss: 0.7067, Elapsed: 4m38s
Starting testing the validation set with 200 subgraphs!
2020-05-10 14:05:26.936554: Epoch: 1, Batch: 1139, Loss: 0.6835, Elapsed: 6m9s
2020-05-10 14:10:06.528364: Epoch: 1, Batch: 1140, Loss: 0.6947, Elapsed: 4m39s
2020-05-10 14:11:38.319530: Validation Test:  Loss: 0.6939,  Acc: 58.3015, AUC: 0.6029, Precision: 0.6361 -- Elapsed: 52m57s
2020-05-10 14:16:47.309981: Epoch: 1, Batch: 1141, Loss: 0.7055, Elapsed: 6m40s
2020-05-10 14:22:55.449465: Epoch: 1, Batch: 1142, Loss: 0.6981, Elapsed: 6m8s
2020-05-10 14:23:52.640119: Epoch: 1, Batch: 651, Loss: 0.7045, Elapsed: 12m14s
2020-05-10 14:28:50.145448: Epoch: 1, Batch: 1143, Loss: 0.7061, Elapsed: 5m54s
2020-05-10 14:30:07.488040: Epoch: 1, Batch: 652, Loss: 0.7137, Elapsed: 6m14s
2020-05-10 14:31:59.718449: Epoch: 1, Batch: 1144, Loss: 0.7067, Elapsed: 3m9s
2020-05-10 14:39:01.735812: Epoch: 1, Batch: 1145, Loss: 0.6964, Elapsed: 7m1s
2020-05-10 14:39:50.880591: Epoch: 1, Batch: 653, Loss: 0.7011, Elapsed: 9m43s
2020-05-10 14:41:03.973573: Validation Test:  Loss: 0.7069,  Acc: 53.0985, AUC: 0.5413, Precision: 0.5986 -- Elapsed: 41m1s
2020-05-10 14:43:50.601651: Epoch: 1, Batch: 1146, Loss: 0.7049, Elapsed: 4m48s
2020-05-10 14:51:28.713991: Epoch: 1, Batch: 1147, Loss: 0.6933, Elapsed: 7m38s
2020-05-10 14:53:59.152432: Epoch: 1, Batch: 1151, Loss: 0.7304, Elapsed: 12m55s
2020-05-10 14:58:34.678312: Epoch: 1, Batch: 1148, Loss: 0.6953, Elapsed: 7m5s
2020-05-10 14:58:36.929251: Epoch: 1, Batch: 654, Loss: 0.6880, Elapsed: 18m46s
2020-05-10 15:01:57.159808: Epoch: 1, Batch: 1152, Loss: 0.7050, Elapsed: 7m57s
2020-05-10 15:08:32.946220: Epoch: 1, Batch: 655, Loss: 0.7253, Elapsed: 9m56s
2020-05-10 15:09:15.182617: Epoch: 1, Batch: 1153, Loss: 0.7161, Elapsed: 7m18s
2020-05-10 15:15:32.660571: Epoch: 1, Batch: 1149, Loss: 0.6854, Elapsed: 16m57s
2020-05-10 15:16:56.421976: Epoch: 1, Batch: 1154, Loss: 0.7044, Elapsed: 7m41s
2020-05-10 15:20:01.836906: Epoch: 1, Batch: 1150, Loss: 0.6902, Elapsed: 4m29s
Starting testing the validation set with 200 subgraphs!
2020-05-10 15:22:39.177592: Epoch: 1, Batch: 1155, Loss: 0.7097, Elapsed: 5m42s
2020-05-10 15:27:44.855640: Epoch: 1, Batch: 656, Loss: 0.6664, Elapsed: 19m11s
2020-05-10 15:29:38.246245: Epoch: 1, Batch: 1156, Loss: 0.6908, Elapsed: 6m59s
2020-05-10 15:35:53.858954: Epoch: 1, Batch: 1157, Loss: 0.6752, Elapsed: 6m15s
2020-05-10 15:38:12.991534: Epoch: 1, Batch: 657, Loss: 0.7011, Elapsed: 10m28s
2020-05-10 15:44:06.718812: Epoch: 1, Batch: 1158, Loss: 0.7075, Elapsed: 8m12s
2020-05-10 15:47:56.663686: Epoch: 1, Batch: 658, Loss: 0.7034, Elapsed: 9m43s
2020-05-10 15:49:25.458979: Epoch: 1, Batch: 1159, Loss: 0.6950, Elapsed: 5m18s
2020-05-10 15:53:57.351725: Epoch: 1, Batch: 1160, Loss: 0.7192, Elapsed: 4m31s
2020-05-10 16:01:00.648740: Epoch: 1, Batch: 1161, Loss: 0.7264, Elapsed: 7m3s
2020-05-10 16:01:35.590350: Epoch: 1, Batch: 659, Loss: 0.7007, Elapsed: 13m38s
2020-05-10 16:02:19.114512: Validation Test:  Loss: 0.6996,  Acc: 54.5461, AUC: 0.5468, Precision: 0.6116 -- Elapsed: 42m17s
2020-05-10 16:06:41.303182: Epoch: 1, Batch: 1151, Loss: 0.7043, Elapsed: 4m22s
2020-05-10 16:07:35.223861: Epoch: 1, Batch: 1162, Loss: 0.6907, Elapsed: 6m34s
2020-05-10 16:11:11.131752: Epoch: 1, Batch: 660, Loss: 0.7042, Elapsed: 9m35s
2020-05-10 16:12:12.116877: Epoch: 1, Batch: 1163, Loss: 0.7079, Elapsed: 4m36s
2020-05-10 16:12:10.596658: Epoch: 1, Batch: 1152, Loss: 0.7224, Elapsed: 5m29s
2020-05-10 16:16:30.652359: Epoch: 1, Batch: 1153, Loss: 0.7219, Elapsed: 4m20s
2020-05-10 16:22:20.286243: Epoch: 1, Batch: 1164, Loss: 0.6971, Elapsed: 10m8s
2020-05-10 16:25:55.083015: Epoch: 1, Batch: 661, Loss: 0.6867, Elapsed: 14m43s
2020-05-10 16:31:49.656480: Epoch: 1, Batch: 1165, Loss: 0.7045, Elapsed: 9m29s
2020-05-10 16:36:04.575673: Epoch: 1, Batch: 1166, Loss: 0.7039, Elapsed: 4m14s
2020-05-10 16:36:10.529686: Epoch: 1, Batch: 662, Loss: 0.7051, Elapsed: 10m15s
2020-05-10 16:43:20.766833: Epoch: 1, Batch: 1167, Loss: 0.6929, Elapsed: 7m16s
2020-05-10 16:46:03.805338: Epoch: 1, Batch: 663, Loss: 0.7057, Elapsed: 9m53s
2020-05-10 16:47:30.773969: Epoch: 1, Batch: 1154, Loss: 0.6560, Elapsed: 31m0s
2020-05-10 16:50:24.809166: Epoch: 1, Batch: 1168, Loss: 0.6765, Elapsed: 7m4s
2020-05-10 16:53:51.109547: Epoch: 1, Batch: 664, Loss: 0.7077, Elapsed: 7m47s
2020-05-10 16:55:00.047116: Epoch: 1, Batch: 1169, Loss: 0.7073, Elapsed: 4m35s
2020-05-10 16:55:33.944222: Epoch: 1, Batch: 1155, Loss: 0.6945, Elapsed: 8m3s
2020-05-10 17:02:28.680909: Epoch: 1, Batch: 1170, Loss: 0.7057, Elapsed: 7m28s
2020-05-10 17:04:36.570906: Epoch: 1, Batch: 1156, Loss: 0.6914, Elapsed: 9m2s
2020-05-10 17:05:26.982311: Epoch: 1, Batch: 665, Loss: 0.7089, Elapsed: 11m35s
2020-05-10 17:07:59.407812: Epoch: 1, Batch: 1171, Loss: 0.6781, Elapsed: 5m30s
2020-05-10 17:11:29.099690: Epoch: 1, Batch: 1157, Loss: 0.7045, Elapsed: 6m52s
2020-05-10 17:15:15.878046: Epoch: 1, Batch: 1172, Loss: 0.7038, Elapsed: 7m16s
2020-05-10 17:19:28.070959: Epoch: 1, Batch: 1158, Loss: 0.6942, Elapsed: 7m58s
2020-05-10 17:20:07.246376: Epoch: 1, Batch: 666, Loss: 0.6843, Elapsed: 14m40s
2020-05-10 17:24:14.862555: Epoch: 1, Batch: 1173, Loss: 0.7053, Elapsed: 8m58s
2020-05-10 17:26:38.120981: Epoch: 1, Batch: 1159, Loss: 0.6924, Elapsed: 7m10s
2020-05-10 17:29:43.496941: Epoch: 1, Batch: 667, Loss: 0.7077, Elapsed: 9m36s
2020-05-10 17:29:46.383696: Epoch: 1, Batch: 1174, Loss: 0.6954, Elapsed: 5m31s
2020-05-10 17:32:52.204125: Epoch: 1, Batch: 1160, Loss: 0.6955, Elapsed: 6m14s
2020-05-10 17:38:52.781298: Epoch: 1, Batch: 1175, Loss: 0.6995, Elapsed: 9m6s
2020-05-10 17:38:56.872200: Epoch: 1, Batch: 1161, Loss: 0.6805, Elapsed: 6m4s
2020-05-10 17:45:19.047686: Epoch: 1, Batch: 1162, Loss: 0.6777, Elapsed: 6m22s
2020-05-10 17:46:46.846300: Epoch: 1, Batch: 1176, Loss: 0.7232, Elapsed: 7m54s
2020-05-10 17:50:13.819886: Epoch: 1, Batch: 1163, Loss: 0.6925, Elapsed: 4m54s
2020-05-10 17:51:17.150479: Epoch: 1, Batch: 668, Loss: 0.6643, Elapsed: 21m33s
2020-05-10 17:54:07.076105: Epoch: 1, Batch: 1177, Loss: 0.7083, Elapsed: 7m20s
2020-05-10 17:56:18.484096: Epoch: 1, Batch: 1164, Loss: 0.6986, Elapsed: 6m4s
2020-05-10 18:01:27.350703: Epoch: 1, Batch: 1178, Loss: 0.6805, Elapsed: 7m20s
2020-05-10 18:06:12.193531: Epoch: 1, Batch: 669, Loss: 0.6807, Elapsed: 14m55s
2020-05-10 18:08:57.249179: Epoch: 1, Batch: 1179, Loss: 0.6864, Elapsed: 7m29s
2020-05-10 18:13:13.552140: Epoch: 1, Batch: 1165, Loss: 0.6933, Elapsed: 16m55s
2020-05-10 18:16:03.363562: Epoch: 1, Batch: 1180, Loss: 0.7024, Elapsed: 7m6s
2020-05-10 18:20:38.202021: Epoch: 1, Batch: 670, Loss: 0.7080, Elapsed: 14m26s
2020-05-10 18:24:30.944043: Epoch: 1, Batch: 1181, Loss: 0.7010, Elapsed: 8m27s
2020-05-10 18:29:43.201199: Epoch: 1, Batch: 1166, Loss: 0.6864, Elapsed: 16m29s
2020-05-10 18:32:03.568242: Epoch: 1, Batch: 1182, Loss: 0.7091, Elapsed: 7m32s
2020-05-10 18:32:50.222596: Epoch: 1, Batch: 671, Loss: 0.6982, Elapsed: 12m12s
2020-05-10 18:37:24.491880: Epoch: 1, Batch: 1167, Loss: 0.6958, Elapsed: 7m41s
2020-05-10 18:41:09.007089: Epoch: 1, Batch: 1183, Loss: 0.7115, Elapsed: 9m5s
2020-05-10 18:43:25.286356: Epoch: 1, Batch: 672, Loss: 0.6942, Elapsed: 10m35s
2020-05-10 18:48:17.925502: Epoch: 1, Batch: 1168, Loss: 0.6905, Elapsed: 10m53s
2020-05-10 18:48:51.193061: Epoch: 1, Batch: 1184, Loss: 0.7024, Elapsed: 7m42s
2020-05-10 18:52:28.055776: Epoch: 1, Batch: 673, Loss: 0.7169, Elapsed: 9m2s
2020-05-10 18:56:43.169960: Epoch: 1, Batch: 1169, Loss: 0.6899, Elapsed: 8m25s
2020-05-10 18:57:49.884059: Epoch: 1, Batch: 1185, Loss: 0.6441, Elapsed: 8m58s
2020-05-10 19:04:33.612294: Epoch: 1, Batch: 674, Loss: 0.6959, Elapsed: 12m5s
2020-05-10 19:05:50.854860: Epoch: 1, Batch: 1170, Loss: 0.6783, Elapsed: 9m7s
2020-05-10 19:06:45.472053: Epoch: 1, Batch: 1186, Loss: 0.6877, Elapsed: 8m55s
2020-05-10 19:14:07.739815: Epoch: 1, Batch: 1187, Loss: 0.7199, Elapsed: 7m22s
2020-05-10 19:14:54.682054: Epoch: 1, Batch: 675, Loss: 0.6970, Elapsed: 10m21s
2020-05-10 19:15:09.361121: Epoch: 1, Batch: 1171, Loss: 0.6807, Elapsed: 9m18s
2020-05-10 19:19:16.978096: Epoch: 1, Batch: 1188, Loss: 0.6906, Elapsed: 5m9s
2020-05-10 19:21:45.268780: Epoch: 1, Batch: 1172, Loss: 0.6706, Elapsed: 6m35s
2020-05-10 19:23:14.415801: Epoch: 1, Batch: 676, Loss: 0.7102, Elapsed: 8m19s
2020-05-10 19:28:02.076812: Epoch: 1, Batch: 1189, Loss: 0.7064, Elapsed: 8m45s
2020-05-10 19:28:28.028380: Epoch: 1, Batch: 1173, Loss: 0.6695, Elapsed: 6m42s
2020-05-10 19:35:20.707705: Epoch: 1, Batch: 1190, Loss: 0.6984, Elapsed: 7m18s
2020-05-10 19:36:15.817382: Epoch: 1, Batch: 1174, Loss: 0.6831, Elapsed: 7m47s
2020-05-10 19:37:58.662085: Epoch: 1, Batch: 677, Loss: 0.6921, Elapsed: 14m44s
2020-05-10 19:42:07.304665: Epoch: 1, Batch: 1191, Loss: 0.7043, Elapsed: 6m46s
2020-05-10 19:43:08.542140: Epoch: 1, Batch: 1175, Loss: 0.6857, Elapsed: 6m52s
2020-05-10 19:49:42.763647: Epoch: 1, Batch: 678, Loss: 0.6963, Elapsed: 11m44s
2020-05-10 19:51:04.292138: Epoch: 1, Batch: 1192, Loss: 0.7047, Elapsed: 8m56s
2020-05-10 19:53:16.205401: Epoch: 1, Batch: 1176, Loss: 0.6764, Elapsed: 10m7s
2020-05-10 19:58:59.384022: Epoch: 1, Batch: 1177, Loss: 0.6795, Elapsed: 5m43s
2020-05-10 20:01:14.386790: Epoch: 1, Batch: 1193, Loss: 0.6906, Elapsed: 10m10s
2020-05-10 20:03:02.987199: Epoch: 1, Batch: 679, Loss: 0.6905, Elapsed: 13m20s
2020-05-10 20:03:20.429119: Epoch: 1, Batch: 1178, Loss: 0.6760, Elapsed: 4m21s
2020-05-10 20:08:13.223502: Epoch: 1, Batch: 1194, Loss: 0.6959, Elapsed: 6m58s
2020-05-10 20:10:54.070272: Epoch: 1, Batch: 1179, Loss: 0.6808, Elapsed: 7m33s
2020-05-10 20:11:41.322584: Epoch: 1, Batch: 680, Loss: 0.7012, Elapsed: 8m38s
2020-05-10 20:15:31.257224: Epoch: 1, Batch: 1195, Loss: 0.6937, Elapsed: 7m18s
2020-05-10 20:15:36.972793: Epoch: 1, Batch: 1180, Loss: 0.6680, Elapsed: 4m42s
2020-05-10 20:22:01.736390: Epoch: 1, Batch: 1181, Loss: 0.6742, Elapsed: 6m24s
2020-05-10 20:23:53.928331: Epoch: 1, Batch: 1196, Loss: 0.7154, Elapsed: 8m22s
2020-05-10 20:24:03.083049: Epoch: 1, Batch: 681, Loss: 0.6887, Elapsed: 12m21s
2020-05-10 20:26:28.862079: Epoch: 1, Batch: 1182, Loss: 0.6608, Elapsed: 4m27s
2020-05-10 20:32:29.087121: Epoch: 1, Batch: 1197, Loss: 0.6971, Elapsed: 8m35s
2020-05-10 20:40:19.889823: Epoch: 1, Batch: 1198, Loss: 0.7191, Elapsed: 7m50s
2020-05-10 20:40:17.584614: Epoch: 1, Batch: 1183, Loss: 0.6766, Elapsed: 13m48s
2020-05-10 20:41:42.827900: Epoch: 1, Batch: 682, Loss: 0.6840, Elapsed: 17m39s
2020-05-10 20:47:20.371607: Epoch: 1, Batch: 1184, Loss: 0.6688, Elapsed: 7m2s
2020-05-10 20:47:58.426923: Epoch: 1, Batch: 1199, Loss: 0.6898, Elapsed: 7m38s
2020-05-10 20:53:03.595681: Epoch: 1, Batch: 1185, Loss: 0.6636, Elapsed: 5m43s
2020-05-10 20:53:47.414530: Epoch: 1, Batch: 683, Loss: 0.7052, Elapsed: 12m4s
2020-05-10 20:54:56.874366: Epoch: 1, Batch: 1200, Loss: 0.7194, Elapsed: 6m58s
Starting testing the validation set with 200 subgraphs!
2020-05-10 20:59:05.068249: Epoch: 1, Batch: 1186, Loss: 0.6794, Elapsed: 6m1s
2020-05-10 21:03:40.791272: Epoch: 1, Batch: 684, Loss: 0.6909, Elapsed: 9m53s
2020-05-10 21:06:48.194515: Epoch: 1, Batch: 1187, Loss: 0.6699, Elapsed: 7m43s
2020-05-10 21:09:51.516286: Epoch: 1, Batch: 685, Loss: 0.6975, Elapsed: 6m10s
2020-05-10 21:15:25.935141: Epoch: 1, Batch: 1188, Loss: 0.6912, Elapsed: 8m37s
2020-05-10 21:23:22.182534: Epoch: 1, Batch: 686, Loss: 0.6947, Elapsed: 13m30s
2020-05-10 21:23:26.286485: Epoch: 1, Batch: 1189, Loss: 0.6686, Elapsed: 8m0s
2020-05-10 21:34:06.671945: Epoch: 1, Batch: 1190, Loss: 0.6873, Elapsed: 10m40s
2020-05-10 21:36:59.966663: Validation Test:  Loss: 0.7019,  Acc: 54.4021, AUC: 0.5563, Precision: 0.6072 -- Elapsed: 42m3s
2020-05-10 21:40:11.024796: Epoch: 1, Batch: 1191, Loss: 0.6854, Elapsed: 6m4s
2020-05-10 21:40:48.247270: Epoch: 1, Batch: 687, Loss: 0.6871, Elapsed: 17m26s
2020-05-10 21:44:00.252036: Epoch: 1, Batch: 1201, Loss: 0.7235, Elapsed: 7m0s
2020-05-10 21:47:22.766794: Epoch: 1, Batch: 1192, Loss: 0.6532, Elapsed: 7m11s
2020-05-10 21:51:49.266763: Epoch: 1, Batch: 688, Loss: 0.6899, Elapsed: 11m1s
2020-05-10 21:52:40.880335: Epoch: 1, Batch: 1202, Loss: 0.6874, Elapsed: 8m40s
2020-05-10 21:56:49.713848: Epoch: 1, Batch: 1193, Loss: 0.7059, Elapsed: 9m26s
2020-05-10 22:01:17.064299: Epoch: 1, Batch: 689, Loss: 0.7111, Elapsed: 9m27s
2020-05-10 22:04:00.094656: Epoch: 1, Batch: 1203, Loss: 0.6899, Elapsed: 11m19s
2020-05-10 22:04:01.777647: Epoch: 1, Batch: 1194, Loss: 0.6670, Elapsed: 7m12s
2020-05-10 22:09:26.505830: Epoch: 1, Batch: 1195, Loss: 0.6834, Elapsed: 5m24s
2020-05-10 22:11:36.723988: Epoch: 1, Batch: 1204, Loss: 0.6944, Elapsed: 7m36s
2020-05-10 22:12:53.071383: Epoch: 1, Batch: 690, Loss: 0.7110, Elapsed: 11m36s
2020-05-10 22:15:12.582153: Epoch: 1, Batch: 1196, Loss: 0.6912, Elapsed: 5m46s
2020-05-10 22:18:46.618961: Epoch: 1, Batch: 1205, Loss: 0.6820, Elapsed: 7m9s
2020-05-10 22:22:02.577605: Epoch: 1, Batch: 1197, Loss: 0.6534, Elapsed: 6m49s
2020-05-10 22:23:59.700698: Epoch: 1, Batch: 691, Loss: 0.6998, Elapsed: 11m6s
2020-05-10 22:25:31.090690: Epoch: 1, Batch: 1206, Loss: 0.7106, Elapsed: 6m44s
2020-05-10 22:28:28.334980: Epoch: 1, Batch: 1198, Loss: 0.6720, Elapsed: 6m25s
2020-05-10 22:31:03.544020: Epoch: 1, Batch: 1207, Loss: 0.6892, Elapsed: 5m32s
2020-05-10 22:33:20.958447: Epoch: 1, Batch: 1199, Loss: 0.6742, Elapsed: 4m52s
2020-05-10 22:37:19.462926: Epoch: 1, Batch: 1208, Loss: 0.6884, Elapsed: 6m15s
2020-05-10 22:38:58.497814: Epoch: 1, Batch: 692, Loss: 0.6886, Elapsed: 14m58s
2020-05-10 22:41:18.698724: Epoch: 1, Batch: 1200, Loss: 0.7005, Elapsed: 7m57s
Starting testing the validation set with 200 subgraphs!
2020-05-10 22:41:39.794582: Epoch: 1, Batch: 1209, Loss: 0.7107, Elapsed: 4m20s
2020-05-10 22:47:09.776154: Epoch: 1, Batch: 1210, Loss: 0.7014, Elapsed: 5m29s
2020-05-10 22:47:48.709530: Epoch: 1, Batch: 693, Loss: 0.7074, Elapsed: 8m50s
2020-05-10 22:54:34.065949: Epoch: 1, Batch: 1211, Loss: 0.6924, Elapsed: 7m24s
2020-05-10 23:00:20.316481: Epoch: 1, Batch: 1212, Loss: 0.7477, Elapsed: 5m46s
2020-05-10 23:04:18.567794: Epoch: 1, Batch: 694, Loss: 0.6846, Elapsed: 16m29s
2020-05-10 23:09:22.095080: Epoch: 1, Batch: 1213, Loss: 0.7381, Elapsed: 9m1s
2020-05-10 23:15:26.590361: Epoch: 1, Batch: 695, Loss: 0.6876, Elapsed: 11m8s
2020-05-10 23:16:18.658267: Epoch: 1, Batch: 1214, Loss: 0.7398, Elapsed: 6m56s
2020-05-10 23:23:33.834390: Validation Test:  Loss: 0.6797,  Acc: 57.4362, AUC: 0.5918, Precision: 0.6273 -- Elapsed: 42m15s
2020-05-10 23:24:36.570072: Epoch: 1, Batch: 1215, Loss: 0.7424, Elapsed: 8m17s
2020-05-10 23:26:21.391312: Epoch: 1, Batch: 696, Loss: 0.6941, Elapsed: 10m54s
2020-05-10 23:31:18.386376: Epoch: 1, Batch: 1201, Loss: 0.7077, Elapsed: 7m44s
2020-05-10 23:32:32.881326: Epoch: 1, Batch: 1216, Loss: 0.7387, Elapsed: 7m56s
2020-05-10 23:38:15.161580: Epoch: 1, Batch: 697, Loss: 0.6940, Elapsed: 11m53s
2020-05-10 23:40:33.225981: Epoch: 1, Batch: 1217, Loss: 0.7066, Elapsed: 8m0s
2020-05-10 23:41:05.134776: Epoch: 1, Batch: 1202, Loss: 0.7149, Elapsed: 9m46s
2020-05-10 23:46:14.445120: Epoch: 1, Batch: 1203, Loss: 0.6700, Elapsed: 5m9s
2020-05-10 23:49:14.334685: Epoch: 1, Batch: 1218, Loss: 0.7073, Elapsed: 8m41s
2020-05-10 23:50:01.827540: Epoch: 1, Batch: 698, Loss: 0.6938, Elapsed: 11m46s
2020-05-10 23:54:23.866244: Epoch: 1, Batch: 1204, Loss: 0.6621, Elapsed: 8m9s
2020-05-10 23:54:48.984597: Epoch: 1, Batch: 1219, Loss: 0.7109, Elapsed: 5m34s
2020-05-10 23:58:52.249920: Epoch: 1, Batch: 1220, Loss: 0.7101, Elapsed: 4m3s
2020-05-10 23:59:08.451752: Epoch: 1, Batch: 699, Loss: 0.6922, Elapsed: 9m6s
2020-05-10 23:59:29.012344: Epoch: 1, Batch: 1205, Loss: 0.6606, Elapsed: 5m5s
2020-05-11 00:07:57.516383: Epoch: 1, Batch: 1221, Loss: 0.7528, Elapsed: 9m5s
2020-05-11 00:09:52.755573: Epoch: 1, Batch: 1206, Loss: 0.7037, Elapsed: 10m23s
2020-05-11 00:13:57.134780: Epoch: 1, Batch: 700, Loss: 0.6862, Elapsed: 14m48s
Starting testing the validation set with 200 subgraphs!
2020-05-11 00:14:18.353183: Epoch: 1, Batch: 1222, Loss: 0.7341, Elapsed: 6m20s
2020-05-11 00:21:14.481662: Epoch: 1, Batch: 1207, Loss: 0.6959, Elapsed: 11m21s
2020-05-11 00:21:28.652472: Epoch: 1, Batch: 1223, Loss: 0.7108, Elapsed: 7m10s
2020-05-11 00:26:21.985568: Epoch: 1, Batch: 1208, Loss: 0.6748, Elapsed: 5m7s
2020-05-11 00:27:27.118567: Epoch: 1, Batch: 1224, Loss: 0.7368, Elapsed: 5m58s
2020-05-11 00:33:16.226871: Epoch: 1, Batch: 1209, Loss: 0.6902, Elapsed: 6m54s
2020-05-11 00:34:50.056268: Epoch: 1, Batch: 1225, Loss: 0.7348, Elapsed: 7m22s
2020-05-11 00:36:54.902560: Epoch: 1, Batch: 1210, Loss: 0.6410, Elapsed: 3m38s
2020-05-11 00:40:33.161749: Epoch: 1, Batch: 1226, Loss: 0.7227, Elapsed: 5m43s
2020-05-11 00:45:14.023902: Epoch: 1, Batch: 1211, Loss: 0.6989, Elapsed: 8m19s
2020-05-11 00:47:17.141446: Epoch: 1, Batch: 1227, Loss: 0.7138, Elapsed: 6m43s
2020-05-11 00:51:11.337245: Epoch: 1, Batch: 1212, Loss: 0.6649, Elapsed: 5m57s
2020-05-11 00:54:48.378360: Epoch: 1, Batch: 1213, Loss: 0.6593, Elapsed: 3m37s
2020-05-11 00:54:54.475674: Epoch: 1, Batch: 1228, Loss: 0.7111, Elapsed: 7m37s
2020-05-11 00:58:41.175164: Epoch: 1, Batch: 1229, Loss: 0.7182, Elapsed: 3m46s
2020-05-11 01:03:20.972441: Epoch: 1, Batch: 1214, Loss: 0.7045, Elapsed: 8m32s
2020-05-11 01:04:44.797187: Epoch: 1, Batch: 1230, Loss: 0.7041, Elapsed: 6m3s
2020-05-11 01:07:56.258410: Validation Test:  Loss: 0.6903,  Acc: 58.3958, AUC: 0.6054, Precision: 0.6380 -- Elapsed: 53m59s
2020-05-11 01:08:42.708089: Epoch: 1, Batch: 1231, Loss: 0.7024, Elapsed: 3m57s
2020-05-11 01:09:22.920775: Epoch: 1, Batch: 1215, Loss: 0.7018, Elapsed: 6m1s
2020-05-11 01:15:13.775281: Epoch: 1, Batch: 1216, Loss: 0.6667, Elapsed: 5m50s
2020-05-11 01:15:36.586373: Epoch: 1, Batch: 1232, Loss: 0.7407, Elapsed: 6m53s
2020-05-11 01:18:07.004333: Epoch: 1, Batch: 701, Loss: 0.6905, Elapsed: 10m10s
2020-05-11 01:20:10.897291: Epoch: 1, Batch: 1233, Loss: 0.6978, Elapsed: 4m34s
2020-05-11 01:22:48.940075: Epoch: 1, Batch: 1217, Loss: 0.6750, Elapsed: 7m35s
2020-05-11 01:25:36.527252: Epoch: 1, Batch: 1234, Loss: 0.7021, Elapsed: 5m25s
2020-05-11 01:27:33.904142: Epoch: 1, Batch: 1218, Loss: 0.6475, Elapsed: 4m44s
2020-05-11 01:28:59.472660: Epoch: 1, Batch: 702, Loss: 0.6933, Elapsed: 10m52s
2020-05-11 01:32:01.289309: Epoch: 1, Batch: 1235, Loss: 0.7112, Elapsed: 6m24s
2020-05-11 01:36:10.478742: Epoch: 1, Batch: 1219, Loss: 0.6698, Elapsed: 8m36s
2020-05-11 01:38:33.718663: Epoch: 1, Batch: 1236, Loss: 0.7139, Elapsed: 6m32s
2020-05-11 01:42:25.503798: Epoch: 1, Batch: 1220, Loss: 0.6666, Elapsed: 6m14s
2020-05-11 01:45:16.466358: Epoch: 1, Batch: 703, Loss: 0.6942, Elapsed: 16m16s
2020-05-11 01:46:22.542048: Epoch: 1, Batch: 1221, Loss: 0.6546, Elapsed: 3m57s
2020-05-11 01:51:39.782138: Epoch: 1, Batch: 1237, Loss: 0.7205, Elapsed: 13m6s
2020-05-11 01:54:14.601932: Epoch: 1, Batch: 1222, Loss: 0.7213, Elapsed: 7m52s
2020-05-11 01:57:55.581002: Epoch: 1, Batch: 1238, Loss: 0.7222, Elapsed: 6m15s
2020-05-11 01:58:57.917375: Epoch: 1, Batch: 704, Loss: 0.7031, Elapsed: 13m41s
2020-05-11 02:01:14.939426: Epoch: 1, Batch: 1223, Loss: 0.6942, Elapsed: 7m0s
2020-05-11 02:04:10.824360: Epoch: 1, Batch: 1239, Loss: 0.6904, Elapsed: 6m15s
2020-05-11 02:05:25.267292: Epoch: 1, Batch: 1224, Loss: 0.6927, Elapsed: 4m10s
2020-05-11 02:06:17.044592: Epoch: 1, Batch: 705, Loss: 0.7191, Elapsed: 7m19s
2020-05-11 02:09:42.202726: Epoch: 1, Batch: 1240, Loss: 0.6988, Elapsed: 5m31s
2020-05-11 02:12:36.826307: Epoch: 1, Batch: 1225, Loss: 0.6723, Elapsed: 7m11s
2020-05-11 02:15:52.290382: Epoch: 1, Batch: 1241, Loss: 0.6828, Elapsed: 6m10s
2020-05-11 02:19:44.666405: Epoch: 1, Batch: 1226, Loss: 0.7310, Elapsed: 7m7s
2020-05-11 02:27:07.189408: Epoch: 1, Batch: 1227, Loss: 0.6738, Elapsed: 7m22s
2020-05-11 02:28:15.652199: Epoch: 1, Batch: 706, Loss: 0.6928, Elapsed: 21m58s
2020-05-11 02:31:48.858219: Epoch: 1, Batch: 1242, Loss: 0.7125, Elapsed: 15m56s
2020-05-11 02:34:37.941861: Epoch: 1, Batch: 1228, Loss: 0.6955, Elapsed: 7m30s
2020-05-11 02:39:56.035592: Epoch: 1, Batch: 1229, Loss: 0.6480, Elapsed: 5m18s
2020-05-11 02:41:43.627388: Epoch: 1, Batch: 1243, Loss: 0.7002, Elapsed: 9m54s
2020-05-11 02:44:33.343976: Epoch: 1, Batch: 707, Loss: 0.6846, Elapsed: 16m17s
2020-05-11 02:48:24.010060: Epoch: 1, Batch: 1230, Loss: 0.7326, Elapsed: 8m27s
2020-05-11 02:50:57.545036: Epoch: 1, Batch: 1244, Loss: 0.7096, Elapsed: 9m13s
2020-05-11 02:53:59.929597: Epoch: 1, Batch: 708, Loss: 0.7086, Elapsed: 9m26s
2020-05-11 02:55:42.918478: Epoch: 1, Batch: 1231, Loss: 0.6807, Elapsed: 7m18s
2020-05-11 02:56:32.060769: Epoch: 1, Batch: 1245, Loss: 0.7082, Elapsed: 5m34s
2020-05-11 03:01:41.909694: Epoch: 1, Batch: 1232, Loss: 0.6935, Elapsed: 5m58s
2020-05-11 03:03:26.703804: Epoch: 1, Batch: 1246, Loss: 0.6853, Elapsed: 6m54s
2020-05-11 03:09:12.529107: Epoch: 1, Batch: 709, Loss: 0.6852, Elapsed: 15m12s
2020-05-11 03:13:05.063943: Epoch: 1, Batch: 1233, Loss: 0.7562, Elapsed: 11m23s
2020-05-11 03:14:57.533762: Epoch: 1, Batch: 1247, Loss: 0.7029, Elapsed: 11m30s
2020-05-11 03:19:04.434737: Epoch: 1, Batch: 1234, Loss: 0.6993, Elapsed: 5m59s
2020-05-11 03:19:53.035580: Epoch: 1, Batch: 710, Loss: 0.7096, Elapsed: 10m40s
2020-05-11 03:22:45.111492: Epoch: 1, Batch: 1248, Loss: 0.6816, Elapsed: 7m47s
2020-05-11 03:25:00.904081: Epoch: 1, Batch: 1235, Loss: 0.6547, Elapsed: 5m56s
2020-05-11 03:30:47.737007: Epoch: 1, Batch: 1236, Loss: 0.6996, Elapsed: 5m46s
2020-05-11 03:32:40.806126: Epoch: 1, Batch: 711, Loss: 0.6846, Elapsed: 12m47s
2020-05-11 03:32:55.879990: Epoch: 1, Batch: 1249, Loss: 0.7252, Elapsed: 10m10s
2020-05-11 03:37:07.169461: Epoch: 1, Batch: 1237, Loss: 0.6733, Elapsed: 6m19s
2020-05-11 03:42:56.944859: Epoch: 1, Batch: 1238, Loss: 0.6827, Elapsed: 5m49s
2020-05-11 03:43:31.707598: Epoch: 1, Batch: 1250, Loss: 0.6994, Elapsed: 10m35s
Starting testing the validation set with 200 subgraphs!
2020-05-11 03:47:46.778420: Epoch: 1, Batch: 712, Loss: 0.7021, Elapsed: 15m5s
2020-05-11 03:49:10.003639: Epoch: 1, Batch: 1239, Loss: 0.6674, Elapsed: 6m13s
2020-05-11 03:52:25.258562: Epoch: 1, Batch: 1240, Loss: 0.6618, Elapsed: 3m15s
2020-05-11 03:53:50.412832: Epoch: 1, Batch: 713, Loss: 0.7489, Elapsed: 6m3s
2020-05-11 03:57:54.549185: Epoch: 1, Batch: 1241, Loss: 0.6964, Elapsed: 5m29s
2020-05-11 04:02:38.334804: Epoch: 1, Batch: 1242, Loss: 0.6773, Elapsed: 4m43s
2020-05-11 04:08:04.702249: Epoch: 1, Batch: 714, Loss: 0.6847, Elapsed: 14m14s
2020-05-11 04:08:59.675452: Epoch: 1, Batch: 1243, Loss: 0.6735, Elapsed: 6m21s
2020-05-11 04:18:25.596495: Epoch: 1, Batch: 1244, Loss: 0.6772, Elapsed: 9m25s
2020-05-11 04:19:52.855055: Epoch: 1, Batch: 715, Loss: 0.6903, Elapsed: 11m48s
2020-05-11 04:23:51.805500: Epoch: 1, Batch: 1245, Loss: 0.6550, Elapsed: 5m26s
2020-05-11 04:25:03.209398: Epoch: 1, Batch: 716, Loss: 0.7452, Elapsed: 5m10s
2020-05-11 04:25:40.574306: Validation Test:  Loss: 0.7041,  Acc: 54.3270, AUC: 0.5529, Precision: 0.6023 -- Elapsed: 42m8s
2020-05-11 04:28:05.640808: Epoch: 1, Batch: 1246, Loss: 0.6637, Elapsed: 4m13s
2020-05-11 04:35:26.314676: Epoch: 1, Batch: 717, Loss: 0.7082, Elapsed: 10m23s
2020-05-11 04:35:36.978066: Epoch: 1, Batch: 1251, Loss: 0.6956, Elapsed: 9m56s
2020-05-11 04:36:41.241604: Epoch: 1, Batch: 1247, Loss: 0.6804, Elapsed: 8m35s
2020-05-11 04:44:18.897301: Epoch: 1, Batch: 1248, Loss: 0.6744, Elapsed: 7m37s
2020-05-11 04:45:09.717307: Epoch: 1, Batch: 1252, Loss: 0.6977, Elapsed: 9m32s
2020-05-11 04:50:22.953356: Epoch: 1, Batch: 1253, Loss: 0.7060, Elapsed: 5m13s
2020-05-11 04:52:05.650251: Epoch: 1, Batch: 1249, Loss: 0.6722, Elapsed: 7m46s
2020-05-11 04:53:33.187556: Epoch: 1, Batch: 718, Loss: 0.6838, Elapsed: 18m6s
2020-05-11 04:54:32.027275: Epoch: 1, Batch: 1254, Loss: 0.7113, Elapsed: 4m9s
2020-05-11 04:59:55.179969: Epoch: 1, Batch: 1250, Loss: 0.6913, Elapsed: 7m49s
Starting testing the validation set with 200 subgraphs!
2020-05-11 05:01:34.932690: Epoch: 1, Batch: 1255, Loss: 0.7069, Elapsed: 7m2s
2020-05-11 05:02:37.996510: Epoch: 1, Batch: 719, Loss: 0.6904, Elapsed: 9m4s
2020-05-11 05:06:12.520943: Epoch: 1, Batch: 1256, Loss: 0.7008, Elapsed: 4m37s
2020-05-11 05:10:41.310406: Epoch: 1, Batch: 720, Loss: 0.7381, Elapsed: 8m3s
2020-05-11 05:12:19.138167: Epoch: 1, Batch: 1257, Loss: 0.6904, Elapsed: 6m6s
2020-05-11 05:18:08.228348: Epoch: 1, Batch: 1258, Loss: 0.7135, Elapsed: 5m49s
2020-05-11 05:21:46.376991: Epoch: 1, Batch: 1259, Loss: 0.6785, Elapsed: 3m38s
2020-05-11 05:25:38.668148: Epoch: 1, Batch: 721, Loss: 0.6846, Elapsed: 14m57s
2020-05-11 05:30:03.746590: Epoch: 1, Batch: 1260, Loss: 0.6887, Elapsed: 8m17s
2020-05-11 05:34:56.837045: Epoch: 1, Batch: 722, Loss: 0.6843, Elapsed: 9m18s
2020-05-11 05:36:07.468338: Epoch: 1, Batch: 1261, Loss: 0.7129, Elapsed: 6m3s
2020-05-11 05:42:19.895855: Validation Test:  Loss: 0.6771,  Acc: 57.6482, AUC: 0.5979, Precision: 0.6339 -- Elapsed: 42m24s
2020-05-11 05:45:38.372185: Epoch: 1, Batch: 1262, Loss: 0.7047, Elapsed: 9m30s
2020-05-11 05:46:15.171909: Epoch: 1, Batch: 723, Loss: 0.7083, Elapsed: 11m18s
2020-05-11 05:48:54.721908: Epoch: 1, Batch: 1251, Loss: 0.6871, Elapsed: 6m34s
2020-05-11 05:54:23.175018: Epoch: 1, Batch: 724, Loss: 0.7276, Elapsed: 8m7s
2020-05-11 05:55:35.752383: Epoch: 1, Batch: 1252, Loss: 0.6990, Elapsed: 6m41s
2020-05-11 05:55:55.774312: Epoch: 1, Batch: 1263, Loss: 0.7060, Elapsed: 10m17s
2020-05-11 06:04:36.061080: Epoch: 1, Batch: 1264, Loss: 0.7066, Elapsed: 8m40s
2020-05-11 06:08:37.644737: Epoch: 1, Batch: 1265, Loss: 0.6849, Elapsed: 4m1s
2020-05-11 06:10:35.395782: Epoch: 1, Batch: 725, Loss: 0.6759, Elapsed: 16m12s
2020-05-11 06:10:50.582708: Epoch: 1, Batch: 1253, Loss: 0.6858, Elapsed: 15m14s
2020-05-11 06:19:07.048882: Epoch: 1, Batch: 1266, Loss: 0.6920, Elapsed: 10m29s
2020-05-11 06:19:04.345985: Epoch: 1, Batch: 1254, Loss: 0.6819, Elapsed: 8m13s
2020-05-11 06:24:13.324761: Epoch: 1, Batch: 1255, Loss: 0.6594, Elapsed: 5m8s
2020-05-11 06:24:23.038547: Epoch: 1, Batch: 726, Loss: 0.7041, Elapsed: 13m47s
2020-05-11 06:27:54.612374: Epoch: 1, Batch: 1267, Loss: 0.6935, Elapsed: 8m47s
2020-05-11 06:28:14.878243: Epoch: 1, Batch: 1256, Loss: 0.6367, Elapsed: 4m1s
2020-05-11 06:34:07.882655: Epoch: 1, Batch: 1268, Loss: 0.6979, Elapsed: 6m13s
2020-05-11 06:35:02.694081: Epoch: 1, Batch: 727, Loss: 0.7194, Elapsed: 10m39s
2020-05-11 06:35:31.337703: Epoch: 1, Batch: 1257, Loss: 0.6710, Elapsed: 7m16s
2020-05-11 06:39:35.323896: Epoch: 1, Batch: 1269, Loss: 0.6914, Elapsed: 5m27s
2020-05-11 06:43:06.262122: Epoch: 1, Batch: 1258, Loss: 0.6612, Elapsed: 7m34s
2020-05-11 06:46:49.520545: Epoch: 1, Batch: 1270, Loss: 0.7009, Elapsed: 7m14s
2020-05-11 06:47:24.524206: Epoch: 1, Batch: 1259, Loss: 0.6551, Elapsed: 4m18s
2020-05-11 06:52:15.857861: Epoch: 1, Batch: 1271, Loss: 0.7076, Elapsed: 5m26s
2020-05-11 06:56:36.091922: Epoch: 1, Batch: 1260, Loss: 0.6805, Elapsed: 9m11s
2020-05-11 06:57:37.592942: Epoch: 1, Batch: 728, Loss: 0.6653, Elapsed: 22m34s
2020-05-11 07:01:04.438321: Epoch: 1, Batch: 1261, Loss: 0.6298, Elapsed: 4m28s
2020-05-11 07:04:00.240940: Epoch: 1, Batch: 1262, Loss: 0.6237, Elapsed: 2m55s
2020-05-11 07:06:29.135820: Epoch: 1, Batch: 1272, Loss: 0.7113, Elapsed: 14m13s
2020-05-11 07:09:49.113494: Epoch: 1, Batch: 1263, Loss: 0.6720, Elapsed: 5m48s
2020-05-11 07:10:46.903904: Epoch: 1, Batch: 729, Loss: 0.7126, Elapsed: 13m9s
2020-05-11 07:12:14.140557: Epoch: 1, Batch: 1273, Loss: 0.6815, Elapsed: 5m44s
2020-05-11 07:13:27.327903: Epoch: 1, Batch: 1264, Loss: 0.6350, Elapsed: 3m38s
2020-05-11 07:18:59.566836: Epoch: 1, Batch: 1274, Loss: 0.7170, Elapsed: 6m45s
2020-05-11 07:19:13.887067: Epoch: 1, Batch: 1265, Loss: 0.6497, Elapsed: 5m46s
2020-05-11 07:24:22.246203: Epoch: 1, Batch: 1275, Loss: 0.6945, Elapsed: 5m22s
2020-05-11 07:25:43.715399: Epoch: 1, Batch: 1266, Loss: 0.6886, Elapsed: 6m29s
2020-05-11 07:29:23.572051: Epoch: 1, Batch: 730, Loss: 0.6640, Elapsed: 18m36s
2020-05-11 07:33:25.091259: Epoch: 1, Batch: 1267, Loss: 0.6959, Elapsed: 7m41s
2020-05-11 07:35:55.984934: Epoch: 1, Batch: 1276, Loss: 0.7101, Elapsed: 11m33s
2020-05-11 07:40:32.545539: Epoch: 1, Batch: 1268, Loss: 0.6949, Elapsed: 7m7s
2020-05-11 07:42:19.282804: Epoch: 1, Batch: 1277, Loss: 0.7048, Elapsed: 6m23s
2020-05-11 07:45:14.452126: Epoch: 1, Batch: 731, Loss: 0.7046, Elapsed: 15m50s
2020-05-11 07:47:59.308393: Epoch: 1, Batch: 1278, Loss: 0.6947, Elapsed: 5m39s
2020-05-11 07:48:08.944779: Epoch: 1, Batch: 1269, Loss: 0.7094, Elapsed: 7m36s
2020-05-11 07:52:32.903335: Epoch: 1, Batch: 1270, Loss: 0.6586, Elapsed: 4m23s
2020-05-11 07:55:47.103189: Epoch: 1, Batch: 1279, Loss: 0.7122, Elapsed: 7m47s
2020-05-11 07:58:39.472653: Epoch: 1, Batch: 1271, Loss: 0.6687, Elapsed: 6m6s
2020-05-11 08:00:16.114390: Epoch: 1, Batch: 732, Loss: 0.6744, Elapsed: 15m1s
2020-05-11 08:01:40.843661: Epoch: 1, Batch: 1280, Loss: 0.6942, Elapsed: 5m53s
2020-05-11 08:05:28.694473: Epoch: 1, Batch: 1272, Loss: 0.6740, Elapsed: 6m49s
2020-05-11 08:06:23.649269: Epoch: 1, Batch: 1281, Loss: 0.6900, Elapsed: 4m42s
2020-05-11 08:12:26.091940: Epoch: 1, Batch: 1273, Loss: 0.6848, Elapsed: 6m57s
2020-05-11 08:13:42.549053: Epoch: 1, Batch: 733, Loss: 0.6956, Elapsed: 13m26s
2020-05-11 08:17:01.785583: Epoch: 1, Batch: 1274, Loss: 0.6515, Elapsed: 4m35s
2020-05-11 08:17:32.775576: Epoch: 1, Batch: 1282, Loss: 0.7255, Elapsed: 11m9s
2020-05-11 08:23:40.252522: Epoch: 1, Batch: 1275, Loss: 0.6824, Elapsed: 6m38s
2020-05-11 08:25:43.669769: Epoch: 1, Batch: 734, Loss: 0.7092, Elapsed: 12m1s
2020-05-11 08:27:33.064468: Epoch: 1, Batch: 1283, Loss: 0.7280, Elapsed: 10m0s
2020-05-11 08:28:54.005189: Epoch: 1, Batch: 1276, Loss: 0.6645, Elapsed: 5m13s
2020-05-11 08:34:30.993146: Epoch: 1, Batch: 1284, Loss: 0.6929, Elapsed: 6m57s
2020-05-11 08:38:05.683899: Epoch: 1, Batch: 1277, Loss: 0.6867, Elapsed: 9m11s
2020-05-11 08:42:06.889407: Epoch: 1, Batch: 1285, Loss: 0.7145, Elapsed: 7m35s
2020-05-11 08:42:38.746168: Epoch: 1, Batch: 1278, Loss: 0.6415, Elapsed: 4m33s
2020-05-11 08:42:54.014297: Epoch: 1, Batch: 735, Loss: 0.6832, Elapsed: 17m10s
2020-05-11 08:47:20.182471: Epoch: 1, Batch: 1279, Loss: 0.6410, Elapsed: 4m41s
2020-05-11 08:53:09.939218: Epoch: 1, Batch: 1286, Loss: 0.7141, Elapsed: 11m3s
2020-05-11 08:54:35.733459: Epoch: 1, Batch: 736, Loss: 0.7153, Elapsed: 11m41s
2020-05-11 08:57:52.883526: Epoch: 1, Batch: 1280, Loss: 0.7110, Elapsed: 10m32s
2020-05-11 08:58:06.893404: Epoch: 1, Batch: 1287, Loss: 0.7228, Elapsed: 4m56s
2020-05-11 09:04:23.018554: Epoch: 1, Batch: 1288, Loss: 0.7285, Elapsed: 6m16s
2020-05-11 09:05:31.637275: Epoch: 1, Batch: 1281, Loss: 0.6894, Elapsed: 7m38s
2020-05-11 09:08:48.561547: Epoch: 1, Batch: 737, Loss: 0.7031, Elapsed: 14m12s
2020-05-11 09:10:59.381904: Epoch: 1, Batch: 1289, Loss: 0.7077, Elapsed: 6m36s
2020-05-11 09:14:04.221873: Epoch: 1, Batch: 1282, Loss: 0.6884, Elapsed: 8m32s
2020-05-11 09:18:13.850888: Epoch: 1, Batch: 1290, Loss: 0.6791, Elapsed: 7m14s
2020-05-11 09:18:31.699866: Epoch: 1, Batch: 1283, Loss: 0.6421, Elapsed: 4m27s
2020-05-11 09:24:00.342280: Epoch: 1, Batch: 738, Loss: 0.7017, Elapsed: 15m11s
2020-05-11 09:26:11.717386: Epoch: 1, Batch: 1291, Loss: 0.7154, Elapsed: 7m57s
2020-05-11 09:26:43.941281: Epoch: 1, Batch: 1284, Loss: 0.6962, Elapsed: 8m12s
2020-05-11 09:33:28.393023: Epoch: 1, Batch: 1292, Loss: 0.6992, Elapsed: 7m16s
2020-05-11 09:35:22.508830: Epoch: 1, Batch: 739, Loss: 0.6977, Elapsed: 11m22s
2020-05-11 09:37:06.578095: Epoch: 1, Batch: 1285, Loss: 0.7232, Elapsed: 10m22s
2020-05-11 09:39:28.762246: Epoch: 1, Batch: 1293, Loss: 0.6926, Elapsed: 6m0s
2020-05-11 09:46:37.410239: Epoch: 1, Batch: 1294, Loss: 0.6965, Elapsed: 7m8s
2020-05-11 09:47:07.641024: Epoch: 1, Batch: 1286, Loss: 0.6811, Elapsed: 10m1s
2020-05-11 09:52:37.311026: Epoch: 1, Batch: 740, Loss: 0.6733, Elapsed: 17m14s
2020-05-11 09:54:30.719030: Epoch: 1, Batch: 1287, Loss: 0.6757, Elapsed: 7m23s
2020-05-11 09:55:38.420556: Epoch: 1, Batch: 1295, Loss: 0.7249, Elapsed: 9m0s
2020-05-11 09:59:52.981774: Epoch: 1, Batch: 1296, Loss: 0.7036, Elapsed: 4m14s
2020-05-11 10:03:40.111881: Epoch: 1, Batch: 1297, Loss: 0.7052, Elapsed: 3m47s
2020-05-11 10:05:26.412704: Epoch: 1, Batch: 741, Loss: 0.7024, Elapsed: 12m49s
2020-05-11 10:06:05.054045: Epoch: 1, Batch: 1288, Loss: 0.6911, Elapsed: 11m34s
2020-05-11 10:06:48.242405: Epoch: 1, Batch: 1298, Loss: 0.6648, Elapsed: 3m8s
2020-05-11 10:12:22.930089: Epoch: 1, Batch: 1299, Loss: 0.6826, Elapsed: 5m34s
2020-05-11 10:13:55.626884: Epoch: 1, Batch: 1289, Loss: 0.6739, Elapsed: 7m50s
2020-05-11 10:21:16.014631: Epoch: 1, Batch: 1290, Loss: 0.6590, Elapsed: 7m20s
2020-05-11 10:24:06.526557: Epoch: 1, Batch: 1300, Loss: 0.6903, Elapsed: 11m43s
Starting testing the validation set with 200 subgraphs!
2020-05-11 10:28:09.052120: Epoch: 1, Batch: 742, Loss: 0.6654, Elapsed: 22m42s
2020-05-11 10:29:04.218799: Epoch: 1, Batch: 1291, Loss: 0.6962, Elapsed: 7m48s
2020-05-11 10:36:08.329071: Epoch: 1, Batch: 743, Loss: 0.6724, Elapsed: 7m59s
2020-05-11 10:38:07.761147: Epoch: 1, Batch: 1292, Loss: 0.6597, Elapsed: 9m3s
2020-05-11 10:43:35.886525: Epoch: 1, Batch: 1293, Loss: 0.6565, Elapsed: 5m28s
2020-05-11 10:47:02.962358: Epoch: 1, Batch: 744, Loss: 0.6948, Elapsed: 10m54s
2020-05-11 10:49:14.860063: Epoch: 1, Batch: 1294, Loss: 0.6603, Elapsed: 5m38s
2020-05-11 10:55:03.133883: Epoch: 1, Batch: 745, Loss: 0.7090, Elapsed: 8m0s
2020-05-11 10:56:18.522893: Epoch: 1, Batch: 1295, Loss: 0.6906, Elapsed: 7m3s
2020-05-11 11:03:00.490872: Epoch: 1, Batch: 1296, Loss: 0.6821, Elapsed: 6m41s
2020-05-11 11:06:11.143156: Validation Test:  Loss: 0.6998,  Acc: 54.3523, AUC: 0.5565, Precision: 0.6048 -- Elapsed: 42m4s
2020-05-11 11:08:45.509779: Epoch: 1, Batch: 1297, Loss: 0.6685, Elapsed: 5m44s
2020-05-11 11:09:05.795890: Epoch: 1, Batch: 746, Loss: 0.6806, Elapsed: 14m2s
2020-05-11 11:13:49.831658: Epoch: 1, Batch: 1301, Loss: 0.6922, Elapsed: 7m38s
2020-05-11 11:19:32.285440: Epoch: 1, Batch: 1302, Loss: 0.6883, Elapsed: 5m42s
2020-05-11 11:19:56.613032: Epoch: 1, Batch: 1298, Loss: 0.6716, Elapsed: 11m11s
2020-05-11 11:25:17.083404: Epoch: 1, Batch: 1299, Loss: 0.6538, Elapsed: 5m20s
2020-05-11 11:26:18.491486: Epoch: 1, Batch: 747, Loss: 0.6832, Elapsed: 17m12s
2020-05-11 11:27:23.079378: Epoch: 1, Batch: 1303, Loss: 0.6994, Elapsed: 7m50s
2020-05-11 11:33:43.572271: Epoch: 1, Batch: 1304, Loss: 0.6886, Elapsed: 6m20s
2020-05-11 11:37:06.239909: Epoch: 1, Batch: 1300, Loss: 0.7548, Elapsed: 11m49s
Starting testing the validation set with 200 subgraphs!
2020-05-11 11:38:38.515429: Epoch: 1, Batch: 748, Loss: 0.6877, Elapsed: 12m20s
2020-05-11 11:38:44.695129: Epoch: 1, Batch: 1305, Loss: 0.7092, Elapsed: 5m1s
2020-05-11 11:46:18.380016: Epoch: 1, Batch: 1306, Loss: 0.7077, Elapsed: 7m33s
2020-05-11 11:55:42.722565: Epoch: 1, Batch: 749, Loss: 0.6722, Elapsed: 17m4s
2020-05-11 11:57:42.897028: Epoch: 1, Batch: 1307, Loss: 0.6846, Elapsed: 11m24s
2020-05-11 12:01:00.209086: Epoch: 1, Batch: 1308, Loss: 0.6715, Elapsed: 3m17s
2020-05-11 12:06:21.342322: Epoch: 1, Batch: 1309, Loss: 0.7104, Elapsed: 5m21s
2020-05-11 12:06:45.251879: Epoch: 1, Batch: 750, Loss: 0.6898, Elapsed: 11m2s
Starting testing the validation set with 200 subgraphs!
2020-05-11 12:10:58.578929: Epoch: 1, Batch: 1310, Loss: 0.6786, Elapsed: 4m37s
2020-05-11 12:17:03.352276: Epoch: 1, Batch: 1311, Loss: 0.7044, Elapsed: 6m4s
2020-05-11 12:18:31.441138: Validation Test:  Loss: 0.6742,  Acc: 58.0722, AUC: 0.6032, Precision: 0.6223 -- Elapsed: 41m25s
2020-05-11 12:20:59.173138: Epoch: 1, Batch: 1312, Loss: 0.6857, Elapsed: 3m55s
2020-05-11 12:23:16.426758: Epoch: 1, Batch: 1301, Loss: 0.6738, Elapsed: 4m44s
2020-05-11 12:27:03.207534: Epoch: 1, Batch: 1302, Loss: 0.6888, Elapsed: 3m46s
2020-05-11 12:27:50.958280: Epoch: 1, Batch: 1313, Loss: 0.6907, Elapsed: 6m51s
2020-05-11 12:33:35.594380: Epoch: 1, Batch: 1303, Loss: 0.6464, Elapsed: 6m32s
2020-05-11 12:34:09.064930: Epoch: 1, Batch: 1314, Loss: 0.7048, Elapsed: 6m18s
2020-05-11 12:37:33.707863: Epoch: 1, Batch: 1315, Loss: 0.6975, Elapsed: 3m24s
2020-05-11 12:37:36.233040: Epoch: 1, Batch: 1304, Loss: 0.6510, Elapsed: 4m0s
2020-05-11 12:41:47.552285: Epoch: 1, Batch: 1305, Loss: 0.6783, Elapsed: 4m11s
2020-05-11 12:45:25.794844: Epoch: 1, Batch: 1316, Loss: 0.6805, Elapsed: 7m52s
2020-05-11 12:46:30.619555: Epoch: 1, Batch: 1306, Loss: 0.6346, Elapsed: 4m43s
2020-05-11 12:51:02.644786: Epoch: 1, Batch: 1317, Loss: 0.7033, Elapsed: 5m36s
2020-05-11 12:53:13.839978: Epoch: 1, Batch: 1307, Loss: 0.7047, Elapsed: 6m43s
2020-05-11 12:56:25.901059: Epoch: 1, Batch: 1318, Loss: 0.7026, Elapsed: 5m23s
2020-05-11 12:58:01.822110: Epoch: 1, Batch: 1308, Loss: 0.7029, Elapsed: 4m47s
2020-05-11 12:59:30.692463: Validation Test:  Loss: 0.6911,  Acc: 56.4400, AUC: 0.5843, Precision: 0.6179 -- Elapsed: 52m45s
2020-05-11 13:02:29.702048: Epoch: 1, Batch: 1319, Loss: 0.6930, Elapsed: 6m3s
2020-05-11 13:08:02.996467: Epoch: 1, Batch: 1309, Loss: 0.6854, Elapsed: 10m1s
2020-05-11 13:13:35.711088: Epoch: 1, Batch: 1320, Loss: 0.6907, Elapsed: 11m5s
2020-05-11 13:15:10.051613: Epoch: 1, Batch: 751, Loss: 0.6838, Elapsed: 15m39s
2020-05-11 13:21:02.152024: Epoch: 1, Batch: 1321, Loss: 0.7036, Elapsed: 7m26s
2020-05-11 13:23:06.192537: Epoch: 1, Batch: 1310, Loss: 0.7879, Elapsed: 15m3s
2020-05-11 13:26:51.216333: Epoch: 1, Batch: 752, Loss: 0.6936, Elapsed: 11m41s
2020-05-11 13:29:56.638991: Epoch: 1, Batch: 1322, Loss: 0.6962, Elapsed: 8m54s
2020-05-11 13:34:58.818826: Epoch: 1, Batch: 1311, Loss: 0.7060, Elapsed: 11m52s
2020-05-11 13:35:43.736974: Epoch: 1, Batch: 1323, Loss: 0.6811, Elapsed: 5m47s
2020-05-11 13:37:53.110445: Epoch: 1, Batch: 753, Loss: 0.6911, Elapsed: 11m1s
2020-05-11 13:41:19.615968: Epoch: 1, Batch: 1312, Loss: 0.6776, Elapsed: 6m20s
2020-05-11 13:43:19.750878: Epoch: 1, Batch: 1324, Loss: 0.6998, Elapsed: 7m35s
2020-05-11 13:46:04.468827: Epoch: 1, Batch: 754, Loss: 0.6961, Elapsed: 8m11s
2020-05-11 13:48:15.786967: Epoch: 1, Batch: 1325, Loss: 0.6965, Elapsed: 4m56s
2020-05-11 13:48:31.439449: Epoch: 1, Batch: 1313, Loss: 0.6981, Elapsed: 7m11s
2020-05-11 13:55:46.025130: Epoch: 1, Batch: 1326, Loss: 0.6962, Elapsed: 7m30s
2020-05-11 13:56:36.646290: Epoch: 1, Batch: 1314, Loss: 0.6769, Elapsed: 8m5s
2020-05-11 13:57:17.900348: Epoch: 1, Batch: 755, Loss: 0.6969, Elapsed: 11m13s
2020-05-11 14:02:40.087104: Epoch: 1, Batch: 1327, Loss: 0.7045, Elapsed: 6m54s
2020-05-11 14:03:55.685354: Epoch: 1, Batch: 1315, Loss: 0.6708, Elapsed: 7m19s
2020-05-11 14:05:40.553765: Epoch: 1, Batch: 1328, Loss: 0.6807, Elapsed: 3m0s
2020-05-11 14:12:22.177779: Epoch: 1, Batch: 1316, Loss: 0.6784, Elapsed: 8m26s
2020-05-11 14:12:34.665963: Epoch: 1, Batch: 756, Loss: 0.6921, Elapsed: 15m16s
2020-05-11 14:13:44.920102: Epoch: 1, Batch: 1329, Loss: 0.6731, Elapsed: 8m4s
2020-05-11 14:19:10.767391: Epoch: 1, Batch: 1317, Loss: 0.6989, Elapsed: 6m48s
2020-05-11 14:20:08.805739: Epoch: 1, Batch: 757, Loss: 0.6883, Elapsed: 7m34s
2020-05-11 14:25:26.847317: Epoch: 1, Batch: 1330, Loss: 0.7061, Elapsed: 11m41s
2020-05-11 14:25:50.000058: Epoch: 1, Batch: 1318, Loss: 0.6947, Elapsed: 6m39s
2020-05-11 14:31:36.983663: Epoch: 1, Batch: 758, Loss: 0.6856, Elapsed: 11m28s
2020-05-11 14:31:52.068176: Epoch: 1, Batch: 1331, Loss: 0.6798, Elapsed: 6m25s
2020-05-11 14:32:50.327123: Epoch: 1, Batch: 1319, Loss: 0.6600, Elapsed: 7m0s
2020-05-11 14:37:58.524838: Epoch: 1, Batch: 1332, Loss: 0.6964, Elapsed: 6m6s
2020-05-11 14:44:19.421925: Epoch: 1, Batch: 1320, Loss: 0.6836, Elapsed: 11m29s
2020-05-11 14:46:05.562746: Epoch: 1, Batch: 759, Loss: 0.6945, Elapsed: 14m28s
2020-05-11 14:50:18.661647: Epoch: 1, Batch: 1333, Loss: 0.7040, Elapsed: 12m20s
2020-05-11 14:51:33.300908: Epoch: 1, Batch: 1321, Loss: 0.6723, Elapsed: 7m13s
2020-05-11 14:54:51.854729: Epoch: 1, Batch: 760, Loss: 0.7082, Elapsed: 8m46s
2020-05-11 15:00:27.484880: Epoch: 1, Batch: 1334, Loss: 0.7115, Elapsed: 10m8s
2020-05-11 15:02:58.710385: Epoch: 1, Batch: 1322, Loss: 0.7018, Elapsed: 11m25s
2020-05-11 15:04:53.678801: Epoch: 1, Batch: 1335, Loss: 0.6895, Elapsed: 4m26s
2020-05-11 15:09:50.296923: Epoch: 1, Batch: 761, Loss: 0.6983, Elapsed: 14m58s
2020-05-11 15:11:56.308059: Epoch: 1, Batch: 1323, Loss: 0.6815, Elapsed: 8m57s
2020-05-11 15:12:21.628720: Epoch: 1, Batch: 1336, Loss: 0.6802, Elapsed: 7m27s
2020-05-11 15:15:16.567582: Epoch: 1, Batch: 1337, Loss: 0.6962, Elapsed: 2m54s
2020-05-11 15:20:49.859320: Epoch: 1, Batch: 1324, Loss: 0.6700, Elapsed: 8m53s
2020-05-11 15:21:07.503059: Epoch: 1, Batch: 1338, Loss: 0.7058, Elapsed: 5m50s
2020-05-11 15:26:47.665911: Epoch: 1, Batch: 1339, Loss: 0.7128, Elapsed: 5m40s
2020-05-11 15:28:11.269925: Epoch: 1, Batch: 762, Loss: 0.6908, Elapsed: 18m20s
2020-05-11 15:28:21.781191: Epoch: 1, Batch: 1325, Loss: 0.6696, Elapsed: 7m31s
2020-05-11 15:32:52.252257: Epoch: 1, Batch: 1340, Loss: 0.6840, Elapsed: 6m4s
2020-05-11 15:33:49.487111: Epoch: 1, Batch: 1326, Loss: 0.6870, Elapsed: 5m27s
2020-05-11 15:38:27.257844: Epoch: 1, Batch: 1341, Loss: 0.6783, Elapsed: 5m34s
2020-05-11 15:41:04.785604: Epoch: 1, Batch: 763, Loss: 0.6979, Elapsed: 12m53s
2020-05-11 15:42:43.382583: Epoch: 1, Batch: 1327, Loss: 0.6912, Elapsed: 8m53s
2020-05-11 15:48:18.889231: Epoch: 1, Batch: 1342, Loss: 0.7215, Elapsed: 9m51s
2020-05-11 15:50:30.642640: Epoch: 1, Batch: 1328, Loss: 0.6779, Elapsed: 7m47s
2020-05-11 15:53:35.213773: Epoch: 1, Batch: 1343, Loss: 0.6905, Elapsed: 5m16s
2020-05-11 15:53:43.277284: Epoch: 1, Batch: 764, Loss: 0.7007, Elapsed: 12m38s
2020-05-11 15:57:07.244534: Epoch: 1, Batch: 1329, Loss: 0.7066, Elapsed: 6m36s
2020-05-11 15:57:30.754075: Epoch: 1, Batch: 1344, Loss: 0.6941, Elapsed: 3m55s
2020-05-11 16:03:02.459761: Epoch: 1, Batch: 1330, Loss: 0.6504, Elapsed: 5m55s
2020-05-11 16:03:19.952615: Epoch: 1, Batch: 1345, Loss: 0.6820, Elapsed: 5m49s
2020-05-11 16:04:03.790629: Epoch: 1, Batch: 765, Loss: 0.6967, Elapsed: 10m20s
2020-05-11 16:08:22.345126: Epoch: 1, Batch: 1346, Loss: 0.6955, Elapsed: 5m2s
2020-05-11 16:13:33.797429: Epoch: 1, Batch: 1331, Loss: 0.6659, Elapsed: 10m31s
2020-05-11 16:15:27.551054: Epoch: 1, Batch: 1347, Loss: 0.7054, Elapsed: 7m5s
2020-05-11 16:19:48.468183: Epoch: 1, Batch: 1332, Loss: 0.6921, Elapsed: 6m14s
2020-05-11 16:20:24.378701: Epoch: 1, Batch: 766, Loss: 0.6882, Elapsed: 16m20s
2020-05-11 16:22:49.917829: Epoch: 1, Batch: 1348, Loss: 0.7007, Elapsed: 7m22s
2020-05-11 16:31:24.146300: Epoch: 1, Batch: 1333, Loss: 0.7038, Elapsed: 11m35s
2020-05-11 16:31:56.968300: Epoch: 1, Batch: 1349, Loss: 0.7029, Elapsed: 9m7s
2020-05-11 16:33:42.274676: Epoch: 1, Batch: 767, Loss: 0.6975, Elapsed: 13m17s
2020-05-11 16:37:49.031973: Epoch: 1, Batch: 1334, Loss: 0.6540, Elapsed: 6m24s
2020-05-11 16:38:58.443648: Epoch: 1, Batch: 1350, Loss: 0.6932, Elapsed: 7m1s
Starting testing the validation set with 200 subgraphs!
2020-05-11 16:43:01.834744: Epoch: 1, Batch: 1335, Loss: 0.6990, Elapsed: 5m12s
2020-05-11 16:48:00.249011: Epoch: 1, Batch: 1336, Loss: 0.6669, Elapsed: 4m58s
2020-05-11 16:51:49.965994: Epoch: 1, Batch: 1337, Loss: 0.6676, Elapsed: 3m49s
2020-05-11 16:52:23.794767: Epoch: 1, Batch: 768, Loss: 0.6791, Elapsed: 18m41s
2020-05-11 16:58:37.714383: Epoch: 1, Batch: 1338, Loss: 0.7032, Elapsed: 6m47s
2020-05-11 17:03:26.431553: Epoch: 1, Batch: 1339, Loss: 0.6857, Elapsed: 4m48s
2020-05-11 17:08:00.059994: Epoch: 1, Batch: 769, Loss: 0.6969, Elapsed: 15m36s
2020-05-11 17:09:45.263598: Epoch: 1, Batch: 1340, Loss: 0.6640, Elapsed: 6m18s
2020-05-11 17:16:14.538719: Epoch: 1, Batch: 770, Loss: 0.6962, Elapsed: 8m14s
2020-05-11 17:17:45.269201: Epoch: 1, Batch: 1341, Loss: 0.6960, Elapsed: 7m59s
2020-05-11 17:21:08.632111: Validation Test:  Loss: 0.6966,  Acc: 54.1413, AUC: 0.5583, Precision: 0.6099 -- Elapsed: 42m10s
2020-05-11 17:22:01.014895: Epoch: 1, Batch: 1342, Loss: 0.6785, Elapsed: 4m15s
2020-05-11 17:28:45.098248: Epoch: 1, Batch: 1351, Loss: 0.7019, Elapsed: 7m36s
2020-05-11 17:28:58.006570: Epoch: 1, Batch: 1343, Loss: 0.6824, Elapsed: 6m56s
2020-05-11 17:29:31.968236: Epoch: 1, Batch: 771, Loss: 0.7026, Elapsed: 13m17s
2020-05-11 17:35:07.118667: Epoch: 1, Batch: 1352, Loss: 0.7211, Elapsed: 6m21s
2020-05-11 17:38:11.585624: Epoch: 1, Batch: 1344, Loss: 0.7203, Elapsed: 9m13s
2020-05-11 17:40:59.464769: Epoch: 1, Batch: 1353, Loss: 0.7183, Elapsed: 5m52s
2020-05-11 17:42:45.937074: Epoch: 1, Batch: 772, Loss: 0.6950, Elapsed: 13m13s
2020-05-11 17:46:58.565117: Epoch: 1, Batch: 1354, Loss: 0.6958, Elapsed: 5m59s
2020-05-11 17:47:10.561309: Epoch: 1, Batch: 1345, Loss: 0.7084, Elapsed: 8m58s
2020-05-11 17:51:52.987419: Epoch: 1, Batch: 1355, Loss: 0.6998, Elapsed: 4m54s
2020-05-11 17:57:06.756027: Epoch: 1, Batch: 773, Loss: 0.6795, Elapsed: 14m20s
2020-05-11 17:59:14.324598: Epoch: 1, Batch: 1346, Loss: 0.7254, Elapsed: 12m3s
2020-05-11 17:59:32.762203: Epoch: 1, Batch: 1356, Loss: 0.6942, Elapsed: 7m39s
2020-05-11 18:03:49.978024: Epoch: 1, Batch: 1357, Loss: 0.6913, Elapsed: 4m17s
2020-05-11 18:05:06.124476: Epoch: 1, Batch: 774, Loss: 0.6991, Elapsed: 7m59s
2020-05-11 18:06:48.862035: Epoch: 1, Batch: 1347, Loss: 0.7120, Elapsed: 7m34s
2020-05-11 18:09:12.537757: Epoch: 1, Batch: 1358, Loss: 0.7062, Elapsed: 5m22s
2020-05-11 18:12:06.045899: Epoch: 1, Batch: 1348, Loss: 0.6655, Elapsed: 5m17s
2020-05-11 18:15:55.321623: Epoch: 1, Batch: 775, Loss: 0.6933, Elapsed: 10m49s
2020-05-11 18:17:20.229260: Epoch: 1, Batch: 1359, Loss: 0.7210, Elapsed: 8m7s
2020-05-11 18:23:07.604088: Epoch: 1, Batch: 1349, Loss: 0.7464, Elapsed: 11m1s
2020-05-11 18:23:15.652431: Epoch: 1, Batch: 1360, Loss: 0.6951, Elapsed: 5m55s
2020-05-11 18:24:24.507907: Epoch: 1, Batch: 776, Loss: 0.6980, Elapsed: 8m29s
2020-05-11 18:27:55.580874: Epoch: 1, Batch: 1361, Loss: 0.6982, Elapsed: 4m39s
2020-05-11 18:33:08.862873: Epoch: 1, Batch: 1362, Loss: 0.6959, Elapsed: 5m13s
2020-05-11 18:34:25.104404: Epoch: 1, Batch: 777, Loss: 0.7087, Elapsed: 10m0s
2020-05-11 18:34:38.923351: Epoch: 1, Batch: 1350, Loss: 0.7377, Elapsed: 11m31s
Starting testing the validation set with 200 subgraphs!
2020-05-11 18:40:04.482167: Epoch: 1, Batch: 1363, Loss: 0.6824, Elapsed: 6m55s
2020-05-11 18:44:41.748086: Epoch: 1, Batch: 778, Loss: 0.7045, Elapsed: 10m16s
2020-05-11 18:49:47.762040: Epoch: 1, Batch: 1364, Loss: 0.6837, Elapsed: 9m43s
2020-05-11 18:58:06.888940: Epoch: 1, Batch: 1365, Loss: 0.7046, Elapsed: 8m19s
2020-05-11 18:59:52.221688: Epoch: 1, Batch: 779, Loss: 0.6955, Elapsed: 15m10s
2020-05-11 19:03:44.509501: Epoch: 1, Batch: 1366, Loss: 0.7015, Elapsed: 5m37s
2020-05-11 19:06:04.520711: Epoch: 1, Batch: 780, Loss: 0.6871, Elapsed: 6m12s
2020-05-11 19:10:42.031736: Epoch: 1, Batch: 1367, Loss: 0.6791, Elapsed: 6m57s
2020-05-11 19:16:59.112330: Validation Test:  Loss: 0.6865,  Acc: 57.9555, AUC: 0.5802, Precision: 0.6119 -- Elapsed: 42m20s
2020-05-11 19:17:31.301027: Epoch: 1, Batch: 781, Loss: 0.7002, Elapsed: 11m26s
2020-05-11 19:21:57.364989: Epoch: 1, Batch: 1368, Loss: 0.6842, Elapsed: 11m15s
2020-05-11 19:22:49.300297: Epoch: 1, Batch: 1351, Loss: 0.6868, Elapsed: 5m50s
2020-05-11 19:28:10.609164: Epoch: 1, Batch: 1369, Loss: 0.7339, Elapsed: 6m13s
2020-05-11 19:28:33.178014: Epoch: 1, Batch: 1352, Loss: 0.6594, Elapsed: 5m43s
2020-05-11 19:28:42.003365: Epoch: 1, Batch: 782, Loss: 0.7090, Elapsed: 11m10s
2020-05-11 19:36:06.764714: Epoch: 1, Batch: 1370, Loss: 0.6939, Elapsed: 7m56s
2020-05-11 19:36:21.188297: Epoch: 1, Batch: 1353, Loss: 0.7000, Elapsed: 7m47s
2020-05-11 19:37:31.822778: Epoch: 1, Batch: 783, Loss: 0.7070, Elapsed: 8m49s
2020-05-11 19:43:51.304593: Epoch: 1, Batch: 1371, Loss: 0.6972, Elapsed: 7m44s
2020-05-11 19:44:59.962681: Epoch: 1, Batch: 1354, Loss: 0.6765, Elapsed: 8m38s
2020-05-11 19:45:32.004819: Epoch: 1, Batch: 784, Loss: 0.7056, Elapsed: 8m0s
2020-05-11 19:51:45.979775: Epoch: 1, Batch: 1355, Loss: 0.6841, Elapsed: 6m45s
2020-05-11 19:53:59.939626: Epoch: 1, Batch: 1372, Loss: 0.6872, Elapsed: 10m8s
2020-05-11 19:54:51.710537: Epoch: 1, Batch: 785, Loss: 0.6913, Elapsed: 9m19s
2020-05-11 19:58:36.943771: Epoch: 1, Batch: 1356, Loss: 0.6843, Elapsed: 6m50s
2020-05-11 20:02:37.420843: Epoch: 1, Batch: 1373, Loss: 0.6940, Elapsed: 8m37s
2020-05-11 20:02:49.960880: Epoch: 1, Batch: 786, Loss: 0.7154, Elapsed: 7m58s
2020-05-11 20:06:12.203868: Epoch: 1, Batch: 1357, Loss: 0.6809, Elapsed: 7m35s
2020-05-11 20:09:43.757113: Epoch: 1, Batch: 1358, Loss: 0.6749, Elapsed: 3m31s
2020-05-11 20:11:00.275947: Epoch: 1, Batch: 1374, Loss: 0.6789, Elapsed: 8m22s
2020-05-11 20:15:26.169104: Epoch: 1, Batch: 1375, Loss: 0.6981, Elapsed: 4m25s
2020-05-11 20:18:04.107885: Epoch: 1, Batch: 787, Loss: 0.7095, Elapsed: 15m14s
2020-05-11 20:18:21.951066: Epoch: 1, Batch: 1359, Loss: 0.6712, Elapsed: 8m38s
2020-05-11 20:20:24.090726: Epoch: 1, Batch: 1376, Loss: 0.6473, Elapsed: 4m57s
2020-05-11 20:23:45.380483: Epoch: 1, Batch: 1360, Loss: 0.6781, Elapsed: 5m23s
2020-05-11 20:29:11.614878: Epoch: 1, Batch: 1377, Loss: 0.7075, Elapsed: 8m47s
2020-05-11 20:32:19.788085: Epoch: 1, Batch: 788, Loss: 0.7026, Elapsed: 14m15s
2020-05-11 20:34:40.656890: Epoch: 1, Batch: 1378, Loss: 0.7066, Elapsed: 5m29s
2020-05-11 20:36:08.669161: Epoch: 1, Batch: 1361, Loss: 0.6794, Elapsed: 12m23s
2020-05-11 20:42:59.546982: Epoch: 1, Batch: 1379, Loss: 0.6988, Elapsed: 8m18s
2020-05-11 20:43:27.193784: Epoch: 1, Batch: 1362, Loss: 0.6902, Elapsed: 7m18s
2020-05-11 20:45:32.095912: Epoch: 1, Batch: 789, Loss: 0.6968, Elapsed: 13m12s
2020-05-11 20:48:29.503372: Epoch: 1, Batch: 1380, Loss: 0.6599, Elapsed: 5m29s
2020-05-11 20:55:01.770735: Epoch: 1, Batch: 1363, Loss: 0.7310, Elapsed: 11m34s
2020-05-11 20:56:37.772911: Epoch: 1, Batch: 1381, Loss: 0.7043, Elapsed: 8m8s
2020-05-11 20:58:47.496207: Epoch: 1, Batch: 790, Loss: 0.6901, Elapsed: 13m15s
2020-05-11 21:00:07.683959: Epoch: 1, Batch: 1364, Loss: 0.6852, Elapsed: 5m5s
2020-05-11 21:02:12.170660: Epoch: 1, Batch: 1382, Loss: 0.6965, Elapsed: 5m34s
2020-05-11 21:07:51.198178: Epoch: 1, Batch: 1365, Loss: 0.7030, Elapsed: 7m43s
2020-05-11 21:07:58.200454: Epoch: 1, Batch: 1383, Loss: 0.6665, Elapsed: 5m46s
2020-05-11 21:08:22.833827: Epoch: 1, Batch: 791, Loss: 0.6992, Elapsed: 9m35s
2020-05-11 21:14:11.321367: Epoch: 1, Batch: 1366, Loss: 0.7176, Elapsed: 6m20s
2020-05-11 21:15:16.524769: Epoch: 1, Batch: 1384, Loss: 0.6845, Elapsed: 7m18s
2020-05-11 21:18:22.989762: Epoch: 1, Batch: 792, Loss: 0.7024, Elapsed: 10m0s
2020-05-11 21:21:22.822602: Epoch: 1, Batch: 1385, Loss: 0.7041, Elapsed: 6m6s
2020-05-11 21:23:52.834276: Epoch: 1, Batch: 1367, Loss: 0.7405, Elapsed: 9m41s
2020-05-11 21:28:01.631558: Epoch: 1, Batch: 1386, Loss: 0.6997, Elapsed: 6m38s
2020-05-11 21:28:48.614331: Epoch: 1, Batch: 1368, Loss: 0.7324, Elapsed: 4m55s
2020-05-11 21:30:53.806148: Epoch: 1, Batch: 793, Loss: 0.7193, Elapsed: 12m30s
2020-05-11 21:34:26.478553: Epoch: 1, Batch: 1369, Loss: 0.7522, Elapsed: 5m37s
2020-05-11 21:36:41.536317: Epoch: 1, Batch: 1387, Loss: 0.7118, Elapsed: 8m39s
2020-05-11 21:39:44.027675: Epoch: 1, Batch: 1370, Loss: 0.7618, Elapsed: 5m17s
2020-05-11 21:42:40.281603: Epoch: 1, Batch: 794, Loss: 0.7046, Elapsed: 11m46s
2020-05-11 21:45:51.228597: Epoch: 1, Batch: 1371, Loss: 0.7520, Elapsed: 6m7s
2020-05-11 21:52:09.724449: Epoch: 1, Batch: 1372, Loss: 0.7442, Elapsed: 6m18s
2020-05-11 21:53:31.378736: Epoch: 1, Batch: 1388, Loss: 0.7351, Elapsed: 16m49s
2020-05-11 21:57:31.173241: Epoch: 1, Batch: 795, Loss: 0.7128, Elapsed: 14m50s
2020-05-11 21:59:26.588540: Epoch: 1, Batch: 1389, Loss: 0.6690, Elapsed: 5m55s
2020-05-11 22:02:13.319507: Epoch: 1, Batch: 1373, Loss: 0.7774, Elapsed: 10m3s
2020-05-11 22:04:45.307837: Epoch: 1, Batch: 1390, Loss: 0.6879, Elapsed: 5m18s
2020-05-11 22:07:22.699791: Epoch: 1, Batch: 1374, Loss: 0.8130, Elapsed: 5m9s
2020-05-11 22:08:55.738337: Epoch: 1, Batch: 796, Loss: 0.6970, Elapsed: 11m24s
2020-05-11 22:09:28.692085: Epoch: 1, Batch: 1391, Loss: 0.6831, Elapsed: 4m43s
2020-05-11 22:14:22.822624: Epoch: 1, Batch: 1375, Loss: 0.7485, Elapsed: 7m0s
2020-05-11 22:16:37.031249: Epoch: 1, Batch: 797, Loss: 0.6976, Elapsed: 7m41s
2020-05-11 22:16:51.528787: Epoch: 1, Batch: 1392, Loss: 0.6781, Elapsed: 7m22s
2020-05-11 22:25:42.672526: Epoch: 1, Batch: 1376, Loss: 0.7995, Elapsed: 11m19s
2020-05-11 22:27:24.057218: Epoch: 1, Batch: 1393, Loss: 0.7010, Elapsed: 10m32s
2020-05-11 22:29:24.675667: Epoch: 1, Batch: 798, Loss: 0.7007, Elapsed: 12m47s
2020-05-11 22:32:54.778039: Epoch: 1, Batch: 1377, Loss: 0.7934, Elapsed: 7m12s
2020-05-11 22:36:42.163912: Epoch: 1, Batch: 1394, Loss: 0.6851, Elapsed: 9m18s
2020-05-11 22:39:29.438857: Epoch: 1, Batch: 1378, Loss: 0.8472, Elapsed: 6m34s
2020-05-11 22:41:37.084477: Epoch: 1, Batch: 799, Loss: 0.7021, Elapsed: 12m12s
2020-05-11 22:43:07.922314: Epoch: 1, Batch: 1395, Loss: 0.6806, Elapsed: 6m25s
2020-05-11 22:44:48.389634: Epoch: 1, Batch: 1379, Loss: 0.7369, Elapsed: 5m18s
2020-05-11 22:50:04.554672: Epoch: 1, Batch: 1396, Loss: 0.6873, Elapsed: 6m56s
2020-05-11 22:50:57.039626: Epoch: 1, Batch: 800, Loss: 0.6974, Elapsed: 9m19s
Starting testing the validation set with 200 subgraphs!
2020-05-11 22:56:11.272708: Epoch: 1, Batch: 1380, Loss: 0.8075, Elapsed: 11m22s
2020-05-11 22:56:39.458516: Epoch: 1, Batch: 1397, Loss: 0.7132, Elapsed: 6m34s
2020-05-11 23:02:12.678246: Epoch: 1, Batch: 1398, Loss: 0.6828, Elapsed: 5m33s
2020-05-11 23:03:54.398333: Epoch: 1, Batch: 1381, Loss: 0.8053, Elapsed: 7m43s
2020-05-11 23:09:03.973472: Epoch: 1, Batch: 1382, Loss: 0.8047, Elapsed: 5m9s
2020-05-11 23:10:19.781114: Epoch: 1, Batch: 1399, Loss: 0.7056, Elapsed: 8m7s
2020-05-11 23:13:28.332286: Epoch: 1, Batch: 1383, Loss: 0.8130, Elapsed: 4m24s
2020-05-11 23:18:10.544773: Epoch: 1, Batch: 1384, Loss: 0.7857, Elapsed: 4m42s
2020-05-11 23:21:42.387257: Epoch: 1, Batch: 1400, Loss: 0.6845, Elapsed: 11m22s
Starting testing the validation set with 200 subgraphs!
2020-05-11 23:24:47.405983: Epoch: 1, Batch: 1385, Loss: 0.8015, Elapsed: 6m36s
2020-05-11 23:32:32.739386: Epoch: 1, Batch: 1386, Loss: 0.8260, Elapsed: 7m45s
2020-05-11 23:34:44.598139: Epoch: 1, Batch: 1387, Loss: 0.7722, Elapsed: 2m11s
2020-05-11 23:38:48.831694: Epoch: 1, Batch: 1388, Loss: 0.7515, Elapsed: 4m4s
2020-05-11 23:42:49.822176: Validation Test:  Loss: 0.7081,  Acc: 48.2405, AUC: 0.4808, Precision: 0.5306 -- Elapsed: 51m52s
2020-05-11 23:43:45.131487: Epoch: 1, Batch: 1389, Loss: 0.7796, Elapsed: 4m56s
2020-05-11 23:52:15.502003: Epoch: 1, Batch: 1390, Loss: 0.8280, Elapsed: 8m30s
2020-05-11 23:55:16.967388: Epoch: 1, Batch: 801, Loss: 0.7099, Elapsed: 12m27s
2020-05-12 00:01:04.365248: Epoch: 1, Batch: 1391, Loss: 0.7861, Elapsed: 8m48s
2020-05-12 00:02:12.154577: Validation Test:  Loss: 0.6926,  Acc: 56.3122, AUC: 0.5828, Precision: 0.6221 -- Elapsed: 40m29s
Starting testing the validation set with 200 subgraphs!
2020-05-12 00:07:02.228526: Epoch: 1, Batch: 802, Loss: 0.6914, Elapsed: 11m45s
2020-05-12 00:09:25.820630: Epoch: 1, Batch: 1392, Loss: 0.8181, Elapsed: 8m21s
2020-05-12 00:16:40.654746: Epoch: 1, Batch: 1393, Loss: 0.7839, Elapsed: 7m14s
2020-05-12 00:17:04.793183: Epoch: 1, Batch: 803, Loss: 0.6966, Elapsed: 10m2s
2020-05-12 00:21:48.362358: Epoch: 1, Batch: 1394, Loss: 0.7440, Elapsed: 5m7s
2020-05-12 00:29:41.366046: Epoch: 1, Batch: 1395, Loss: 0.7755, Elapsed: 7m52s
2020-05-12 00:32:30.743521: Epoch: 1, Batch: 804, Loss: 0.7188, Elapsed: 15m25s
2020-05-12 00:37:31.878017: Epoch: 1, Batch: 1396, Loss: 0.8130, Elapsed: 7m50s
2020-05-12 00:43:36.066566: Epoch: 1, Batch: 1397, Loss: 0.7757, Elapsed: 6m4s
2020-05-12 00:44:17.851642: Validation Test:  Loss: 0.6926,  Acc: 56.3112, AUC: 0.5828, Precision: 0.6221 -- Elapsed: 42m5s
2020-05-12 00:44:17.851739: Training completed!
Singularity> 2020-05-12 00:46:08.231464: Epoch: 1, Batch: 805, Loss: 0.7052, Elapsed: 13m37s
2020-05-12 00:48:59.720570: Epoch: 1, Batch: 1398, Loss: 0.7869, Elapsed: 5m23s
2020-05-12 00:51:05.000706: Epoch: 1, Batch: 806, Loss: 0.6907, Elapsed: 4m56s
2020-05-12 00:56:16.318001: Epoch: 1, Batch: 1399, Loss: 0.7933, Elapsed: 7m16s
2020-05-12 01:00:00.511099: Epoch: 1, Batch: 1400, Loss: 0.7618, Elapsed: 3m44s
Starting testing the validation set with 200 subgraphs!
2020-05-12 01:06:39.511237: Epoch: 1, Batch: 807, Loss: 0.7276, Elapsed: 15m34s
2020-05-12 01:16:06.428393: Epoch: 1, Batch: 808, Loss: 0.7021, Elapsed: 9m26s
2020-05-12 01:22:51.963540: Epoch: 1, Batch: 809, Loss: 0.6989, Elapsed: 6m45s
2020-05-12 01:32:54.144180: Epoch: 1, Batch: 810, Loss: 0.6878, Elapsed: 10m2s
2020-05-12 01:36:31.066729: Validation Test:  Loss: 0.7808,  Acc: 46.2218, AUC: 0.4433, Precision: 0.5125 -- Elapsed: 36m30s
Starting testing the validation set with 200 subgraphs!
2020-05-12 01:40:09.631763: Epoch: 1, Batch: 811, Loss: 0.6915, Elapsed: 7m15s
2020-05-12 01:48:42.446396: Epoch: 1, Batch: 812, Loss: 0.6913, Elapsed: 8m32s
2020-05-12 02:02:24.463737: Epoch: 1, Batch: 813, Loss: 0.7157, Elapsed: 13m42s
2020-05-12 02:09:57.586462: Epoch: 1, Batch: 814, Loss: 0.6925, Elapsed: 7m33s
2020-05-12 02:13:13.652089: Validation Test:  Loss: 0.7808,  Acc: 46.2218, AUC: 0.4433, Precision: 0.5125 -- Elapsed: 36m42s
2020-05-12 02:13:13.652203: Training completed!
Singularity> 2020-05-12 02:21:20.653941: Epoch: 1, Batch: 815, Loss: 0.7091, Elapsed: 11m23s
2020-05-12 02:28:27.843361: Epoch: 1, Batch: 816, Loss: 0.6917, Elapsed: 7m7s
2020-05-12 02:34:54.951785: Epoch: 1, Batch: 817, Loss: 0.6930, Elapsed: 6m27s
2020-05-12 02:44:01.082992: Epoch: 1, Batch: 818, Loss: 0.6838, Elapsed: 9m6s
2020-05-12 02:48:54.692313: Epoch: 1, Batch: 819, Loss: 0.6947, Elapsed: 4m53s
2020-05-12 02:55:55.233173: Epoch: 1, Batch: 820, Loss: 0.6826, Elapsed: 7m0s
2020-05-12 03:03:03.652488: Epoch: 1, Batch: 821, Loss: 0.6941, Elapsed: 7m8s
2020-05-12 03:11:30.913884: Epoch: 1, Batch: 822, Loss: 0.6824, Elapsed: 8m27s
2020-05-12 03:17:37.158335: Epoch: 1, Batch: 823, Loss: 0.6761, Elapsed: 6m6s
2020-05-12 03:30:19.184486: Epoch: 1, Batch: 824, Loss: 0.6820, Elapsed: 12m42s
2020-05-12 03:36:47.408507: Epoch: 1, Batch: 825, Loss: 0.6875, Elapsed: 6m28s
2020-05-12 03:42:54.564911: Epoch: 1, Batch: 826, Loss: 0.6887, Elapsed: 6m7s
2020-05-12 03:49:11.522010: Epoch: 1, Batch: 827, Loss: 0.6903, Elapsed: 6m16s
2020-05-12 03:59:03.157552: Epoch: 1, Batch: 828, Loss: 0.6920, Elapsed: 9m51s
2020-05-12 04:08:33.164138: Epoch: 1, Batch: 829, Loss: 0.6888, Elapsed: 9m29s
2020-05-12 04:19:47.730513: Epoch: 1, Batch: 830, Loss: 0.6936, Elapsed: 11m14s
2020-05-12 04:25:11.922182: Epoch: 1, Batch: 831, Loss: 0.6911, Elapsed: 5m24s
2020-05-12 04:27:47.814125: Epoch: 1, Batch: 832, Loss: 0.7017, Elapsed: 2m35s
2020-05-12 04:34:42.514886: Epoch: 1, Batch: 833, Loss: 0.6847, Elapsed: 6m54s
2020-05-12 04:41:57.062741: Epoch: 1, Batch: 834, Loss: 0.6932, Elapsed: 7m14s
2020-05-12 04:51:06.086819: Epoch: 1, Batch: 835, Loss: 0.6871, Elapsed: 9m9s
2020-05-12 04:55:47.154474: Epoch: 1, Batch: 836, Loss: 0.6997, Elapsed: 4m41s
2020-05-12 05:06:04.594136: Epoch: 1, Batch: 837, Loss: 0.6805, Elapsed: 10m17s
2020-05-12 05:14:23.620489: Epoch: 1, Batch: 838, Loss: 0.6929, Elapsed: 8m19s
2020-05-12 05:19:01.842565: Epoch: 1, Batch: 839, Loss: 0.6920, Elapsed: 4m38s
2020-05-12 05:25:58.204539: Epoch: 1, Batch: 840, Loss: 0.6938, Elapsed: 6m56s
2020-05-12 05:31:47.143292: Epoch: 1, Batch: 841, Loss: 0.6780, Elapsed: 5m48s
2020-05-12 05:37:38.445111: Epoch: 1, Batch: 842, Loss: 0.6867, Elapsed: 5m51s
2020-05-12 05:44:58.973208: Epoch: 1, Batch: 843, Loss: 0.7033, Elapsed: 7m20s
2020-05-12 05:52:42.220616: Epoch: 1, Batch: 844, Loss: 0.6926, Elapsed: 7m43s
2020-05-12 05:57:57.924408: Epoch: 1, Batch: 845, Loss: 0.6882, Elapsed: 5m15s
2020-05-12 06:02:50.815630: Epoch: 1, Batch: 846, Loss: 0.6852, Elapsed: 4m52s
2020-05-12 06:06:15.277519: Epoch: 1, Batch: 847, Loss: 0.6918, Elapsed: 3m24s
2020-05-12 06:11:28.078719: Epoch: 1, Batch: 848, Loss: 0.6882, Elapsed: 5m12s
2020-05-12 06:17:57.554882: Epoch: 1, Batch: 849, Loss: 0.6821, Elapsed: 6m29s
2020-05-12 06:25:57.528941: Epoch: 1, Batch: 850, Loss: 0.6927, Elapsed: 7m59s
Starting testing the validation set with 200 subgraphs!
2020-05-12 07:04:54.385879: Validation Test:  Loss: 0.6876,  Acc: 56.0982, AUC: 0.5762, Precision: 0.6166 -- Elapsed: 38m56s
2020-05-12 07:11:09.806667: Epoch: 1, Batch: 851, Loss: 0.6959, Elapsed: 6m15s
2020-05-12 07:16:31.199949: Epoch: 1, Batch: 852, Loss: 0.6887, Elapsed: 5m21s
2020-05-12 07:26:40.623513: Epoch: 1, Batch: 853, Loss: 0.6975, Elapsed: 10m9s
2020-05-12 07:34:58.790408: Epoch: 1, Batch: 854, Loss: 0.6862, Elapsed: 8m18s
2020-05-12 07:43:00.611445: Epoch: 1, Batch: 855, Loss: 0.6833, Elapsed: 8m1s
2020-05-12 07:50:17.116191: Epoch: 1, Batch: 856, Loss: 0.7034, Elapsed: 7m16s
2020-05-12 08:00:15.071232: Epoch: 1, Batch: 857, Loss: 0.7021, Elapsed: 9m57s
2020-05-12 08:08:51.069148: Epoch: 1, Batch: 858, Loss: 0.7044, Elapsed: 8m35s
2020-05-12 08:14:00.180821: Epoch: 1, Batch: 859, Loss: 0.6764, Elapsed: 5m9s
2020-05-12 08:20:15.574678: Epoch: 1, Batch: 860, Loss: 0.6872, Elapsed: 6m15s
2020-05-12 08:26:37.957169: Epoch: 1, Batch: 861, Loss: 0.6916, Elapsed: 6m22s
2020-05-12 08:35:46.086294: Epoch: 1, Batch: 862, Loss: 0.6902, Elapsed: 9m8s
2020-05-12 08:41:40.141512: Epoch: 1, Batch: 863, Loss: 0.6888, Elapsed: 5m54s
2020-05-12 08:48:16.747827: Epoch: 1, Batch: 864, Loss: 0.6920, Elapsed: 6m36s
2020-05-12 08:56:03.910997: Epoch: 1, Batch: 865, Loss: 0.6860, Elapsed: 7m47s
2020-05-12 09:06:09.770579: Epoch: 1, Batch: 866, Loss: 0.6903, Elapsed: 10m5s
2020-05-12 09:16:03.705433: Epoch: 1, Batch: 867, Loss: 0.6703, Elapsed: 9m53s
2020-05-12 09:22:55.008964: Epoch: 1, Batch: 868, Loss: 0.6993, Elapsed: 6m51s
2020-05-12 09:33:18.624113: Epoch: 1, Batch: 869, Loss: 0.6850, Elapsed: 10m23s
2020-05-12 09:45:08.767007: Epoch: 1, Batch: 870, Loss: 0.6746, Elapsed: 11m50s
2020-05-12 09:54:51.291358: Epoch: 1, Batch: 871, Loss: 0.6857, Elapsed: 9m42s
2020-05-12 10:03:19.162839: Epoch: 1, Batch: 872, Loss: 0.6806, Elapsed: 8m27s
2020-05-12 10:11:10.259819: Epoch: 1, Batch: 873, Loss: 0.6887, Elapsed: 7m51s
2020-05-12 10:18:40.271989: Epoch: 1, Batch: 874, Loss: 0.6836, Elapsed: 7m30s
2020-05-12 10:23:28.222593: Epoch: 1, Batch: 875, Loss: 0.6977, Elapsed: 4m47s
2020-05-12 10:33:30.353469: Epoch: 1, Batch: 876, Loss: 0.6845, Elapsed: 10m2s
2020-05-12 10:39:43.217703: Epoch: 1, Batch: 877, Loss: 0.6959, Elapsed: 6m12s
2020-05-12 10:50:06.530035: Epoch: 1, Batch: 878, Loss: 0.6895, Elapsed: 10m23s
2020-05-12 10:55:57.070150: Epoch: 1, Batch: 879, Loss: 0.6961, Elapsed: 5m50s
2020-05-12 11:02:38.931022: Epoch: 1, Batch: 880, Loss: 0.6884, Elapsed: 6m41s
2020-05-12 11:09:45.554341: Epoch: 1, Batch: 881, Loss: 0.6874, Elapsed: 7m6s
2020-05-12 11:18:03.351359: Epoch: 1, Batch: 882, Loss: 0.6831, Elapsed: 8m17s
2020-05-12 11:22:55.335089: Epoch: 1, Batch: 883, Loss: 0.6850, Elapsed: 4m51s
2020-05-12 11:31:22.357711: Epoch: 1, Batch: 884, Loss: 0.6845, Elapsed: 8m27s
2020-05-12 11:35:42.661046: Epoch: 1, Batch: 885, Loss: 0.6908, Elapsed: 4m20s
2020-05-12 11:43:32.603696: Epoch: 1, Batch: 886, Loss: 0.6887, Elapsed: 7m49s
2020-05-12 11:53:35.061005: Epoch: 1, Batch: 887, Loss: 0.6846, Elapsed: 10m2s
2020-05-12 12:01:46.942713: Epoch: 1, Batch: 888, Loss: 0.6912, Elapsed: 8m11s
2020-05-12 12:09:21.388633: Epoch: 1, Batch: 889, Loss: 0.6865, Elapsed: 7m34s
2020-05-12 12:20:51.379677: Epoch: 1, Batch: 890, Loss: 0.7003, Elapsed: 11m29s
2020-05-12 12:26:22.060355: Epoch: 1, Batch: 891, Loss: 0.6947, Elapsed: 5m30s
2020-05-12 12:35:10.799082: Epoch: 1, Batch: 892, Loss: 0.6829, Elapsed: 8m48s
2020-05-12 12:40:47.174665: Epoch: 1, Batch: 893, Loss: 0.6928, Elapsed: 5m36s
2020-05-12 12:46:47.008584: Epoch: 1, Batch: 894, Loss: 0.6935, Elapsed: 5m59s
2020-05-12 12:53:55.704411: Epoch: 1, Batch: 895, Loss: 0.6926, Elapsed: 7m8s
2020-05-12 13:00:41.839603: Epoch: 1, Batch: 896, Loss: 0.6894, Elapsed: 6m46s
2020-05-12 13:08:38.257163: Epoch: 1, Batch: 897, Loss: 0.6833, Elapsed: 7m56s
2020-05-12 13:17:26.973917: Epoch: 1, Batch: 898, Loss: 0.6784, Elapsed: 8m48s
2020-05-12 13:23:23.872410: Epoch: 1, Batch: 899, Loss: 0.6937, Elapsed: 5m56s
2020-05-12 13:29:44.275116: Epoch: 1, Batch: 900, Loss: 0.6923, Elapsed: 6m20s
Starting testing the validation set with 200 subgraphs!
2020-05-12 14:08:37.022814: Validation Test:  Loss: 0.6872,  Acc: 57.1684, AUC: 0.5837, Precision: 0.6241 -- Elapsed: 38m52s
2020-05-12 14:15:58.155309: Epoch: 1, Batch: 901, Loss: 0.6827, Elapsed: 7m21s
2020-05-12 14:26:45.545631: Epoch: 1, Batch: 902, Loss: 0.6856, Elapsed: 10m47s
2020-05-12 14:33:50.698312: Epoch: 1, Batch: 903, Loss: 0.6835, Elapsed: 7m5s
2020-05-12 14:41:47.706478: Epoch: 1, Batch: 904, Loss: 0.6840, Elapsed: 7m57s
2020-05-12 14:48:02.013992: Epoch: 1, Batch: 905, Loss: 0.6855, Elapsed: 6m14s
2020-05-12 14:54:30.499733: Epoch: 1, Batch: 906, Loss: 0.6802, Elapsed: 6m28s
2020-05-12 15:01:50.733800: Epoch: 1, Batch: 907, Loss: 0.6838, Elapsed: 7m20s
2020-05-12 15:07:57.735179: Epoch: 1, Batch: 908, Loss: 0.6875, Elapsed: 6m6s
2020-05-12 15:15:44.838190: Epoch: 1, Batch: 909, Loss: 0.6834, Elapsed: 7m47s
2020-05-12 15:22:30.257134: Epoch: 1, Batch: 910, Loss: 0.6892, Elapsed: 6m45s
2020-05-12 15:31:43.620649: Epoch: 1, Batch: 911, Loss: 0.6764, Elapsed: 9m13s
2020-05-12 15:41:31.777872: Epoch: 1, Batch: 912, Loss: 0.7038, Elapsed: 9m48s
2020-05-12 15:46:35.490859: Epoch: 1, Batch: 913, Loss: 0.6963, Elapsed: 5m3s
2020-05-12 15:53:13.022831: Epoch: 1, Batch: 914, Loss: 0.6847, Elapsed: 6m37s
2020-05-12 16:00:42.051541: Epoch: 1, Batch: 915, Loss: 0.6992, Elapsed: 7m29s
2020-05-12 16:08:50.389425: Epoch: 1, Batch: 916, Loss: 0.6918, Elapsed: 8m8s
2020-05-12 16:16:42.089449: Epoch: 1, Batch: 917, Loss: 0.6899, Elapsed: 7m51s
2020-05-12 16:24:51.161549: Epoch: 1, Batch: 918, Loss: 0.6881, Elapsed: 8m9s
2020-05-12 16:33:08.188112: Epoch: 1, Batch: 919, Loss: 0.6912, Elapsed: 8m17s
2020-05-12 16:40:06.982556: Epoch: 1, Batch: 920, Loss: 0.6983, Elapsed: 6m58s
2020-05-12 16:49:05.123441: Epoch: 1, Batch: 921, Loss: 0.6909, Elapsed: 8m58s
2020-05-12 16:53:34.690041: Epoch: 1, Batch: 922, Loss: 0.6845, Elapsed: 4m29s
2020-05-12 16:59:22.062671: Epoch: 1, Batch: 923, Loss: 0.6831, Elapsed: 5m47s
2020-05-12 17:07:56.327796: Epoch: 1, Batch: 924, Loss: 0.6976, Elapsed: 8m34s
2020-05-12 17:15:21.376965: Epoch: 1, Batch: 925, Loss: 0.7005, Elapsed: 7m25s
2020-05-12 17:20:23.708204: Epoch: 1, Batch: 926, Loss: 0.6886, Elapsed: 5m2s
2020-05-12 17:27:16.474690: Epoch: 1, Batch: 927, Loss: 0.6927, Elapsed: 6m52s
2020-05-12 17:34:24.198940: Epoch: 1, Batch: 928, Loss: 0.6825, Elapsed: 7m7s
2020-05-12 17:41:59.183066: Epoch: 1, Batch: 929, Loss: 0.6857, Elapsed: 7m34s
2020-05-12 17:48:39.697275: Epoch: 1, Batch: 930, Loss: 0.6884, Elapsed: 6m40s
2020-05-12 17:52:33.929868: Epoch: 1, Batch: 931, Loss: 0.6861, Elapsed: 3m54s
2020-05-12 18:02:19.642604: Epoch: 1, Batch: 932, Loss: 0.6880, Elapsed: 9m45s
2020-05-12 18:08:08.371795: Epoch: 1, Batch: 933, Loss: 0.6905, Elapsed: 5m48s
2020-05-12 18:14:41.734531: Epoch: 1, Batch: 934, Loss: 0.6800, Elapsed: 6m33s
2020-05-12 18:21:40.552336: Epoch: 1, Batch: 935, Loss: 0.6851, Elapsed: 6m58s
2020-05-12 18:31:43.716104: Epoch: 1, Batch: 936, Loss: 0.6752, Elapsed: 10m3s
2020-05-12 18:38:58.116289: Epoch: 1, Batch: 937, Loss: 0.6895, Elapsed: 7m14s
2020-05-12 18:45:21.049751: Epoch: 1, Batch: 938, Loss: 0.6820, Elapsed: 6m22s
2020-05-12 18:53:06.171909: Epoch: 1, Batch: 939, Loss: 0.6997, Elapsed: 7m45s
2020-05-12 18:59:41.479610: Epoch: 1, Batch: 940, Loss: 0.6973, Elapsed: 6m35s
2020-05-12 19:08:38.549570: Epoch: 1, Batch: 941, Loss: 0.6892, Elapsed: 8m57s
2020-05-12 19:16:46.011038: Epoch: 1, Batch: 942, Loss: 0.6891, Elapsed: 8m7s
2020-05-12 19:22:40.962915: Epoch: 1, Batch: 943, Loss: 0.6861, Elapsed: 5m54s
2020-05-12 19:28:59.543646: Epoch: 1, Batch: 944, Loss: 0.6891, Elapsed: 6m18s
2020-05-12 19:37:55.662047: Epoch: 1, Batch: 945, Loss: 0.6922, Elapsed: 8m56s
2020-05-12 19:43:48.456125: Epoch: 1, Batch: 946, Loss: 0.6919, Elapsed: 5m52s
2020-05-12 19:51:37.153069: Epoch: 1, Batch: 947, Loss: 0.6870, Elapsed: 7m48s
2020-05-12 19:58:49.032115: Epoch: 1, Batch: 948, Loss: 0.6830, Elapsed: 7m11s
2020-05-12 20:07:33.802653: Epoch: 1, Batch: 949, Loss: 0.7036, Elapsed: 8m44s
2020-05-12 20:13:27.482574: Epoch: 1, Batch: 950, Loss: 0.6794, Elapsed: 5m53s
Starting testing the validation set with 200 subgraphs!
2020-05-12 20:52:23.780072: Validation Test:  Loss: 0.6902,  Acc: 53.8746, AUC: 0.5452, Precision: 0.6033 -- Elapsed: 38m56s
2020-05-12 21:00:25.642751: Epoch: 1, Batch: 951, Loss: 0.6888, Elapsed: 8m1s
2020-05-12 21:08:34.191562: Epoch: 1, Batch: 952, Loss: 0.6887, Elapsed: 8m8s
2020-05-12 21:16:03.609761: Epoch: 1, Batch: 953, Loss: 0.6811, Elapsed: 7m29s
2020-05-12 21:22:26.312875: Epoch: 1, Batch: 954, Loss: 0.6763, Elapsed: 6m22s
2020-05-12 21:30:32.982752: Epoch: 1, Batch: 955, Loss: 0.6886, Elapsed: 8m6s
2020-05-12 21:39:30.745956: Epoch: 1, Batch: 956, Loss: 0.6882, Elapsed: 8m57s
2020-05-12 21:45:16.681914: Epoch: 1, Batch: 957, Loss: 0.6852, Elapsed: 5m45s
2020-05-12 21:50:48.857959: Epoch: 1, Batch: 958, Loss: 0.7052, Elapsed: 5m32s
2020-05-12 22:01:21.217258: Epoch: 1, Batch: 959, Loss: 0.6844, Elapsed: 10m32s
2020-05-12 22:07:02.388562: Epoch: 1, Batch: 960, Loss: 0.6853, Elapsed: 5m41s
2020-05-12 22:11:44.696426: Epoch: 1, Batch: 961, Loss: 0.6863, Elapsed: 4m42s
2020-05-12 22:21:50.468371: Epoch: 1, Batch: 962, Loss: 0.6979, Elapsed: 10m5s
2020-05-12 22:28:09.288325: Epoch: 1, Batch: 963, Loss: 0.6929, Elapsed: 6m18s
2020-05-12 22:33:30.520716: Epoch: 1, Batch: 964, Loss: 0.6810, Elapsed: 5m21s
2020-05-12 22:40:27.709225: Epoch: 1, Batch: 965, Loss: 0.6906, Elapsed: 6m57s
2020-05-12 22:46:08.815312: Epoch: 1, Batch: 966, Loss: 0.6891, Elapsed: 5m41s
2020-05-12 22:51:51.181638: Epoch: 1, Batch: 967, Loss: 0.6839, Elapsed: 5m42s
2020-05-12 23:01:26.748700: Epoch: 1, Batch: 968, Loss: 0.6863, Elapsed: 9m35s
2020-05-12 23:09:19.715534: Epoch: 1, Batch: 969, Loss: 0.6964, Elapsed: 7m52s
2020-05-12 23:15:43.064851: Epoch: 1, Batch: 970, Loss: 0.6864, Elapsed: 6m23s
2020-05-12 23:26:05.852740: Epoch: 1, Batch: 971, Loss: 0.6977, Elapsed: 10m22s
2020-05-12 23:33:47.199477: Epoch: 1, Batch: 972, Loss: 0.6911, Elapsed: 7m41s
2020-05-12 23:41:14.138391: Epoch: 1, Batch: 973, Loss: 0.6837, Elapsed: 7m26s
2020-05-12 23:51:50.446150: Epoch: 1, Batch: 974, Loss: 0.6913, Elapsed: 10m36s
2020-05-13 00:05:14.312089: Epoch: 1, Batch: 975, Loss: 0.6874, Elapsed: 13m23s
2020-05-13 00:12:23.328878: Epoch: 1, Batch: 976, Loss: 0.6806, Elapsed: 7m9s
2020-05-13 00:18:34.418109: Epoch: 1, Batch: 977, Loss: 0.6957, Elapsed: 6m11s
2020-05-13 00:22:44.333926: Epoch: 1, Batch: 978, Loss: 0.6865, Elapsed: 4m9s
2020-05-13 00:34:30.403513: Epoch: 1, Batch: 979, Loss: 0.6822, Elapsed: 11m46s
2020-05-13 00:42:44.922988: Epoch: 1, Batch: 980, Loss: 0.7013, Elapsed: 8m14s
2020-05-13 00:48:43.150562: Epoch: 1, Batch: 981, Loss: 0.6916, Elapsed: 5m58s
2020-05-13 00:57:15.762899: Epoch: 1, Batch: 982, Loss: 0.6894, Elapsed: 8m32s
2020-05-13 01:06:12.275623: Epoch: 1, Batch: 983, Loss: 0.6951, Elapsed: 8m56s
2020-05-13 01:16:04.477638: Epoch: 1, Batch: 984, Loss: 0.6864, Elapsed: 9m52s
2020-05-13 01:22:13.793236: Epoch: 1, Batch: 985, Loss: 0.6815, Elapsed: 6m9s
2020-05-13 01:26:57.463932: Epoch: 1, Batch: 986, Loss: 0.6909, Elapsed: 4m43s
2020-05-13 01:37:37.803704: Epoch: 1, Batch: 987, Loss: 0.6932, Elapsed: 10m40s
2020-05-13 01:42:46.487682: Epoch: 1, Batch: 988, Loss: 0.6892, Elapsed: 5m8s
2020-05-13 01:48:07.894379: Epoch: 1, Batch: 989, Loss: 0.6968, Elapsed: 5m21s
2020-05-13 01:59:06.084767: Epoch: 1, Batch: 990, Loss: 0.6873, Elapsed: 10m58s
2020-05-13 02:03:32.212117: Epoch: 1, Batch: 991, Loss: 0.6888, Elapsed: 4m26s
2020-05-13 02:07:59.524116: Epoch: 1, Batch: 992, Loss: 0.6848, Elapsed: 4m27s
2020-05-13 02:13:39.604727: Epoch: 1, Batch: 993, Loss: 0.6754, Elapsed: 5m40s
2020-05-13 02:18:50.476532: Epoch: 1, Batch: 994, Loss: 0.6906, Elapsed: 5m10s
2020-05-13 02:29:49.284292: Epoch: 1, Batch: 995, Loss: 0.6900, Elapsed: 10m58s
2020-05-13 02:36:35.717856: Epoch: 1, Batch: 996, Loss: 0.6874, Elapsed: 6m46s
2020-05-13 02:43:07.002953: Epoch: 1, Batch: 997, Loss: 0.6846, Elapsed: 6m31s
2020-05-13 02:56:32.566759: Epoch: 1, Batch: 998, Loss: 0.6820, Elapsed: 13m25s
2020-05-13 03:02:52.559785: Epoch: 1, Batch: 999, Loss: 0.6943, Elapsed: 6m19s
2020-05-13 03:10:21.596316: Epoch: 1, Batch: 1000, Loss: 0.6937, Elapsed: 7m29s
Starting testing the validation set with 200 subgraphs!
2020-05-13 03:49:20.961592: Validation Test:  Loss: 0.6873,  Acc: 54.7693, AUC: 0.5667, Precision: 0.6113 -- Elapsed: 38m59s
2020-05-13 03:57:31.232579: Epoch: 1, Batch: 1001, Loss: 0.6777, Elapsed: 8m10s
2020-05-13 04:05:15.787206: Epoch: 1, Batch: 1002, Loss: 0.6857, Elapsed: 7m44s
2020-05-13 04:16:35.041192: Epoch: 1, Batch: 1003, Loss: 0.6812, Elapsed: 11m19s
2020-05-13 04:23:40.119049: Epoch: 1, Batch: 1004, Loss: 0.6818, Elapsed: 7m5s
2020-05-13 04:31:13.803093: Epoch: 1, Batch: 1005, Loss: 0.6970, Elapsed: 7m33s
2020-05-13 04:40:28.124027: Epoch: 1, Batch: 1006, Loss: 0.6972, Elapsed: 9m14s
2020-05-13 04:47:07.738875: Epoch: 1, Batch: 1007, Loss: 0.6838, Elapsed: 6m39s
2020-05-13 04:53:13.209807: Epoch: 1, Batch: 1008, Loss: 0.6865, Elapsed: 6m5s
2020-05-13 05:03:31.966996: Epoch: 1, Batch: 1009, Loss: 0.6870, Elapsed: 10m18s
2020-05-13 05:12:39.131501: Epoch: 1, Batch: 1010, Loss: 0.6864, Elapsed: 9m7s
2020-05-13 05:17:32.513535: Epoch: 1, Batch: 1011, Loss: 0.6855, Elapsed: 4m53s
2020-05-13 05:21:45.462600: Epoch: 1, Batch: 1012, Loss: 0.6920, Elapsed: 4m12s
2020-05-13 05:27:54.512773: Epoch: 1, Batch: 1013, Loss: 0.6745, Elapsed: 6m9s
2020-05-13 05:35:26.292848: Epoch: 1, Batch: 1014, Loss: 0.6820, Elapsed: 7m31s
2020-05-13 05:41:38.303247: Epoch: 1, Batch: 1015, Loss: 0.6826, Elapsed: 6m12s
2020-05-13 05:50:50.326415: Epoch: 1, Batch: 1016, Loss: 0.6789, Elapsed: 9m12s
2020-05-13 05:59:05.787264: Epoch: 1, Batch: 1017, Loss: 0.6954, Elapsed: 8m15s
2020-05-13 06:07:25.792321: Epoch: 1, Batch: 1018, Loss: 0.6855, Elapsed: 8m19s
2020-05-13 06:15:00.859472: Epoch: 1, Batch: 1019, Loss: 0.6869, Elapsed: 7m35s
2020-05-13 06:21:38.081945: Epoch: 1, Batch: 1020, Loss: 0.6796, Elapsed: 6m37s
2020-05-13 06:27:26.131265: Epoch: 1, Batch: 1021, Loss: 0.6914, Elapsed: 5m48s
2020-05-13 06:31:35.881894: Epoch: 1, Batch: 1022, Loss: 0.6754, Elapsed: 4m9s
2020-05-13 06:38:58.182853: Epoch: 1, Batch: 1023, Loss: 0.6852, Elapsed: 7m22s
2020-05-13 06:47:42.828594: Epoch: 1, Batch: 1024, Loss: 0.6860, Elapsed: 8m44s
2020-05-13 06:54:55.472408: Epoch: 1, Batch: 1025, Loss: 0.6807, Elapsed: 7m12s
2020-05-13 06:59:11.054442: Epoch: 1, Batch: 1026, Loss: 0.6912, Elapsed: 4m15s
2020-05-13 07:08:47.897810: Epoch: 1, Batch: 1027, Loss: 0.6679, Elapsed: 9m36s
2020-05-13 07:19:26.424365: Epoch: 1, Batch: 1028, Loss: 0.6727, Elapsed: 10m38s
2020-05-13 07:23:33.193526: Epoch: 1, Batch: 1029, Loss: 0.7076, Elapsed: 4m6s
2020-05-13 07:33:45.843769: Epoch: 1, Batch: 1030, Loss: 0.6937, Elapsed: 10m12s
2020-05-13 07:38:01.729776: Epoch: 1, Batch: 1031, Loss: 0.6886, Elapsed: 4m15s
2020-05-13 07:47:02.658759: Epoch: 1, Batch: 1032, Loss: 0.6841, Elapsed: 9m0s
2020-05-13 07:52:36.228434: Epoch: 1, Batch: 1033, Loss: 0.6760, Elapsed: 5m33s
2020-05-13 08:01:15.484122: Epoch: 1, Batch: 1034, Loss: 0.6758, Elapsed: 8m39s
2020-05-13 08:06:46.994341: Epoch: 1, Batch: 1035, Loss: 0.6879, Elapsed: 5m31s
2020-05-13 08:15:25.645045: Epoch: 1, Batch: 1036, Loss: 0.6809, Elapsed: 8m38s
2020-05-13 08:21:20.406481: Epoch: 1, Batch: 1037, Loss: 0.6852, Elapsed: 5m54s
2020-05-13 08:27:22.429015: Epoch: 1, Batch: 1038, Loss: 0.6939, Elapsed: 6m2s
2020-05-13 08:34:30.442468: Epoch: 1, Batch: 1039, Loss: 0.6846, Elapsed: 7m8s
2020-05-13 08:42:48.759351: Epoch: 1, Batch: 1040, Loss: 0.6799, Elapsed: 8m18s
2020-05-13 08:51:03.922426: Epoch: 1, Batch: 1041, Loss: 0.6631, Elapsed: 8m15s
2020-05-13 08:57:12.999033: Epoch: 1, Batch: 1042, Loss: 0.6861, Elapsed: 6m9s
2020-05-13 09:01:23.994647: Epoch: 1, Batch: 1043, Loss: 0.6969, Elapsed: 4m10s
2020-05-13 09:08:35.866390: Epoch: 1, Batch: 1044, Loss: 0.6836, Elapsed: 7m11s
2020-05-13 09:18:31.061364: Epoch: 1, Batch: 1045, Loss: 0.6812, Elapsed: 9m55s
2020-05-13 09:25:34.972439: Epoch: 1, Batch: 1046, Loss: 0.6751, Elapsed: 7m3s
2020-05-13 09:33:14.980089: Epoch: 1, Batch: 1047, Loss: 0.6662, Elapsed: 7m40s
2020-05-13 09:40:33.411176: Epoch: 1, Batch: 1048, Loss: 0.6805, Elapsed: 7m18s
2020-05-13 09:50:51.724198: Epoch: 1, Batch: 1049, Loss: 0.6739, Elapsed: 10m18s
2020-05-13 10:00:17.003643: Epoch: 1, Batch: 1050, Loss: 0.6887, Elapsed: 9m25s
Starting testing the validation set with 200 subgraphs!
2020-05-13 10:39:23.509289: Validation Test:  Loss: 0.6869,  Acc: 58.9385, AUC: 0.6016, Precision: 0.6338 -- Elapsed: 39m6s
2020-05-13 10:45:40.325359: Epoch: 1, Batch: 1051, Loss: 0.7016, Elapsed: 6m16s
2020-05-13 10:51:32.194679: Epoch: 1, Batch: 1052, Loss: 0.7025, Elapsed: 5m51s
2020-05-13 11:00:17.037451: Epoch: 1, Batch: 1053, Loss: 0.6803, Elapsed: 8m44s
2020-05-13 11:07:02.670463: Epoch: 1, Batch: 1054, Loss: 0.6943, Elapsed: 6m45s
2020-05-13 11:18:18.783468: Epoch: 1, Batch: 1055, Loss: 0.6787, Elapsed: 11m16s
2020-05-13 11:25:14.606446: Epoch: 1, Batch: 1056, Loss: 0.6941, Elapsed: 6m55s
2020-05-13 11:30:51.509540: Epoch: 1, Batch: 1057, Loss: 0.6979, Elapsed: 5m36s
2020-05-13 11:41:23.860520: Epoch: 1, Batch: 1058, Loss: 0.6868, Elapsed: 10m32s
2020-05-13 11:46:04.337426: Epoch: 1, Batch: 1059, Loss: 0.7092, Elapsed: 4m40s
2020-05-13 11:53:22.453849: Epoch: 1, Batch: 1060, Loss: 0.6891, Elapsed: 7m18s
2020-05-13 12:00:33.444935: Epoch: 1, Batch: 1061, Loss: 0.6989, Elapsed: 7m10s
2020-05-13 12:08:25.576921: Epoch: 1, Batch: 1062, Loss: 0.6930, Elapsed: 7m52s
2020-05-13 12:16:09.052666: Epoch: 1, Batch: 1063, Loss: 0.6765, Elapsed: 7m43s
2020-05-13 12:23:52.369490: Epoch: 1, Batch: 1064, Loss: 0.6868, Elapsed: 7m43s
2020-05-13 12:31:11.211320: Epoch: 1, Batch: 1065, Loss: 0.6847, Elapsed: 7m18s
2020-05-13 12:35:53.911670: Epoch: 1, Batch: 1066, Loss: 0.6928, Elapsed: 4m42s
2020-05-13 12:41:19.911892: Epoch: 1, Batch: 1067, Loss: 0.6934, Elapsed: 5m25s
2020-05-13 12:47:49.677429: Epoch: 1, Batch: 1068, Loss: 0.6859, Elapsed: 6m29s
2020-05-13 12:55:31.057316: Epoch: 1, Batch: 1069, Loss: 0.6987, Elapsed: 7m41s
2020-05-13 13:06:44.502172: Epoch: 1, Batch: 1070, Loss: 0.7025, Elapsed: 11m13s
2020-05-13 13:15:45.987737: Epoch: 1, Batch: 1071, Loss: 0.6792, Elapsed: 9m1s
2020-05-13 13:26:21.806126: Epoch: 1, Batch: 1072, Loss: 0.6848, Elapsed: 10m35s
2020-05-13 13:31:54.736518: Epoch: 1, Batch: 1073, Loss: 0.6816, Elapsed: 5m32s
2020-05-13 13:41:12.844639: Epoch: 1, Batch: 1074, Loss: 0.6820, Elapsed: 9m18s
2020-05-13 13:44:58.934763: Epoch: 1, Batch: 1075, Loss: 0.7069, Elapsed: 3m46s
2020-05-13 13:54:02.281473: Epoch: 1, Batch: 1076, Loss: 0.6819, Elapsed: 9m3s
2020-05-13 14:11:39.690724: Epoch: 1, Batch: 1077, Loss: 0.6706, Elapsed: 17m37s
2020-05-13 14:19:39.576478: Epoch: 1, Batch: 1078, Loss: 0.6946, Elapsed: 7m59s
2020-05-13 14:27:43.298295: Epoch: 1, Batch: 1079, Loss: 0.6937, Elapsed: 8m3s
2020-05-13 14:32:31.539073: Epoch: 1, Batch: 1080, Loss: 0.6878, Elapsed: 4m48s
2020-05-13 14:37:21.861004: Epoch: 1, Batch: 1081, Loss: 0.7018, Elapsed: 4m50s
2020-05-13 14:46:02.970563: Epoch: 1, Batch: 1082, Loss: 0.6850, Elapsed: 8m41s
2020-05-13 14:51:33.693024: Epoch: 1, Batch: 1083, Loss: 0.6931, Elapsed: 5m30s
2020-05-13 15:03:40.072757: Epoch: 1, Batch: 1084, Loss: 0.6877, Elapsed: 12m6s
2020-05-13 15:11:18.974246: Epoch: 1, Batch: 1085, Loss: 0.6922, Elapsed: 7m38s
2020-05-13 15:19:55.524277: Epoch: 1, Batch: 1086, Loss: 0.6950, Elapsed: 8m36s
2020-05-13 15:24:32.806641: Epoch: 1, Batch: 1087, Loss: 0.6969, Elapsed: 4m37s
2020-05-13 15:34:23.322724: Epoch: 1, Batch: 1088, Loss: 0.6896, Elapsed: 9m50s
2020-05-13 15:38:31.282820: Epoch: 1, Batch: 1089, Loss: 0.6702, Elapsed: 4m7s
2020-05-13 15:46:28.065103: Epoch: 1, Batch: 1090, Loss: 0.6916, Elapsed: 7m56s
2020-05-13 15:55:25.999047: Epoch: 1, Batch: 1091, Loss: 0.6614, Elapsed: 8m57s
2020-05-13 16:05:17.483854: Epoch: 1, Batch: 1092, Loss: 0.6952, Elapsed: 9m51s
2020-05-13 16:14:54.598567: Epoch: 1, Batch: 1093, Loss: 0.6806, Elapsed: 9m37s
2020-05-13 16:19:06.815350: Epoch: 1, Batch: 1094, Loss: 0.6740, Elapsed: 4m12s
2020-05-13 16:24:52.619819: Epoch: 1, Batch: 1095, Loss: 0.6893, Elapsed: 5m45s
2020-05-13 16:30:46.763510: Epoch: 1, Batch: 1096, Loss: 0.7027, Elapsed: 5m54s
2020-05-13 16:34:29.941821: Epoch: 1, Batch: 1097, Loss: 0.6783, Elapsed: 3m43s
2020-05-13 16:40:24.570529: Epoch: 1, Batch: 1098, Loss: 0.6739, Elapsed: 5m54s
2020-05-13 16:51:55.967477: Epoch: 1, Batch: 1099, Loss: 0.6938, Elapsed: 11m31s
2020-05-13 16:59:41.936659: Epoch: 1, Batch: 1100, Loss: 0.6774, Elapsed: 7m45s
Starting testing the validation set with 200 subgraphs!
2020-05-13 17:38:42.089832: Validation Test:  Loss: 0.6897,  Acc: 54.6648, AUC: 0.5584, Precision: 0.6014 -- Elapsed: 39m0s
2020-05-13 17:45:26.714369: Epoch: 1, Batch: 1101, Loss: 0.6870, Elapsed: 6m44s
2020-05-13 17:53:49.404387: Epoch: 1, Batch: 1102, Loss: 0.6859, Elapsed: 8m22s
2020-05-13 17:58:28.997909: Epoch: 1, Batch: 1103, Loss: 0.6955, Elapsed: 4m39s
2020-05-13 18:05:31.271761: Epoch: 1, Batch: 1104, Loss: 0.6838, Elapsed: 7m2s
2020-05-13 18:14:01.036124: Epoch: 1, Batch: 1105, Loss: 0.6960, Elapsed: 8m29s
2020-05-13 18:21:25.004780: Epoch: 1, Batch: 1106, Loss: 0.7077, Elapsed: 7m23s
2020-05-13 18:29:50.640232: Epoch: 1, Batch: 1107, Loss: 0.6744, Elapsed: 8m25s
2020-05-13 18:34:20.845327: Epoch: 1, Batch: 1108, Loss: 0.6692, Elapsed: 4m30s
2020-05-13 18:41:39.532270: Epoch: 1, Batch: 1109, Loss: 0.6925, Elapsed: 7m18s
2020-05-13 18:51:48.648259: Epoch: 1, Batch: 1110, Loss: 0.6816, Elapsed: 10m9s
2020-05-13 19:00:33.056610: Epoch: 1, Batch: 1111, Loss: 0.6951, Elapsed: 8m44s
2020-05-13 19:07:14.415490: Epoch: 1, Batch: 1112, Loss: 0.6834, Elapsed: 6m41s
2020-05-13 19:16:28.739110: Epoch: 1, Batch: 1113, Loss: 0.6876, Elapsed: 9m14s
2020-05-13 19:23:31.113260: Epoch: 1, Batch: 1114, Loss: 0.6865, Elapsed: 7m2s
2020-05-13 19:29:58.262415: Epoch: 1, Batch: 1115, Loss: 0.6839, Elapsed: 6m27s
2020-05-13 19:38:12.342960: Epoch: 1, Batch: 1116, Loss: 0.6903, Elapsed: 8m14s
2020-05-13 19:46:01.033509: Epoch: 1, Batch: 1117, Loss: 0.6769, Elapsed: 7m48s
2020-05-13 19:56:21.416181: Epoch: 1, Batch: 1118, Loss: 0.6862, Elapsed: 10m20s
2020-05-13 20:04:10.403259: Epoch: 1, Batch: 1119, Loss: 0.6834, Elapsed: 7m48s
2020-05-13 20:10:27.391570: Epoch: 1, Batch: 1120, Loss: 0.7052, Elapsed: 6m16s
2020-05-13 20:17:58.687700: Epoch: 1, Batch: 1121, Loss: 0.6883, Elapsed: 7m31s
2020-05-13 20:24:33.621202: Epoch: 1, Batch: 1122, Loss: 0.6808, Elapsed: 6m34s
2020-05-13 20:29:09.732296: Epoch: 1, Batch: 1123, Loss: 0.6876, Elapsed: 4m36s
2020-05-13 20:37:45.506617: Epoch: 1, Batch: 1124, Loss: 0.6971, Elapsed: 8m35s
2020-05-13 20:43:45.829911: Epoch: 1, Batch: 1125, Loss: 0.6856, Elapsed: 6m0s
2020-05-13 20:50:52.294467: Epoch: 1, Batch: 1126, Loss: 0.6884, Elapsed: 7m6s
2020-05-13 20:58:28.573068: Epoch: 1, Batch: 1127, Loss: 0.6816, Elapsed: 7m36s
2020-05-13 21:03:21.816488: Epoch: 1, Batch: 1128, Loss: 0.6792, Elapsed: 4m53s
2020-05-13 21:11:27.937963: Epoch: 1, Batch: 1129, Loss: 0.7021, Elapsed: 8m6s
2020-05-13 21:23:09.990416: Epoch: 1, Batch: 1130, Loss: 0.6897, Elapsed: 11m42s
2020-05-13 21:37:10.335239: Epoch: 1, Batch: 1131, Loss: 0.6693, Elapsed: 14m0s
2020-05-13 21:51:25.219098: Epoch: 1, Batch: 1132, Loss: 0.7210, Elapsed: 14m14s
2020-05-13 22:00:14.296670: Epoch: 1, Batch: 1133, Loss: 0.6710, Elapsed: 8m49s
2020-05-13 22:09:14.210432: Epoch: 1, Batch: 1134, Loss: 0.6758, Elapsed: 8m59s
2020-05-13 22:14:29.656553: Epoch: 1, Batch: 1135, Loss: 0.6769, Elapsed: 5m15s
2020-05-13 22:22:33.961183: Epoch: 1, Batch: 1136, Loss: 0.6871, Elapsed: 8m4s
2020-05-13 22:26:32.886057: Epoch: 1, Batch: 1137, Loss: 0.6817, Elapsed: 3m58s
2020-05-13 22:37:14.937118: Epoch: 1, Batch: 1138, Loss: 0.6788, Elapsed: 10m42s
2020-05-13 22:42:53.428206: Epoch: 1, Batch: 1139, Loss: 0.6837, Elapsed: 5m38s
2020-05-13 22:48:01.807423: Epoch: 1, Batch: 1140, Loss: 0.7105, Elapsed: 5m8s
2020-05-13 22:52:41.561397: Epoch: 1, Batch: 1141, Loss: 0.6964, Elapsed: 4m39s
2020-05-13 22:59:04.232404: Epoch: 1, Batch: 1142, Loss: 0.6799, Elapsed: 6m22s
2020-05-13 23:03:59.203846: Epoch: 1, Batch: 1143, Loss: 0.7174, Elapsed: 4m54s
2020-05-13 23:09:47.057115: Epoch: 1, Batch: 1144, Loss: 0.6969, Elapsed: 5m47s
2020-05-13 23:15:49.637394: Epoch: 1, Batch: 1145, Loss: 0.6976, Elapsed: 6m2s
2020-05-13 23:21:04.739639: Epoch: 1, Batch: 1146, Loss: 0.7037, Elapsed: 5m15s
2020-05-13 23:28:42.691814: Epoch: 1, Batch: 1147, Loss: 0.6731, Elapsed: 7m37s
2020-05-13 23:38:13.598348: Epoch: 1, Batch: 1148, Loss: 0.6795, Elapsed: 9m30s
2020-05-13 23:44:28.301334: Epoch: 1, Batch: 1149, Loss: 0.6936, Elapsed: 6m14s
2020-05-13 23:50:39.908387: Epoch: 1, Batch: 1150, Loss: 0.6934, Elapsed: 6m11s
Starting testing the validation set with 200 subgraphs!
2020-05-14 00:29:33.754566: Validation Test:  Loss: 0.6929,  Acc: 58.5054, AUC: 0.5924, Precision: 0.6310 -- Elapsed: 38m53s
2020-05-14 00:34:50.723711: Epoch: 1, Batch: 1151, Loss: 0.6918, Elapsed: 5m16s
2020-05-14 00:45:19.118158: Epoch: 1, Batch: 1152, Loss: 0.6759, Elapsed: 10m28s
2020-05-14 00:54:03.511336: Epoch: 1, Batch: 1153, Loss: 0.6929, Elapsed: 8m44s
2020-05-14 01:02:44.832007: Epoch: 1, Batch: 1154, Loss: 0.6814, Elapsed: 8m41s
2020-05-14 01:11:06.899875: Epoch: 1, Batch: 1155, Loss: 0.6954, Elapsed: 8m22s
2020-05-14 01:18:13.683970: Epoch: 1, Batch: 1156, Loss: 0.7211, Elapsed: 7m6s
2020-05-14 01:25:55.954814: Epoch: 1, Batch: 1157, Loss: 0.6783, Elapsed: 7m42s
2020-05-14 01:30:18.825342: Epoch: 1, Batch: 1158, Loss: 0.7045, Elapsed: 4m22s
2020-05-14 01:46:51.223391: Epoch: 1, Batch: 1159, Loss: 0.6802, Elapsed: 16m32s
2020-05-14 01:51:38.326219: Epoch: 1, Batch: 1160, Loss: 0.6822, Elapsed: 4m47s
2020-05-14 02:07:46.806450: Epoch: 1, Batch: 1161, Loss: 0.7121, Elapsed: 16m8s
2020-05-14 02:16:28.418699: Epoch: 1, Batch: 1162, Loss: 0.6912, Elapsed: 8m41s
2020-05-14 02:24:39.648832: Epoch: 1, Batch: 1163, Loss: 0.6779, Elapsed: 8m11s
2020-05-14 02:33:10.004377: Epoch: 1, Batch: 1164, Loss: 0.6922, Elapsed: 8m30s
2020-05-14 02:41:42.973851: Epoch: 1, Batch: 1165, Loss: 0.6969, Elapsed: 8m32s
2020-05-14 02:50:57.564509: Epoch: 1, Batch: 1166, Loss: 0.6902, Elapsed: 9m14s
2020-05-14 03:04:02.117962: Epoch: 1, Batch: 1167, Loss: 0.6637, Elapsed: 13m4s
2020-05-14 03:08:37.674238: Epoch: 1, Batch: 1168, Loss: 0.7268, Elapsed: 4m35s
2020-05-14 03:20:08.119056: Epoch: 1, Batch: 1169, Loss: 0.6966, Elapsed: 11m30s
2020-05-14 03:26:41.332129: Epoch: 1, Batch: 1170, Loss: 0.6963, Elapsed: 6m33s
2020-05-14 03:37:08.682630: Epoch: 1, Batch: 1171, Loss: 0.6919, Elapsed: 10m27s
2020-05-14 03:49:33.545730: Epoch: 1, Batch: 1172, Loss: 0.6742, Elapsed: 12m24s
2020-05-14 03:57:27.718026: Epoch: 1, Batch: 1173, Loss: 0.6794, Elapsed: 7m54s
2020-05-14 04:06:28.514763: Epoch: 1, Batch: 1174, Loss: 0.6935, Elapsed: 9m0s
2020-05-14 04:15:57.926117: Epoch: 1, Batch: 1175, Loss: 0.6910, Elapsed: 9m29s
2020-05-14 04:23:52.974726: Epoch: 1, Batch: 1176, Loss: 0.6729, Elapsed: 7m55s
2020-05-14 04:31:00.562304: Epoch: 1, Batch: 1177, Loss: 0.6998, Elapsed: 7m7s
2020-05-14 04:34:48.385532: Epoch: 1, Batch: 1178, Loss: 0.7392, Elapsed: 3m47s
2020-05-14 04:42:31.548223: Epoch: 1, Batch: 1179, Loss: 0.6884, Elapsed: 7m43s
2020-05-14 04:50:47.928083: Epoch: 1, Batch: 1180, Loss: 0.6851, Elapsed: 8m16s
2020-05-14 04:59:48.233218: Epoch: 1, Batch: 1181, Loss: 0.6917, Elapsed: 9m0s
2020-05-14 05:08:04.821959: Epoch: 1, Batch: 1182, Loss: 0.6763, Elapsed: 8m16s
2020-05-14 05:15:08.917944: Epoch: 1, Batch: 1183, Loss: 0.6878, Elapsed: 7m4s
2020-05-14 05:21:44.538766: Epoch: 1, Batch: 1184, Loss: 0.7059, Elapsed: 6m35s
2020-05-14 05:31:58.206540: Epoch: 1, Batch: 1185, Loss: 0.6744, Elapsed: 10m13s
2020-05-14 05:38:15.211032: Epoch: 1, Batch: 1186, Loss: 0.7074, Elapsed: 6m16s
2020-05-14 05:43:22.534667: Epoch: 1, Batch: 1187, Loss: 0.7168, Elapsed: 5m7s
2020-05-14 05:51:14.069005: Epoch: 1, Batch: 1188, Loss: 0.6746, Elapsed: 7m51s
2020-05-14 05:56:44.140757: Epoch: 1, Batch: 1189, Loss: 0.6963, Elapsed: 5m30s
2020-05-14 06:04:47.441226: Epoch: 1, Batch: 1190, Loss: 0.6747, Elapsed: 8m3s
2020-05-14 06:11:48.000056: Epoch: 1, Batch: 1191, Loss: 0.6991, Elapsed: 7m0s
2020-05-14 06:17:39.184703: Epoch: 1, Batch: 1192, Loss: 0.6914, Elapsed: 5m51s
2020-05-14 06:24:10.339624: Epoch: 1, Batch: 1193, Loss: 0.7165, Elapsed: 6m31s
2020-05-14 06:30:36.523367: Epoch: 1, Batch: 1194, Loss: 0.6885, Elapsed: 6m26s
2020-05-14 06:35:49.062260: Epoch: 1, Batch: 1195, Loss: 0.7040, Elapsed: 5m12s
2020-05-14 06:39:51.609530: Epoch: 1, Batch: 1196, Loss: 0.6834, Elapsed: 4m2s
2020-05-14 06:48:17.759534: Epoch: 1, Batch: 1197, Loss: 0.6790, Elapsed: 8m26s
2020-05-14 06:58:41.382590: Epoch: 1, Batch: 1198, Loss: 0.6643, Elapsed: 10m23s
2020-05-14 07:05:44.226125: Epoch: 1, Batch: 1199, Loss: 0.6946, Elapsed: 7m2s
2020-05-14 07:13:43.774810: Epoch: 1, Batch: 1200, Loss: 0.6889, Elapsed: 7m59s
Starting testing the validation set with 200 subgraphs!
2020-05-14 07:52:41.593208: Validation Test:  Loss: 0.6885,  Acc: 58.3096, AUC: 0.5998, Precision: 0.6285 -- Elapsed: 38m57s
2020-05-14 08:01:40.972047: Epoch: 1, Batch: 1201, Loss: 0.6876, Elapsed: 8m59s
2020-05-14 08:04:43.245019: Epoch: 1, Batch: 1202, Loss: 0.6951, Elapsed: 3m2s
2020-05-14 08:12:30.051422: Epoch: 1, Batch: 1203, Loss: 0.6821, Elapsed: 7m46s
2020-05-14 08:19:03.477865: Epoch: 1, Batch: 1204, Loss: 0.7053, Elapsed: 6m33s
2020-05-14 08:28:03.042731: Epoch: 1, Batch: 1205, Loss: 0.6760, Elapsed: 8m59s
2020-05-14 08:34:12.998159: Epoch: 1, Batch: 1206, Loss: 0.6871, Elapsed: 6m9s
2020-05-14 08:42:01.227689: Epoch: 1, Batch: 1207, Loss: 0.6874, Elapsed: 7m48s
2020-05-14 08:49:58.717112: Epoch: 1, Batch: 1208, Loss: 0.6837, Elapsed: 7m57s
2020-05-14 08:58:58.999601: Epoch: 1, Batch: 1209, Loss: 0.6751, Elapsed: 9m0s
2020-05-14 09:07:43.042888: Epoch: 1, Batch: 1210, Loss: 0.6908, Elapsed: 8m44s
2020-05-14 09:17:35.943147: Epoch: 1, Batch: 1211, Loss: 0.6906, Elapsed: 9m52s
2020-05-14 09:26:03.461145: Epoch: 1, Batch: 1212, Loss: 0.6936, Elapsed: 8m27s
2020-05-14 09:32:28.578851: Epoch: 1, Batch: 1213, Loss: 0.6879, Elapsed: 6m25s
2020-05-14 09:39:29.863361: Epoch: 1, Batch: 1214, Loss: 0.6796, Elapsed: 7m1s
2020-05-14 09:46:26.819018: Epoch: 1, Batch: 1215, Loss: 0.6843, Elapsed: 6m56s
2020-05-14 09:51:13.641626: Epoch: 1, Batch: 1216, Loss: 0.7029, Elapsed: 4m46s
2020-05-14 09:58:12.698567: Epoch: 1, Batch: 1217, Loss: 0.6894, Elapsed: 6m59s
2020-05-14 10:05:31.494427: Epoch: 1, Batch: 1218, Loss: 0.6705, Elapsed: 7m18s
2020-05-14 10:13:47.850318: Epoch: 1, Batch: 1219, Loss: 0.6780, Elapsed: 8m16s
2020-05-14 10:19:37.020151: Epoch: 1, Batch: 1220, Loss: 0.6932, Elapsed: 5m49s
2020-05-14 10:28:00.862454: Epoch: 1, Batch: 1221, Loss: 0.6944, Elapsed: 8m23s
2020-05-14 10:37:01.357599: Epoch: 1, Batch: 1222, Loss: 0.6708, Elapsed: 9m0s
2020-05-14 10:42:28.610889: Epoch: 1, Batch: 1223, Loss: 0.6821, Elapsed: 5m27s
2020-05-14 10:50:11.492909: Epoch: 1, Batch: 1224, Loss: 0.6835, Elapsed: 7m42s
2020-05-14 11:00:03.857165: Epoch: 1, Batch: 1225, Loss: 0.6942, Elapsed: 9m52s
2020-05-14 11:07:30.226773: Epoch: 1, Batch: 1226, Loss: 0.6831, Elapsed: 7m26s
2020-05-14 11:14:15.471229: Epoch: 1, Batch: 1227, Loss: 0.6792, Elapsed: 6m45s
2020-05-14 11:22:18.929631: Epoch: 1, Batch: 1228, Loss: 0.6740, Elapsed: 8m3s
2020-05-14 11:28:15.828111: Epoch: 1, Batch: 1229, Loss: 0.6945, Elapsed: 5m56s
2020-05-14 11:35:53.845793: Epoch: 1, Batch: 1230, Loss: 0.6844, Elapsed: 7m38s
2020-05-14 11:45:05.097297: Epoch: 1, Batch: 1231, Loss: 0.6875, Elapsed: 9m11s
2020-05-14 11:49:42.233294: Epoch: 1, Batch: 1232, Loss: 0.6968, Elapsed: 4m37s
2020-05-14 11:55:19.438863: Epoch: 1, Batch: 1233, Loss: 0.6850, Elapsed: 5m37s
2020-05-14 12:00:45.167973: Epoch: 1, Batch: 1234, Loss: 0.6907, Elapsed: 5m25s
2020-05-14 12:07:35.210692: Epoch: 1, Batch: 1235, Loss: 0.6811, Elapsed: 6m50s
2020-05-14 12:10:54.466787: Epoch: 1, Batch: 1236, Loss: 0.6778, Elapsed: 3m19s
2020-05-14 12:18:52.693859: Epoch: 1, Batch: 1237, Loss: 0.6914, Elapsed: 7m58s
2020-05-14 12:25:31.957622: Epoch: 1, Batch: 1238, Loss: 0.6771, Elapsed: 6m39s
2020-05-14 12:32:45.558397: Epoch: 1, Batch: 1239, Loss: 0.6997, Elapsed: 7m13s
2020-05-14 12:37:32.493881: Epoch: 1, Batch: 1240, Loss: 0.6804, Elapsed: 4m46s
2020-05-14 12:44:04.710662: Epoch: 1, Batch: 1241, Loss: 0.6828, Elapsed: 6m32s
2020-05-14 12:49:27.596392: Epoch: 1, Batch: 1242, Loss: 0.6837, Elapsed: 5m22s
2020-05-14 12:55:16.057493: Epoch: 1, Batch: 1243, Loss: 0.6808, Elapsed: 5m48s
2020-05-14 13:00:53.684497: Epoch: 1, Batch: 1244, Loss: 0.6813, Elapsed: 5m37s
2020-05-14 13:07:54.149868: Epoch: 1, Batch: 1245, Loss: 0.6902, Elapsed: 7m0s
2020-05-14 13:16:38.494949: Epoch: 1, Batch: 1246, Loss: 0.6861, Elapsed: 8m44s
2020-05-14 13:24:05.885966: Epoch: 1, Batch: 1247, Loss: 0.6836, Elapsed: 7m27s
2020-05-14 13:31:16.972848: Epoch: 1, Batch: 1248, Loss: 0.6766, Elapsed: 7m11s
2020-05-14 13:37:08.671665: Epoch: 1, Batch: 1249, Loss: 0.6897, Elapsed: 5m51s
2020-05-14 13:47:11.953902: Epoch: 1, Batch: 1250, Loss: 0.6883, Elapsed: 10m3s
Starting testing the validation set with 200 subgraphs!
2020-05-14 14:26:04.560008: Validation Test:  Loss: 0.6879,  Acc: 55.8760, AUC: 0.5743, Precision: 0.6095 -- Elapsed: 38m52s
2020-05-14 14:29:30.097527: Epoch: 1, Batch: 1251, Loss: 0.6850, Elapsed: 3m25s
2020-05-14 14:34:43.951198: Epoch: 1, Batch: 1252, Loss: 0.6883, Elapsed: 5m13s
2020-05-14 14:42:38.198435: Epoch: 1, Batch: 1253, Loss: 0.6881, Elapsed: 7m54s
2020-05-14 14:53:36.572699: Epoch: 1, Batch: 1254, Loss: 0.6997, Elapsed: 10m58s
2020-05-14 14:57:56.375654: Epoch: 1, Batch: 1255, Loss: 0.6750, Elapsed: 4m19s
2020-05-14 15:03:55.527523: Epoch: 1, Batch: 1256, Loss: 0.6856, Elapsed: 5m59s
2020-05-14 15:10:37.522073: Epoch: 1, Batch: 1257, Loss: 0.6857, Elapsed: 6m41s
2020-05-14 15:21:35.876324: Epoch: 1, Batch: 1258, Loss: 0.6978, Elapsed: 10m58s
2020-05-14 15:29:22.924692: Epoch: 1, Batch: 1259, Loss: 0.6883, Elapsed: 7m47s
2020-05-14 15:40:31.145354: Epoch: 1, Batch: 1260, Loss: 0.6826, Elapsed: 11m8s
2020-05-14 15:49:33.179824: Epoch: 1, Batch: 1261, Loss: 0.6783, Elapsed: 9m2s
2020-05-14 15:58:29.758509: Epoch: 1, Batch: 1262, Loss: 0.6759, Elapsed: 8m56s
2020-05-14 16:06:16.721374: Epoch: 1, Batch: 1263, Loss: 0.6762, Elapsed: 7m46s
2020-05-14 16:12:01.148499: Epoch: 1, Batch: 1264, Loss: 0.6952, Elapsed: 5m44s
2020-05-14 16:19:31.442133: Epoch: 1, Batch: 1265, Loss: 0.6807, Elapsed: 7m30s
2020-05-14 16:24:36.176244: Epoch: 1, Batch: 1266, Loss: 0.6741, Elapsed: 5m4s
2020-05-14 16:34:00.172162: Epoch: 1, Batch: 1267, Loss: 0.7019, Elapsed: 9m23s
2020-05-14 16:40:29.854216: Epoch: 1, Batch: 1268, Loss: 0.6952, Elapsed: 6m29s
2020-05-14 16:48:21.660530: Epoch: 1, Batch: 1269, Loss: 0.6860, Elapsed: 7m51s
2020-05-14 16:57:20.931169: Epoch: 1, Batch: 1270, Loss: 0.6726, Elapsed: 8m59s
2020-05-14 17:03:58.615981: Epoch: 1, Batch: 1271, Loss: 0.6725, Elapsed: 6m37s
2020-05-14 17:09:54.199006: Epoch: 1, Batch: 1272, Loss: 0.6680, Elapsed: 5m55s
2020-05-14 17:14:13.317770: Epoch: 1, Batch: 1273, Loss: 0.6797, Elapsed: 4m19s
2020-05-14 17:20:45.752936: Epoch: 1, Batch: 1274, Loss: 0.6786, Elapsed: 6m32s
2020-05-14 17:29:08.097669: Epoch: 1, Batch: 1275, Loss: 0.6603, Elapsed: 8m22s
2020-05-14 17:39:36.271996: Epoch: 1, Batch: 1276, Loss: 0.6840, Elapsed: 10m28s
2020-05-14 17:48:37.994896: Epoch: 1, Batch: 1277, Loss: 0.6890, Elapsed: 9m1s
2020-05-14 17:57:02.030491: Epoch: 1, Batch: 1278, Loss: 0.6944, Elapsed: 8m24s
2020-05-14 18:07:07.738749: Epoch: 1, Batch: 1279, Loss: 0.6833, Elapsed: 10m5s
2020-05-14 18:13:48.663018: Epoch: 1, Batch: 1280, Loss: 0.6904, Elapsed: 6m40s
2020-05-14 18:19:24.475681: Epoch: 1, Batch: 1281, Loss: 0.6789, Elapsed: 5m35s
2020-05-14 18:25:44.342783: Epoch: 1, Batch: 1282, Loss: 0.6922, Elapsed: 6m19s
2020-05-14 18:33:07.885623: Epoch: 1, Batch: 1283, Loss: 0.6733, Elapsed: 7m23s
2020-05-14 18:41:10.282954: Epoch: 1, Batch: 1284, Loss: 0.6804, Elapsed: 8m2s
2020-05-14 18:44:57.928344: Epoch: 1, Batch: 1285, Loss: 0.6799, Elapsed: 3m47s
2020-05-14 18:56:13.593706: Epoch: 1, Batch: 1286, Loss: 0.6905, Elapsed: 11m15s
2020-05-14 18:59:03.270870: Epoch: 1, Batch: 1287, Loss: 0.6885, Elapsed: 2m49s
2020-05-14 19:07:08.130533: Epoch: 1, Batch: 1288, Loss: 0.6928, Elapsed: 8m4s
2020-05-14 19:14:28.554421: Epoch: 1, Batch: 1289, Loss: 0.6850, Elapsed: 7m20s
2020-05-14 19:22:50.330288: Epoch: 1, Batch: 1290, Loss: 0.6875, Elapsed: 8m21s
2020-05-14 19:30:50.104801: Epoch: 1, Batch: 1291, Loss: 0.6877, Elapsed: 7m59s
2020-05-14 19:39:30.573265: Epoch: 1, Batch: 1292, Loss: 0.6681, Elapsed: 8m40s
2020-05-14 19:46:07.012923: Epoch: 1, Batch: 1293, Loss: 0.6858, Elapsed: 6m36s
2020-05-14 19:49:28.941274: Epoch: 1, Batch: 1294, Loss: 0.6748, Elapsed: 3m21s
2020-05-14 19:55:54.102907: Epoch: 1, Batch: 1295, Loss: 0.6833, Elapsed: 6m25s
2020-05-14 20:01:13.990527: Epoch: 1, Batch: 1296, Loss: 0.6675, Elapsed: 5m19s
2020-05-14 20:07:54.118102: Epoch: 1, Batch: 1297, Loss: 0.6876, Elapsed: 6m40s
2020-05-14 20:14:04.345482: Epoch: 1, Batch: 1298, Loss: 0.6891, Elapsed: 6m10s
2020-05-14 20:23:24.464348: Epoch: 1, Batch: 1299, Loss: 0.6738, Elapsed: 9m20s
2020-05-14 20:36:44.815320: Epoch: 1, Batch: 1300, Loss: 0.6913, Elapsed: 13m20s
Starting testing the validation set with 200 subgraphs!
2020-05-14 21:15:39.455917: Validation Test:  Loss: 0.6833,  Acc: 58.5175, AUC: 0.6016, Precision: 0.6282 -- Elapsed: 38m54s
2020-05-14 21:23:14.055479: Epoch: 1, Batch: 1301, Loss: 0.6764, Elapsed: 7m34s
2020-05-14 21:30:26.482566: Epoch: 1, Batch: 1302, Loss: 0.6803, Elapsed: 7m12s
2020-05-14 21:36:02.948910: Epoch: 1, Batch: 1303, Loss: 0.6892, Elapsed: 5m36s
2020-05-14 21:42:15.843436: Epoch: 1, Batch: 1304, Loss: 0.7049, Elapsed: 6m12s
2020-05-14 21:46:30.449372: Epoch: 1, Batch: 1305, Loss: 0.6796, Elapsed: 4m14s
2020-05-14 21:53:44.649433: Epoch: 1, Batch: 1306, Loss: 0.6922, Elapsed: 7m14s
2020-05-14 21:59:28.316402: Epoch: 1, Batch: 1307, Loss: 0.6891, Elapsed: 5m43s
2020-05-14 22:05:43.993241: Epoch: 1, Batch: 1308, Loss: 0.6803, Elapsed: 6m15s
2020-05-14 22:09:20.339612: Epoch: 1, Batch: 1309, Loss: 0.6872, Elapsed: 3m36s
2020-05-14 22:14:29.819570: Epoch: 1, Batch: 1310, Loss: 0.6865, Elapsed: 5m9s
2020-05-14 22:23:14.238824: Epoch: 1, Batch: 1311, Loss: 0.6821, Elapsed: 8m44s
2020-05-14 22:29:05.283392: Epoch: 1, Batch: 1312, Loss: 0.6792, Elapsed: 5m51s
2020-05-14 22:37:04.457801: Epoch: 1, Batch: 1313, Loss: 0.6873, Elapsed: 7m59s
2020-05-14 22:44:53.561923: Epoch: 1, Batch: 1314, Loss: 0.6815, Elapsed: 7m49s
2020-05-14 22:53:59.056505: Epoch: 1, Batch: 1315, Loss: 0.6763, Elapsed: 9m5s
2020-05-14 22:59:24.198657: Epoch: 1, Batch: 1316, Loss: 0.6986, Elapsed: 5m25s
2020-05-14 23:05:52.869042: Epoch: 1, Batch: 1317, Loss: 0.6936, Elapsed: 6m28s
2020-05-14 23:16:57.087500: Epoch: 1, Batch: 1318, Loss: 0.7994, Elapsed: 11m4s
2020-05-14 23:23:27.602970: Epoch: 1, Batch: 1319, Loss: 0.6717, Elapsed: 6m30s
2020-05-14 23:29:37.728862: Epoch: 1, Batch: 1320, Loss: 0.6693, Elapsed: 6m10s
2020-05-14 23:34:53.145777: Epoch: 1, Batch: 1321, Loss: 0.6833, Elapsed: 5m15s
2020-05-14 23:42:06.622668: Epoch: 1, Batch: 1322, Loss: 0.6784, Elapsed: 7m13s
2020-05-14 23:47:32.076774: Epoch: 1, Batch: 1323, Loss: 0.6900, Elapsed: 5m25s
2020-05-14 23:52:16.424626: Epoch: 1, Batch: 1324, Loss: 0.6850, Elapsed: 4m44s
2020-05-14 23:57:52.609813: Epoch: 1, Batch: 1325, Loss: 0.6885, Elapsed: 5m36s
2020-05-15 00:05:27.573117: Epoch: 1, Batch: 1326, Loss: 0.6931, Elapsed: 7m34s
2020-05-15 00:12:40.675811: Epoch: 1, Batch: 1327, Loss: 0.6816, Elapsed: 7m13s
2020-05-15 00:18:23.422204: Epoch: 1, Batch: 1328, Loss: 0.6891, Elapsed: 5m42s
2020-05-15 00:23:36.700559: Epoch: 1, Batch: 1329, Loss: 0.7009, Elapsed: 5m13s
2020-05-15 00:31:55.565198: Epoch: 1, Batch: 1330, Loss: 0.6809, Elapsed: 8m18s
2020-05-15 00:40:29.247173: Epoch: 1, Batch: 1331, Loss: 0.7031, Elapsed: 8m33s
2020-05-15 00:47:18.047057: Epoch: 1, Batch: 1332, Loss: 0.6791, Elapsed: 6m48s
2020-05-15 00:53:44.005026: Epoch: 1, Batch: 1333, Loss: 0.6981, Elapsed: 6m25s
2020-05-15 00:58:29.417393: Epoch: 1, Batch: 1334, Loss: 0.6843, Elapsed: 4m45s
2020-05-15 01:04:48.256289: Epoch: 1, Batch: 1335, Loss: 0.7316, Elapsed: 6m18s
2020-05-15 01:09:04.710103: Epoch: 1, Batch: 1336, Loss: 0.6721, Elapsed: 4m16s
2020-05-15 01:14:18.257202: Epoch: 1, Batch: 1337, Loss: 0.7054, Elapsed: 5m13s
2020-05-15 01:23:16.422831: Epoch: 1, Batch: 1338, Loss: 0.7606, Elapsed: 8m58s
2020-05-15 01:30:30.035907: Epoch: 1, Batch: 1339, Loss: 0.7080, Elapsed: 7m13s
2020-05-15 01:37:08.032155: Epoch: 1, Batch: 1340, Loss: 0.7002, Elapsed: 6m37s
2020-05-15 01:44:35.611282: Epoch: 1, Batch: 1341, Loss: 0.7045, Elapsed: 7m27s
2020-05-15 01:50:44.902478: Epoch: 1, Batch: 1342, Loss: 0.6960, Elapsed: 6m9s
2020-05-15 01:57:12.044473: Epoch: 1, Batch: 1343, Loss: 0.7184, Elapsed: 6m27s
2020-05-15 02:01:28.231522: Epoch: 1, Batch: 1344, Loss: 0.7522, Elapsed: 4m16s
2020-05-15 02:09:24.685834: Epoch: 1, Batch: 1345, Loss: 0.7707, Elapsed: 7m56s
2020-05-15 02:17:31.203430: Epoch: 1, Batch: 1346, Loss: 0.7329, Elapsed: 8m6s
2020-05-15 02:25:00.222777: Epoch: 1, Batch: 1347, Loss: 0.7823, Elapsed: 7m29s
2020-05-15 02:30:28.573941: Epoch: 1, Batch: 1348, Loss: 0.7137, Elapsed: 5m28s
2020-05-15 02:38:06.965122: Epoch: 1, Batch: 1349, Loss: 0.7030, Elapsed: 7m38s
2020-05-15 02:46:54.395366: Epoch: 1, Batch: 1350, Loss: 0.7068, Elapsed: 8m47s
Starting testing the validation set with 200 subgraphs!
2020-05-15 03:25:51.645164: Validation Test:  Loss: 0.7175,  Acc: 50.0350, AUC: 0.4960, Precision: 0.5355 -- Elapsed: 38m57s
2020-05-15 03:32:45.203968: Epoch: 1, Batch: 1351, Loss: 0.7011, Elapsed: 6m53s
2020-05-15 03:42:13.828535: Epoch: 1, Batch: 1352, Loss: 0.7357, Elapsed: 9m28s
2020-05-15 03:47:41.700110: Epoch: 1, Batch: 1353, Loss: 0.7419, Elapsed: 5m27s
2020-05-15 03:58:32.310767: Epoch: 1, Batch: 1354, Loss: 0.6831, Elapsed: 10m50s
2020-05-15 04:05:14.205808: Epoch: 1, Batch: 1355, Loss: 0.7350, Elapsed: 6m41s
2020-05-15 04:11:41.715696: Epoch: 1, Batch: 1356, Loss: 0.7017, Elapsed: 6m27s
2020-05-15 04:17:43.969585: Epoch: 1, Batch: 1357, Loss: 0.7255, Elapsed: 6m2s
2020-05-15 04:24:24.646784: Epoch: 1, Batch: 1358, Loss: 0.7411, Elapsed: 6m40s
2020-05-15 04:35:39.448689: Epoch: 1, Batch: 1359, Loss: 0.6841, Elapsed: 11m14s
2020-05-15 04:44:41.544963: Epoch: 1, Batch: 1360, Loss: 0.7553, Elapsed: 9m2s
2020-05-15 04:52:35.311472: Epoch: 1, Batch: 1361, Loss: 0.7083, Elapsed: 7m53s
2020-05-15 05:01:29.577994: Epoch: 1, Batch: 1362, Loss: 0.7518, Elapsed: 8m54s
2020-05-15 05:12:01.847677: Epoch: 1, Batch: 1363, Loss: 0.7397, Elapsed: 10m32s
2020-05-15 05:21:41.234545: Epoch: 1, Batch: 1364, Loss: 0.7081, Elapsed: 9m39s
2020-05-15 05:30:51.051715: Epoch: 1, Batch: 1365, Loss: 0.7183, Elapsed: 9m9s
2020-05-15 05:38:34.552515: Epoch: 1, Batch: 1366, Loss: 0.7226, Elapsed: 7m43s
2020-05-15 05:49:16.185412: Epoch: 1, Batch: 1367, Loss: 0.6669, Elapsed: 10m41s
2020-05-15 05:53:54.063057: Epoch: 1, Batch: 1368, Loss: 0.7125, Elapsed: 4m37s
2020-05-15 05:59:42.959888: Epoch: 1, Batch: 1369, Loss: 0.7077, Elapsed: 5m48s
2020-05-15 06:07:23.868652: Epoch: 1, Batch: 1370, Loss: 0.6898, Elapsed: 7m40s
2020-05-15 06:17:25.356107: Epoch: 1, Batch: 1371, Loss: 0.7025, Elapsed: 10m1s
2020-05-15 06:24:13.768548: Epoch: 1, Batch: 1372, Loss: 0.6895, Elapsed: 6m48s
2020-05-15 06:31:20.776499: Epoch: 1, Batch: 1373, Loss: 0.7059, Elapsed: 7m7s
2020-05-15 06:37:03.974472: Epoch: 1, Batch: 1374, Loss: 0.7014, Elapsed: 5m43s
2020-05-15 06:43:02.974664: Epoch: 1, Batch: 1375, Loss: 0.6892, Elapsed: 5m58s
2020-05-15 06:52:23.139997: Epoch: 1, Batch: 1376, Loss: 0.6907, Elapsed: 9m20s
2020-05-15 07:01:21.743239: Epoch: 1, Batch: 1377, Loss: 0.7098, Elapsed: 8m58s
2020-05-15 07:09:17.330341: Epoch: 1, Batch: 1378, Loss: 0.7054, Elapsed: 7m55s
2020-05-15 07:22:29.968425: Epoch: 1, Batch: 1379, Loss: 0.6755, Elapsed: 13m12s
2020-05-15 07:30:37.395835: Epoch: 1, Batch: 1380, Loss: 0.6787, Elapsed: 8m7s
2020-05-15 07:35:11.797102: Epoch: 1, Batch: 1381, Loss: 0.7158, Elapsed: 4m34s
2020-05-15 07:43:03.790851: Epoch: 1, Batch: 1382, Loss: 0.6872, Elapsed: 7m51s
2020-05-15 07:46:31.506525: Epoch: 1, Batch: 1383, Loss: 0.7333, Elapsed: 3m27s
2020-05-15 07:57:47.193575: Epoch: 1, Batch: 1384, Loss: 0.6890, Elapsed: 11m15s
2020-05-15 08:03:42.948705: Epoch: 1, Batch: 1385, Loss: 0.6936, Elapsed: 5m55s
2020-05-15 08:13:20.166714: Epoch: 1, Batch: 1386, Loss: 0.7049, Elapsed: 9m37s
2020-05-15 08:16:28.866191: Epoch: 1, Batch: 1387, Loss: 0.7392, Elapsed: 3m8s
2020-05-15 08:21:19.496969: Epoch: 1, Batch: 1388, Loss: 0.7127, Elapsed: 4m50s
2020-05-15 08:28:34.559100: Epoch: 1, Batch: 1389, Loss: 0.7094, Elapsed: 7m15s
2020-05-15 08:39:15.245350: Epoch: 1, Batch: 1390, Loss: 0.6730, Elapsed: 10m40s
2020-05-15 08:45:00.553535: Epoch: 1, Batch: 1391, Loss: 0.7163, Elapsed: 5m45s
2020-05-15 08:55:09.040841: Epoch: 1, Batch: 1392, Loss: 0.6896, Elapsed: 10m8s
2020-05-15 09:03:56.592426: Epoch: 1, Batch: 1393, Loss: 0.6910, Elapsed: 8m47s
2020-05-15 09:10:38.871389: Epoch: 1, Batch: 1394, Loss: 0.7097, Elapsed: 6m42s
2020-05-15 09:15:42.300841: Epoch: 1, Batch: 1395, Loss: 0.6918, Elapsed: 5m3s
2020-05-15 09:23:50.601302: Epoch: 1, Batch: 1396, Loss: 0.6960, Elapsed: 8m8s
2020-05-15 09:30:41.780115: Epoch: 1, Batch: 1397, Loss: 0.7073, Elapsed: 6m51s
2020-05-15 09:36:15.452792: Epoch: 1, Batch: 1398, Loss: 0.7115, Elapsed: 5m33s
2020-05-15 09:41:04.294723: Epoch: 1, Batch: 1399, Loss: 0.7156, Elapsed: 4m48s
2020-05-15 09:46:57.568383: Epoch: 1, Batch: 1400, Loss: 0.7033, Elapsed: 5m53s
Starting testing the validation set with 200 subgraphs!
2020-05-15 10:25:59.231667: Validation Test:  Loss: 0.6969,  Acc: 56.2483, AUC: 0.5781, Precision: 0.6190 -- Elapsed: 39m1s
Starting testing the validation set with 200 subgraphs!
2020-05-15 11:04:54.827122: Validation Test:  Loss: 0.6969,  Acc: 56.2483, AUC: 0.5781, Precision: 0.6190 -- Elapsed: 38m55s
2020-05-15 11:04:54.827231: Training completed!
Singularity> [KSingularity> exit
exit
kctuysuz@culture-plate-sm:~/HepTrkX-quantum\[ctuysuz@culture-plate-sm HepTrkX-quantum]$ exit
exit
[KSingularity> exit
exit
kctuysuz@culture-plate-sm:~/HepTrkX-quantum\[ctuysuz@culture-plate-sm HepTrkX-quantum]$ exit
exit
[KSingularity> clear
[H[JSingularity> python  3 train.py configs/test.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/test/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/test/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
2020-05-17 23:04:05.687040 Deleted old log: logs/test/log_params_NN.csv
2020-05-17 23:04:05.687326 Deleted old log: logs/test/summary.csv
2020-05-17 23:04:05.687594 Deleted old log: logs/test/log_params_IN.csv
2020-05-17 23:04:05.687844 Deleted old log: logs/test/log_grads_EN.csv
2020-05-17 23:04:05.688096 Deleted old log: logs/test/log_grads_NN.csv
2020-05-17 23:04:05.688337 Deleted old log: logs/test/log_params_EN.csv
2020-05-17 23:04:05.688592 Deleted old log: logs/test/log_grads_IN.csv
2020-05-17 23:04:05.688823 Deleted old log: logs/test/log_validation.csv
2020-05-17 23:04:05.689052 Deleted old log: logs/test/log_loss.csv
Starting testing the validation set with 200 subgraphs!
2020-05-17 23:35:33.570831: Validation Test:  Loss: 0.7569,  Acc: 52.9261, AUC: 0.5480, Precision: 0.5872 -- Elapsed: 31m25s
2020-05-17 23:35:33.570930: Training is starting!
2020-05-17 23:38:55.421096: Epoch: 1, Batch: 1, Loss: 0.7493, Elapsed: 3m21s
2020-05-17 23:43:43.278718: Epoch: 1, Batch: 2, Loss: 0.7749, Elapsed: 4m47s
2020-05-17 23:46:37.720256: Epoch: 1, Batch: 3, Loss: 0.7476, Elapsed: 2m54s
2020-05-17 23:51:59.245595: Epoch: 1, Batch: 4, Loss: 0.7511, Elapsed: 5m21s
2020-05-17 23:54:45.385246: Epoch: 1, Batch: 5, Loss: 0.7698, Elapsed: 2m46s
2020-05-17 23:59:15.660822: Epoch: 1, Batch: 6, Loss: 0.7318, Elapsed: 4m30s
2020-05-18 00:04:25.805453: Epoch: 1, Batch: 7, Loss: 0.7282, Elapsed: 5m10s
2020-05-18 00:06:52.330841: Epoch: 1, Batch: 8, Loss: 0.7635, Elapsed: 2m26s
2020-05-18 00:12:15.570235: Epoch: 1, Batch: 9, Loss: 0.7134, Elapsed: 5m23s
2020-05-18 00:16:21.807073: Epoch: 1, Batch: 10, Loss: 0.7529, Elapsed: 4m6s
2020-05-18 00:19:40.088767: Epoch: 1, Batch: 11, Loss: 0.7396, Elapsed: 3m18s
2020-05-18 00:23:10.814240: Epoch: 1, Batch: 12, Loss: 0.7047, Elapsed: 3m30s
2020-05-18 00:26:37.176000: Epoch: 1, Batch: 13, Loss: 0.7242, Elapsed: 3m26s
2020-05-18 00:31:27.095728: Epoch: 1, Batch: 14, Loss: 0.7241, Elapsed: 4m49s
2020-05-18 00:36:17.698666: Epoch: 1, Batch: 15, Loss: 0.7025, Elapsed: 4m50s
2020-05-18 00:41:26.850721: Epoch: 1, Batch: 16, Loss: 0.7037, Elapsed: 5m9s
2020-05-18 00:47:11.834640: Epoch: 1, Batch: 17, Loss: 0.7220, Elapsed: 5m44s
2020-05-18 00:52:12.786335: Epoch: 1, Batch: 18, Loss: 0.7219, Elapsed: 5m0s
2020-05-18 00:57:14.474852: Epoch: 1, Batch: 19, Loss: 0.6952, Elapsed: 5m1s
2020-05-18 01:02:26.225908: Epoch: 1, Batch: 20, Loss: 0.7209, Elapsed: 5m11s
2020-05-18 01:08:28.161148: Epoch: 1, Batch: 21, Loss: 0.7081, Elapsed: 6m1s
2020-05-18 01:13:01.429017: Epoch: 1, Batch: 22, Loss: 0.6962, Elapsed: 4m33s
2020-05-18 01:16:45.057074: Epoch: 1, Batch: 23, Loss: 0.7057, Elapsed: 3m43s
2020-05-18 01:21:36.917332: Epoch: 1, Batch: 24, Loss: 0.7110, Elapsed: 4m51s
2020-05-18 01:25:13.586688: Epoch: 1, Batch: 25, Loss: 0.6972, Elapsed: 3m36s
2020-05-18 01:30:30.996351: Epoch: 1, Batch: 26, Loss: 0.7049, Elapsed: 5m17s
2020-05-18 01:36:19.085840: Epoch: 1, Batch: 27, Loss: 0.6844, Elapsed: 5m48s
2020-05-18 01:41:57.023768: Epoch: 1, Batch: 28, Loss: 0.6961, Elapsed: 5m37s
2020-05-18 01:45:27.622702: Epoch: 1, Batch: 29, Loss: 0.7158, Elapsed: 3m30s
2020-05-18 01:51:02.610009: Epoch: 1, Batch: 30, Loss: 0.6911, Elapsed: 5m34s
2020-05-18 01:56:24.917497: Epoch: 1, Batch: 31, Loss: 0.6876, Elapsed: 5m22s
2020-05-18 01:59:03.223338: Epoch: 1, Batch: 32, Loss: 0.7060, Elapsed: 2m38s
2020-05-18 02:02:08.475285: Epoch: 1, Batch: 33, Loss: 0.7021, Elapsed: 3m5s
2020-05-18 02:08:13.979026: Epoch: 1, Batch: 34, Loss: 0.6984, Elapsed: 6m5s
2020-05-18 02:11:58.725145: Epoch: 1, Batch: 35, Loss: 0.7034, Elapsed: 3m44s
2020-05-18 02:15:36.667443: Epoch: 1, Batch: 36, Loss: 0.7022, Elapsed: 3m37s
2020-05-18 02:19:54.281053: Epoch: 1, Batch: 37, Loss: 0.6944, Elapsed: 4m17s
2020-05-18 02:22:46.571233: Epoch: 1, Batch: 38, Loss: 0.7146, Elapsed: 2m52s
2020-05-18 02:28:24.582779: Epoch: 1, Batch: 39, Loss: 0.6865, Elapsed: 5m37s
2020-05-18 02:32:20.952975: Epoch: 1, Batch: 40, Loss: 0.7002, Elapsed: 3m56s
2020-05-18 02:35:10.242664: Epoch: 1, Batch: 41, Loss: 0.6894, Elapsed: 2m49s
2020-05-18 02:40:05.943768: Epoch: 1, Batch: 42, Loss: 0.6811, Elapsed: 4m55s
2020-05-18 02:45:09.952128: Epoch: 1, Batch: 43, Loss: 0.6861, Elapsed: 5m3s
2020-05-18 02:50:03.490347: Epoch: 1, Batch: 44, Loss: 0.6969, Elapsed: 4m53s
2020-05-18 02:53:06.923588: Epoch: 1, Batch: 45, Loss: 0.6879, Elapsed: 3m3s
2020-05-18 02:59:33.117577: Epoch: 1, Batch: 46, Loss: 0.6786, Elapsed: 6m26s
2020-05-18 03:02:52.988615: Epoch: 1, Batch: 47, Loss: 0.6912, Elapsed: 3m19s
2020-05-18 03:08:20.913236: Epoch: 1, Batch: 48, Loss: 0.6909, Elapsed: 5m27s
2020-05-18 03:11:48.398726: Epoch: 1, Batch: 49, Loss: 0.6961, Elapsed: 3m27s
2020-05-18 03:17:41.594911: Epoch: 1, Batch: 50, Loss: 0.6822, Elapsed: 5m53s
Starting testing the validation set with 200 subgraphs!
2020-05-18 03:49:08.019040: Validation Test:  Loss: 0.6922,  Acc: 56.4755, AUC: 0.5783, Precision: 0.6138 -- Elapsed: 31m26s
2020-05-18 03:54:06.481118: Epoch: 1, Batch: 51, Loss: 0.7091, Elapsed: 4m58s
2020-05-18 03:59:20.042378: Epoch: 1, Batch: 52, Loss: 0.6880, Elapsed: 5m13s
2020-05-18 04:02:39.102692: Epoch: 1, Batch: 53, Loss: 0.7054, Elapsed: 3m19s
2020-05-18 04:07:11.144533: Epoch: 1, Batch: 54, Loss: 0.6690, Elapsed: 4m32s
2020-05-18 04:10:13.587406: Epoch: 1, Batch: 55, Loss: 0.6912, Elapsed: 3m2s
2020-05-18 04:14:05.752793: Epoch: 1, Batch: 56, Loss: 0.6984, Elapsed: 3m52s
2020-05-18 04:16:27.101743: Epoch: 1, Batch: 57, Loss: 0.6964, Elapsed: 2m21s
2020-05-18 04:20:57.684655: Epoch: 1, Batch: 58, Loss: 0.6856, Elapsed: 4m30s
2020-05-18 04:24:41.228811: Epoch: 1, Batch: 59, Loss: 0.6963, Elapsed: 3m43s
2020-05-18 04:27:55.003452: Epoch: 1, Batch: 60, Loss: 0.7015, Elapsed: 3m13s
2020-05-18 04:31:36.237074: Epoch: 1, Batch: 61, Loss: 0.6865, Elapsed: 3m41s
2020-05-18 04:35:54.772006: Epoch: 1, Batch: 62, Loss: 0.6709, Elapsed: 4m18s
2020-05-18 04:42:25.415015: Epoch: 1, Batch: 63, Loss: 0.6919, Elapsed: 6m30s
2020-05-18 04:48:34.475495: Epoch: 1, Batch: 64, Loss: 0.6699, Elapsed: 6m9s
2020-05-18 04:51:43.275833: Epoch: 1, Batch: 65, Loss: 0.7007, Elapsed: 3m8s
2020-05-18 04:54:43.327344: Epoch: 1, Batch: 66, Loss: 0.6835, Elapsed: 3m0s
2020-05-18 04:58:59.911333: Epoch: 1, Batch: 67, Loss: 0.6907, Elapsed: 4m16s
2020-05-18 05:01:33.290526: Epoch: 1, Batch: 68, Loss: 0.6812, Elapsed: 2m33s
2020-05-18 05:05:27.684025: Epoch: 1, Batch: 69, Loss: 0.6757, Elapsed: 3m54s
2020-05-18 05:08:09.120440: Epoch: 1, Batch: 70, Loss: 0.6901, Elapsed: 2m41s
2020-05-18 05:13:14.495273: Epoch: 1, Batch: 71, Loss: 0.6614, Elapsed: 5m5s
2020-05-18 05:19:42.869588: Epoch: 1, Batch: 72, Loss: 0.6850, Elapsed: 6m28s
2020-05-18 05:25:10.239801: Epoch: 1, Batch: 73, Loss: 0.6929, Elapsed: 5m27s
2020-05-18 05:27:46.798638: Epoch: 1, Batch: 74, Loss: 0.6899, Elapsed: 2m36s
2020-05-18 05:33:10.996458: Epoch: 1, Batch: 75, Loss: 0.6762, Elapsed: 5m24s
2020-05-18 05:35:33.411731: Epoch: 1, Batch: 76, Loss: 0.7094, Elapsed: 2m22s
2020-05-18 05:40:07.799098: Epoch: 1, Batch: 77, Loss: 0.6917, Elapsed: 4m34s
2020-05-18 05:43:46.204861: Epoch: 1, Batch: 78, Loss: 0.6912, Elapsed: 3m38s
2020-05-18 05:47:25.886318: Epoch: 1, Batch: 79, Loss: 0.6866, Elapsed: 3m39s
2020-05-18 05:52:45.177421: Epoch: 1, Batch: 80, Loss: 0.6724, Elapsed: 5m19s
2020-05-18 05:56:49.030667: Epoch: 1, Batch: 81, Loss: 0.6949, Elapsed: 4m3s
2020-05-18 06:00:53.527303: Epoch: 1, Batch: 82, Loss: 0.6772, Elapsed: 4m4s
2020-05-18 06:04:03.554910: Epoch: 1, Batch: 83, Loss: 0.6921, Elapsed: 3m10s
2020-05-18 06:06:56.602716: Epoch: 1, Batch: 84, Loss: 0.6916, Elapsed: 2m53s
2020-05-18 06:11:06.617119: Epoch: 1, Batch: 85, Loss: 0.6870, Elapsed: 4m9s
2020-05-18 06:15:49.701614: Epoch: 1, Batch: 86, Loss: 0.6685, Elapsed: 4m43s
2020-05-18 06:20:51.131427: Epoch: 1, Batch: 87, Loss: 0.7015, Elapsed: 5m1s
2020-05-18 06:25:59.711781: Epoch: 1, Batch: 88, Loss: 0.7021, Elapsed: 5m8s
2020-05-18 06:29:22.883055: Epoch: 1, Batch: 89, Loss: 0.6936, Elapsed: 3m23s
2020-05-18 06:32:35.096852: Epoch: 1, Batch: 90, Loss: 0.6845, Elapsed: 3m12s
2020-05-18 06:39:19.248464: Epoch: 1, Batch: 91, Loss: 0.6851, Elapsed: 6m44s
2020-05-18 06:41:34.630059: Epoch: 1, Batch: 92, Loss: 0.6998, Elapsed: 2m15s
2020-05-18 06:44:26.128062: Epoch: 1, Batch: 93, Loss: 0.6933, Elapsed: 2m51s
2020-05-18 06:49:35.270104: Epoch: 1, Batch: 94, Loss: 0.6694, Elapsed: 5m9s
2020-05-18 06:51:43.299475: Epoch: 1, Batch: 95, Loss: 0.7081, Elapsed: 2m8s
2020-05-18 06:55:04.568409: Epoch: 1, Batch: 96, Loss: 0.7020, Elapsed: 3m21s
2020-05-18 07:01:14.378171: Epoch: 1, Batch: 97, Loss: 0.6673, Elapsed: 6m9s
2020-05-18 07:05:29.332466: Epoch: 1, Batch: 98, Loss: 0.6929, Elapsed: 4m14s
2020-05-18 07:09:07.761755: Epoch: 1, Batch: 99, Loss: 0.6998, Elapsed: 3m38s
2020-05-18 07:11:55.934368: Epoch: 1, Batch: 100, Loss: 0.6764, Elapsed: 2m48s
Starting testing the validation set with 200 subgraphs!
2020-05-18 07:43:16.559643: Validation Test:  Loss: 0.6860,  Acc: 58.9436, AUC: 0.6009, Precision: 0.6303 -- Elapsed: 31m20s
2020-05-18 07:46:13.152628: Epoch: 1, Batch: 101, Loss: 0.6874, Elapsed: 2m56s
2020-05-18 07:52:29.627892: Epoch: 1, Batch: 102, Loss: 0.6678, Elapsed: 6m16s
2020-05-18 07:58:20.734597: Epoch: 1, Batch: 103, Loss: 0.6654, Elapsed: 5m51s
2020-05-18 08:01:30.719411: Epoch: 1, Batch: 104, Loss: 0.6901, Elapsed: 3m9s
2020-05-18 08:05:02.950072: Epoch: 1, Batch: 105, Loss: 0.6930, Elapsed: 3m32s
2020-05-18 08:09:10.628698: Epoch: 1, Batch: 106, Loss: 0.6748, Elapsed: 4m7s
2020-05-18 08:13:54.378813: Epoch: 1, Batch: 107, Loss: 0.7129, Elapsed: 4m43s
2020-05-18 08:23:14.579806: Epoch: 1, Batch: 108, Loss: 0.6792, Elapsed: 9m20s
2020-05-18 08:27:02.021253: Epoch: 1, Batch: 109, Loss: 0.6825, Elapsed: 3m47s
2020-05-18 08:32:03.069599: Epoch: 1, Batch: 110, Loss: 0.7023, Elapsed: 5m1s
2020-05-18 08:34:39.342313: Epoch: 1, Batch: 111, Loss: 0.6907, Elapsed: 2m36s
2020-05-18 08:37:11.391947: Epoch: 1, Batch: 112, Loss: 0.6759, Elapsed: 2m32s
2020-05-18 08:39:28.069661: Epoch: 1, Batch: 113, Loss: 0.6996, Elapsed: 2m16s
2020-05-18 08:43:31.156701: Epoch: 1, Batch: 114, Loss: 0.6866, Elapsed: 4m3s
2020-05-18 08:47:27.793216: Epoch: 1, Batch: 115, Loss: 0.6814, Elapsed: 3m56s
2020-05-18 08:53:32.024778: Epoch: 1, Batch: 116, Loss: 0.6929, Elapsed: 6m4s
2020-05-18 08:56:31.443156: Epoch: 1, Batch: 117, Loss: 0.6791, Elapsed: 2m59s
2020-05-18 09:00:01.692424: Epoch: 1, Batch: 118, Loss: 0.6790, Elapsed: 3m30s
2020-05-18 09:01:51.288829: Epoch: 1, Batch: 119, Loss: 0.6842, Elapsed: 1m49s
2020-05-18 09:06:27.276819: Epoch: 1, Batch: 120, Loss: 0.6846, Elapsed: 4m35s
2020-05-18 09:10:10.245002: Epoch: 1, Batch: 121, Loss: 0.6824, Elapsed: 3m42s
2020-05-18 09:14:31.591959: Epoch: 1, Batch: 122, Loss: 0.6947, Elapsed: 4m21s
2020-05-18 09:22:03.211442: Epoch: 1, Batch: 123, Loss: 0.6804, Elapsed: 7m31s
2020-05-18 09:24:39.639308: Epoch: 1, Batch: 124, Loss: 0.6755, Elapsed: 2m36s
2020-05-18 09:27:24.002827: Epoch: 1, Batch: 125, Loss: 0.6866, Elapsed: 2m44s
2020-05-18 09:30:41.184352: Epoch: 1, Batch: 126, Loss: 0.6757, Elapsed: 3m17s
2020-05-18 09:35:45.539285: Epoch: 1, Batch: 127, Loss: 0.6837, Elapsed: 5m4s
2020-05-18 09:41:20.202955: Epoch: 1, Batch: 128, Loss: 0.6757, Elapsed: 5m34s
2020-05-18 09:44:46.039788: Epoch: 1, Batch: 129, Loss: 0.6862, Elapsed: 3m25s
2020-05-18 09:48:51.682354: Epoch: 1, Batch: 130, Loss: 0.6910, Elapsed: 4m5s
2020-05-18 09:52:50.870841: Epoch: 1, Batch: 131, Loss: 0.6770, Elapsed: 3m59s
2020-05-18 09:56:54.172877: Epoch: 1, Batch: 132, Loss: 0.6770, Elapsed: 4m3s
2020-05-18 10:02:10.803872: Epoch: 1, Batch: 133, Loss: 0.6565, Elapsed: 5m16s
2020-05-18 10:05:33.979291: Epoch: 1, Batch: 134, Loss: 0.6659, Elapsed: 3m23s
2020-05-18 10:10:01.313858: Epoch: 1, Batch: 135, Loss: 0.6727, Elapsed: 4m27s
2020-05-18 10:13:36.417946: Epoch: 1, Batch: 136, Loss: 0.6831, Elapsed: 3m35s
2020-05-18 10:18:11.522410: Epoch: 1, Batch: 137, Loss: 0.6916, Elapsed: 4m35s
2020-05-18 10:22:43.408964: Epoch: 1, Batch: 138, Loss: 0.6860, Elapsed: 4m31s
2020-05-18 10:27:18.870471: Epoch: 1, Batch: 139, Loss: 0.6674, Elapsed: 4m35s
2020-05-18 10:29:59.749316: Epoch: 1, Batch: 140, Loss: 0.6805, Elapsed: 2m40s
2020-05-18 10:34:21.022662: Epoch: 1, Batch: 141, Loss: 0.6895, Elapsed: 4m21s
2020-05-18 10:37:31.412961: Epoch: 1, Batch: 142, Loss: 0.6756, Elapsed: 3m10s
2020-05-18 10:40:38.852074: Epoch: 1, Batch: 143, Loss: 0.6731, Elapsed: 3m7s
2020-05-18 10:45:33.624123: Epoch: 1, Batch: 144, Loss: 0.6739, Elapsed: 4m54s
2020-05-18 10:49:39.269907: Epoch: 1, Batch: 145, Loss: 0.6627, Elapsed: 4m5s
2020-05-18 10:54:49.098961: Epoch: 1, Batch: 146, Loss: 0.6810, Elapsed: 5m9s
2020-05-18 11:01:03.829891: Epoch: 1, Batch: 147, Loss: 0.6740, Elapsed: 6m14s
2020-05-18 11:04:15.221699: Epoch: 1, Batch: 148, Loss: 0.6708, Elapsed: 3m11s
2020-05-18 11:08:04.072604: Epoch: 1, Batch: 149, Loss: 0.6672, Elapsed: 3m48s
2020-05-18 11:12:42.922143: Epoch: 1, Batch: 150, Loss: 0.6678, Elapsed: 4m38s
Starting testing the validation set with 200 subgraphs!
2020-05-18 11:43:52.784333: Validation Test:  Loss: 0.6740,  Acc: 58.7498, AUC: 0.6083, Precision: 0.6406 -- Elapsed: 31m9s
2020-05-18 11:46:42.206778: Epoch: 1, Batch: 151, Loss: 0.6602, Elapsed: 2m49s
2020-05-18 11:50:11.409667: Epoch: 1, Batch: 152, Loss: 0.6712, Elapsed: 3m29s
2020-05-18 11:53:33.782933: Epoch: 1, Batch: 153, Loss: 0.6759, Elapsed: 3m22s
2020-05-18 11:57:49.719331: Epoch: 1, Batch: 154, Loss: 0.6672, Elapsed: 4m15s
2020-05-18 12:02:58.131816: Epoch: 1, Batch: 155, Loss: 0.6743, Elapsed: 5m8s
2020-05-18 12:05:18.058904: Epoch: 1, Batch: 156, Loss: 0.6701, Elapsed: 2m19s
2020-05-18 12:08:03.942073: Epoch: 1, Batch: 157, Loss: 0.6510, Elapsed: 2m45s
2020-05-18 12:10:43.933091: Epoch: 1, Batch: 158, Loss: 0.6731, Elapsed: 2m39s
2020-05-18 12:14:24.785121: Epoch: 1, Batch: 159, Loss: 0.6575, Elapsed: 3m40s
2020-05-18 12:17:59.285087: Epoch: 1, Batch: 160, Loss: 0.6790, Elapsed: 3m34s
2020-05-18 12:22:10.806731: Epoch: 1, Batch: 161, Loss: 0.6639, Elapsed: 4m11s
2020-05-18 12:26:50.468304: Epoch: 1, Batch: 162, Loss: 0.6928, Elapsed: 4m39s
2020-05-18 12:30:45.463434: Epoch: 1, Batch: 163, Loss: 0.6588, Elapsed: 3m54s
2020-05-18 12:35:11.435221: Epoch: 1, Batch: 164, Loss: 0.6929, Elapsed: 4m25s
2020-05-18 12:40:21.818978: Epoch: 1, Batch: 165, Loss: 0.6856, Elapsed: 5m10s
2020-05-18 12:44:17.904118: Epoch: 1, Batch: 166, Loss: 0.6720, Elapsed: 3m56s
2020-05-18 12:48:27.105118: Epoch: 1, Batch: 167, Loss: 0.6673, Elapsed: 4m9s
2020-05-18 12:51:08.360341: Epoch: 1, Batch: 168, Loss: 0.6688, Elapsed: 2m41s
2020-05-18 12:55:25.393186: Epoch: 1, Batch: 169, Loss: 0.7026, Elapsed: 4m17s
2020-05-18 12:59:42.140311: Epoch: 1, Batch: 170, Loss: 0.6653, Elapsed: 4m16s
2020-05-18 13:04:50.276065: Epoch: 1, Batch: 171, Loss: 0.6721, Elapsed: 5m8s
2020-05-18 13:08:04.370907: Epoch: 1, Batch: 172, Loss: 0.6531, Elapsed: 3m14s
2020-05-18 13:13:37.002961: Epoch: 1, Batch: 173, Loss: 0.7114, Elapsed: 5m32s
2020-05-18 13:19:02.517087: Epoch: 1, Batch: 174, Loss: 0.6707, Elapsed: 5m25s
2020-05-18 13:22:00.072816: Epoch: 1, Batch: 175, Loss: 0.6538, Elapsed: 2m57s
2020-05-18 13:24:22.923738: Epoch: 1, Batch: 176, Loss: 0.6396, Elapsed: 2m22s
2020-05-18 13:32:07.734713: Epoch: 1, Batch: 177, Loss: 0.6674, Elapsed: 7m44s
2020-05-18 13:34:34.814633: Epoch: 1, Batch: 178, Loss: 0.6598, Elapsed: 2m27s
2020-05-18 13:39:38.485959: Epoch: 1, Batch: 179, Loss: 0.6747, Elapsed: 5m3s
2020-05-18 13:42:55.641420: Epoch: 1, Batch: 180, Loss: 0.6857, Elapsed: 3m17s
2020-05-18 13:47:22.700309: Epoch: 1, Batch: 181, Loss: 0.6485, Elapsed: 4m27s
2020-05-18 13:54:04.862851: Epoch: 1, Batch: 182, Loss: 0.6916, Elapsed: 6m42s
2020-05-18 13:58:23.140491: Epoch: 1, Batch: 183, Loss: 0.6836, Elapsed: 4m18s
2020-05-18 14:00:59.894318: Epoch: 1, Batch: 184, Loss: 0.6484, Elapsed: 2m36s
2020-05-18 14:06:04.719486: Epoch: 1, Batch: 185, Loss: 0.6665, Elapsed: 5m4s
2020-05-18 14:10:17.642032: Epoch: 1, Batch: 186, Loss: 0.6649, Elapsed: 4m12s
2020-05-18 14:18:25.691142: Epoch: 1, Batch: 187, Loss: 0.7091, Elapsed: 8m8s
2020-05-18 14:22:48.095734: Epoch: 1, Batch: 188, Loss: 0.6684, Elapsed: 4m22s
2020-05-18 14:29:20.517993: Epoch: 1, Batch: 189, Loss: 0.7308, Elapsed: 6m32s
2020-05-18 14:34:13.754075: Epoch: 1, Batch: 190, Loss: 0.6738, Elapsed: 4m53s
2020-05-18 14:38:37.963756: Epoch: 1, Batch: 191, Loss: 0.6932, Elapsed: 4m24s
2020-05-18 14:43:30.809526: Epoch: 1, Batch: 192, Loss: 0.6817, Elapsed: 4m52s
2020-05-18 14:46:43.394139: Epoch: 1, Batch: 193, Loss: 0.6711, Elapsed: 3m12s
2020-05-18 14:49:58.504250: Epoch: 1, Batch: 194, Loss: 0.6413, Elapsed: 3m15s
2020-05-18 14:53:22.206383: Epoch: 1, Batch: 195, Loss: 0.6743, Elapsed: 3m23s
2020-05-18 14:58:54.724081: Epoch: 1, Batch: 196, Loss: 0.6891, Elapsed: 5m32s
2020-05-18 15:02:40.725214: Epoch: 1, Batch: 197, Loss: 0.6494, Elapsed: 3m45s
2020-05-18 15:06:26.352527: Epoch: 1, Batch: 198, Loss: 0.6633, Elapsed: 3m45s
2020-05-18 15:09:40.487760: Epoch: 1, Batch: 199, Loss: 0.6781, Elapsed: 3m14s
2020-05-18 15:14:20.891403: Epoch: 1, Batch: 200, Loss: 0.6842, Elapsed: 4m40s
Starting testing the validation set with 200 subgraphs!
2020-05-18 15:45:45.111694: Validation Test:  Loss: 0.6706,  Acc: 58.3988, AUC: 0.6128, Precision: 0.6416 -- Elapsed: 31m24s
2020-05-18 15:48:08.392283: Epoch: 1, Batch: 201, Loss: 0.6639, Elapsed: 2m23s
2020-05-18 15:55:36.443788: Epoch: 1, Batch: 202, Loss: 0.6810, Elapsed: 7m28s
2020-05-18 16:00:40.203903: Epoch: 1, Batch: 203, Loss: 0.6748, Elapsed: 5m3s
2020-05-18 16:03:58.068599: Epoch: 1, Batch: 204, Loss: 0.6593, Elapsed: 3m17s
2020-05-18 16:07:52.362019: Epoch: 1, Batch: 205, Loss: 0.6613, Elapsed: 3m54s
2020-05-18 16:13:55.119103: Epoch: 1, Batch: 206, Loss: 0.6744, Elapsed: 6m2s
2020-05-18 16:20:48.592152: Epoch: 1, Batch: 207, Loss: 0.7118, Elapsed: 6m53s
2020-05-18 16:24:08.427628: Epoch: 1, Batch: 208, Loss: 0.6663, Elapsed: 3m19s
2020-05-18 16:29:16.405926: Epoch: 1, Batch: 209, Loss: 0.6841, Elapsed: 5m7s
2020-05-18 16:33:45.871457: Epoch: 1, Batch: 210, Loss: 0.6837, Elapsed: 4m29s
2020-05-18 16:39:34.932285: Epoch: 1, Batch: 211, Loss: 0.6671, Elapsed: 5m49s
2020-05-18 16:42:34.885953: Epoch: 1, Batch: 212, Loss: 0.6412, Elapsed: 2m59s
2020-05-18 16:45:59.322147: Epoch: 1, Batch: 213, Loss: 0.6733, Elapsed: 3m24s
2020-05-18 16:49:54.434343: Epoch: 1, Batch: 214, Loss: 0.6653, Elapsed: 3m55s
2020-05-18 16:53:46.094067: Epoch: 1, Batch: 215, Loss: 0.6637, Elapsed: 3m51s
2020-05-18 16:56:53.411319: Epoch: 1, Batch: 216, Loss: 0.6505, Elapsed: 3m7s
2020-05-18 17:00:34.201392: Epoch: 1, Batch: 217, Loss: 0.6699, Elapsed: 3m40s
2020-05-18 17:04:28.916478: Epoch: 1, Batch: 218, Loss: 0.6570, Elapsed: 3m54s
2020-05-18 17:09:13.526400: Epoch: 1, Batch: 219, Loss: 0.6655, Elapsed: 4m44s
2020-05-18 17:15:46.150106: Epoch: 1, Batch: 220, Loss: 0.7515, Elapsed: 6m32s
2020-05-18 17:19:03.473566: Epoch: 1, Batch: 221, Loss: 0.6414, Elapsed: 3m17s
2020-05-18 17:22:49.714936: Epoch: 1, Batch: 222, Loss: 0.6783, Elapsed: 3m46s
2020-05-18 17:27:30.163515: Epoch: 1, Batch: 223, Loss: 0.6694, Elapsed: 4m40s
2020-05-18 17:33:17.264899: Epoch: 1, Batch: 224, Loss: 0.6744, Elapsed: 5m47s
2020-05-18 17:37:09.684862: Epoch: 1, Batch: 225, Loss: 0.6782, Elapsed: 3m52s
2020-05-18 17:41:14.222267: Epoch: 1, Batch: 226, Loss: 0.6641, Elapsed: 4m4s
2020-05-18 17:45:50.118314: Epoch: 1, Batch: 227, Loss: 0.6905, Elapsed: 4m35s
2020-05-18 17:49:38.714428: Epoch: 1, Batch: 228, Loss: 0.6659, Elapsed: 3m48s
2020-05-18 17:52:44.747284: Epoch: 1, Batch: 229, Loss: 0.6625, Elapsed: 3m6s
2020-05-18 17:56:49.184895: Epoch: 1, Batch: 230, Loss: 0.6615, Elapsed: 4m4s
2020-05-18 18:00:16.823397: Epoch: 1, Batch: 231, Loss: 0.6615, Elapsed: 3m27s
2020-05-18 18:03:06.355281: Epoch: 1, Batch: 232, Loss: 0.6629, Elapsed: 2m49s
2020-05-18 18:06:28.563972: Epoch: 1, Batch: 233, Loss: 0.6591, Elapsed: 3m22s
2020-05-18 18:13:03.014209: Epoch: 1, Batch: 234, Loss: 0.6835, Elapsed: 6m34s
2020-05-18 18:18:33.346679: Epoch: 1, Batch: 235, Loss: 0.6904, Elapsed: 5m30s
2020-05-18 18:23:45.798737: Epoch: 1, Batch: 236, Loss: 0.6994, Elapsed: 5m12s
2020-05-18 18:28:59.772241: Epoch: 1, Batch: 237, Loss: 0.6714, Elapsed: 5m13s
2020-05-18 18:34:07.893399: Epoch: 1, Batch: 238, Loss: 0.6857, Elapsed: 5m8s
2020-05-18 18:38:01.962926: Epoch: 1, Batch: 239, Loss: 0.6648, Elapsed: 3m54s
2020-05-18 18:41:35.469677: Epoch: 1, Batch: 240, Loss: 0.6632, Elapsed: 3m33s
2020-05-18 18:45:41.145208: Epoch: 1, Batch: 241, Loss: 0.6671, Elapsed: 4m5s
2020-05-18 18:49:11.782370: Epoch: 1, Batch: 242, Loss: 0.6785, Elapsed: 3m30s
2020-05-18 18:58:43.106453: Epoch: 1, Batch: 243, Loss: 0.7063, Elapsed: 9m31s
2020-05-18 19:03:47.871439: Epoch: 1, Batch: 244, Loss: 0.6675, Elapsed: 5m4s
2020-05-18 19:06:08.902083: Epoch: 1, Batch: 245, Loss: 0.6398, Elapsed: 2m21s
2020-05-18 19:09:29.005566: Epoch: 1, Batch: 246, Loss: 0.6646, Elapsed: 3m20s
2020-05-18 19:13:06.733992: Epoch: 1, Batch: 247, Loss: 0.6723, Elapsed: 3m37s
2020-05-18 19:17:40.202137: Epoch: 1, Batch: 248, Loss: 0.6577, Elapsed: 4m33s
2020-05-18 19:22:16.036395: Epoch: 1, Batch: 249, Loss: 0.6691, Elapsed: 4m35s
2020-05-18 19:23:52.057995: Epoch: 1, Batch: 250, Loss: 0.6629, Elapsed: 1m36s
Starting testing the validation set with 200 subgraphs!
2020-05-18 19:55:12.402557: Validation Test:  Loss: 0.6705,  Acc: 59.2499, AUC: 0.6215, Precision: 0.6511 -- Elapsed: 31m20s
2020-05-18 19:58:59.452609: Epoch: 1, Batch: 251, Loss: 0.6644, Elapsed: 3m47s
2020-05-18 20:05:11.672559: Epoch: 1, Batch: 252, Loss: 0.6773, Elapsed: 6m12s
2020-05-18 20:10:20.469580: Epoch: 1, Batch: 253, Loss: 0.6820, Elapsed: 5m8s
2020-05-18 20:13:19.617989: Epoch: 1, Batch: 254, Loss: 0.6663, Elapsed: 2m59s
2020-05-18 20:15:56.502641: Epoch: 1, Batch: 255, Loss: 0.6622, Elapsed: 2m36s
2020-05-18 20:20:23.471166: Epoch: 1, Batch: 256, Loss: 0.6706, Elapsed: 4m26s
2020-05-18 20:23:16.357714: Epoch: 1, Batch: 257, Loss: 0.6764, Elapsed: 2m52s
2020-05-18 20:29:14.287258: Epoch: 1, Batch: 258, Loss: 0.6547, Elapsed: 5m57s
2020-05-18 20:35:20.534038: Epoch: 1, Batch: 259, Loss: 0.6646, Elapsed: 6m6s
2020-05-18 20:39:29.956459: Epoch: 1, Batch: 260, Loss: 0.6831, Elapsed: 4m9s
2020-05-18 20:44:09.322515: Epoch: 1, Batch: 261, Loss: 0.6751, Elapsed: 4m39s
2020-05-18 20:47:22.834799: Epoch: 1, Batch: 262, Loss: 0.6700, Elapsed: 3m13s
2020-05-18 20:52:37.049022: Epoch: 1, Batch: 263, Loss: 0.6771, Elapsed: 5m14s
2020-05-18 20:54:13.699056: Epoch: 1, Batch: 264, Loss: 0.6656, Elapsed: 1m36s
2020-05-18 20:58:48.822308: Epoch: 1, Batch: 265, Loss: 0.6667, Elapsed: 4m35s
2020-05-18 21:03:58.905253: Epoch: 1, Batch: 266, Loss: 0.6551, Elapsed: 5m10s
2020-05-18 21:08:12.860971: Epoch: 1, Batch: 267, Loss: 0.6762, Elapsed: 4m13s
2020-05-18 21:13:58.277889: Epoch: 1, Batch: 268, Loss: 0.6720, Elapsed: 5m45s
2020-05-18 21:16:43.986882: Epoch: 1, Batch: 269, Loss: 0.6703, Elapsed: 2m45s
2020-05-18 21:19:06.542696: Epoch: 1, Batch: 270, Loss: 0.6643, Elapsed: 2m22s
2020-05-18 21:22:57.785087: Epoch: 1, Batch: 271, Loss: 0.6433, Elapsed: 3m51s
2020-05-18 21:26:35.860948: Epoch: 1, Batch: 272, Loss: 0.6612, Elapsed: 3m38s
2020-05-18 21:31:20.147066: Epoch: 1, Batch: 273, Loss: 0.6653, Elapsed: 4m44s
2020-05-18 21:35:47.515284: Epoch: 1, Batch: 274, Loss: 0.6605, Elapsed: 4m27s
2020-05-18 21:38:58.992990: Epoch: 1, Batch: 275, Loss: 0.6648, Elapsed: 3m11s
2020-05-18 21:44:46.389633: Epoch: 1, Batch: 276, Loss: 0.6757, Elapsed: 5m47s
2020-05-18 21:51:30.276469: Epoch: 1, Batch: 277, Loss: 0.6882, Elapsed: 6m43s
2020-05-18 21:55:50.529979: Epoch: 1, Batch: 278, Loss: 0.6739, Elapsed: 4m20s
2020-05-18 21:59:09.950853: Epoch: 1, Batch: 279, Loss: 0.6504, Elapsed: 3m19s
2020-05-18 22:03:30.703824: Epoch: 1, Batch: 280, Loss: 0.6594, Elapsed: 4m20s
2020-05-18 22:07:11.588520: Epoch: 1, Batch: 281, Loss: 0.6480, Elapsed: 3m40s
2020-05-18 22:11:55.462836: Epoch: 1, Batch: 282, Loss: 0.6643, Elapsed: 4m43s
2020-05-18 22:15:30.614509: Epoch: 1, Batch: 283, Loss: 0.6854, Elapsed: 3m35s
2020-05-18 22:20:18.241194: Epoch: 1, Batch: 284, Loss: 0.6652, Elapsed: 4m47s
2020-05-18 22:25:46.898995: Epoch: 1, Batch: 285, Loss: 0.6677, Elapsed: 5m28s
2020-05-18 22:30:52.897363: Epoch: 1, Batch: 286, Loss: 0.6709, Elapsed: 5m5s
2020-05-18 22:35:25.509704: Epoch: 1, Batch: 287, Loss: 0.6886, Elapsed: 4m32s
2020-05-18 22:39:02.156802: Epoch: 1, Batch: 288, Loss: 0.6602, Elapsed: 3m36s
2020-05-18 22:41:23.450252: Epoch: 1, Batch: 289, Loss: 0.6576, Elapsed: 2m21s
2020-05-18 22:46:20.504105: Epoch: 1, Batch: 290, Loss: 0.6424, Elapsed: 4m57s
2020-05-18 22:48:45.893239: Epoch: 1, Batch: 291, Loss: 0.6472, Elapsed: 2m25s
2020-05-18 22:53:02.730063: Epoch: 1, Batch: 292, Loss: 0.6588, Elapsed: 4m16s
2020-05-18 22:58:05.585003: Epoch: 1, Batch: 293, Loss: 0.6893, Elapsed: 5m2s
2020-05-18 23:00:35.590594: Epoch: 1, Batch: 294, Loss: 0.6470, Elapsed: 2m29s
2020-05-18 23:03:29.055140: Epoch: 1, Batch: 295, Loss: 0.6500, Elapsed: 2m53s
2020-05-18 23:07:49.473448: Epoch: 1, Batch: 296, Loss: 0.7060, Elapsed: 4m20s
2020-05-18 23:11:21.869937: Epoch: 1, Batch: 297, Loss: 0.6479, Elapsed: 3m32s
2020-05-18 23:17:28.655396: Epoch: 1, Batch: 298, Loss: 0.6696, Elapsed: 6m6s
2020-05-18 23:20:38.398819: Epoch: 1, Batch: 299, Loss: 0.6526, Elapsed: 3m9s
2020-05-18 23:24:58.691482: Epoch: 1, Batch: 300, Loss: 0.6414, Elapsed: 4m20s
Starting testing the validation set with 200 subgraphs!
2020-05-18 23:56:26.570753: Validation Test:  Loss: 0.6694,  Acc: 58.3451, AUC: 0.6140, Precision: 0.6476 -- Elapsed: 31m27s
2020-05-19 00:03:13.293957: Epoch: 1, Batch: 301, Loss: 0.7129, Elapsed: 6m46s
2020-05-19 00:07:07.547391: Epoch: 1, Batch: 302, Loss: 0.6664, Elapsed: 3m54s
2020-05-19 00:11:06.765574: Epoch: 1, Batch: 303, Loss: 0.6801, Elapsed: 3m59s
2020-05-19 00:15:12.772348: Epoch: 1, Batch: 304, Loss: 0.6650, Elapsed: 4m5s
2020-05-19 00:18:17.590302: Epoch: 1, Batch: 305, Loss: 0.6295, Elapsed: 3m4s
2020-05-19 00:22:58.216487: Epoch: 1, Batch: 306, Loss: 0.6472, Elapsed: 4m40s
2020-05-19 00:26:36.400788: Epoch: 1, Batch: 307, Loss: 0.6561, Elapsed: 3m38s
2020-05-19 00:32:53.806008: Epoch: 1, Batch: 308, Loss: 0.6913, Elapsed: 6m17s
2020-05-19 00:37:39.312642: Epoch: 1, Batch: 309, Loss: 0.6663, Elapsed: 4m45s
2020-05-19 00:42:36.752852: Epoch: 1, Batch: 310, Loss: 0.6670, Elapsed: 4m57s
2020-05-19 00:47:10.358109: Epoch: 1, Batch: 311, Loss: 0.6601, Elapsed: 4m33s
2020-05-19 00:49:44.838355: Epoch: 1, Batch: 312, Loss: 0.6552, Elapsed: 2m34s
2020-05-19 00:52:17.834000: Epoch: 1, Batch: 313, Loss: 0.6400, Elapsed: 2m32s
2020-05-19 00:55:37.928405: Epoch: 1, Batch: 314, Loss: 0.6717, Elapsed: 3m20s
2020-05-19 00:57:17.333264: Epoch: 1, Batch: 315, Loss: 0.6172, Elapsed: 1m39s
2020-05-19 01:01:48.377138: Epoch: 1, Batch: 316, Loss: 0.6586, Elapsed: 4m31s
2020-05-19 01:05:40.853130: Epoch: 1, Batch: 317, Loss: 0.6654, Elapsed: 3m52s
2020-05-19 01:08:12.481736: Epoch: 1, Batch: 318, Loss: 0.6521, Elapsed: 2m31s
2020-05-19 01:12:26.912207: Epoch: 1, Batch: 319, Loss: 0.6597, Elapsed: 4m14s
2020-05-19 01:20:12.240801: Epoch: 1, Batch: 320, Loss: 0.6945, Elapsed: 7m45s
2020-05-19 01:25:32.782239: Epoch: 1, Batch: 321, Loss: 0.6634, Elapsed: 5m20s
2020-05-19 01:28:49.012863: Epoch: 1, Batch: 322, Loss: 0.6565, Elapsed: 3m16s
2020-05-19 01:31:30.134996: Epoch: 1, Batch: 323, Loss: 0.6497, Elapsed: 2m41s
2020-05-19 01:36:02.635945: Epoch: 1, Batch: 324, Loss: 0.6583, Elapsed: 4m32s
2020-05-19 01:39:08.299855: Epoch: 1, Batch: 325, Loss: 0.6617, Elapsed: 3m5s
2020-05-19 01:48:50.658549: Epoch: 1, Batch: 326, Loss: 0.6749, Elapsed: 9m42s
2020-05-19 01:51:31.073859: Epoch: 1, Batch: 327, Loss: 0.6637, Elapsed: 2m40s
2020-05-19 01:56:10.068683: Epoch: 1, Batch: 328, Loss: 0.6557, Elapsed: 4m38s
2020-05-19 01:58:47.770504: Epoch: 1, Batch: 329, Loss: 0.6510, Elapsed: 2m37s
2020-05-19 02:02:13.378803: Epoch: 1, Batch: 330, Loss: 0.6404, Elapsed: 3m25s
2020-05-19 02:05:16.251898: Epoch: 1, Batch: 331, Loss: 0.6676, Elapsed: 3m2s
2020-05-19 02:09:47.185284: Epoch: 1, Batch: 332, Loss: 0.6749, Elapsed: 4m30s
2020-05-19 02:12:51.498931: Epoch: 1, Batch: 333, Loss: 0.6529, Elapsed: 3m4s
2020-05-19 02:16:24.304005: Epoch: 1, Batch: 334, Loss: 0.6547, Elapsed: 3m32s
2020-05-19 02:19:21.879424: Epoch: 1, Batch: 335, Loss: 0.6674, Elapsed: 2m57s
2020-05-19 02:24:10.347781: Epoch: 1, Batch: 336, Loss: 0.6792, Elapsed: 4m48s
2020-05-19 02:29:01.340400: Epoch: 1, Batch: 337, Loss: 0.6721, Elapsed: 4m50s
2020-05-19 02:33:04.224778: Epoch: 1, Batch: 338, Loss: 0.6570, Elapsed: 4m2s
2020-05-19 02:36:10.457587: Epoch: 1, Batch: 339, Loss: 0.6359, Elapsed: 3m6s
2020-05-19 02:41:14.471131: Epoch: 1, Batch: 340, Loss: 0.6633, Elapsed: 5m3s
2020-05-19 02:47:47.208951: Epoch: 1, Batch: 341, Loss: 0.6869, Elapsed: 6m32s
2020-05-19 02:52:14.827620: Epoch: 1, Batch: 342, Loss: 0.6689, Elapsed: 4m27s
2020-05-19 02:54:37.461600: Epoch: 1, Batch: 343, Loss: 0.6408, Elapsed: 2m22s
2020-05-19 02:58:49.402374: Epoch: 1, Batch: 344, Loss: 0.6519, Elapsed: 4m11s
2020-05-19 03:05:47.580224: Epoch: 1, Batch: 345, Loss: 0.6801, Elapsed: 6m58s
2020-05-19 03:09:35.689302: Epoch: 1, Batch: 346, Loss: 0.6464, Elapsed: 3m48s
2020-05-19 03:13:37.858352: Epoch: 1, Batch: 347, Loss: 0.6664, Elapsed: 4m2s
2020-05-19 03:17:14.362276: Epoch: 1, Batch: 348, Loss: 0.6559, Elapsed: 3m36s
2020-05-19 03:22:32.548635: Epoch: 1, Batch: 349, Loss: 0.6610, Elapsed: 5m18s
2020-05-19 03:26:28.739766: Epoch: 1, Batch: 350, Loss: 0.6668, Elapsed: 3m56s
Starting testing the validation set with 200 subgraphs!
2020-05-19 03:57:50.095015: Validation Test:  Loss: 0.6669,  Acc: 59.3808, AUC: 0.6220, Precision: 0.6554 -- Elapsed: 31m21s
2020-05-19 04:01:52.606661: Epoch: 1, Batch: 351, Loss: 0.6643, Elapsed: 4m2s
2020-05-19 04:05:57.467349: Epoch: 1, Batch: 352, Loss: 0.6414, Elapsed: 4m4s
2020-05-19 04:10:01.015361: Epoch: 1, Batch: 353, Loss: 0.6853, Elapsed: 4m3s
2020-05-19 04:13:37.394200: Epoch: 1, Batch: 354, Loss: 0.6450, Elapsed: 3m36s
2020-05-19 04:16:28.590149: Epoch: 1, Batch: 355, Loss: 0.6452, Elapsed: 2m51s
2020-05-19 04:23:57.574653: Epoch: 1, Batch: 356, Loss: 0.6874, Elapsed: 7m28s
2020-05-19 04:28:15.567623: Epoch: 1, Batch: 357, Loss: 0.6568, Elapsed: 4m17s
2020-05-19 04:32:13.062949: Epoch: 1, Batch: 358, Loss: 0.6722, Elapsed: 3m57s
2020-05-19 04:36:37.192592: Epoch: 1, Batch: 359, Loss: 0.6701, Elapsed: 4m24s
2020-05-19 04:41:03.842190: Epoch: 1, Batch: 360, Loss: 0.6561, Elapsed: 4m26s
2020-05-19 04:45:04.526402: Epoch: 1, Batch: 361, Loss: 0.6861, Elapsed: 4m0s
2020-05-19 04:51:57.509779: Epoch: 1, Batch: 362, Loss: 0.6715, Elapsed: 6m52s
2020-05-19 04:55:40.768403: Epoch: 1, Batch: 363, Loss: 0.6555, Elapsed: 3m43s
2020-05-19 05:01:12.080006: Epoch: 1, Batch: 364, Loss: 0.6805, Elapsed: 5m31s
2020-05-19 05:06:30.522393: Epoch: 1, Batch: 365, Loss: 0.6636, Elapsed: 5m18s
2020-05-19 05:12:06.072769: Epoch: 1, Batch: 366, Loss: 0.6695, Elapsed: 5m35s
2020-05-19 05:16:42.321214: Epoch: 1, Batch: 367, Loss: 0.6908, Elapsed: 4m36s
2020-05-19 05:19:50.406344: Epoch: 1, Batch: 368, Loss: 0.6585, Elapsed: 3m8s
2020-05-19 05:23:07.045893: Epoch: 1, Batch: 369, Loss: 0.6536, Elapsed: 3m16s
2020-05-19 05:25:01.739904: Epoch: 1, Batch: 370, Loss: 0.6524, Elapsed: 1m54s
2020-05-19 05:30:11.454054: Epoch: 1, Batch: 371, Loss: 0.6741, Elapsed: 5m9s
2020-05-19 05:33:28.957220: Epoch: 1, Batch: 372, Loss: 0.6557, Elapsed: 3m17s
2020-05-19 05:38:15.362853: Epoch: 1, Batch: 373, Loss: 0.6493, Elapsed: 4m46s
2020-05-19 05:43:22.660359: Epoch: 1, Batch: 374, Loss: 0.6891, Elapsed: 5m7s
2020-05-19 05:47:19.494541: Epoch: 1, Batch: 375, Loss: 0.6683, Elapsed: 3m56s
2020-05-19 05:52:03.671909: Epoch: 1, Batch: 376, Loss: 0.6921, Elapsed: 4m44s
2020-05-19 05:54:51.685382: Epoch: 1, Batch: 377, Loss: 0.6703, Elapsed: 2m47s
2020-05-19 05:58:38.406452: Epoch: 1, Batch: 378, Loss: 0.6836, Elapsed: 3m46s
2020-05-19 06:01:37.070074: Epoch: 1, Batch: 379, Loss: 0.6686, Elapsed: 2m58s
2020-05-19 06:06:17.788856: Epoch: 1, Batch: 380, Loss: 0.6869, Elapsed: 4m40s
2020-05-19 06:10:54.359688: Epoch: 1, Batch: 381, Loss: 0.6842, Elapsed: 4m36s
2020-05-19 06:15:39.756095: Epoch: 1, Batch: 382, Loss: 0.6422, Elapsed: 4m45s
2020-05-19 06:21:22.824220: Epoch: 1, Batch: 383, Loss: 0.6796, Elapsed: 5m43s
2020-05-19 06:25:25.716994: Epoch: 1, Batch: 384, Loss: 0.6853, Elapsed: 4m2s
2020-05-19 06:30:30.364747: Epoch: 1, Batch: 385, Loss: 0.6940, Elapsed: 5m4s
2020-05-19 06:34:59.963155: Epoch: 1, Batch: 386, Loss: 0.7067, Elapsed: 4m29s
2020-05-19 06:37:54.164037: Epoch: 1, Batch: 387, Loss: 0.6996, Elapsed: 2m54s
2020-05-19 06:41:53.609467: Epoch: 1, Batch: 388, Loss: 0.6838, Elapsed: 3m59s
2020-05-19 06:46:37.485121: Epoch: 1, Batch: 389, Loss: 0.6538, Elapsed: 4m43s
2020-05-19 06:49:48.232883: Epoch: 1, Batch: 390, Loss: 0.6526, Elapsed: 3m10s
2020-05-19 06:52:17.189057: Epoch: 1, Batch: 391, Loss: 0.6350, Elapsed: 2m28s
2020-05-19 06:57:39.013951: Epoch: 1, Batch: 392, Loss: 0.6563, Elapsed: 5m21s
2020-05-19 07:02:49.846949: Epoch: 1, Batch: 393, Loss: 0.6620, Elapsed: 5m10s
2020-05-19 07:06:29.234556: Epoch: 1, Batch: 394, Loss: 0.6430, Elapsed: 3m39s
2020-05-19 07:10:23.673533: Epoch: 1, Batch: 395, Loss: 0.6798, Elapsed: 3m54s
2020-05-19 07:14:51.880508: Epoch: 1, Batch: 396, Loss: 0.6551, Elapsed: 4m28s
2020-05-19 07:20:33.609665: Epoch: 1, Batch: 397, Loss: 0.6881, Elapsed: 5m41s
2020-05-19 07:23:46.624201: Epoch: 1, Batch: 398, Loss: 0.6624, Elapsed: 3m12s
2020-05-19 07:26:41.815266: Epoch: 1, Batch: 399, Loss: 0.6667, Elapsed: 2m55s
2020-05-19 07:30:19.668269: Epoch: 1, Batch: 400, Loss: 0.6801, Elapsed: 3m37s
Starting testing the validation set with 200 subgraphs!
2020-05-19 08:01:44.357363: Validation Test:  Loss: 0.6721,  Acc: 59.1282, AUC: 0.6119, Precision: 0.6439 -- Elapsed: 31m24s
2020-05-19 08:06:37.890163: Epoch: 1, Batch: 401, Loss: 0.6685, Elapsed: 4m53s
2020-05-19 08:09:10.796197: Epoch: 1, Batch: 402, Loss: 0.6504, Elapsed: 2m32s
2020-05-19 08:12:47.781759: Epoch: 1, Batch: 403, Loss: 0.6342, Elapsed: 3m36s
2020-05-19 08:17:57.060875: Epoch: 1, Batch: 404, Loss: 0.6708, Elapsed: 5m9s
2020-05-19 08:21:01.908596: Epoch: 1, Batch: 405, Loss: 0.6292, Elapsed: 3m4s
2020-05-19 08:25:08.098940: Epoch: 1, Batch: 406, Loss: 0.6772, Elapsed: 4m6s
2020-05-19 08:31:25.787955: Epoch: 1, Batch: 407, Loss: 0.6977, Elapsed: 6m17s
2020-05-19 08:34:57.340787: Epoch: 1, Batch: 408, Loss: 0.6739, Elapsed: 3m31s
2020-05-19 08:37:09.709170: Epoch: 1, Batch: 409, Loss: 0.6772, Elapsed: 2m12s
2020-05-19 08:47:31.227573: Epoch: 1, Batch: 410, Loss: 0.6976, Elapsed: 10m21s
2020-05-19 08:50:26.706907: Epoch: 1, Batch: 411, Loss: 0.6473, Elapsed: 2m55s
2020-05-19 08:53:29.911103: Epoch: 1, Batch: 412, Loss: 0.6710, Elapsed: 3m3s
2020-05-19 08:58:02.770394: Epoch: 1, Batch: 413, Loss: 0.6702, Elapsed: 4m32s
2020-05-19 09:03:10.113276: Epoch: 1, Batch: 414, Loss: 0.6660, Elapsed: 5m7s
2020-05-19 09:07:57.879087: Epoch: 1, Batch: 415, Loss: 0.6617, Elapsed: 4m47s
2020-05-19 09:10:05.258490: Epoch: 1, Batch: 416, Loss: 0.6791, Elapsed: 2m7s
2020-05-19 09:14:37.806652: Epoch: 1, Batch: 417, Loss: 0.6633, Elapsed: 4m32s
2020-05-19 09:19:12.837611: Epoch: 1, Batch: 418, Loss: 0.6458, Elapsed: 4m35s
2020-05-19 09:27:17.782586: Epoch: 1, Batch: 419, Loss: 0.7151, Elapsed: 8m4s
2020-05-19 09:30:19.296035: Epoch: 1, Batch: 420, Loss: 0.6730, Elapsed: 3m1s
2020-05-19 09:33:49.645813: Epoch: 1, Batch: 421, Loss: 0.6832, Elapsed: 3m30s
2020-05-19 09:39:57.529142: Epoch: 1, Batch: 422, Loss: 0.7004, Elapsed: 6m7s
2020-05-19 09:43:36.508270: Epoch: 1, Batch: 423, Loss: 0.6634, Elapsed: 3m38s
2020-05-19 09:46:37.876459: Epoch: 1, Batch: 424, Loss: 0.6521, Elapsed: 3m1s
2020-05-19 09:50:12.061450: Epoch: 1, Batch: 425, Loss: 0.6928, Elapsed: 3m34s
2020-05-19 09:53:03.683491: Epoch: 1, Batch: 426, Loss: 0.7186, Elapsed: 2m51s
2020-05-19 09:58:02.311789: Epoch: 1, Batch: 427, Loss: 0.6810, Elapsed: 4m58s
2020-05-19 10:01:09.522406: Epoch: 1, Batch: 428, Loss: 0.7095, Elapsed: 3m7s
2020-05-19 10:04:46.929449: Epoch: 1, Batch: 429, Loss: 0.7086, Elapsed: 3m37s
2020-05-19 10:10:14.782921: Epoch: 1, Batch: 430, Loss: 0.6560, Elapsed: 5m27s
2020-05-19 10:13:29.563489: Epoch: 1, Batch: 431, Loss: 0.7060, Elapsed: 3m14s
2020-05-19 10:17:32.654908: Epoch: 1, Batch: 432, Loss: 0.6924, Elapsed: 4m3s
2020-05-19 10:21:52.338570: Epoch: 1, Batch: 433, Loss: 0.6888, Elapsed: 4m19s
2020-05-19 10:24:29.806443: Epoch: 1, Batch: 434, Loss: 0.7034, Elapsed: 2m37s
2020-05-19 10:28:53.492322: Epoch: 1, Batch: 435, Loss: 0.6858, Elapsed: 4m23s
2020-05-19 10:33:37.246452: Epoch: 1, Batch: 436, Loss: 0.6811, Elapsed: 4m43s
2020-05-19 10:37:45.094514: Epoch: 1, Batch: 437, Loss: 0.7113, Elapsed: 4m7s
2020-05-19 10:41:31.149775: Epoch: 1, Batch: 438, Loss: 0.6828, Elapsed: 3m46s
2020-05-19 10:48:45.691075: Epoch: 1, Batch: 439, Loss: 0.6675, Elapsed: 7m14s
2020-05-19 10:52:45.256203: Epoch: 1, Batch: 440, Loss: 0.6750, Elapsed: 3m59s
2020-05-19 10:56:48.578934: Epoch: 1, Batch: 441, Loss: 0.6856, Elapsed: 4m3s
2020-05-19 11:02:29.105560: Epoch: 1, Batch: 442, Loss: 0.6656, Elapsed: 5m40s
2020-05-19 11:08:13.919949: Epoch: 1, Batch: 443, Loss: 0.6612, Elapsed: 5m44s
2020-05-19 11:12:06.313920: Epoch: 1, Batch: 444, Loss: 0.6797, Elapsed: 3m52s
2020-05-19 11:15:01.656859: Epoch: 1, Batch: 445, Loss: 0.6752, Elapsed: 2m55s
2020-05-19 11:18:55.637647: Epoch: 1, Batch: 446, Loss: 0.6751, Elapsed: 3m53s
2020-05-19 11:21:36.478083: Epoch: 1, Batch: 447, Loss: 0.6763, Elapsed: 2m40s
2020-05-19 11:24:43.355012: Epoch: 1, Batch: 448, Loss: 0.6695, Elapsed: 3m6s
2020-05-19 11:28:57.929801: Epoch: 1, Batch: 449, Loss: 0.6754, Elapsed: 4m14s
2020-05-19 11:32:35.857906: Epoch: 1, Batch: 450, Loss: 0.6666, Elapsed: 3m37s
Starting testing the validation set with 200 subgraphs!
2020-05-19 12:03:42.459754: Validation Test:  Loss: 0.6756,  Acc: 59.3199, AUC: 0.6214, Precision: 0.6541 -- Elapsed: 31m6s
2020-05-19 12:07:48.454271: Epoch: 1, Batch: 451, Loss: 0.6697, Elapsed: 4m5s
2020-05-19 12:12:22.187148: Epoch: 1, Batch: 452, Loss: 0.6791, Elapsed: 4m33s
2020-05-19 12:14:56.372224: Epoch: 1, Batch: 453, Loss: 0.6756, Elapsed: 2m34s
2020-05-19 12:18:06.226373: Epoch: 1, Batch: 454, Loss: 0.6629, Elapsed: 3m9s
2020-05-19 12:21:56.137008: Epoch: 1, Batch: 455, Loss: 0.6782, Elapsed: 3m49s
2020-05-19 12:27:28.072609: Epoch: 1, Batch: 456, Loss: 0.6771, Elapsed: 5m31s
2020-05-19 12:31:27.179388: Epoch: 1, Batch: 457, Loss: 0.6779, Elapsed: 3m59s
2020-05-19 12:37:22.921540: Epoch: 1, Batch: 458, Loss: 0.6633, Elapsed: 5m55s
2020-05-19 12:42:03.158541: Epoch: 1, Batch: 459, Loss: 0.6811, Elapsed: 4m40s
2020-05-19 12:45:15.320066: Epoch: 1, Batch: 460, Loss: 0.6700, Elapsed: 3m12s
2020-05-19 12:53:05.584094: Epoch: 1, Batch: 461, Loss: 0.7245, Elapsed: 7m50s
2020-05-19 12:58:23.387477: Epoch: 1, Batch: 462, Loss: 0.6769, Elapsed: 5m17s
2020-05-19 13:03:18.007751: Epoch: 1, Batch: 463, Loss: 0.6613, Elapsed: 4m54s
2020-05-19 13:07:53.244572: Epoch: 1, Batch: 464, Loss: 0.6562, Elapsed: 4m35s
2020-05-19 13:12:12.734243: Epoch: 1, Batch: 465, Loss: 0.6763, Elapsed: 4m19s
2020-05-19 13:16:29.062941: Epoch: 1, Batch: 466, Loss: 0.6578, Elapsed: 4m16s
2020-05-19 13:19:44.472617: Epoch: 1, Batch: 467, Loss: 0.6511, Elapsed: 3m15s
2020-05-19 13:27:35.602199: Epoch: 1, Batch: 468, Loss: 0.6727, Elapsed: 7m51s
2020-05-19 13:32:53.835971: Epoch: 1, Batch: 469, Loss: 0.6839, Elapsed: 5m18s
2020-05-19 13:36:39.839169: Epoch: 1, Batch: 470, Loss: 0.6690, Elapsed: 3m45s
2020-05-19 13:39:05.866891: Epoch: 1, Batch: 471, Loss: 0.6596, Elapsed: 2m26s
2020-05-19 13:42:57.544608: Epoch: 1, Batch: 472, Loss: 0.6583, Elapsed: 3m51s
2020-05-19 13:46:18.462903: Epoch: 1, Batch: 473, Loss: 0.6656, Elapsed: 3m20s
2020-05-19 13:50:08.481021: Epoch: 1, Batch: 474, Loss: 0.6657, Elapsed: 3m49s
2020-05-19 13:53:12.745670: Epoch: 1, Batch: 475, Loss: 0.6680, Elapsed: 3m4s
2020-05-19 13:56:29.760426: Epoch: 1, Batch: 476, Loss: 0.6668, Elapsed: 3m16s
2020-05-19 14:00:37.325917: Epoch: 1, Batch: 477, Loss: 0.6754, Elapsed: 4m7s
2020-05-19 14:05:45.997866: Epoch: 1, Batch: 478, Loss: 0.6820, Elapsed: 5m8s
2020-05-19 14:08:52.526931: Epoch: 1, Batch: 479, Loss: 0.7000, Elapsed: 3m6s
2020-05-19 14:13:02.373321: Epoch: 1, Batch: 480, Loss: 0.6650, Elapsed: 4m9s
2020-05-19 14:17:21.271103: Epoch: 1, Batch: 481, Loss: 0.6756, Elapsed: 4m18s
2020-05-19 14:24:49.529764: Epoch: 1, Batch: 482, Loss: 0.7311, Elapsed: 7m28s
2020-05-19 14:29:12.629038: Epoch: 1, Batch: 483, Loss: 0.6596, Elapsed: 4m23s
2020-05-19 14:33:22.130251: Epoch: 1, Batch: 484, Loss: 0.6617, Elapsed: 4m9s
2020-05-19 14:37:22.154000: Epoch: 1, Batch: 485, Loss: 0.6457, Elapsed: 3m59s
2020-05-19 14:40:27.032377: Epoch: 1, Batch: 486, Loss: 0.6659, Elapsed: 3m4s
2020-05-19 14:44:33.349104: Epoch: 1, Batch: 487, Loss: 0.6909, Elapsed: 4m6s
2020-05-19 14:47:13.256557: Epoch: 1, Batch: 488, Loss: 0.6884, Elapsed: 2m39s
2020-05-19 14:50:25.056509: Epoch: 1, Batch: 489, Loss: 0.6384, Elapsed: 3m11s
2020-05-19 14:52:33.991892: Epoch: 1, Batch: 490, Loss: 0.6490, Elapsed: 2m8s
2020-05-19 14:58:17.193067: Epoch: 1, Batch: 491, Loss: 0.6698, Elapsed: 5m43s
2020-05-19 15:02:52.717768: Epoch: 1, Batch: 492, Loss: 0.6831, Elapsed: 4m35s
2020-05-19 15:04:37.677078: Epoch: 1, Batch: 493, Loss: 0.7037, Elapsed: 1m44s
2020-05-19 15:09:04.620974: Epoch: 1, Batch: 494, Loss: 0.6641, Elapsed: 4m26s
2020-05-19 15:12:32.129509: Epoch: 1, Batch: 495, Loss: 0.6731, Elapsed: 3m27s
2020-05-19 15:16:07.840192: Epoch: 1, Batch: 496, Loss: 0.6889, Elapsed: 3m35s
2020-05-19 15:20:39.830597: Epoch: 1, Batch: 497, Loss: 0.6698, Elapsed: 4m31s
2020-05-19 15:25:43.034999: Epoch: 1, Batch: 498, Loss: 0.6741, Elapsed: 5m3s
2020-05-19 15:29:21.704669: Epoch: 1, Batch: 499, Loss: 0.6548, Elapsed: 3m38s
2020-05-19 15:33:16.215302: Epoch: 1, Batch: 500, Loss: 0.6705, Elapsed: 3m54s
Starting testing the validation set with 200 subgraphs!
2020-05-19 16:04:46.657049: Validation Test:  Loss: 0.6760,  Acc: 59.1728, AUC: 0.6213, Precision: 0.6435 -- Elapsed: 31m30s
2020-05-19 16:10:46.395023: Epoch: 1, Batch: 501, Loss: 0.6842, Elapsed: 5m59s
2020-05-19 16:14:44.333714: Epoch: 1, Batch: 502, Loss: 0.6639, Elapsed: 3m57s
2020-05-19 16:17:53.743537: Epoch: 1, Batch: 503, Loss: 0.7186, Elapsed: 3m9s
2020-05-19 16:19:51.736174: Epoch: 1, Batch: 504, Loss: 0.6703, Elapsed: 1m57s
2020-05-19 16:24:15.628486: Epoch: 1, Batch: 505, Loss: 0.6962, Elapsed: 4m23s
2020-05-19 16:29:08.021196: Epoch: 1, Batch: 506, Loss: 0.6687, Elapsed: 4m52s
2020-05-19 16:32:14.050744: Epoch: 1, Batch: 507, Loss: 0.6976, Elapsed: 3m6s
2020-05-19 16:36:05.669872: Epoch: 1, Batch: 508, Loss: 0.6798, Elapsed: 3m51s
2020-05-19 16:42:09.302166: Epoch: 1, Batch: 509, Loss: 0.6817, Elapsed: 6m3s
2020-05-19 16:46:17.014763: Epoch: 1, Batch: 510, Loss: 0.6746, Elapsed: 4m7s
2020-05-19 16:49:35.217069: Epoch: 1, Batch: 511, Loss: 0.6863, Elapsed: 3m18s
2020-05-19 16:52:34.935207: Epoch: 1, Batch: 512, Loss: 0.6613, Elapsed: 2m59s
2020-05-19 16:56:11.089911: Epoch: 1, Batch: 513, Loss: 0.6827, Elapsed: 3m36s
2020-05-19 16:59:23.498735: Epoch: 1, Batch: 514, Loss: 0.6823, Elapsed: 3m12s
2020-05-19 17:02:33.333637: Epoch: 1, Batch: 515, Loss: 0.6721, Elapsed: 3m9s
2020-05-19 17:05:51.209720: Epoch: 1, Batch: 516, Loss: 0.6728, Elapsed: 3m17s
2020-05-19 17:09:12.027721: Epoch: 1, Batch: 517, Loss: 0.6850, Elapsed: 3m20s
2020-05-19 17:13:34.066722: Epoch: 1, Batch: 518, Loss: 0.6777, Elapsed: 4m22s
2020-05-19 17:15:11.239256: Epoch: 1, Batch: 519, Loss: 0.6658, Elapsed: 1m37s
2020-05-19 17:21:41.618007: Epoch: 1, Batch: 520, Loss: 0.6771, Elapsed: 6m30s
2020-05-19 17:24:28.666016: Epoch: 1, Batch: 521, Loss: 0.6691, Elapsed: 2m47s
2020-05-19 17:27:32.443044: Epoch: 1, Batch: 522, Loss: 0.6679, Elapsed: 3m3s
2020-05-19 17:29:26.365894: Epoch: 1, Batch: 523, Loss: 0.6636, Elapsed: 1m53s
2020-05-19 17:35:37.566614: Epoch: 1, Batch: 524, Loss: 0.6699, Elapsed: 6m11s
2020-05-19 17:40:11.892165: Epoch: 1, Batch: 525, Loss: 0.6778, Elapsed: 4m34s
2020-05-19 17:43:51.075016: Epoch: 1, Batch: 526, Loss: 0.6655, Elapsed: 3m39s
2020-05-19 17:49:07.802709: Epoch: 1, Batch: 527, Loss: 0.6728, Elapsed: 5m16s
2020-05-19 17:54:03.864796: Epoch: 1, Batch: 528, Loss: 0.6763, Elapsed: 4m56s
2020-05-19 18:00:21.598540: Epoch: 1, Batch: 529, Loss: 0.6985, Elapsed: 6m17s
2020-05-19 18:02:53.438939: Epoch: 1, Batch: 530, Loss: 0.6742, Elapsed: 2m31s
2020-05-19 18:07:15.122774: Epoch: 1, Batch: 531, Loss: 0.6824, Elapsed: 4m21s
2020-05-19 18:10:56.861888: Epoch: 1, Batch: 532, Loss: 0.6881, Elapsed: 3m41s
2020-05-19 18:14:24.552280: Epoch: 1, Batch: 533, Loss: 0.6818, Elapsed: 3m27s
2020-05-19 18:18:57.043003: Epoch: 1, Batch: 534, Loss: 0.6919, Elapsed: 4m32s
2020-05-19 18:23:54.139745: Epoch: 1, Batch: 535, Loss: 0.6685, Elapsed: 4m57s
2020-05-19 18:27:21.241097: Epoch: 1, Batch: 536, Loss: 0.6758, Elapsed: 3m27s
2020-05-19 18:32:40.123623: Epoch: 1, Batch: 537, Loss: 0.6743, Elapsed: 5m18s
2020-05-19 18:38:32.671706: Epoch: 1, Batch: 538, Loss: 0.6822, Elapsed: 5m52s
2020-05-19 18:41:38.349169: Epoch: 1, Batch: 539, Loss: 0.6763, Elapsed: 3m5s
2020-05-19 18:44:49.600128: Epoch: 1, Batch: 540, Loss: 0.6887, Elapsed: 3m11s
2020-05-19 18:48:38.817157: Epoch: 1, Batch: 541, Loss: 0.6858, Elapsed: 3m49s
2020-05-19 18:51:49.250177: Epoch: 1, Batch: 542, Loss: 0.6717, Elapsed: 3m10s
2020-05-19 18:57:48.904259: Epoch: 1, Batch: 543, Loss: 0.6637, Elapsed: 5m59s
2020-05-19 19:02:52.766550: Epoch: 1, Batch: 544, Loss: 0.6789, Elapsed: 5m3s
2020-05-19 19:07:30.624500: Epoch: 1, Batch: 545, Loss: 0.6788, Elapsed: 4m37s
2020-05-19 19:10:07.001479: Epoch: 1, Batch: 546, Loss: 0.6704, Elapsed: 2m36s
2020-05-19 19:13:20.004140: Epoch: 1, Batch: 547, Loss: 0.6787, Elapsed: 3m12s
2020-05-19 19:16:50.014496: Epoch: 1, Batch: 548, Loss: 0.6766, Elapsed: 3m29s
2020-05-19 19:19:14.141935: Epoch: 1, Batch: 549, Loss: 0.6614, Elapsed: 2m24s
2020-05-19 19:21:44.554980: Epoch: 1, Batch: 550, Loss: 0.6810, Elapsed: 2m30s
Starting testing the validation set with 200 subgraphs!
2020-05-19 19:53:05.679293: Validation Test:  Loss: 0.6671,  Acc: 61.2230, AUC: 0.6365, Precision: 0.6668 -- Elapsed: 31m21s
2020-05-19 19:58:42.336875: Epoch: 1, Batch: 551, Loss: 0.6676, Elapsed: 5m36s
2020-05-19 20:03:18.888368: Epoch: 1, Batch: 552, Loss: 0.6597, Elapsed: 4m36s
2020-05-19 20:08:00.158194: Epoch: 1, Batch: 553, Loss: 0.6724, Elapsed: 4m41s
2020-05-19 20:13:29.022837: Epoch: 1, Batch: 554, Loss: 0.6676, Elapsed: 5m28s
2020-05-19 20:16:27.522292: Epoch: 1, Batch: 555, Loss: 0.6722, Elapsed: 2m58s
2020-05-19 20:19:20.501208: Epoch: 1, Batch: 556, Loss: 0.6554, Elapsed: 2m52s
2020-05-19 20:23:05.436568: Epoch: 1, Batch: 557, Loss: 0.6604, Elapsed: 3m44s
2020-05-19 20:26:09.961967: Epoch: 1, Batch: 558, Loss: 0.6433, Elapsed: 3m4s
2020-05-19 20:29:24.004864: Epoch: 1, Batch: 559, Loss: 0.6671, Elapsed: 3m14s
2020-05-19 20:33:14.240436: Epoch: 1, Batch: 560, Loss: 0.6761, Elapsed: 3m50s
2020-05-19 20:39:23.275010: Epoch: 1, Batch: 561, Loss: 0.6752, Elapsed: 6m9s
2020-05-19 20:42:44.709370: Epoch: 1, Batch: 562, Loss: 0.6586, Elapsed: 3m21s
2020-05-19 20:45:53.139272: Epoch: 1, Batch: 563, Loss: 0.6641, Elapsed: 3m8s
2020-05-19 20:51:29.687736: Epoch: 1, Batch: 564, Loss: 0.6567, Elapsed: 5m36s
2020-05-19 20:54:55.450150: Epoch: 1, Batch: 565, Loss: 0.6610, Elapsed: 3m25s
2020-05-19 20:59:58.132242: Epoch: 1, Batch: 566, Loss: 0.6512, Elapsed: 5m2s
2020-05-19 21:03:06.528417: Epoch: 1, Batch: 567, Loss: 0.6458, Elapsed: 3m8s
2020-05-19 21:06:52.985492: Epoch: 1, Batch: 568, Loss: 0.6612, Elapsed: 3m46s
2020-05-19 21:09:27.108373: Epoch: 1, Batch: 569, Loss: 0.6437, Elapsed: 2m34s
2020-05-19 21:13:39.461971: Epoch: 1, Batch: 570, Loss: 0.6841, Elapsed: 4m12s
2020-05-19 21:16:40.218787: Epoch: 1, Batch: 571, Loss: 0.6557, Elapsed: 3m0s
2020-05-19 21:21:34.914318: Epoch: 1, Batch: 572, Loss: 0.6505, Elapsed: 4m54s
2020-05-19 21:25:39.630250: Epoch: 1, Batch: 573, Loss: 0.6587, Elapsed: 4m4s
2020-05-19 21:29:44.247326: Epoch: 1, Batch: 574, Loss: 0.6546, Elapsed: 4m4s
2020-05-19 21:33:53.900579: Epoch: 1, Batch: 575, Loss: 0.6526, Elapsed: 4m9s
2020-05-19 21:37:26.752534: Epoch: 1, Batch: 576, Loss: 0.6508, Elapsed: 3m32s
2020-05-19 21:43:09.522029: Epoch: 1, Batch: 577, Loss: 0.6940, Elapsed: 5m42s
2020-05-19 21:47:16.671206: Epoch: 1, Batch: 578, Loss: 0.6678, Elapsed: 4m7s
2020-05-19 21:51:49.038121: Epoch: 1, Batch: 579, Loss: 0.6517, Elapsed: 4m32s
2020-05-19 21:54:27.695323: Epoch: 1, Batch: 580, Loss: 0.6382, Elapsed: 2m38s
2020-05-19 21:58:03.785575: Epoch: 1, Batch: 581, Loss: 0.6641, Elapsed: 3m36s
2020-05-19 22:02:07.826326: Epoch: 1, Batch: 582, Loss: 0.6498, Elapsed: 4m4s
2020-05-19 22:06:06.565251: Epoch: 1, Batch: 583, Loss: 0.6563, Elapsed: 3m58s
2020-05-19 22:11:51.003244: Epoch: 1, Batch: 584, Loss: 0.6697, Elapsed: 5m44s
2020-05-19 22:14:12.145932: Epoch: 1, Batch: 585, Loss: 0.6371, Elapsed: 2m21s
2020-05-19 22:19:06.384966: Epoch: 1, Batch: 586, Loss: 0.6670, Elapsed: 4m54s
2020-05-19 22:21:45.757064: Epoch: 1, Batch: 587, Loss: 0.6641, Elapsed: 2m39s
2020-05-19 22:26:16.619756: Epoch: 1, Batch: 588, Loss: 0.6810, Elapsed: 4m30s
2020-05-19 22:30:43.835255: Epoch: 1, Batch: 589, Loss: 0.6835, Elapsed: 4m27s
2020-05-19 22:35:14.751142: Epoch: 1, Batch: 590, Loss: 0.6769, Elapsed: 4m30s
2020-05-19 22:40:39.226702: Epoch: 1, Batch: 591, Loss: 0.6698, Elapsed: 5m24s
2020-05-19 22:44:09.924466: Epoch: 1, Batch: 592, Loss: 0.6482, Elapsed: 3m30s
2020-05-19 22:48:19.850446: Epoch: 1, Batch: 593, Loss: 0.6431, Elapsed: 4m9s
2020-05-19 22:53:14.592049: Epoch: 1, Batch: 594, Loss: 0.6655, Elapsed: 4m54s
2020-05-19 22:57:57.891252: Epoch: 1, Batch: 595, Loss: 0.6461, Elapsed: 4m43s
2020-05-19 23:03:08.394274: Epoch: 1, Batch: 596, Loss: 0.6702, Elapsed: 5m10s
2020-05-19 23:06:12.031685: Epoch: 1, Batch: 597, Loss: 0.6289, Elapsed: 3m3s
2020-05-19 23:09:07.408681: Epoch: 1, Batch: 598, Loss: 0.6496, Elapsed: 2m55s
2020-05-19 23:10:34.575075: Epoch: 1, Batch: 599, Loss: 0.6451, Elapsed: 1m27s
2020-05-19 23:13:24.551226: Epoch: 1, Batch: 600, Loss: 0.6353, Elapsed: 2m49s
Starting testing the validation set with 200 subgraphs!
2020-05-19 23:44:42.609537: Validation Test:  Loss: 0.6586,  Acc: 61.3599, AUC: 0.6383, Precision: 0.6820 -- Elapsed: 31m18s
2020-05-19 23:48:55.026313: Epoch: 1, Batch: 601, Loss: 0.6752, Elapsed: 4m12s
2020-05-19 23:53:18.031223: Epoch: 1, Batch: 602, Loss: 0.6874, Elapsed: 4m22s
2020-05-19 23:55:22.520927: Epoch: 1, Batch: 603, Loss: 0.6331, Elapsed: 2m4s
2020-05-19 23:59:47.221328: Epoch: 1, Batch: 604, Loss: 0.6516, Elapsed: 4m24s
2020-05-20 00:02:35.516708: Epoch: 1, Batch: 605, Loss: 0.6776, Elapsed: 2m48s
2020-05-20 00:08:30.355971: Epoch: 1, Batch: 606, Loss: 0.6766, Elapsed: 5m54s
2020-05-20 00:11:08.211689: Epoch: 1, Batch: 607, Loss: 0.6622, Elapsed: 2m37s
2020-05-20 00:16:24.574608: Epoch: 1, Batch: 608, Loss: 0.6975, Elapsed: 5m16s
2020-05-20 00:21:43.036875: Epoch: 1, Batch: 609, Loss: 0.6699, Elapsed: 5m18s
2020-05-20 00:25:00.036253: Epoch: 1, Batch: 610, Loss: 0.6795, Elapsed: 3m16s
2020-05-20 00:28:31.007961: Epoch: 1, Batch: 611, Loss: 0.6401, Elapsed: 3m30s
2020-05-20 00:35:13.404536: Epoch: 1, Batch: 612, Loss: 0.7170, Elapsed: 6m42s
2020-05-20 00:38:21.773099: Epoch: 1, Batch: 613, Loss: 0.6616, Elapsed: 3m8s
2020-05-20 00:43:20.196525: Epoch: 1, Batch: 614, Loss: 0.6914, Elapsed: 4m58s
2020-05-20 00:46:01.077781: Epoch: 1, Batch: 615, Loss: 0.6482, Elapsed: 2m40s
2020-05-20 00:49:49.642513: Epoch: 1, Batch: 616, Loss: 0.6674, Elapsed: 3m48s
2020-05-20 00:53:11.575894: Epoch: 1, Batch: 617, Loss: 0.6606, Elapsed: 3m21s
2020-05-20 00:57:23.326134: Epoch: 1, Batch: 618, Loss: 0.6599, Elapsed: 4m11s
2020-05-20 01:00:57.889095: Epoch: 1, Batch: 619, Loss: 0.6690, Elapsed: 3m34s
2020-05-20 01:05:27.138070: Epoch: 1, Batch: 620, Loss: 0.6733, Elapsed: 4m29s
2020-05-20 01:08:12.198687: Epoch: 1, Batch: 621, Loss: 0.6563, Elapsed: 2m45s
2020-05-20 01:11:46.799140: Epoch: 1, Batch: 622, Loss: 0.6731, Elapsed: 3m34s
2020-05-20 01:16:11.128140: Epoch: 1, Batch: 623, Loss: 0.6454, Elapsed: 4m24s
2020-05-20 01:20:46.209749: Epoch: 1, Batch: 624, Loss: 0.6914, Elapsed: 4m35s
2020-05-20 01:24:25.114161: Epoch: 1, Batch: 625, Loss: 0.6583, Elapsed: 3m38s
2020-05-20 01:27:48.161406: Epoch: 1, Batch: 626, Loss: 0.6509, Elapsed: 3m23s
2020-05-20 01:32:41.340984: Epoch: 1, Batch: 627, Loss: 0.6645, Elapsed: 4m53s
2020-05-20 01:37:02.298696: Epoch: 1, Batch: 628, Loss: 0.6792, Elapsed: 4m20s
2020-05-20 01:42:01.355761: Epoch: 1, Batch: 629, Loss: 0.6683, Elapsed: 4m59s
2020-05-20 01:46:50.529585: Epoch: 1, Batch: 630, Loss: 0.6781, Elapsed: 4m49s
2020-05-20 01:51:16.526705: Epoch: 1, Batch: 631, Loss: 0.7263, Elapsed: 4m25s
2020-05-20 01:55:38.520464: Epoch: 1, Batch: 632, Loss: 0.7007, Elapsed: 4m21s
2020-05-20 02:00:23.419395: Epoch: 1, Batch: 633, Loss: 0.6628, Elapsed: 4m44s
2020-05-20 02:04:38.645872: Epoch: 1, Batch: 634, Loss: 0.6728, Elapsed: 4m15s
2020-05-20 02:08:12.665717: Epoch: 1, Batch: 635, Loss: 0.6398, Elapsed: 3m33s
2020-05-20 02:12:05.176835: Epoch: 1, Batch: 636, Loss: 0.6749, Elapsed: 3m52s
2020-05-20 02:14:06.369984: Epoch: 1, Batch: 637, Loss: 0.6178, Elapsed: 2m1s
2020-05-20 02:18:27.114246: Epoch: 1, Batch: 638, Loss: 0.6768, Elapsed: 4m20s
2020-05-20 02:23:48.403084: Epoch: 1, Batch: 639, Loss: 0.6834, Elapsed: 5m21s
2020-05-20 02:28:55.580800: Epoch: 1, Batch: 640, Loss: 0.6806, Elapsed: 5m7s
2020-05-20 02:33:16.626253: Epoch: 1, Batch: 641, Loss: 0.6437, Elapsed: 4m21s
2020-05-20 02:37:11.186215: Epoch: 1, Batch: 642, Loss: 0.6510, Elapsed: 3m54s
2020-05-20 02:41:04.251073: Epoch: 1, Batch: 643, Loss: 0.6449, Elapsed: 3m53s
2020-05-20 02:46:16.464662: Epoch: 1, Batch: 644, Loss: 0.6482, Elapsed: 5m12s
2020-05-20 02:48:41.513544: Epoch: 1, Batch: 645, Loss: 0.6221, Elapsed: 2m25s
2020-05-20 02:53:27.085850: Epoch: 1, Batch: 646, Loss: 0.6644, Elapsed: 4m45s
2020-05-20 02:56:10.173890: Epoch: 1, Batch: 647, Loss: 0.6461, Elapsed: 2m43s
2020-05-20 02:58:42.791401: Epoch: 1, Batch: 648, Loss: 0.6060, Elapsed: 2m32s
2020-05-20 03:01:20.226710: Epoch: 1, Batch: 649, Loss: 0.6760, Elapsed: 2m37s
2020-05-20 03:04:14.938956: Epoch: 1, Batch: 650, Loss: 0.6526, Elapsed: 2m54s
Starting testing the validation set with 200 subgraphs!
2020-05-20 03:35:24.963199: Validation Test:  Loss: 0.6719,  Acc: 59.3392, AUC: 0.6173, Precision: 0.6427 -- Elapsed: 31m10s
2020-05-20 03:38:43.211446: Epoch: 1, Batch: 651, Loss: 0.6514, Elapsed: 3m18s
2020-05-20 03:40:26.116588: Epoch: 1, Batch: 652, Loss: 0.6615, Elapsed: 1m42s
2020-05-20 03:43:32.361069: Epoch: 1, Batch: 653, Loss: 0.6696, Elapsed: 3m6s
2020-05-20 03:48:42.707621: Epoch: 1, Batch: 654, Loss: 0.6792, Elapsed: 5m10s
2020-05-20 03:51:57.936770: Epoch: 1, Batch: 655, Loss: 0.6843, Elapsed: 3m15s
2020-05-20 03:58:06.902220: Epoch: 1, Batch: 656, Loss: 0.7027, Elapsed: 6m8s
2020-05-20 03:59:59.777580: Epoch: 1, Batch: 657, Loss: 0.5954, Elapsed: 1m52s
2020-05-20 04:03:58.940537: Epoch: 1, Batch: 658, Loss: 0.6623, Elapsed: 3m59s
2020-05-20 04:07:11.730999: Epoch: 1, Batch: 659, Loss: 0.6590, Elapsed: 3m12s
2020-05-20 04:13:09.431964: Epoch: 1, Batch: 660, Loss: 0.7116, Elapsed: 5m57s
2020-05-20 04:19:39.404382: Epoch: 1, Batch: 661, Loss: 0.7182, Elapsed: 6m29s
2020-05-20 04:23:36.143915: Epoch: 1, Batch: 662, Loss: 0.6703, Elapsed: 3m56s
2020-05-20 04:28:34.336832: Epoch: 1, Batch: 663, Loss: 0.6761, Elapsed: 4m58s
2020-05-20 04:31:37.310772: Epoch: 1, Batch: 664, Loss: 0.6616, Elapsed: 3m2s
2020-05-20 04:35:58.245307: Epoch: 1, Batch: 665, Loss: 0.6785, Elapsed: 4m20s
2020-05-20 04:40:03.658601: Epoch: 1, Batch: 666, Loss: 0.7271, Elapsed: 4m5s
2020-05-20 04:43:48.923072: Epoch: 1, Batch: 667, Loss: 0.6543, Elapsed: 3m45s
2020-05-20 04:48:28.574615: Epoch: 1, Batch: 668, Loss: 0.6809, Elapsed: 4m39s
2020-05-20 04:52:18.244303: Epoch: 1, Batch: 669, Loss: 0.6810, Elapsed: 3m49s
2020-05-20 04:55:13.306673: Epoch: 1, Batch: 670, Loss: 0.6563, Elapsed: 2m55s
2020-05-20 05:00:59.749488: Epoch: 1, Batch: 671, Loss: 0.6796, Elapsed: 5m46s
2020-05-20 05:04:04.324504: Epoch: 1, Batch: 672, Loss: 0.6571, Elapsed: 3m4s
2020-05-20 05:08:09.129086: Epoch: 1, Batch: 673, Loss: 0.6836, Elapsed: 4m4s
2020-05-20 05:11:47.112325: Epoch: 1, Batch: 674, Loss: 0.6660, Elapsed: 3m37s
2020-05-20 05:13:31.384775: Epoch: 1, Batch: 675, Loss: 0.6336, Elapsed: 1m44s
2020-05-20 05:17:29.116206: Epoch: 1, Batch: 676, Loss: 0.6674, Elapsed: 3m57s
2020-05-20 05:19:47.827426: Epoch: 1, Batch: 677, Loss: 0.6201, Elapsed: 2m18s
2020-05-20 05:27:34.154541: Epoch: 1, Batch: 678, Loss: 0.6945, Elapsed: 7m46s
2020-05-20 05:32:16.262047: Epoch: 1, Batch: 679, Loss: 0.6789, Elapsed: 4m42s
2020-05-20 05:36:25.185074: Epoch: 1, Batch: 680, Loss: 0.6790, Elapsed: 4m8s
2020-05-20 05:39:31.533284: Epoch: 1, Batch: 681, Loss: 0.6550, Elapsed: 3m6s
2020-05-20 05:44:40.311854: Epoch: 1, Batch: 682, Loss: 0.6579, Elapsed: 5m8s
2020-05-20 05:49:27.294733: Epoch: 1, Batch: 683, Loss: 0.6623, Elapsed: 4m46s
2020-05-20 05:56:53.640410: Epoch: 1, Batch: 684, Loss: 0.6912, Elapsed: 7m26s
2020-05-20 06:01:51.386856: Epoch: 1, Batch: 685, Loss: 0.6825, Elapsed: 4m57s
2020-05-20 06:06:22.486023: Epoch: 1, Batch: 686, Loss: 0.6499, Elapsed: 4m31s
2020-05-20 06:09:27.576836: Epoch: 1, Batch: 687, Loss: 0.6351, Elapsed: 3m5s
2020-05-20 06:16:13.529376: Epoch: 1, Batch: 688, Loss: 0.6712, Elapsed: 6m45s
2020-05-20 06:20:51.216607: Epoch: 1, Batch: 689, Loss: 0.6612, Elapsed: 4m37s
2020-05-20 06:25:35.138645: Epoch: 1, Batch: 690, Loss: 0.6642, Elapsed: 4m43s
2020-05-20 06:31:44.800652: Epoch: 1, Batch: 691, Loss: 0.6982, Elapsed: 6m9s
2020-05-20 06:34:04.293750: Epoch: 1, Batch: 692, Loss: 0.6337, Elapsed: 2m19s
2020-05-20 06:39:50.766164: Epoch: 1, Batch: 693, Loss: 0.6791, Elapsed: 5m46s
2020-05-20 06:46:00.294834: Epoch: 1, Batch: 694, Loss: 0.6543, Elapsed: 6m9s
2020-05-20 06:49:42.310977: Epoch: 1, Batch: 695, Loss: 0.6588, Elapsed: 3m41s
2020-05-20 06:53:58.931028: Epoch: 1, Batch: 696, Loss: 0.6472, Elapsed: 4m16s
2020-05-20 06:59:19.453624: Epoch: 1, Batch: 697, Loss: 0.6845, Elapsed: 5m20s
2020-05-20 07:03:09.713016: Epoch: 1, Batch: 698, Loss: 0.6896, Elapsed: 3m50s
2020-05-20 07:08:13.235737: Epoch: 1, Batch: 699, Loss: 0.6780, Elapsed: 5m3s
2020-05-20 07:11:06.014451: Epoch: 1, Batch: 700, Loss: 0.6636, Elapsed: 2m52s
Starting testing the validation set with 200 subgraphs!
2020-05-20 07:42:28.157925: Validation Test:  Loss: 0.6617,  Acc: 61.6876, AUC: 0.6451, Precision: 0.6774 -- Elapsed: 31m22s
2020-05-20 07:47:08.556689: Epoch: 1, Batch: 701, Loss: 0.6427, Elapsed: 4m40s
2020-05-20 07:50:05.350463: Epoch: 1, Batch: 702, Loss: 0.6556, Elapsed: 2m56s
2020-05-20 07:54:49.241593: Epoch: 1, Batch: 703, Loss: 0.6569, Elapsed: 4m43s
2020-05-20 07:58:22.776972: Epoch: 1, Batch: 704, Loss: 0.6544, Elapsed: 3m33s
2020-05-20 08:03:38.529465: Epoch: 1, Batch: 705, Loss: 0.6494, Elapsed: 5m15s
2020-05-20 08:09:30.892280: Epoch: 1, Batch: 706, Loss: 0.6643, Elapsed: 5m52s
2020-05-20 08:14:09.830778: Epoch: 1, Batch: 707, Loss: 0.6710, Elapsed: 4m38s
2020-05-20 08:17:49.497390: Epoch: 1, Batch: 708, Loss: 0.6991, Elapsed: 3m39s
2020-05-20 08:21:19.015584: Epoch: 1, Batch: 709, Loss: 0.6760, Elapsed: 3m29s
2020-05-20 08:23:25.138650: Epoch: 1, Batch: 710, Loss: 0.6650, Elapsed: 2m6s
2020-05-20 08:28:15.157025: Epoch: 1, Batch: 711, Loss: 0.6368, Elapsed: 4m49s
2020-05-20 08:34:35.861129: Epoch: 1, Batch: 712, Loss: 0.6793, Elapsed: 6m20s
2020-05-20 08:38:28.866153: Epoch: 1, Batch: 713, Loss: 0.6624, Elapsed: 3m52s
2020-05-20 08:43:13.423118: Epoch: 1, Batch: 714, Loss: 0.6756, Elapsed: 4m44s
2020-05-20 08:48:54.763739: Epoch: 1, Batch: 715, Loss: 0.6692, Elapsed: 5m41s
2020-05-20 08:52:09.823455: Epoch: 1, Batch: 716, Loss: 0.6797, Elapsed: 3m15s
2020-05-20 08:55:40.895056: Epoch: 1, Batch: 717, Loss: 0.6595, Elapsed: 3m31s
2020-05-20 09:00:14.189676: Epoch: 1, Batch: 718, Loss: 0.6476, Elapsed: 4m33s
2020-05-20 09:04:35.250321: Epoch: 1, Batch: 719, Loss: 0.6835, Elapsed: 4m21s
2020-05-20 09:08:45.660261: Epoch: 1, Batch: 720, Loss: 0.6735, Elapsed: 4m10s
2020-05-20 09:12:55.968897: Epoch: 1, Batch: 721, Loss: 0.6717, Elapsed: 4m10s
2020-05-20 09:15:28.078496: Epoch: 1, Batch: 722, Loss: 0.6977, Elapsed: 2m32s
2020-05-20 09:19:09.459946: Epoch: 1, Batch: 723, Loss: 0.6505, Elapsed: 3m41s
2020-05-20 09:23:50.309182: Epoch: 1, Batch: 724, Loss: 0.6687, Elapsed: 4m40s
2020-05-20 09:27:18.659696: Epoch: 1, Batch: 725, Loss: 0.6645, Elapsed: 3m28s
2020-05-20 09:30:42.159384: Epoch: 1, Batch: 726, Loss: 0.6716, Elapsed: 3m23s
2020-05-20 09:34:44.257284: Epoch: 1, Batch: 727, Loss: 0.6472, Elapsed: 4m2s
2020-05-20 09:40:22.020788: Epoch: 1, Batch: 728, Loss: 0.6526, Elapsed: 5m37s
2020-05-20 09:45:09.398310: Epoch: 1, Batch: 729, Loss: 0.6856, Elapsed: 4m47s
2020-05-20 09:48:42.791657: Epoch: 1, Batch: 730, Loss: 0.6492, Elapsed: 3m33s
2020-05-20 09:51:23.927391: Epoch: 1, Batch: 731, Loss: 0.6943, Elapsed: 2m41s
2020-05-20 09:55:01.152623: Epoch: 1, Batch: 732, Loss: 0.6454, Elapsed: 3m37s
2020-05-20 09:56:59.902580: Epoch: 1, Batch: 733, Loss: 0.6467, Elapsed: 1m58s
2020-05-20 09:59:31.228650: Epoch: 1, Batch: 734, Loss: 0.6432, Elapsed: 2m31s
2020-05-20 10:06:48.624919: Epoch: 1, Batch: 735, Loss: 0.6687, Elapsed: 7m17s
2020-05-20 10:10:07.022843: Epoch: 1, Batch: 736, Loss: 0.6556, Elapsed: 3m18s
2020-05-20 10:15:43.167536: Epoch: 1, Batch: 737, Loss: 0.6785, Elapsed: 5m36s
2020-05-20 10:23:29.336467: Epoch: 1, Batch: 738, Loss: 0.6709, Elapsed: 7m46s
2020-05-20 10:28:21.330036: Epoch: 1, Batch: 739, Loss: 0.6644, Elapsed: 4m51s
2020-05-20 10:31:49.087816: Epoch: 1, Batch: 740, Loss: 0.6740, Elapsed: 3m27s
2020-05-20 10:35:32.200057: Epoch: 1, Batch: 741, Loss: 0.6850, Elapsed: 3m43s
2020-05-20 10:41:17.774624: Epoch: 1, Batch: 742, Loss: 0.6903, Elapsed: 5m45s
2020-05-20 10:44:24.951924: Epoch: 1, Batch: 743, Loss: 0.6521, Elapsed: 3m7s
2020-05-20 10:49:17.636609: Epoch: 1, Batch: 744, Loss: 0.6847, Elapsed: 4m52s
2020-05-20 10:53:33.366622: Epoch: 1, Batch: 745, Loss: 0.6765, Elapsed: 4m15s
2020-05-20 10:58:23.234378: Epoch: 1, Batch: 746, Loss: 0.6805, Elapsed: 4m49s
2020-05-20 11:02:21.916727: Epoch: 1, Batch: 747, Loss: 0.6664, Elapsed: 3m58s
2020-05-20 11:06:37.484394: Epoch: 1, Batch: 748, Loss: 0.6860, Elapsed: 4m15s
2020-05-20 11:11:53.421503: Epoch: 1, Batch: 749, Loss: 0.6982, Elapsed: 5m15s
2020-05-20 11:17:06.240747: Epoch: 1, Batch: 750, Loss: 0.6918, Elapsed: 5m12s
Starting testing the validation set with 200 subgraphs!
2020-05-20 11:48:23.738374: Validation Test:  Loss: 0.6796,  Acc: 56.6480, AUC: 0.5890, Precision: 0.6428 -- Elapsed: 31m17s
2020-05-20 11:53:32.822360: Epoch: 1, Batch: 751, Loss: 0.6819, Elapsed: 5m9s
2020-05-20 11:58:04.546676: Epoch: 1, Batch: 752, Loss: 0.6751, Elapsed: 4m31s
2020-05-20 12:01:55.380914: Epoch: 1, Batch: 753, Loss: 0.6808, Elapsed: 3m50s
2020-05-20 12:05:55.479250: Epoch: 1, Batch: 754, Loss: 0.6790, Elapsed: 4m0s
2020-05-20 12:08:51.659965: Epoch: 1, Batch: 755, Loss: 0.6633, Elapsed: 2m56s
2020-05-20 12:14:56.789157: Epoch: 1, Batch: 756, Loss: 0.7213, Elapsed: 6m5s
2020-05-20 12:18:11.903233: Epoch: 1, Batch: 757, Loss: 0.6740, Elapsed: 3m15s
2020-05-20 12:21:43.694977: Epoch: 1, Batch: 758, Loss: 0.6768, Elapsed: 3m31s
2020-05-20 12:28:07.433146: Epoch: 1, Batch: 759, Loss: 0.6744, Elapsed: 6m23s
2020-05-20 12:32:39.057106: Epoch: 1, Batch: 760, Loss: 0.6713, Elapsed: 4m31s
2020-05-20 12:36:14.974437: Epoch: 1, Batch: 761, Loss: 0.6855, Elapsed: 3m35s
2020-05-20 12:40:37.811175: Epoch: 1, Batch: 762, Loss: 0.6745, Elapsed: 4m22s
2020-05-20 12:43:50.232808: Epoch: 1, Batch: 763, Loss: 0.6527, Elapsed: 3m12s
2020-05-20 12:47:35.417775: Epoch: 1, Batch: 764, Loss: 0.6683, Elapsed: 3m45s
2020-05-20 12:50:32.876781: Epoch: 1, Batch: 765, Loss: 0.6540, Elapsed: 2m57s
2020-05-20 12:53:03.653312: Epoch: 1, Batch: 766, Loss: 0.6591, Elapsed: 2m30s
2020-05-20 12:57:41.806432: Epoch: 1, Batch: 767, Loss: 0.6905, Elapsed: 4m38s
2020-05-20 13:00:19.847222: Epoch: 1, Batch: 768, Loss: 0.6592, Elapsed: 2m38s
2020-05-20 13:05:06.067241: Epoch: 1, Batch: 769, Loss: 0.6688, Elapsed: 4m46s
2020-05-20 13:07:30.506265: Epoch: 1, Batch: 770, Loss: 0.6659, Elapsed: 2m24s
2020-05-20 13:14:16.988501: Epoch: 1, Batch: 771, Loss: 0.7647, Elapsed: 6m46s
2020-05-20 13:18:21.897299: Epoch: 1, Batch: 772, Loss: 0.6524, Elapsed: 4m4s
2020-05-20 13:21:42.321854: Epoch: 1, Batch: 773, Loss: 0.6853, Elapsed: 3m20s
2020-05-20 13:25:09.981957: Epoch: 1, Batch: 774, Loss: 0.6537, Elapsed: 3m27s
2020-05-20 13:30:35.229508: Epoch: 1, Batch: 775, Loss: 0.6748, Elapsed: 5m25s
2020-05-20 13:37:25.048594: Epoch: 1, Batch: 776, Loss: 0.6743, Elapsed: 6m49s
2020-05-20 13:41:43.996046: Epoch: 1, Batch: 777, Loss: 0.6812, Elapsed: 4m18s
2020-05-20 13:45:23.535952: Epoch: 1, Batch: 778, Loss: 0.6770, Elapsed: 3m39s
2020-05-20 13:49:19.890728: Epoch: 1, Batch: 779, Loss: 0.6966, Elapsed: 3m56s
2020-05-20 13:53:22.574456: Epoch: 1, Batch: 780, Loss: 0.6990, Elapsed: 4m2s
2020-05-20 14:00:14.367870: Epoch: 1, Batch: 781, Loss: 0.6757, Elapsed: 6m51s
2020-05-20 14:04:43.381314: Epoch: 1, Batch: 782, Loss: 0.6758, Elapsed: 4m28s
2020-05-20 14:07:19.251323: Epoch: 1, Batch: 783, Loss: 0.6870, Elapsed: 2m35s
2020-05-20 14:12:33.956865: Epoch: 1, Batch: 784, Loss: 0.6676, Elapsed: 5m14s
2020-05-20 14:18:36.479447: Epoch: 1, Batch: 785, Loss: 0.6781, Elapsed: 6m2s
2020-05-20 14:20:58.702217: Epoch: 1, Batch: 786, Loss: 0.7187, Elapsed: 2m22s
2020-05-20 14:25:02.205401: Epoch: 1, Batch: 787, Loss: 0.6887, Elapsed: 4m3s
2020-05-20 14:30:28.328128: Epoch: 1, Batch: 788, Loss: 0.6616, Elapsed: 5m26s
2020-05-20 14:36:14.803090: Epoch: 1, Batch: 789, Loss: 0.6792, Elapsed: 5m46s
2020-05-20 14:41:16.518217: Epoch: 1, Batch: 790, Loss: 0.6830, Elapsed: 5m1s
2020-05-20 14:45:38.513038: Epoch: 1, Batch: 791, Loss: 0.6663, Elapsed: 4m21s
2020-05-20 14:48:07.941304: Epoch: 1, Batch: 792, Loss: 0.6833, Elapsed: 2m29s
2020-05-20 14:51:29.933783: Epoch: 1, Batch: 793, Loss: 0.6686, Elapsed: 3m21s
2020-05-20 14:54:50.341119: Epoch: 1, Batch: 794, Loss: 0.6821, Elapsed: 3m20s
2020-05-20 14:58:56.328932: Epoch: 1, Batch: 795, Loss: 0.6806, Elapsed: 4m5s
2020-05-20 15:02:19.656732: Epoch: 1, Batch: 796, Loss: 0.6597, Elapsed: 3m23s
2020-05-20 15:07:53.008553: Epoch: 1, Batch: 797, Loss: 0.6968, Elapsed: 5m33s
2020-05-20 15:11:58.049846: Epoch: 1, Batch: 798, Loss: 0.6805, Elapsed: 4m5s
2020-05-20 15:15:58.952850: Epoch: 1, Batch: 799, Loss: 0.6690, Elapsed: 4m0s
2020-05-20 15:20:15.634935: Epoch: 1, Batch: 800, Loss: 0.6849, Elapsed: 4m16s
Starting testing the validation set with 200 subgraphs!
2020-05-20 15:51:42.554740: Validation Test:  Loss: 0.6731,  Acc: 60.5210, AUC: 0.6279, Precision: 0.6558 -- Elapsed: 31m26s
2020-05-20 15:56:46.229717: Epoch: 1, Batch: 801, Loss: 0.6661, Elapsed: 5m3s
2020-05-20 16:01:14.039328: Epoch: 1, Batch: 802, Loss: 0.7068, Elapsed: 4m27s
2020-05-20 16:05:19.981282: Epoch: 1, Batch: 803, Loss: 0.6386, Elapsed: 4m5s
2020-05-20 16:08:49.529717: Epoch: 1, Batch: 804, Loss: 0.6381, Elapsed: 3m29s
2020-05-20 16:17:25.454844: Epoch: 1, Batch: 805, Loss: 0.6803, Elapsed: 8m35s
2020-05-20 16:23:47.190762: Epoch: 1, Batch: 806, Loss: 0.6896, Elapsed: 6m21s
2020-05-20 16:28:18.874711: Epoch: 1, Batch: 807, Loss: 0.6813, Elapsed: 4m31s
2020-05-20 16:31:08.933219: Epoch: 1, Batch: 808, Loss: 0.6821, Elapsed: 2m50s
2020-05-20 16:34:51.590281: Epoch: 1, Batch: 809, Loss: 0.6441, Elapsed: 3m42s
2020-05-20 16:37:27.503703: Epoch: 1, Batch: 810, Loss: 0.6522, Elapsed: 2m35s
2020-05-20 16:41:11.609005: Epoch: 1, Batch: 811, Loss: 0.6802, Elapsed: 3m44s
2020-05-20 16:44:47.106204: Epoch: 1, Batch: 812, Loss: 0.6559, Elapsed: 3m35s
2020-05-20 16:47:43.176362: Epoch: 1, Batch: 813, Loss: 0.6801, Elapsed: 2m56s
2020-05-20 16:50:21.140339: Epoch: 1, Batch: 814, Loss: 0.6914, Elapsed: 2m37s
2020-05-20 16:55:58.986551: Epoch: 1, Batch: 815, Loss: 0.6697, Elapsed: 5m37s
2020-05-20 17:01:36.941260: Epoch: 1, Batch: 816, Loss: 0.6730, Elapsed: 5m37s
2020-05-20 17:05:52.563897: Epoch: 1, Batch: 817, Loss: 0.6821, Elapsed: 4m15s
2020-05-20 17:09:46.883074: Epoch: 1, Batch: 818, Loss: 0.6737, Elapsed: 3m54s
2020-05-20 17:16:56.947871: Epoch: 1, Batch: 819, Loss: 0.6877, Elapsed: 7m10s
2020-05-20 17:21:20.190178: Epoch: 1, Batch: 820, Loss: 0.6839, Elapsed: 4m23s
2020-05-20 17:25:41.333990: Epoch: 1, Batch: 821, Loss: 0.6745, Elapsed: 4m21s
2020-05-20 17:29:19.618276: Epoch: 1, Batch: 822, Loss: 0.6650, Elapsed: 3m38s
2020-05-20 17:31:36.921416: Epoch: 1, Batch: 823, Loss: 0.6483, Elapsed: 2m17s
2020-05-20 17:35:12.236994: Epoch: 1, Batch: 824, Loss: 0.6686, Elapsed: 3m35s
2020-05-20 17:40:08.221772: Epoch: 1, Batch: 825, Loss: 0.6830, Elapsed: 4m55s
2020-05-20 17:43:24.511989: Epoch: 1, Batch: 826, Loss: 0.6837, Elapsed: 3m16s
2020-05-20 17:47:48.508610: Epoch: 1, Batch: 827, Loss: 0.6629, Elapsed: 4m23s
2020-05-20 17:51:59.497087: Epoch: 1, Batch: 828, Loss: 0.6791, Elapsed: 4m10s
2020-05-20 17:55:02.735624: Epoch: 1, Batch: 829, Loss: 0.6792, Elapsed: 3m3s
2020-05-20 18:01:26.713074: Epoch: 1, Batch: 830, Loss: 0.6643, Elapsed: 6m23s
2020-05-20 18:05:56.701957: Epoch: 1, Batch: 831, Loss: 0.6684, Elapsed: 4m29s
2020-05-20 18:10:28.388102: Epoch: 1, Batch: 832, Loss: 0.6582, Elapsed: 4m31s
2020-05-20 18:15:51.813435: Epoch: 1, Batch: 833, Loss: 0.6701, Elapsed: 5m23s
2020-05-20 18:18:30.350652: Epoch: 1, Batch: 834, Loss: 0.6261, Elapsed: 2m38s
2020-05-20 18:22:02.486871: Epoch: 1, Batch: 835, Loss: 0.6771, Elapsed: 3m32s
2020-05-20 18:27:03.310103: Epoch: 1, Batch: 836, Loss: 0.6721, Elapsed: 5m0s
2020-05-20 18:31:18.308826: Epoch: 1, Batch: 837, Loss: 0.6306, Elapsed: 4m14s
2020-05-20 18:34:49.224332: Epoch: 1, Batch: 838, Loss: 0.6380, Elapsed: 3m30s
2020-05-20 18:39:51.616385: Epoch: 1, Batch: 839, Loss: 0.6748, Elapsed: 5m2s
2020-05-20 18:43:55.640125: Epoch: 1, Batch: 840, Loss: 0.6788, Elapsed: 4m3s
2020-05-20 18:46:34.030251: Epoch: 1, Batch: 841, Loss: 0.6507, Elapsed: 2m38s
2020-05-20 18:51:27.567968: Epoch: 1, Batch: 842, Loss: 0.6643, Elapsed: 4m53s
2020-05-20 18:54:51.897640: Epoch: 1, Batch: 843, Loss: 0.6637, Elapsed: 3m24s
2020-05-20 18:58:10.817419: Epoch: 1, Batch: 844, Loss: 0.6878, Elapsed: 3m18s
2020-05-20 19:02:35.698148: Epoch: 1, Batch: 845, Loss: 0.6600, Elapsed: 4m24s
2020-05-20 19:06:18.383125: Epoch: 1, Batch: 846, Loss: 0.6494, Elapsed: 3m42s
2020-05-20 19:09:49.282574: Epoch: 1, Batch: 847, Loss: 0.6454, Elapsed: 3m30s
2020-05-20 19:14:44.176419: Epoch: 1, Batch: 848, Loss: 0.6768, Elapsed: 4m54s
2020-05-20 19:17:57.646681: Epoch: 1, Batch: 849, Loss: 0.6419, Elapsed: 3m13s
2020-05-20 19:22:10.693189: Epoch: 1, Batch: 850, Loss: 0.6539, Elapsed: 4m13s
Starting testing the validation set with 200 subgraphs!
2020-05-20 19:53:38.452153: Validation Test:  Loss: 0.6667,  Acc: 60.2755, AUC: 0.6312, Precision: 0.6518 -- Elapsed: 31m27s
2020-05-20 20:02:21.932071: Epoch: 1, Batch: 851, Loss: 0.6723, Elapsed: 8m43s
2020-05-20 20:06:08.369764: Epoch: 1, Batch: 852, Loss: 0.6366, Elapsed: 3m46s
2020-05-20 20:10:03.334904: Epoch: 1, Batch: 853, Loss: 0.6551, Elapsed: 3m54s
2020-05-20 20:15:29.754864: Epoch: 1, Batch: 854, Loss: 0.6785, Elapsed: 5m26s
2020-05-20 20:21:30.317709: Epoch: 1, Batch: 855, Loss: 0.6668, Elapsed: 6m0s
2020-05-20 20:27:50.456427: Epoch: 1, Batch: 856, Loss: 0.6751, Elapsed: 6m20s
2020-05-20 20:33:39.314879: Epoch: 1, Batch: 857, Loss: 0.7104, Elapsed: 5m48s
2020-05-20 20:36:42.969618: Epoch: 1, Batch: 858, Loss: 0.6403, Elapsed: 3m3s
2020-05-20 20:41:45.359571: Epoch: 1, Batch: 859, Loss: 0.6758, Elapsed: 5m2s
2020-05-20 20:47:50.600635: Epoch: 1, Batch: 860, Loss: 0.6654, Elapsed: 6m5s
2020-05-20 20:53:33.700063: Epoch: 1, Batch: 861, Loss: 0.6728, Elapsed: 5m43s
2020-05-20 20:57:32.806185: Epoch: 1, Batch: 862, Loss: 0.6736, Elapsed: 3m59s
2020-05-20 21:01:55.824078: Epoch: 1, Batch: 863, Loss: 0.6764, Elapsed: 4m22s
2020-05-20 21:06:29.995978: Epoch: 1, Batch: 864, Loss: 0.6635, Elapsed: 4m34s
2020-05-20 21:09:58.612035: Epoch: 1, Batch: 865, Loss: 0.6743, Elapsed: 3m28s
2020-05-20 21:14:07.474985: Epoch: 1, Batch: 866, Loss: 0.6707, Elapsed: 4m8s
2020-05-20 21:17:43.808087: Epoch: 1, Batch: 867, Loss: 0.6795, Elapsed: 3m36s
2020-05-20 21:24:28.126057: Epoch: 1, Batch: 868, Loss: 0.6735, Elapsed: 6m44s
2020-05-20 21:28:24.144900: Epoch: 1, Batch: 869, Loss: 0.6634, Elapsed: 3m55s
2020-05-20 21:31:45.475833: Epoch: 1, Batch: 870, Loss: 0.6630, Elapsed: 3m21s
2020-05-20 21:34:48.920945: Epoch: 1, Batch: 871, Loss: 0.6584, Elapsed: 3m3s
2020-05-20 21:38:04.370933: Epoch: 1, Batch: 872, Loss: 0.6474, Elapsed: 3m15s
2020-05-20 21:44:26.684675: Epoch: 1, Batch: 873, Loss: 0.6687, Elapsed: 6m22s
2020-05-20 21:48:34.113903: Epoch: 1, Batch: 874, Loss: 0.6568, Elapsed: 4m7s
2020-05-20 21:51:44.085563: Epoch: 1, Batch: 875, Loss: 0.6517, Elapsed: 3m9s
2020-05-20 21:56:37.923458: Epoch: 1, Batch: 876, Loss: 0.6713, Elapsed: 4m53s
2020-05-20 21:59:18.201470: Epoch: 1, Batch: 877, Loss: 0.6475, Elapsed: 2m40s
2020-05-20 22:03:18.622583: Epoch: 1, Batch: 878, Loss: 0.6587, Elapsed: 4m0s
2020-05-20 22:06:52.521800: Epoch: 1, Batch: 879, Loss: 0.6574, Elapsed: 3m33s
2020-05-20 22:10:43.464984: Epoch: 1, Batch: 880, Loss: 0.6620, Elapsed: 3m50s
2020-05-20 22:15:14.680017: Epoch: 1, Batch: 881, Loss: 0.6645, Elapsed: 4m31s
2020-05-20 22:19:15.326736: Epoch: 1, Batch: 882, Loss: 0.6686, Elapsed: 4m0s
2020-05-20 22:23:48.353277: Epoch: 1, Batch: 883, Loss: 0.6855, Elapsed: 4m33s
2020-05-20 22:26:56.454403: Epoch: 1, Batch: 884, Loss: 0.6512, Elapsed: 3m8s
2020-05-20 22:30:48.722609: Epoch: 1, Batch: 885, Loss: 0.6708, Elapsed: 3m52s
2020-05-20 22:33:19.636312: Epoch: 1, Batch: 886, Loss: 0.6622, Elapsed: 2m30s
2020-05-20 22:38:43.111773: Epoch: 1, Batch: 887, Loss: 0.6822, Elapsed: 5m23s
2020-05-20 22:42:15.305013: Epoch: 1, Batch: 888, Loss: 0.6742, Elapsed: 3m32s
2020-05-20 22:45:43.200591: Epoch: 1, Batch: 889, Loss: 0.6302, Elapsed: 3m27s
2020-05-20 22:50:53.655163: Epoch: 1, Batch: 890, Loss: 0.6539, Elapsed: 5m10s
2020-05-20 22:53:17.901158: Epoch: 1, Batch: 891, Loss: 0.6450, Elapsed: 2m24s
2020-05-20 22:57:13.649028: Epoch: 1, Batch: 892, Loss: 0.6448, Elapsed: 3m55s
2020-05-20 23:01:45.262567: Epoch: 1, Batch: 893, Loss: 0.6705, Elapsed: 4m31s
2020-05-20 23:05:14.004654: Epoch: 1, Batch: 894, Loss: 0.6529, Elapsed: 3m28s
2020-05-20 23:09:08.135571: Epoch: 1, Batch: 895, Loss: 0.6852, Elapsed: 3m54s
2020-05-20 23:12:15.625978: Epoch: 1, Batch: 896, Loss: 0.6660, Elapsed: 3m7s
2020-05-20 23:16:58.517898: Epoch: 1, Batch: 897, Loss: 0.6778, Elapsed: 4m42s
2020-05-20 23:20:44.049112: Epoch: 1, Batch: 898, Loss: 0.6644, Elapsed: 3m45s
2020-05-20 23:23:08.199480: Epoch: 1, Batch: 899, Loss: 0.6802, Elapsed: 2m24s
2020-05-20 23:27:43.953433: Epoch: 1, Batch: 900, Loss: 0.6746, Elapsed: 4m35s
Starting testing the validation set with 200 subgraphs!
2020-05-20 23:59:08.587139: Validation Test:  Loss: 0.6690,  Acc: 60.2745, AUC: 0.6233, Precision: 0.6413 -- Elapsed: 31m24s
2020-05-21 00:02:43.057890: Epoch: 1, Batch: 901, Loss: 0.6594, Elapsed: 3m34s
2020-05-21 00:06:32.744322: Epoch: 1, Batch: 902, Loss: 0.6506, Elapsed: 3m49s
2020-05-21 00:09:37.051300: Epoch: 1, Batch: 903, Loss: 0.6506, Elapsed: 3m4s
2020-05-21 00:12:13.202108: Epoch: 1, Batch: 904, Loss: 0.6631, Elapsed: 2m36s
2020-05-21 00:15:49.700395: Epoch: 1, Batch: 905, Loss: 0.6481, Elapsed: 3m36s
2020-05-21 00:18:05.953556: Epoch: 1, Batch: 906, Loss: 0.6670, Elapsed: 2m16s
2020-05-21 00:21:42.394577: Epoch: 1, Batch: 907, Loss: 0.6761, Elapsed: 3m36s
2020-05-21 00:25:33.481686: Epoch: 1, Batch: 908, Loss: 0.6644, Elapsed: 3m51s
2020-05-21 00:28:25.859837: Epoch: 1, Batch: 909, Loss: 0.6662, Elapsed: 2m52s
2020-05-21 00:32:08.797873: Epoch: 1, Batch: 910, Loss: 0.6670, Elapsed: 3m42s
2020-05-21 00:35:45.997877: Epoch: 1, Batch: 911, Loss: 0.6655, Elapsed: 3m37s
2020-05-21 00:40:30.789453: Epoch: 1, Batch: 912, Loss: 0.6940, Elapsed: 4m44s
2020-05-21 00:44:22.563674: Epoch: 1, Batch: 913, Loss: 0.6377, Elapsed: 3m51s
2020-05-21 00:50:18.975039: Epoch: 1, Batch: 914, Loss: 0.6803, Elapsed: 5m56s
2020-05-21 00:53:17.857351: Epoch: 1, Batch: 915, Loss: 0.6480, Elapsed: 2m58s
2020-05-21 00:58:19.680957: Epoch: 1, Batch: 916, Loss: 0.6650, Elapsed: 5m1s
2020-05-21 01:04:40.590933: Epoch: 1, Batch: 917, Loss: 0.6970, Elapsed: 6m20s
2020-05-21 01:09:03.080092: Epoch: 1, Batch: 918, Loss: 0.6502, Elapsed: 4m22s
2020-05-21 01:13:00.457899: Epoch: 1, Batch: 919, Loss: 0.6503, Elapsed: 3m57s
2020-05-21 01:17:15.960190: Epoch: 1, Batch: 920, Loss: 0.6698, Elapsed: 4m15s
2020-05-21 01:20:08.819321: Epoch: 1, Batch: 921, Loss: 0.6491, Elapsed: 2m52s
2020-05-21 01:27:16.606562: Epoch: 1, Batch: 922, Loss: 0.6900, Elapsed: 7m7s
2020-05-21 01:29:45.019648: Epoch: 1, Batch: 923, Loss: 0.6242, Elapsed: 2m28s
2020-05-21 01:33:05.283729: Epoch: 1, Batch: 924, Loss: 0.6388, Elapsed: 3m20s
2020-05-21 01:36:27.030687: Epoch: 1, Batch: 925, Loss: 0.6427, Elapsed: 3m21s
2020-05-21 01:39:25.841340: Epoch: 1, Batch: 926, Loss: 0.6333, Elapsed: 2m58s
2020-05-21 01:42:51.275328: Epoch: 1, Batch: 927, Loss: 0.6267, Elapsed: 3m25s
2020-05-21 01:45:08.552881: Epoch: 1, Batch: 928, Loss: 0.6511, Elapsed: 2m17s
2020-05-21 01:50:33.597476: Epoch: 1, Batch: 929, Loss: 0.6866, Elapsed: 5m25s
2020-05-21 01:56:17.853769: Epoch: 1, Batch: 930, Loss: 0.6851, Elapsed: 5m44s
2020-05-21 02:02:50.128712: Epoch: 1, Batch: 931, Loss: 0.6730, Elapsed: 6m32s
2020-05-21 02:10:28.499573: Epoch: 1, Batch: 932, Loss: 0.6811, Elapsed: 7m38s
2020-05-21 02:14:54.779838: Epoch: 1, Batch: 933, Loss: 0.6772, Elapsed: 4m26s
2020-05-21 02:17:07.992240: Epoch: 1, Batch: 934, Loss: 0.6296, Elapsed: 2m13s
2020-05-21 02:19:33.252842: Epoch: 1, Batch: 935, Loss: 0.6740, Elapsed: 2m25s
2020-05-21 02:26:01.014516: Epoch: 1, Batch: 936, Loss: 0.6793, Elapsed: 6m27s
2020-05-21 02:31:23.590917: Epoch: 1, Batch: 937, Loss: 0.6729, Elapsed: 5m22s
2020-05-21 02:35:02.983089: Epoch: 1, Batch: 938, Loss: 0.6963, Elapsed: 3m39s
2020-05-21 02:39:43.277843: Epoch: 1, Batch: 939, Loss: 0.6525, Elapsed: 4m40s
2020-05-21 02:43:16.993733: Epoch: 1, Batch: 940, Loss: 0.6398, Elapsed: 3m33s
2020-05-21 02:50:02.331204: Epoch: 1, Batch: 941, Loss: 0.6821, Elapsed: 6m45s
2020-05-21 02:52:21.445944: Epoch: 1, Batch: 942, Loss: 0.6300, Elapsed: 2m19s
2020-05-21 02:55:43.015260: Epoch: 1, Batch: 943, Loss: 0.6714, Elapsed: 3m21s
2020-05-21 02:58:57.829189: Epoch: 1, Batch: 944, Loss: 0.6749, Elapsed: 3m14s
2020-05-21 03:03:49.986073: Epoch: 1, Batch: 945, Loss: 0.6674, Elapsed: 4m52s
2020-05-21 03:06:13.523858: Epoch: 1, Batch: 946, Loss: 0.6310, Elapsed: 2m23s
2020-05-21 03:10:46.535864: Epoch: 1, Batch: 947, Loss: 0.6544, Elapsed: 4m32s
2020-05-21 03:14:04.276882: Epoch: 1, Batch: 948, Loss: 0.6287, Elapsed: 3m17s
2020-05-21 03:19:21.280824: Epoch: 1, Batch: 949, Loss: 0.6835, Elapsed: 5m16s
2020-05-21 03:22:08.416516: Epoch: 1, Batch: 950, Loss: 0.6589, Elapsed: 2m47s
Starting testing the validation set with 200 subgraphs!
2020-05-21 03:53:34.730833: Validation Test:  Loss: 0.6645,  Acc: 61.2270, AUC: 0.6329, Precision: 0.6466 -- Elapsed: 31m26s
2020-05-21 03:56:46.316837: Epoch: 1, Batch: 951, Loss: 0.6495, Elapsed: 3m11s
2020-05-21 04:00:42.961559: Epoch: 1, Batch: 952, Loss: 0.6556, Elapsed: 3m56s
2020-05-21 04:04:53.939057: Epoch: 1, Batch: 953, Loss: 0.6609, Elapsed: 4m10s
2020-05-21 04:09:07.509875: Epoch: 1, Batch: 954, Loss: 0.6588, Elapsed: 4m13s
2020-05-21 04:13:17.140245: Epoch: 1, Batch: 955, Loss: 0.6528, Elapsed: 4m9s
2020-05-21 04:16:49.970398: Epoch: 1, Batch: 956, Loss: 0.6634, Elapsed: 3m32s
2020-05-21 04:23:56.565959: Epoch: 1, Batch: 957, Loss: 0.6748, Elapsed: 7m6s
2020-05-21 04:27:06.876426: Epoch: 1, Batch: 958, Loss: 0.6503, Elapsed: 3m10s
2020-05-21 04:33:05.303883: Epoch: 1, Batch: 959, Loss: 0.6853, Elapsed: 5m58s
2020-05-21 04:37:59.512119: Epoch: 1, Batch: 960, Loss: 0.6620, Elapsed: 4m54s
2020-05-21 04:41:03.096257: Epoch: 1, Batch: 961, Loss: 0.6459, Elapsed: 3m3s
2020-05-21 04:45:25.103019: Epoch: 1, Batch: 962, Loss: 0.6367, Elapsed: 4m21s
2020-05-21 04:47:29.746631: Epoch: 1, Batch: 963, Loss: 0.6179, Elapsed: 2m4s
2020-05-21 04:51:42.416032: Epoch: 1, Batch: 964, Loss: 0.6770, Elapsed: 4m12s
2020-05-21 04:57:00.790984: Epoch: 1, Batch: 965, Loss: 0.6743, Elapsed: 5m18s
2020-05-21 05:00:43.767298: Epoch: 1, Batch: 966, Loss: 0.6638, Elapsed: 3m42s
2020-05-21 05:04:36.979484: Epoch: 1, Batch: 967, Loss: 0.6612, Elapsed: 3m53s
2020-05-21 05:07:29.751416: Epoch: 1, Batch: 968, Loss: 0.6152, Elapsed: 2m52s
2020-05-21 05:11:10.540020: Epoch: 1, Batch: 969, Loss: 0.6605, Elapsed: 3m40s
2020-05-21 05:15:46.887950: Epoch: 1, Batch: 970, Loss: 0.6617, Elapsed: 4m36s
2020-05-21 05:22:16.352910: Epoch: 1, Batch: 971, Loss: 0.6894, Elapsed: 6m29s
2020-05-21 05:30:47.486017: Epoch: 1, Batch: 972, Loss: 0.7417, Elapsed: 8m31s
2020-05-21 05:35:06.268014: Epoch: 1, Batch: 973, Loss: 0.6667, Elapsed: 4m18s
2020-05-21 05:39:16.026563: Epoch: 1, Batch: 974, Loss: 0.6881, Elapsed: 4m9s
2020-05-21 05:43:21.319732: Epoch: 1, Batch: 975, Loss: 0.6693, Elapsed: 4m5s
2020-05-21 05:45:45.689726: Epoch: 1, Batch: 976, Loss: 0.6538, Elapsed: 2m24s
2020-05-21 05:48:29.704109: Epoch: 1, Batch: 977, Loss: 0.6533, Elapsed: 2m43s
2020-05-21 05:53:33.685720: Epoch: 1, Batch: 978, Loss: 0.6686, Elapsed: 5m3s
2020-05-21 05:56:34.430100: Epoch: 1, Batch: 979, Loss: 0.6449, Elapsed: 3m0s
2020-05-21 06:01:26.299048: Epoch: 1, Batch: 980, Loss: 0.6756, Elapsed: 4m51s
2020-05-21 06:04:40.170432: Epoch: 1, Batch: 981, Loss: 0.6666, Elapsed: 3m13s
2020-05-21 06:07:44.884920: Epoch: 1, Batch: 982, Loss: 0.6508, Elapsed: 3m4s
2020-05-21 06:12:48.501620: Epoch: 1, Batch: 983, Loss: 0.6725, Elapsed: 5m3s
2020-05-21 06:15:36.747267: Epoch: 1, Batch: 984, Loss: 0.6364, Elapsed: 2m48s
2020-05-21 06:18:46.168425: Epoch: 1, Batch: 985, Loss: 0.6703, Elapsed: 3m9s
2020-05-21 06:22:30.194863: Epoch: 1, Batch: 986, Loss: 0.6608, Elapsed: 3m44s
2020-05-21 06:25:39.206376: Epoch: 1, Batch: 987, Loss: 0.6496, Elapsed: 3m8s
2020-05-21 06:29:03.106225: Epoch: 1, Batch: 988, Loss: 0.6495, Elapsed: 3m23s
2020-05-21 06:31:30.220506: Epoch: 1, Batch: 989, Loss: 0.6344, Elapsed: 2m27s
2020-05-21 06:39:35.466033: Epoch: 1, Batch: 990, Loss: 0.6836, Elapsed: 8m5s
2020-05-21 06:43:47.319148: Epoch: 1, Batch: 991, Loss: 0.6671, Elapsed: 4m11s
2020-05-21 06:47:17.110113: Epoch: 1, Batch: 992, Loss: 0.6579, Elapsed: 3m29s
2020-05-21 06:51:51.742359: Epoch: 1, Batch: 993, Loss: 0.7117, Elapsed: 4m34s
2020-05-21 06:55:51.327503: Epoch: 1, Batch: 994, Loss: 0.6766, Elapsed: 3m59s
2020-05-21 07:00:02.504003: Epoch: 1, Batch: 995, Loss: 0.6624, Elapsed: 4m11s
2020-05-21 07:05:00.161681: Epoch: 1, Batch: 996, Loss: 0.6821, Elapsed: 4m57s
2020-05-21 07:08:21.329612: Epoch: 1, Batch: 997, Loss: 0.6519, Elapsed: 3m21s
2020-05-21 07:14:44.776926: Epoch: 1, Batch: 998, Loss: 0.6803, Elapsed: 6m23s
2020-05-21 07:17:02.679395: Epoch: 1, Batch: 999, Loss: 0.6447, Elapsed: 2m17s
2020-05-21 07:22:28.350226: Epoch: 1, Batch: 1000, Loss: 0.6742, Elapsed: 5m25s
Starting testing the validation set with 200 subgraphs!
2020-05-21 07:53:40.096357: Validation Test:  Loss: 0.6794,  Acc: 56.7372, AUC: 0.5838, Precision: 0.6260 -- Elapsed: 31m11s
2020-05-21 07:58:49.626838: Epoch: 1, Batch: 1001, Loss: 0.6860, Elapsed: 5m9s
2020-05-21 08:02:00.445714: Epoch: 1, Batch: 1002, Loss: 0.6510, Elapsed: 3m10s
2020-05-21 08:06:46.529353: Epoch: 1, Batch: 1003, Loss: 0.6805, Elapsed: 4m46s
2020-05-21 08:10:56.759118: Epoch: 1, Batch: 1004, Loss: 0.6575, Elapsed: 4m10s
2020-05-21 08:14:47.605939: Epoch: 1, Batch: 1005, Loss: 0.6751, Elapsed: 3m50s
2020-05-21 08:20:35.292284: Epoch: 1, Batch: 1006, Loss: 0.6789, Elapsed: 5m47s
2020-05-21 08:25:15.422531: Epoch: 1, Batch: 1007, Loss: 0.6819, Elapsed: 4m40s
2020-05-21 08:29:25.386134: Epoch: 1, Batch: 1008, Loss: 0.6791, Elapsed: 4m9s
2020-05-21 08:33:04.138576: Epoch: 1, Batch: 1009, Loss: 0.6702, Elapsed: 3m38s
2020-05-21 08:36:05.990837: Epoch: 1, Batch: 1010, Loss: 0.6711, Elapsed: 3m1s
2020-05-21 08:41:39.174472: Epoch: 1, Batch: 1011, Loss: 0.6859, Elapsed: 5m33s
2020-05-21 08:44:26.912919: Epoch: 1, Batch: 1012, Loss: 0.6823, Elapsed: 2m47s
2020-05-21 08:48:57.993526: Epoch: 1, Batch: 1013, Loss: 0.6776, Elapsed: 4m31s
2020-05-21 08:54:15.581834: Epoch: 1, Batch: 1014, Loss: 0.6973, Elapsed: 5m17s
2020-05-21 08:57:09.290062: Epoch: 1, Batch: 1015, Loss: 0.6656, Elapsed: 2m53s
2020-05-21 09:02:10.639148: Epoch: 1, Batch: 1016, Loss: 0.6746, Elapsed: 5m1s
2020-05-21 09:05:57.419612: Epoch: 1, Batch: 1017, Loss: 0.6639, Elapsed: 3m46s
2020-05-21 09:13:18.479146: Epoch: 1, Batch: 1018, Loss: 0.6858, Elapsed: 7m21s
2020-05-21 09:17:15.661667: Epoch: 1, Batch: 1019, Loss: 0.6906, Elapsed: 3m57s
2020-05-21 09:22:11.813914: Epoch: 1, Batch: 1020, Loss: 0.6762, Elapsed: 4m56s
2020-05-21 09:27:03.022396: Epoch: 1, Batch: 1021, Loss: 0.6888, Elapsed: 4m51s
2020-05-21 09:30:00.416165: Epoch: 1, Batch: 1022, Loss: 0.6582, Elapsed: 2m57s
2020-05-21 09:34:30.991342: Epoch: 1, Batch: 1023, Loss: 0.6832, Elapsed: 4m30s
2020-05-21 09:39:15.943473: Epoch: 1, Batch: 1024, Loss: 0.6781, Elapsed: 4m44s
2020-05-21 09:41:50.550919: Epoch: 1, Batch: 1025, Loss: 0.6743, Elapsed: 2m34s
2020-05-21 09:44:07.777185: Epoch: 1, Batch: 1026, Loss: 0.6497, Elapsed: 2m17s
2020-05-21 09:48:06.421602: Epoch: 1, Batch: 1027, Loss: 0.6840, Elapsed: 3m58s
2020-05-21 09:53:40.646072: Epoch: 1, Batch: 1028, Loss: 0.6753, Elapsed: 5m34s
2020-05-21 09:58:25.412747: Epoch: 1, Batch: 1029, Loss: 0.6794, Elapsed: 4m44s
2020-05-21 10:00:55.854560: Epoch: 1, Batch: 1030, Loss: 0.6599, Elapsed: 2m30s
2020-05-21 10:05:22.147190: Epoch: 1, Batch: 1031, Loss: 0.6812, Elapsed: 4m26s
2020-05-21 10:08:30.524434: Epoch: 1, Batch: 1032, Loss: 0.6769, Elapsed: 3m8s
2020-05-21 10:10:55.509071: Epoch: 1, Batch: 1033, Loss: 0.6684, Elapsed: 2m24s
2020-05-21 10:13:22.598165: Epoch: 1, Batch: 1034, Loss: 0.6659, Elapsed: 2m27s
2020-05-21 10:16:41.033722: Epoch: 1, Batch: 1035, Loss: 0.6736, Elapsed: 3m18s
2020-05-21 10:21:39.665628: Epoch: 1, Batch: 1036, Loss: 0.6788, Elapsed: 4m58s
2020-05-21 10:25:01.554250: Epoch: 1, Batch: 1037, Loss: 0.6710, Elapsed: 3m21s
2020-05-21 10:30:39.738625: Epoch: 1, Batch: 1038, Loss: 0.6808, Elapsed: 5m38s
2020-05-21 10:33:52.995249: Epoch: 1, Batch: 1039, Loss: 0.6802, Elapsed: 3m13s
2020-05-21 10:39:01.271958: Epoch: 1, Batch: 1040, Loss: 0.6717, Elapsed: 5m8s
2020-05-21 10:42:56.255355: Epoch: 1, Batch: 1041, Loss: 0.6748, Elapsed: 3m54s
2020-05-21 10:47:03.894976: Epoch: 1, Batch: 1042, Loss: 0.6752, Elapsed: 4m7s
2020-05-21 10:52:53.526568: Epoch: 1, Batch: 1043, Loss: 0.6670, Elapsed: 5m49s
2020-05-21 10:57:14.417801: Epoch: 1, Batch: 1044, Loss: 0.6678, Elapsed: 4m20s
2020-05-21 11:01:20.318558: Epoch: 1, Batch: 1045, Loss: 0.6629, Elapsed: 4m5s
2020-05-21 11:04:50.426135: Epoch: 1, Batch: 1046, Loss: 0.6603, Elapsed: 3m30s
2020-05-21 11:11:54.722791: Epoch: 1, Batch: 1047, Loss: 0.6682, Elapsed: 7m4s
2020-05-21 11:15:04.225665: Epoch: 1, Batch: 1048, Loss: 0.6631, Elapsed: 3m9s
2020-05-21 11:19:12.375909: Epoch: 1, Batch: 1049, Loss: 0.6529, Elapsed: 4m8s
2020-05-21 11:25:04.861843: Epoch: 1, Batch: 1050, Loss: 0.6717, Elapsed: 5m52s
Starting testing the validation set with 200 subgraphs!
2020-05-21 11:56:22.917057: Validation Test:  Loss: 0.6663,  Acc: 60.0523, AUC: 0.6367, Precision: 0.6642 -- Elapsed: 31m18s
2020-05-21 12:00:43.931094: Epoch: 1, Batch: 1051, Loss: 0.6592, Elapsed: 4m21s
2020-05-21 12:03:58.052999: Epoch: 1, Batch: 1052, Loss: 0.6631, Elapsed: 3m14s
2020-05-21 12:08:44.067353: Epoch: 1, Batch: 1053, Loss: 0.6648, Elapsed: 4m45s
2020-05-21 12:12:12.542835: Epoch: 1, Batch: 1054, Loss: 0.6574, Elapsed: 3m28s
2020-05-21 12:15:42.398817: Epoch: 1, Batch: 1055, Loss: 0.6537, Elapsed: 3m29s
2020-05-21 12:21:20.750683: Epoch: 1, Batch: 1056, Loss: 0.6793, Elapsed: 5m38s
2020-05-21 12:27:07.751159: Epoch: 1, Batch: 1057, Loss: 0.6739, Elapsed: 5m46s
2020-05-21 12:31:58.536688: Epoch: 1, Batch: 1058, Loss: 0.6659, Elapsed: 4m50s
2020-05-21 12:34:10.388063: Epoch: 1, Batch: 1059, Loss: 0.6443, Elapsed: 2m11s
2020-05-21 12:37:43.669048: Epoch: 1, Batch: 1060, Loss: 0.6696, Elapsed: 3m33s
2020-05-21 12:42:36.673632: Epoch: 1, Batch: 1061, Loss: 0.6697, Elapsed: 4m52s
2020-05-21 12:46:41.031263: Epoch: 1, Batch: 1062, Loss: 0.6685, Elapsed: 4m4s
2020-05-21 12:51:04.608760: Epoch: 1, Batch: 1063, Loss: 0.6773, Elapsed: 4m23s
2020-05-21 12:53:45.865861: Epoch: 1, Batch: 1064, Loss: 0.6641, Elapsed: 2m41s
2020-05-21 12:57:56.199271: Epoch: 1, Batch: 1065, Loss: 0.6613, Elapsed: 4m10s
2020-05-21 13:00:42.765763: Epoch: 1, Batch: 1066, Loss: 0.6522, Elapsed: 2m46s
2020-05-21 13:04:20.731446: Epoch: 1, Batch: 1067, Loss: 0.6439, Elapsed: 3m37s
2020-05-21 13:06:16.208662: Epoch: 1, Batch: 1068, Loss: 0.6477, Elapsed: 1m55s
2020-05-21 13:12:52.814110: Epoch: 1, Batch: 1069, Loss: 0.6858, Elapsed: 6m36s
2020-05-21 13:16:22.259199: Epoch: 1, Batch: 1070, Loss: 0.6509, Elapsed: 3m29s
2020-05-21 13:20:34.796735: Epoch: 1, Batch: 1071, Loss: 0.6572, Elapsed: 4m12s
2020-05-21 13:26:06.671321: Epoch: 1, Batch: 1072, Loss: 0.6649, Elapsed: 5m31s
2020-05-21 13:30:37.687370: Epoch: 1, Batch: 1073, Loss: 0.6554, Elapsed: 4m30s
2020-05-21 13:34:25.808498: Epoch: 1, Batch: 1074, Loss: 0.6670, Elapsed: 3m48s
2020-05-21 13:39:29.415966: Epoch: 1, Batch: 1075, Loss: 0.6723, Elapsed: 5m3s
2020-05-21 13:43:11.210886: Epoch: 1, Batch: 1076, Loss: 0.6487, Elapsed: 3m41s
2020-05-21 13:46:55.741971: Epoch: 1, Batch: 1077, Loss: 0.6423, Elapsed: 3m44s
2020-05-21 13:49:05.742883: Epoch: 1, Batch: 1078, Loss: 0.6049, Elapsed: 2m9s
2020-05-21 13:54:37.827230: Epoch: 1, Batch: 1079, Loss: 0.6480, Elapsed: 5m32s
2020-05-21 13:56:29.291439: Epoch: 1, Batch: 1080, Loss: 0.6064, Elapsed: 1m51s
2020-05-21 14:00:57.881195: Epoch: 1, Batch: 1081, Loss: 0.6798, Elapsed: 4m28s
2020-05-21 14:04:58.546220: Epoch: 1, Batch: 1082, Loss: 0.6686, Elapsed: 4m0s
2020-05-21 14:08:37.959342: Epoch: 1, Batch: 1083, Loss: 0.6436, Elapsed: 3m39s
2020-05-21 14:12:20.288198: Epoch: 1, Batch: 1084, Loss: 0.6352, Elapsed: 3m42s
2020-05-21 14:15:51.491891: Epoch: 1, Batch: 1085, Loss: 0.6613, Elapsed: 3m31s
2020-05-21 14:18:55.676385: Epoch: 1, Batch: 1086, Loss: 0.6285, Elapsed: 3m4s
2020-05-21 14:25:58.694229: Epoch: 1, Batch: 1087, Loss: 0.7067, Elapsed: 7m2s
2020-05-21 14:30:51.181632: Epoch: 1, Batch: 1088, Loss: 0.6698, Elapsed: 4m52s
2020-05-21 14:33:48.831393: Epoch: 1, Batch: 1089, Loss: 0.6654, Elapsed: 2m57s
2020-05-21 14:36:43.772050: Epoch: 1, Batch: 1090, Loss: 0.6283, Elapsed: 2m54s
2020-05-21 14:41:16.328719: Epoch: 1, Batch: 1091, Loss: 0.6493, Elapsed: 4m32s
2020-05-21 14:46:47.770727: Epoch: 1, Batch: 1092, Loss: 0.6577, Elapsed: 5m31s
2020-05-21 14:50:57.106250: Epoch: 1, Batch: 1093, Loss: 0.6609, Elapsed: 4m9s
2020-05-21 14:54:57.819029: Epoch: 1, Batch: 1094, Loss: 0.6376, Elapsed: 4m0s
2020-05-21 14:58:19.261121: Epoch: 1, Batch: 1095, Loss: 0.6384, Elapsed: 3m21s
2020-05-21 15:01:38.765828: Epoch: 1, Batch: 1096, Loss: 0.6403, Elapsed: 3m19s
2020-05-21 15:05:26.755137: Epoch: 1, Batch: 1097, Loss: 0.6424, Elapsed: 3m47s
2020-05-21 15:11:44.182148: Epoch: 1, Batch: 1098, Loss: 0.6748, Elapsed: 6m17s
2020-05-21 15:14:51.877284: Epoch: 1, Batch: 1099, Loss: 0.6543, Elapsed: 3m7s
2020-05-21 15:18:42.368414: Epoch: 1, Batch: 1100, Loss: 0.6500, Elapsed: 3m50s
Starting testing the validation set with 200 subgraphs!
2020-05-21 15:50:02.971817: Validation Test:  Loss: 0.6583,  Acc: 61.2757, AUC: 0.6442, Precision: 0.6714 -- Elapsed: 31m20s
2020-05-21 15:56:48.759523: Epoch: 1, Batch: 1101, Loss: 0.6559, Elapsed: 6m45s
2020-05-21 16:03:49.105042: Epoch: 1, Batch: 1102, Loss: 0.6908, Elapsed: 7m0s
2020-05-21 16:08:14.504049: Epoch: 1, Batch: 1103, Loss: 0.6574, Elapsed: 4m25s
2020-05-21 16:13:04.230069: Epoch: 1, Batch: 1104, Loss: 0.6597, Elapsed: 4m49s
2020-05-21 16:19:25.938526: Epoch: 1, Batch: 1105, Loss: 0.6734, Elapsed: 6m21s
2020-05-21 16:24:56.309543: Epoch: 1, Batch: 1106, Loss: 0.6588, Elapsed: 5m30s
2020-05-21 16:29:57.214872: Epoch: 1, Batch: 1107, Loss: 0.6860, Elapsed: 5m0s
2020-05-21 16:35:51.864352: Epoch: 1, Batch: 1108, Loss: 0.6582, Elapsed: 5m54s
2020-05-21 16:39:50.374586: Epoch: 1, Batch: 1109, Loss: 0.6381, Elapsed: 3m58s
2020-05-21 16:44:25.795240: Epoch: 1, Batch: 1110, Loss: 0.6575, Elapsed: 4m35s
2020-05-21 16:48:01.161607: Epoch: 1, Batch: 1111, Loss: 0.6541, Elapsed: 3m35s
2020-05-21 16:52:03.611413: Epoch: 1, Batch: 1112, Loss: 0.6725, Elapsed: 4m2s
2020-05-21 16:56:08.428359: Epoch: 1, Batch: 1113, Loss: 0.6668, Elapsed: 4m4s
2020-05-21 16:59:13.777129: Epoch: 1, Batch: 1114, Loss: 0.6525, Elapsed: 3m5s
2020-05-21 17:07:05.388317: Epoch: 1, Batch: 1115, Loss: 0.6871, Elapsed: 7m51s
2020-05-21 17:11:33.629353: Epoch: 1, Batch: 1116, Loss: 0.6476, Elapsed: 4m28s
2020-05-21 17:18:00.038109: Epoch: 1, Batch: 1117, Loss: 0.6651, Elapsed: 6m26s
2020-05-21 17:22:30.797157: Epoch: 1, Batch: 1118, Loss: 0.6647, Elapsed: 4m30s
2020-05-21 17:26:12.896542: Epoch: 1, Batch: 1119, Loss: 0.6621, Elapsed: 3m42s
2020-05-21 17:30:24.442607: Epoch: 1, Batch: 1120, Loss: 0.6438, Elapsed: 4m11s
2020-05-21 17:32:16.246888: Epoch: 1, Batch: 1121, Loss: 0.6484, Elapsed: 1m51s
2020-05-21 17:34:31.451946: Epoch: 1, Batch: 1122, Loss: 0.6323, Elapsed: 2m15s
2020-05-21 17:38:44.287895: Epoch: 1, Batch: 1123, Loss: 0.6367, Elapsed: 4m12s
2020-05-21 17:43:34.607588: Epoch: 1, Batch: 1124, Loss: 0.6491, Elapsed: 4m50s
2020-05-21 17:46:20.775394: Epoch: 1, Batch: 1125, Loss: 0.6482, Elapsed: 2m46s
2020-05-21 17:50:06.546096: Epoch: 1, Batch: 1126, Loss: 0.6485, Elapsed: 3m45s
2020-05-21 17:54:20.499626: Epoch: 1, Batch: 1127, Loss: 0.6712, Elapsed: 4m13s
2020-05-21 17:59:55.043152: Epoch: 1, Batch: 1128, Loss: 0.6563, Elapsed: 5m34s
2020-05-21 18:02:40.325365: Epoch: 1, Batch: 1129, Loss: 0.6282, Elapsed: 2m45s
2020-05-21 18:06:24.507030: Epoch: 1, Batch: 1130, Loss: 0.6441, Elapsed: 3m44s
2020-05-21 18:13:02.588119: Epoch: 1, Batch: 1131, Loss: 0.6640, Elapsed: 6m38s
2020-05-21 18:19:29.434466: Epoch: 1, Batch: 1132, Loss: 0.6680, Elapsed: 6m26s
2020-05-21 18:22:33.660312: Epoch: 1, Batch: 1133, Loss: 0.6416, Elapsed: 3m4s
2020-05-21 18:26:23.387673: Epoch: 1, Batch: 1134, Loss: 0.6589, Elapsed: 3m49s
2020-05-21 18:31:38.789542: Epoch: 1, Batch: 1135, Loss: 0.6750, Elapsed: 5m15s
2020-05-21 18:35:55.224494: Epoch: 1, Batch: 1136, Loss: 0.6297, Elapsed: 4m16s
2020-05-21 18:40:20.154780: Epoch: 1, Batch: 1137, Loss: 0.6536, Elapsed: 4m24s
2020-05-21 18:44:19.559526: Epoch: 1, Batch: 1138, Loss: 0.6651, Elapsed: 3m59s
2020-05-21 18:46:15.807339: Epoch: 1, Batch: 1139, Loss: 0.6203, Elapsed: 1m56s
2020-05-21 18:54:46.802245: Epoch: 1, Batch: 1140, Loss: 0.6759, Elapsed: 8m30s
2020-05-21 19:00:53.910077: Epoch: 1, Batch: 1141, Loss: 0.6719, Elapsed: 6m7s
2020-05-21 19:07:51.241067: Epoch: 1, Batch: 1142, Loss: 0.6645, Elapsed: 6m57s
2020-05-21 19:13:40.599295: Epoch: 1, Batch: 1143, Loss: 0.6530, Elapsed: 5m49s
2020-05-21 19:20:51.530524: Epoch: 1, Batch: 1144, Loss: 0.6923, Elapsed: 7m10s
2020-05-21 19:25:45.585313: Epoch: 1, Batch: 1145, Loss: 0.6215, Elapsed: 4m54s
2020-05-21 19:34:47.558196: Epoch: 1, Batch: 1146, Loss: 0.6805, Elapsed: 9m1s
2020-05-21 19:44:21.926099: Epoch: 1, Batch: 1147, Loss: 0.6946, Elapsed: 9m34s
2020-05-21 19:51:00.782434: Epoch: 1, Batch: 1148, Loss: 0.6545, Elapsed: 6m38s
2020-05-21 19:55:23.497718: Epoch: 1, Batch: 1149, Loss: 0.6330, Elapsed: 4m22s
2020-05-21 20:03:02.215952: Epoch: 1, Batch: 1150, Loss: 0.6601, Elapsed: 7m38s
Starting testing the validation set with 200 subgraphs!
2020-05-21 20:44:49.584208: Validation Test:  Loss: 0.6607,  Acc: 60.3638, AUC: 0.6378, Precision: 0.6622 -- Elapsed: 41m47s
2020-05-21 20:51:00.795666: Epoch: 1, Batch: 1151, Loss: 0.6439, Elapsed: 6m11s
2020-05-21 20:57:33.749769: Epoch: 1, Batch: 1152, Loss: 0.6672, Elapsed: 6m32s
2020-05-21 21:04:42.079089: Epoch: 1, Batch: 1153, Loss: 0.6370, Elapsed: 7m8s
2020-05-21 21:08:52.725008: Epoch: 1, Batch: 1154, Loss: 0.6098, Elapsed: 4m10s
2020-05-21 21:11:33.487175: Epoch: 1, Batch: 1155, Loss: 0.6366, Elapsed: 2m40s
2020-05-21 21:17:59.851696: Epoch: 1, Batch: 1156, Loss: 0.6411, Elapsed: 6m26s
2020-05-21 21:26:30.469289: Epoch: 1, Batch: 1157, Loss: 0.6429, Elapsed: 8m30s
2020-05-21 21:32:40.236385: Epoch: 1, Batch: 1158, Loss: 0.6594, Elapsed: 6m9s
2020-05-21 21:36:36.431512: Epoch: 1, Batch: 1159, Loss: 0.6262, Elapsed: 3m56s
2020-05-21 21:41:44.866127: Epoch: 1, Batch: 1160, Loss: 0.6376, Elapsed: 5m8s
2020-05-21 21:45:13.157989: Epoch: 1, Batch: 1161, Loss: 0.6190, Elapsed: 3m28s
2020-05-21 21:49:46.353445: Epoch: 1, Batch: 1162, Loss: 0.6586, Elapsed: 4m33s
2020-05-21 21:54:39.869822: Epoch: 1, Batch: 1163, Loss: 0.6334, Elapsed: 4m53s
2020-05-21 22:01:43.013381: Epoch: 1, Batch: 1164, Loss: 0.6690, Elapsed: 7m3s
2020-05-21 22:10:07.022782: Epoch: 1, Batch: 1165, Loss: 0.6638, Elapsed: 8m23s
2020-05-21 22:20:30.090973: Epoch: 1, Batch: 1166, Loss: 0.6912, Elapsed: 10m23s
2020-05-21 22:27:08.135565: Epoch: 1, Batch: 1167, Loss: 0.6422, Elapsed: 6m38s
2020-05-21 22:36:43.754953: Epoch: 1, Batch: 1168, Loss: 0.6882, Elapsed: 9m35s
2020-05-21 22:45:49.718318: Epoch: 1, Batch: 1169, Loss: 0.6662, Elapsed: 9m5s
2020-05-21 22:53:13.492955: Epoch: 1, Batch: 1170, Loss: 0.6567, Elapsed: 7m23s
2020-05-21 23:01:50.870511: Epoch: 1, Batch: 1171, Loss: 0.6823, Elapsed: 8m37s
2020-05-21 23:09:41.478074: Epoch: 1, Batch: 1172, Loss: 0.6298, Elapsed: 7m50s
2020-05-21 23:19:18.437195: Epoch: 1, Batch: 1173, Loss: 0.6734, Elapsed: 9m36s
2020-05-21 23:27:01.825803: Epoch: 1, Batch: 1174, Loss: 0.6724, Elapsed: 7m43s
2020-05-21 23:34:13.723437: Epoch: 1, Batch: 1175, Loss: 0.6428, Elapsed: 7m11s
2020-05-21 23:44:19.194850: Epoch: 1, Batch: 1176, Loss: 0.6804, Elapsed: 10m5s
2020-05-21 23:49:37.790340: Epoch: 1, Batch: 1177, Loss: 0.6521, Elapsed: 5m18s
2020-05-21 23:54:59.985448: Epoch: 1, Batch: 1178, Loss: 0.6634, Elapsed: 5m22s
2020-05-22 00:00:26.530855: Epoch: 1, Batch: 1179, Loss: 0.6376, Elapsed: 5m26s
2020-05-22 00:08:19.634223: Epoch: 1, Batch: 1180, Loss: 0.6575, Elapsed: 7m53s
2020-05-22 00:13:44.711076: Epoch: 1, Batch: 1181, Loss: 0.6256, Elapsed: 5m25s
2020-05-22 00:20:35.029513: Epoch: 1, Batch: 1182, Loss: 0.6802, Elapsed: 6m50s
2020-05-22 00:23:43.051221: Epoch: 1, Batch: 1183, Loss: 0.6190, Elapsed: 3m8s
2020-05-22 00:33:06.783723: Epoch: 1, Batch: 1184, Loss: 0.6926, Elapsed: 9m23s
2020-05-22 00:37:09.632184: Epoch: 1, Batch: 1185, Loss: 0.6497, Elapsed: 4m2s
2020-05-22 00:45:08.076220: Epoch: 1, Batch: 1186, Loss: 0.6448, Elapsed: 7m58s
2020-05-22 00:49:57.830544: Epoch: 1, Batch: 1187, Loss: 0.6356, Elapsed: 4m49s
2020-05-22 00:55:26.398137: Epoch: 1, Batch: 1188, Loss: 0.6279, Elapsed: 5m28s
2020-05-22 01:03:34.860015: Epoch: 1, Batch: 1189, Loss: 0.6239, Elapsed: 8m8s
2020-05-22 01:09:07.057050: Epoch: 1, Batch: 1190, Loss: 0.6446, Elapsed: 5m32s
2020-05-22 01:12:54.690271: Epoch: 1, Batch: 1191, Loss: 0.6699, Elapsed: 3m47s
2020-05-22 01:19:25.970852: Epoch: 1, Batch: 1192, Loss: 0.6325, Elapsed: 6m31s
2020-05-22 01:27:01.388966: Epoch: 1, Batch: 1193, Loss: 0.6432, Elapsed: 7m35s
2020-05-22 01:31:33.019274: Epoch: 1, Batch: 1194, Loss: 0.6300, Elapsed: 4m31s
2020-05-22 01:39:07.839695: Epoch: 1, Batch: 1195, Loss: 0.6404, Elapsed: 7m34s
2020-05-22 01:43:45.910494: Epoch: 1, Batch: 1196, Loss: 0.6439, Elapsed: 4m38s
2020-05-22 01:50:16.418957: Epoch: 1, Batch: 1197, Loss: 0.6690, Elapsed: 6m30s
2020-05-22 01:54:09.515500: Epoch: 1, Batch: 1198, Loss: 0.6424, Elapsed: 3m53s
2020-05-22 02:01:46.129066: Epoch: 1, Batch: 1199, Loss: 0.6551, Elapsed: 7m36s
2020-05-22 02:15:47.285549: Epoch: 1, Batch: 1200, Loss: 0.6635, Elapsed: 14m1s
Starting testing the validation set with 200 subgraphs!
2020-05-22 02:57:38.818921: Validation Test:  Loss: 0.6572,  Acc: 60.6529, AUC: 0.6468, Precision: 0.6621 -- Elapsed: 41m51s
2020-05-22 03:04:45.558437: Epoch: 1, Batch: 1201, Loss: 0.6606, Elapsed: 7m6s
2020-05-22 03:11:32.789687: Epoch: 1, Batch: 1202, Loss: 0.6646, Elapsed: 6m47s
2020-05-22 03:16:24.273841: Epoch: 1, Batch: 1203, Loss: 0.6313, Elapsed: 4m51s
2020-05-22 03:26:44.966052: Epoch: 1, Batch: 1204, Loss: 0.6614, Elapsed: 10m20s
2020-05-22 03:33:08.863914: Epoch: 1, Batch: 1205, Loss: 0.6555, Elapsed: 6m23s
2020-05-22 03:39:25.772642: Epoch: 1, Batch: 1206, Loss: 0.6779, Elapsed: 6m16s
2020-05-22 03:44:45.022453: Epoch: 1, Batch: 1207, Loss: 0.6582, Elapsed: 5m19s
2020-05-22 03:52:47.598856: Epoch: 1, Batch: 1208, Loss: 0.6734, Elapsed: 8m2s
2020-05-22 03:56:27.451280: Epoch: 1, Batch: 1209, Loss: 0.6197, Elapsed: 3m39s
2020-05-22 04:02:09.393328: Epoch: 1, Batch: 1210, Loss: 0.6575, Elapsed: 5m41s
2020-05-22 04:05:27.141538: Epoch: 1, Batch: 1211, Loss: 0.6287, Elapsed: 3m17s
2020-05-22 04:08:44.840629: Epoch: 1, Batch: 1212, Loss: 0.6179, Elapsed: 3m17s
2020-05-22 04:13:20.752477: Epoch: 1, Batch: 1213, Loss: 0.6253, Elapsed: 4m35s
2020-05-22 04:20:55.702241: Epoch: 1, Batch: 1214, Loss: 0.6667, Elapsed: 7m34s
2020-05-22 04:48:39.279125: Epoch: 1, Batch: 1215, Loss: 0.8855, Elapsed: 27m43s
2020-05-22 04:56:06.932008: Epoch: 1, Batch: 1216, Loss: 0.6612, Elapsed: 7m27s
2020-05-22 05:02:55.247402: Epoch: 1, Batch: 1217, Loss: 0.7161, Elapsed: 6m48s
2020-05-22 05:07:15.689104: Epoch: 1, Batch: 1218, Loss: 0.6439, Elapsed: 4m20s
2020-05-22 05:14:33.065442: Epoch: 1, Batch: 1219, Loss: 0.6973, Elapsed: 7m17s
2020-05-22 05:18:39.317991: Epoch: 1, Batch: 1220, Loss: 0.6733, Elapsed: 4m6s
2020-05-22 05:24:49.300688: Epoch: 1, Batch: 1221, Loss: 0.7094, Elapsed: 6m9s
2020-05-22 05:32:44.595072: Epoch: 1, Batch: 1222, Loss: 0.7286, Elapsed: 7m55s
2020-05-22 05:38:02.592723: Epoch: 1, Batch: 1223, Loss: 0.7063, Elapsed: 5m17s
2020-05-22 05:43:24.723672: Epoch: 1, Batch: 1224, Loss: 0.6861, Elapsed: 5m22s
2020-05-22 05:47:59.293662: Epoch: 1, Batch: 1225, Loss: 0.7546, Elapsed: 4m34s
2020-05-22 05:51:33.513552: Epoch: 1, Batch: 1226, Loss: 0.7390, Elapsed: 3m34s
2020-05-22 05:56:10.144371: Epoch: 1, Batch: 1227, Loss: 0.7459, Elapsed: 4m36s
2020-05-22 06:01:01.016432: Epoch: 1, Batch: 1228, Loss: 0.7686, Elapsed: 4m50s
2020-05-22 06:05:43.286098: Epoch: 1, Batch: 1229, Loss: 0.7855, Elapsed: 4m42s
2020-05-22 06:12:17.424786: Epoch: 1, Batch: 1230, Loss: 0.7379, Elapsed: 6m34s
2020-05-22 06:24:09.111671: Epoch: 1, Batch: 1231, Loss: 0.7530, Elapsed: 11m51s
2020-05-22 06:30:42.466849: Epoch: 1, Batch: 1232, Loss: 0.7356, Elapsed: 6m33s
2020-05-22 06:36:52.206659: Epoch: 1, Batch: 1233, Loss: 0.7383, Elapsed: 6m9s
2020-05-22 06:42:28.121331: Epoch: 1, Batch: 1234, Loss: 0.7295, Elapsed: 5m35s
2020-05-22 06:46:46.532114: Epoch: 1, Batch: 1235, Loss: 0.7355, Elapsed: 4m18s
2020-05-22 06:52:30.401890: Epoch: 1, Batch: 1236, Loss: 0.7244, Elapsed: 5m43s
2020-05-22 06:57:49.315633: Epoch: 1, Batch: 1237, Loss: 0.7198, Elapsed: 5m18s
2020-05-22 07:06:08.181832: Epoch: 1, Batch: 1238, Loss: 0.7396, Elapsed: 8m18s
2020-05-22 07:12:28.050493: Epoch: 1, Batch: 1239, Loss: 0.7142, Elapsed: 6m19s
2020-05-22 07:19:27.661985: Epoch: 1, Batch: 1240, Loss: 0.7188, Elapsed: 6m59s
2020-05-22 07:24:28.350563: Epoch: 1, Batch: 1241, Loss: 0.7459, Elapsed: 5m0s
2020-05-22 07:30:36.983835: Epoch: 1, Batch: 1242, Loss: 0.7003, Elapsed: 6m8s
2020-05-22 07:36:08.676069: Epoch: 1, Batch: 1243, Loss: 0.7409, Elapsed: 5m31s
2020-05-22 07:41:34.584894: Epoch: 1, Batch: 1244, Loss: 0.7367, Elapsed: 5m25s
2020-05-22 07:48:44.130086: Epoch: 1, Batch: 1245, Loss: 0.7344, Elapsed: 7m9s
2020-05-22 07:58:04.626932: Epoch: 1, Batch: 1246, Loss: 0.7011, Elapsed: 9m20s
2020-05-22 08:06:11.831119: Epoch: 1, Batch: 1247, Loss: 0.6914, Elapsed: 8m7s
2020-05-22 08:15:48.081352: Epoch: 1, Batch: 1248, Loss: 0.7388, Elapsed: 9m36s
2020-05-22 08:21:36.119625: Epoch: 1, Batch: 1249, Loss: 0.7310, Elapsed: 5m48s
2020-05-22 08:27:53.553843: Epoch: 1, Batch: 1250, Loss: 0.6934, Elapsed: 6m17s
Starting testing the validation set with 200 subgraphs!
2020-05-22 09:09:44.248272: Validation Test:  Loss: 0.7048,  Acc: 55.3211, AUC: 0.5764, Precision: 0.6201 -- Elapsed: 41m50s
2020-05-22 09:13:27.649930: Epoch: 1, Batch: 1251, Loss: 0.7093, Elapsed: 3m43s
2020-05-22 09:18:44.099695: Epoch: 1, Batch: 1252, Loss: 0.7309, Elapsed: 5m16s
2020-05-22 09:23:41.256278: Epoch: 1, Batch: 1253, Loss: 0.7422, Elapsed: 4m57s
2020-05-22 09:29:03.261553: Epoch: 1, Batch: 1254, Loss: 0.6911, Elapsed: 5m21s
2020-05-22 09:35:37.046629: Epoch: 1, Batch: 1255, Loss: 0.7160, Elapsed: 6m33s
2020-05-22 09:42:28.952247: Epoch: 1, Batch: 1256, Loss: 0.7309, Elapsed: 6m51s
2020-05-22 09:50:54.163231: Epoch: 1, Batch: 1257, Loss: 0.7318, Elapsed: 8m25s
2020-05-22 10:00:29.954794: Epoch: 1, Batch: 1258, Loss: 0.7305, Elapsed: 9m35s
2020-05-22 10:08:22.858936: Epoch: 1, Batch: 1259, Loss: 0.7331, Elapsed: 7m52s
2020-05-22 10:12:10.622988: Epoch: 1, Batch: 1260, Loss: 0.7209, Elapsed: 3m47s
2020-05-22 10:17:45.274026: Epoch: 1, Batch: 1261, Loss: 0.6984, Elapsed: 5m34s
2020-05-22 10:24:45.370373: Epoch: 1, Batch: 1262, Loss: 0.7226, Elapsed: 7m0s
2020-05-22 10:33:36.677545: Epoch: 1, Batch: 1263, Loss: 0.7237, Elapsed: 8m51s
2020-05-22 10:38:56.552275: Epoch: 1, Batch: 1264, Loss: 0.7055, Elapsed: 5m19s
2020-05-22 10:44:44.042055: Epoch: 1, Batch: 1265, Loss: 0.7174, Elapsed: 5m47s
2020-05-22 10:50:57.958281: Epoch: 1, Batch: 1266, Loss: 0.7235, Elapsed: 6m13s
2020-05-22 10:56:11.721695: Epoch: 1, Batch: 1267, Loss: 0.7479, Elapsed: 5m13s
2020-05-22 11:06:13.366658: Epoch: 1, Batch: 1268, Loss: 0.7255, Elapsed: 10m1s
2020-05-22 11:14:59.064571: Epoch: 1, Batch: 1269, Loss: 0.7034, Elapsed: 8m45s
2020-05-22 11:19:49.673913: Epoch: 1, Batch: 1270, Loss: 0.7108, Elapsed: 4m50s
2020-05-22 11:25:47.783160: Epoch: 1, Batch: 1271, Loss: 0.7155, Elapsed: 5m58s
2020-05-22 11:32:12.071750: Epoch: 1, Batch: 1272, Loss: 0.7076, Elapsed: 6m24s
2020-05-22 11:38:02.659199: Epoch: 1, Batch: 1273, Loss: 0.7062, Elapsed: 5m50s
2020-05-22 11:42:40.231148: Epoch: 1, Batch: 1274, Loss: 0.7283, Elapsed: 4m37s
2020-05-22 11:48:16.229256: Epoch: 1, Batch: 1275, Loss: 0.7160, Elapsed: 5m35s
2020-05-22 11:54:37.674806: Epoch: 1, Batch: 1276, Loss: 0.7152, Elapsed: 6m21s
2020-05-22 12:02:29.191276: Epoch: 1, Batch: 1277, Loss: 0.6936, Elapsed: 7m51s
2020-05-22 12:07:21.606668: Epoch: 1, Batch: 1278, Loss: 0.7163, Elapsed: 4m52s
2020-05-22 12:11:47.832744: Epoch: 1, Batch: 1279, Loss: 0.7096, Elapsed: 4m26s
2020-05-22 12:19:47.501068: Epoch: 1, Batch: 1280, Loss: 0.7082, Elapsed: 7m59s
2020-05-22 12:25:27.190510: Epoch: 1, Batch: 1281, Loss: 0.7031, Elapsed: 5m39s
2020-05-22 12:36:02.879801: Epoch: 1, Batch: 1282, Loss: 0.7263, Elapsed: 10m35s
2020-05-22 12:43:23.758436: Epoch: 1, Batch: 1283, Loss: 0.6953, Elapsed: 7m20s
2020-05-22 12:50:27.195922: Epoch: 1, Batch: 1284, Loss: 0.7127, Elapsed: 7m3s
2020-05-22 12:56:16.021737: Epoch: 1, Batch: 1285, Loss: 0.7459, Elapsed: 5m48s
2020-05-22 13:02:41.391370: Epoch: 1, Batch: 1286, Loss: 0.7254, Elapsed: 6m25s
2020-05-22 13:11:01.322793: Epoch: 1, Batch: 1287, Loss: 0.7156, Elapsed: 8m19s
2020-05-22 13:16:53.946086: Epoch: 1, Batch: 1288, Loss: 0.6960, Elapsed: 5m52s
2020-05-22 13:24:31.832826: Epoch: 1, Batch: 1289, Loss: 0.7031, Elapsed: 7m37s
2020-05-22 13:29:37.097413: Epoch: 1, Batch: 1290, Loss: 0.7097, Elapsed: 5m5s
2020-05-22 13:34:22.573242: Epoch: 1, Batch: 1291, Loss: 0.6850, Elapsed: 4m45s
2020-05-22 13:38:32.132189: Epoch: 1, Batch: 1292, Loss: 0.7220, Elapsed: 4m9s
2020-05-22 13:44:16.770285: Epoch: 1, Batch: 1293, Loss: 0.7186, Elapsed: 5m44s
2020-05-22 13:50:52.487131: Epoch: 1, Batch: 1294, Loss: 0.7110, Elapsed: 6m35s
2020-05-22 13:58:12.411855: Epoch: 1, Batch: 1295, Loss: 0.7246, Elapsed: 7m19s
2020-05-22 14:06:51.332654: Epoch: 1, Batch: 1296, Loss: 0.7197, Elapsed: 8m38s
2020-05-22 14:13:00.071507: Epoch: 1, Batch: 1297, Loss: 0.7074, Elapsed: 6m8s
2020-05-22 14:17:42.856527: Epoch: 1, Batch: 1298, Loss: 0.6922, Elapsed: 4m42s
2020-05-22 14:21:56.764677: Epoch: 1, Batch: 1299, Loss: 0.6945, Elapsed: 4m13s
2020-05-22 14:27:59.622853: Epoch: 1, Batch: 1300, Loss: 0.7102, Elapsed: 6m2s
Starting testing the validation set with 200 subgraphs!
2020-05-22 15:10:01.674610: Validation Test:  Loss: 0.7058,  Acc: 49.9031, AUC: 0.4998, Precision: 0.5634 -- Elapsed: 42m2s
2020-05-22 15:17:00.565507: Epoch: 1, Batch: 1301, Loss: 0.7004, Elapsed: 6m58s
2020-05-22 15:21:55.439061: Epoch: 1, Batch: 1302, Loss: 0.7188, Elapsed: 4m54s
2020-05-22 15:30:15.521983: Epoch: 1, Batch: 1303, Loss: 0.7057, Elapsed: 8m20s
2020-05-22 15:36:09.594360: Epoch: 1, Batch: 1304, Loss: 0.7095, Elapsed: 5m54s
2020-05-22 15:42:34.230266: Epoch: 1, Batch: 1305, Loss: 0.7042, Elapsed: 6m24s
2020-05-22 15:52:11.273783: Epoch: 1, Batch: 1306, Loss: 0.7045, Elapsed: 9m37s
2020-05-22 16:01:35.220603: Epoch: 1, Batch: 1307, Loss: 0.7045, Elapsed: 9m23s
2020-05-22 16:10:55.484817: Epoch: 1, Batch: 1308, Loss: 0.6997, Elapsed: 9m20s
2020-05-22 16:16:35.692028: Epoch: 1, Batch: 1309, Loss: 0.7077, Elapsed: 5m40s
2020-05-22 16:22:35.519449: Epoch: 1, Batch: 1310, Loss: 0.7045, Elapsed: 5m59s
2020-05-22 16:29:44.974075: Epoch: 1, Batch: 1311, Loss: 0.7154, Elapsed: 7m9s
2020-05-22 16:37:26.147310: Epoch: 1, Batch: 1312, Loss: 0.7031, Elapsed: 7m41s
2020-05-22 16:43:25.152159: Epoch: 1, Batch: 1313, Loss: 0.7108, Elapsed: 5m58s
2020-05-22 16:50:35.207886: Epoch: 1, Batch: 1314, Loss: 0.7095, Elapsed: 7m10s
2020-05-22 16:58:12.386492: Epoch: 1, Batch: 1315, Loss: 0.7030, Elapsed: 7m37s
2020-05-22 17:05:13.444010: Epoch: 1, Batch: 1316, Loss: 0.7098, Elapsed: 7m1s
2020-05-22 17:10:07.580631: Epoch: 1, Batch: 1317, Loss: 0.7155, Elapsed: 4m54s
2020-05-22 17:14:35.515922: Epoch: 1, Batch: 1318, Loss: 0.7125, Elapsed: 4m27s
2020-05-22 17:19:03.663829: Epoch: 1, Batch: 1319, Loss: 0.6967, Elapsed: 4m28s
2020-05-22 17:25:34.880268: Epoch: 1, Batch: 1320, Loss: 0.6968, Elapsed: 6m31s
2020-05-22 17:34:06.930115: Epoch: 1, Batch: 1321, Loss: 0.6997, Elapsed: 8m32s
2020-05-22 17:38:15.683162: Epoch: 1, Batch: 1322, Loss: 0.7064, Elapsed: 4m8s
2020-05-22 17:42:20.102185: Epoch: 1, Batch: 1323, Loss: 0.7036, Elapsed: 4m4s
2020-05-22 17:50:21.277830: Epoch: 1, Batch: 1324, Loss: 0.6905, Elapsed: 8m1s
2020-05-22 17:58:57.383687: Epoch: 1, Batch: 1325, Loss: 0.6998, Elapsed: 8m36s
2020-05-22 18:05:57.161822: Epoch: 1, Batch: 1326, Loss: 0.6943, Elapsed: 6m59s
2020-05-22 18:14:18.734292: Epoch: 1, Batch: 1327, Loss: 0.7053, Elapsed: 8m21s
2020-05-22 18:19:23.422608: Epoch: 1, Batch: 1328, Loss: 0.7033, Elapsed: 5m4s
2020-05-22 18:24:55.771465: Epoch: 1, Batch: 1329, Loss: 0.7039, Elapsed: 5m32s
2020-05-22 18:33:49.685365: Epoch: 1, Batch: 1330, Loss: 0.6976, Elapsed: 8m53s
2020-05-22 18:37:41.998672: Epoch: 1, Batch: 1331, Loss: 0.6949, Elapsed: 3m52s
2020-05-22 18:41:20.240973: Epoch: 1, Batch: 1332, Loss: 0.7028, Elapsed: 3m38s
2020-05-22 18:47:43.219164: Epoch: 1, Batch: 1333, Loss: 0.6997, Elapsed: 6m22s
2020-05-22 18:52:27.601761: Epoch: 1, Batch: 1334, Loss: 0.6991, Elapsed: 4m44s
2020-05-22 18:56:06.322879: Epoch: 1, Batch: 1335, Loss: 0.6849, Elapsed: 3m38s
2020-05-22 19:02:01.999974: Epoch: 1, Batch: 1336, Loss: 0.6955, Elapsed: 5m55s
2020-05-22 19:09:41.545262: Epoch: 1, Batch: 1337, Loss: 0.6975, Elapsed: 7m39s
2020-05-22 19:14:20.656108: Epoch: 1, Batch: 1338, Loss: 0.6956, Elapsed: 4m39s
2020-05-22 19:21:22.707533: Epoch: 1, Batch: 1339, Loss: 0.6956, Elapsed: 7m2s
2020-05-22 19:25:43.424258: Epoch: 1, Batch: 1340, Loss: 0.7039, Elapsed: 4m20s
2020-05-22 19:30:34.345174: Epoch: 1, Batch: 1341, Loss: 0.7040, Elapsed: 4m50s
2020-05-22 19:36:32.379161: Epoch: 1, Batch: 1342, Loss: 0.6911, Elapsed: 5m58s
2020-05-22 19:42:08.118155: Epoch: 1, Batch: 1343, Loss: 0.6981, Elapsed: 5m35s
2020-05-22 19:49:20.205896: Epoch: 1, Batch: 1344, Loss: 0.7031, Elapsed: 7m12s
2020-05-22 19:54:18.210388: Epoch: 1, Batch: 1345, Loss: 0.6978, Elapsed: 4m57s
2020-05-22 19:59:43.156136: Epoch: 1, Batch: 1346, Loss: 0.7022, Elapsed: 5m24s
2020-05-22 20:04:18.665433: Epoch: 1, Batch: 1347, Loss: 0.6937, Elapsed: 4m35s
2020-05-22 20:08:34.168206: Epoch: 1, Batch: 1348, Loss: 0.7089, Elapsed: 4m15s
2020-05-22 20:20:52.634581: Epoch: 1, Batch: 1349, Loss: 0.7076, Elapsed: 12m18s
2020-05-22 20:25:18.408388: Epoch: 1, Batch: 1350, Loss: 0.7020, Elapsed: 4m25s
Starting testing the validation set with 200 subgraphs!
2020-05-22 21:07:30.310135: Validation Test:  Loss: 0.6986,  Acc: 49.6749, AUC: 0.4954, Precision: 0.5684 -- Elapsed: 42m11s
2020-05-22 21:13:39.505849: Epoch: 1, Batch: 1351, Loss: 0.7056, Elapsed: 6m9s
2020-05-22 21:19:18.434497: Epoch: 1, Batch: 1352, Loss: 0.7000, Elapsed: 5m38s
2020-05-22 21:29:16.321023: Epoch: 1, Batch: 1353, Loss: 0.6959, Elapsed: 9m57s
2020-05-22 21:36:29.551413: Epoch: 1, Batch: 1354, Loss: 0.7081, Elapsed: 7m13s
2020-05-22 21:44:57.980421: Epoch: 1, Batch: 1355, Loss: 0.6979, Elapsed: 8m28s
2020-05-22 21:50:27.540538: Epoch: 1, Batch: 1356, Loss: 0.6887, Elapsed: 5m29s
2020-05-22 21:57:30.058957: Epoch: 1, Batch: 1357, Loss: 0.6956, Elapsed: 7m2s
2020-05-22 22:08:11.336304: Epoch: 1, Batch: 1358, Loss: 0.6960, Elapsed: 10m41s
2020-05-22 22:13:25.057643: Epoch: 1, Batch: 1359, Loss: 0.6997, Elapsed: 5m13s
2020-05-22 22:19:33.383571: Epoch: 1, Batch: 1360, Loss: 0.7025, Elapsed: 6m8s
2020-05-22 22:24:33.714135: Epoch: 1, Batch: 1361, Loss: 0.6943, Elapsed: 5m0s
2020-05-22 22:28:33.548910: Epoch: 1, Batch: 1362, Loss: 0.6978, Elapsed: 3m59s
2020-05-22 22:33:22.042353: Epoch: 1, Batch: 1363, Loss: 0.7003, Elapsed: 4m48s
2020-05-22 22:43:32.043480: Epoch: 1, Batch: 1364, Loss: 0.6979, Elapsed: 10m9s
2020-05-22 22:50:53.684128: Epoch: 1, Batch: 1365, Loss: 0.6964, Elapsed: 7m21s
2020-05-22 22:56:12.099561: Epoch: 1, Batch: 1366, Loss: 0.6982, Elapsed: 5m18s
2020-05-22 23:04:15.108548: Epoch: 1, Batch: 1367, Loss: 0.6918, Elapsed: 8m2s
2020-05-22 23:09:01.386855: Epoch: 1, Batch: 1368, Loss: 0.6897, Elapsed: 4m46s
2020-05-22 23:16:16.368205: Epoch: 1, Batch: 1369, Loss: 0.7039, Elapsed: 7m14s
2020-05-22 23:25:28.495134: Epoch: 1, Batch: 1370, Loss: 0.6934, Elapsed: 9m12s
2020-05-22 23:32:51.542260: Epoch: 1, Batch: 1371, Loss: 0.6911, Elapsed: 7m23s
2020-05-22 23:39:47.544884: Epoch: 1, Batch: 1372, Loss: 0.6959, Elapsed: 6m55s
2020-05-22 23:43:49.381895: Epoch: 1, Batch: 1373, Loss: 0.7063, Elapsed: 4m1s
2020-05-22 23:50:47.054935: Epoch: 1, Batch: 1374, Loss: 0.7145, Elapsed: 6m57s
2020-05-22 23:57:23.124887: Epoch: 1, Batch: 1375, Loss: 0.6959, Elapsed: 6m36s
2020-05-23 00:05:44.713916: Epoch: 1, Batch: 1376, Loss: 0.6918, Elapsed: 8m21s
2020-05-23 00:11:30.162258: Epoch: 1, Batch: 1377, Loss: 0.7006, Elapsed: 5m45s
2020-05-23 00:18:14.117708: Epoch: 1, Batch: 1378, Loss: 0.7015, Elapsed: 6m43s
2020-05-23 00:24:24.340294: Epoch: 1, Batch: 1379, Loss: 0.7063, Elapsed: 6m10s
2020-05-23 00:31:09.576235: Epoch: 1, Batch: 1380, Loss: 0.7023, Elapsed: 6m45s
2020-05-23 00:38:42.916449: Epoch: 1, Batch: 1381, Loss: 0.6958, Elapsed: 7m33s
2020-05-23 00:47:33.355780: Epoch: 1, Batch: 1382, Loss: 0.6989, Elapsed: 8m50s
2020-05-23 00:55:15.907108: Epoch: 1, Batch: 1383, Loss: 0.7003, Elapsed: 7m42s
2020-05-23 01:05:51.081130: Epoch: 1, Batch: 1384, Loss: 0.7041, Elapsed: 10m35s
2020-05-23 01:10:41.545277: Epoch: 1, Batch: 1385, Loss: 0.6981, Elapsed: 4m50s
2020-05-23 01:15:33.482452: Epoch: 1, Batch: 1386, Loss: 0.7032, Elapsed: 4m51s
2020-05-23 01:22:41.195227: Epoch: 1, Batch: 1387, Loss: 0.6998, Elapsed: 7m7s
2020-05-23 01:25:19.616535: Epoch: 1, Batch: 1388, Loss: 0.7045, Elapsed: 2m38s
2020-05-23 01:33:10.240918: Epoch: 1, Batch: 1389, Loss: 0.7088, Elapsed: 7m50s
2020-05-23 01:36:17.116086: Epoch: 1, Batch: 1390, Loss: 0.6904, Elapsed: 3m6s
2020-05-23 01:42:48.759657: Epoch: 1, Batch: 1391, Loss: 0.7039, Elapsed: 6m31s
2020-05-23 01:47:25.275817: Epoch: 1, Batch: 1392, Loss: 0.7058, Elapsed: 4m36s
2020-05-23 01:52:05.613716: Epoch: 1, Batch: 1393, Loss: 0.7025, Elapsed: 4m40s
2020-05-23 01:57:17.072004: Epoch: 1, Batch: 1394, Loss: 0.6937, Elapsed: 5m11s
2020-05-23 02:04:08.247073: Epoch: 1, Batch: 1395, Loss: 0.6973, Elapsed: 6m51s
2020-05-23 02:13:13.656247: Epoch: 1, Batch: 1396, Loss: 0.7032, Elapsed: 9m5s
2020-05-23 02:19:26.538940: Epoch: 1, Batch: 1397, Loss: 0.7059, Elapsed: 6m12s
2020-05-23 02:28:26.826649: Epoch: 1, Batch: 1398, Loss: 0.7038, Elapsed: 9m0s
2020-05-23 02:36:45.372994: Epoch: 1, Batch: 1399, Loss: 0.7114, Elapsed: 8m18s
2020-05-23 02:42:16.384129: Epoch: 1, Batch: 1400, Loss: 0.6835, Elapsed: 5m30s
Starting testing the validation set with 200 subgraphs!
2020-05-23 03:24:21.649503: Validation Test:  Loss: 0.6996,  Acc: 50.2105, AUC: 0.5041, Precision: 0.5515 -- Elapsed: 42m5s
2020-05-23 03:24:21.649603: Training completed!
Singularity> [KSingularity> clear
[H[JSingularity> python3 train.py configs/lo  comparisons/qgnn/arch_comparison/TTN_general.py 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/arch_comparison/TTN_general/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN_general
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/arch_comparison/TTN_general/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
Traceback (most recent call last):
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 97, in decompose_queue
    new_ops.extend(_decompose_queue([op], device))
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 76, in _decompose_queue
    decomposition = _decompose_queue(decomposed_ops, device)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 75, in _decompose_queue
    decomposed_ops = op.decomposition(*op.params, wires=op.wires)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/operation.py", line 502, in decomposition
    raise NotImplementedError
NotImplementedError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 70, in <module>
    test_validation(config,block)
  File "/storage/user/ctuysuz/HepTrkX-quantum/test.py", line 22, in test_validation
    preds  = np.append(preds,network(graph_array))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN1_general.py", line 179, in call
    e = self.EdgeNet(H, Ri, Ro)         # execute EdgeNet
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN1_general.py", line 131, in call
    return edge_forward(B,self.theta_learn)
  File "/storage/user/ctuysuz/HepTrkX-quantum/qnetworks/GNN1_general.py", line 104, in edge_forward
    out = tf.constant((1-TTN_edge_forward(edge_array[i,:],theta_learn))/2.,dtype=tf.float64)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 256, in __call__
    return self._d(self._f, a, k)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 210, in decorated
    return _eager_mode_decorator(wrapped, args, kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 406, in _eager_mode_decorator
    result, grad_fn = f(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/interfaces/tf.py", line 77, in _TFQNode
    res = qnode(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 606, in __call__
    return self.evaluate(args, **kwargs)  # args as one tuple
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/autograd/tracer.py", line 48, in f_wrapped
    return f_raw(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 641, in evaluate
    self.construct(args, kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 354, in construct
    self.queue = decompose_queue(self.queue, self.device)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 99, in decompose_queue
    raise qml.DeviceError("Gate {} not supported on device {}".format(op.name, device.short_name))
pennylane._device.DeviceError: Gate U3 not supported on device qulacs.simulator
Singularity> clear
[H[JSingularity> python3 train.py configs/comparisons/qgnn/dimension_comparison/dim2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/dimension_comparison/dim2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 2
network: QGNN
n_thread: 4
log_verbosity: 1
Log dir: logs/comparisons/qgnn/dimension_comparison/dim2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
2020-05-27 02:30:36.035365 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/summary.csv
2020-05-27 02:30:36.035659 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_validation.csv
2020-05-27 02:30:36.035922 Deleted old log: logs/comparisons/qgnn/dimension_comparison/dim2/log_loss.csv
Starting testing the validation set with 200 subgraphs!
2020-05-27 03:29:02.195336: Validation Test:  Loss: 0.7999,  Acc: 55.1192, AUC: 0.5560, Precision: 0.6118 -- Elapsed: 58m25s
2020-05-27 03:29:02.195423: Training is starting!
2020-05-27 03:47:48.544027: Epoch: 1, Batch: 1, Loss: 0.7930, Elapsed: 18m46s
2020-05-27 04:00:43.369032: Epoch: 1, Batch: 2, Loss: 0.7830, Elapsed: 12m54s
2020-05-27 04:25:42.464036: Epoch: 1, Batch: 3, Loss: 0.7501, Elapsed: 24m59s
2020-05-27 04:40:57.708901: Epoch: 1, Batch: 4, Loss: 0.7887, Elapsed: 15m15s
2020-05-27 05:05:02.117609: Epoch: 1, Batch: 5, Loss: 0.7278, Elapsed: 24m4s
2020-05-27 05:21:36.075333: Epoch: 1, Batch: 6, Loss: 0.7495, Elapsed: 16m33s
2020-05-27 05:34:53.109164: Epoch: 1, Batch: 7, Loss: 0.7223, Elapsed: 13m17s
2020-05-27 05:46:55.289047: Epoch: 1, Batch: 8, Loss: 0.7515, Elapsed: 12m2s
2020-05-27 06:01:54.767982: Epoch: 1, Batch: 9, Loss: 0.7247, Elapsed: 14m59s
2020-05-27 06:18:22.552253: Epoch: 1, Batch: 10, Loss: 0.7489, Elapsed: 16m27s
2020-05-27 06:37:35.279282: Epoch: 1, Batch: 11, Loss: 0.7068, Elapsed: 19m12s
2020-05-27 06:56:41.126113: Epoch: 1, Batch: 12, Loss: 0.7272, Elapsed: 19m5s
2020-05-27 07:08:17.499400: Epoch: 1, Batch: 13, Loss: 0.7343, Elapsed: 11m36s
2020-05-27 07:27:05.635186: Epoch: 1, Batch: 14, Loss: 0.7062, Elapsed: 18m48s
2020-05-27 07:52:06.948579: Epoch: 1, Batch: 15, Loss: 0.6923, Elapsed: 25m1s
2020-05-27 08:09:27.007257: Epoch: 1, Batch: 16, Loss: 0.6996, Elapsed: 17m20s
2020-05-27 08:25:15.621909: Epoch: 1, Batch: 17, Loss: 0.6910, Elapsed: 15m48s
2020-05-27 08:39:56.081628: Epoch: 1, Batch: 18, Loss: 0.6995, Elapsed: 14m40s
2020-05-27 08:56:50.778476: Epoch: 1, Batch: 19, Loss: 0.7013, Elapsed: 16m54s
2020-05-27 09:13:59.372539: Epoch: 1, Batch: 20, Loss: 0.6605, Elapsed: 17m8s
2020-05-27 09:26:18.887241: Epoch: 1, Batch: 21, Loss: 0.6892, Elapsed: 12m19s
2020-05-27 09:32:29.184447: Epoch: 1, Batch: 22, Loss: 0.7116, Elapsed: 6m10s
2020-05-27 09:47:07.862277: Epoch: 1, Batch: 23, Loss: 0.6779, Elapsed: 14m38s
2020-05-27 10:08:15.447128: Epoch: 1, Batch: 24, Loss: 0.6994, Elapsed: 21m7s
2020-05-27 10:25:44.226396: Epoch: 1, Batch: 25, Loss: 0.7086, Elapsed: 17m28s
2020-05-27 10:39:56.884132: Epoch: 1, Batch: 26, Loss: 0.6766, Elapsed: 14m12s
2020-05-27 11:01:36.598349: Epoch: 1, Batch: 27, Loss: 0.6740, Elapsed: 21m39s
2020-05-27 11:21:54.615837: Epoch: 1, Batch: 28, Loss: 0.6786, Elapsed: 20m18s
2020-05-27 11:40:04.485048: Epoch: 1, Batch: 29, Loss: 0.6839, Elapsed: 18m9s
2020-05-27 11:52:54.635616: Epoch: 1, Batch: 30, Loss: 0.6949, Elapsed: 12m50s
2020-05-27 12:05:25.190872: Epoch: 1, Batch: 31, Loss: 0.6941, Elapsed: 12m30s
2020-05-27 12:17:59.954223: Epoch: 1, Batch: 32, Loss: 0.6734, Elapsed: 12m34s
2020-05-27 12:35:14.070097: Epoch: 1, Batch: 33, Loss: 0.6710, Elapsed: 17m14s
2020-05-27 12:52:37.948009: Epoch: 1, Batch: 34, Loss: 0.6889, Elapsed: 17m23s
2020-05-27 13:05:36.969190: Epoch: 1, Batch: 35, Loss: 0.6814, Elapsed: 12m59s
2020-05-27 13:26:07.682644: Epoch: 1, Batch: 36, Loss: 0.6780, Elapsed: 20m30s
2020-05-27 13:47:03.262553: Epoch: 1, Batch: 37, Loss: 0.6734, Elapsed: 20m55s
2020-05-27 14:08:28.235239: Epoch: 1, Batch: 38, Loss: 0.7084, Elapsed: 21m24s
2020-05-27 14:20:48.708380: Epoch: 1, Batch: 39, Loss: 0.6831, Elapsed: 12m20s
2020-05-27 14:40:44.371437: Epoch: 1, Batch: 40, Loss: 0.6786, Elapsed: 19m55s
2020-05-27 14:57:43.680628: Epoch: 1, Batch: 41, Loss: 0.6702, Elapsed: 16m59s
2020-05-27 15:15:46.966633: Epoch: 1, Batch: 42, Loss: 0.6797, Elapsed: 18m3s
2020-05-27 15:34:47.542771: Epoch: 1, Batch: 43, Loss: 0.6837, Elapsed: 19m0s
2020-05-27 15:52:02.773105: Epoch: 1, Batch: 44, Loss: 0.6765, Elapsed: 17m15s
2020-05-27 16:08:25.055140: Epoch: 1, Batch: 45, Loss: 0.6623, Elapsed: 16m22s
2020-05-27 16:24:14.176769: Epoch: 1, Batch: 46, Loss: 0.6812, Elapsed: 15m49s
2020-05-27 16:42:40.554401: Epoch: 1, Batch: 47, Loss: 0.6589, Elapsed: 18m26s
2020-05-27 16:55:07.595112: Epoch: 1, Batch: 48, Loss: 0.6494, Elapsed: 12m27s
2020-05-27 17:10:29.578406: Epoch: 1, Batch: 49, Loss: 0.6574, Elapsed: 15m21s
2020-05-27 17:25:38.011351: Epoch: 1, Batch: 50, Loss: 0.6746, Elapsed: 15m8s
Starting testing the validation set with 200 subgraphs!
2020-05-27 18:24:23.790383: Validation Test:  Loss: 0.6704,  Acc: 60.6275, AUC: 0.6327, Precision: 0.6648 -- Elapsed: 58m45s
2020-05-27 18:41:11.833137: Epoch: 1, Batch: 51, Loss: 0.6589, Elapsed: 16m48s
2020-05-27 18:51:11.637652: Epoch: 1, Batch: 52, Loss: 0.6694, Elapsed: 9m59s
2020-05-27 19:19:13.536298: Epoch: 1, Batch: 53, Loss: 0.6787, Elapsed: 28m1s
2020-05-27 19:37:15.937896: Epoch: 1, Batch: 54, Loss: 0.6817, Elapsed: 18m2s
2020-05-27 19:50:04.620081: Epoch: 1, Batch: 55, Loss: 0.6694, Elapsed: 12m48s
2020-05-27 20:02:46.775823: Epoch: 1, Batch: 56, Loss: 0.6512, Elapsed: 12m42s
2020-05-27 20:25:19.607564: Epoch: 1, Batch: 57, Loss: 0.7036, Elapsed: 22m32s
2020-05-27 20:33:41.638015: Epoch: 1, Batch: 58, Loss: 0.6716, Elapsed: 8m22s
2020-05-27 20:45:50.086972: Epoch: 1, Batch: 59, Loss: 0.6688, Elapsed: 12m8s
2020-05-27 21:01:42.327811: Epoch: 1, Batch: 60, Loss: 0.6599, Elapsed: 15m52s
2020-05-27 21:19:23.498534: Epoch: 1, Batch: 61, Loss: 0.6641, Elapsed: 17m41s
2020-05-27 22:01:18.631614: Epoch: 1, Batch: 62, Loss: 0.6664, Elapsed: 41m55s
2020-05-27 22:18:12.724638: Epoch: 1, Batch: 63, Loss: 0.6655, Elapsed: 16m54s
2020-05-27 22:33:03.568312: Epoch: 1, Batch: 64, Loss: 0.6645, Elapsed: 14m50s
2020-05-27 22:44:18.526123: Epoch: 1, Batch: 65, Loss: 0.6542, Elapsed: 11m14s
2020-05-27 22:59:26.130684: Epoch: 1, Batch: 66, Loss: 0.6490, Elapsed: 15m7s
2020-05-27 23:21:40.478707: Epoch: 1, Batch: 67, Loss: 0.6931, Elapsed: 22m14s
2020-05-27 23:35:36.921490: Epoch: 1, Batch: 68, Loss: 0.6591, Elapsed: 13m56s
2020-05-27 23:48:03.383117: Epoch: 1, Batch: 69, Loss: 0.6965, Elapsed: 12m26s
2020-05-28 00:08:43.460258: Epoch: 1, Batch: 70, Loss: 0.6606, Elapsed: 20m40s
2020-05-28 00:21:50.360852: Epoch: 1, Batch: 71, Loss: 0.6803, Elapsed: 13m6s
2020-05-28 00:38:31.688589: Epoch: 1, Batch: 72, Loss: 0.6721, Elapsed: 16m41s
2020-05-28 00:49:23.727478: Epoch: 1, Batch: 73, Loss: 0.6533, Elapsed: 10m52s
2020-05-28 01:08:11.348233: Epoch: 1, Batch: 74, Loss: 0.6795, Elapsed: 18m47s
2020-05-28 01:18:30.942614: Epoch: 1, Batch: 75, Loss: 0.6953, Elapsed: 10m19s
2020-05-28 01:35:53.377138: Epoch: 1, Batch: 76, Loss: 0.6635, Elapsed: 17m22s
2020-05-28 01:49:04.221188: Epoch: 1, Batch: 77, Loss: 0.6768, Elapsed: 13m10s
2020-05-28 02:13:47.625138: Epoch: 1, Batch: 78, Loss: 0.6833, Elapsed: 24m43s
2020-05-28 02:33:26.827309: Epoch: 1, Batch: 79, Loss: 0.6552, Elapsed: 19m39s
2020-05-28 02:44:36.987034: Epoch: 1, Batch: 80, Loss: 0.6747, Elapsed: 11m10s
2020-05-28 02:56:40.864948: Epoch: 1, Batch: 81, Loss: 0.6841, Elapsed: 12m3s
2020-05-28 03:17:11.275238: Epoch: 1, Batch: 82, Loss: 0.6846, Elapsed: 20m30s
2020-05-28 03:25:07.116001: Epoch: 1, Batch: 83, Loss: 0.6364, Elapsed: 7m55s
2020-05-28 03:38:15.292465: Epoch: 1, Batch: 84, Loss: 0.6562, Elapsed: 13m8s
2020-05-28 03:45:44.478189: Epoch: 1, Batch: 85, Loss: 0.6697, Elapsed: 7m29s
2020-05-28 03:50:42.645954: Epoch: 1, Batch: 86, Loss: 0.6658, Elapsed: 4m58s
2020-05-28 03:58:42.952078: Epoch: 1, Batch: 87, Loss: 0.6587, Elapsed: 8m0s
2020-05-28 04:06:07.131651: Epoch: 1, Batch: 88, Loss: 0.6556, Elapsed: 7m24s
2020-05-28 04:11:22.260629: Epoch: 1, Batch: 89, Loss: 0.6496, Elapsed: 5m15s
2020-05-28 04:17:44.726872: Epoch: 1, Batch: 90, Loss: 0.6657, Elapsed: 6m22s
2020-05-28 04:25:19.659554: Epoch: 1, Batch: 91, Loss: 0.6636, Elapsed: 7m34s
2020-05-28 04:29:10.119662: Epoch: 1, Batch: 92, Loss: 0.6788, Elapsed: 3m50s
2020-05-28 04:42:28.653508: Epoch: 1, Batch: 93, Loss: 0.6566, Elapsed: 13m18s
2020-05-28 04:52:08.710242: Epoch: 1, Batch: 94, Loss: 0.6512, Elapsed: 9m40s
2020-05-28 05:00:33.366535: Epoch: 1, Batch: 95, Loss: 0.6701, Elapsed: 8m24s
2020-05-28 05:10:55.524272: Epoch: 1, Batch: 96, Loss: 0.6524, Elapsed: 10m22s
2020-05-28 05:19:52.248803: Epoch: 1, Batch: 97, Loss: 0.6609, Elapsed: 8m56s
2020-05-28 05:26:13.372819: Epoch: 1, Batch: 98, Loss: 0.6599, Elapsed: 6m21s
2020-05-28 05:31:11.665455: Epoch: 1, Batch: 99, Loss: 0.6276, Elapsed: 4m58s
2020-05-28 05:39:32.290819: Epoch: 1, Batch: 100, Loss: 0.6570, Elapsed: 8m20s
Starting testing the validation set with 200 subgraphs!
2020-05-28 06:22:41.047521: Validation Test:  Loss: 0.6629,  Acc: 60.7655, AUC: 0.6408, Precision: 0.6662 -- Elapsed: 43m8s
2020-05-28 06:31:52.037168: Epoch: 1, Batch: 101, Loss: 0.6474, Elapsed: 9m10s
2020-05-28 06:41:34.396424: Epoch: 1, Batch: 102, Loss: 0.6707, Elapsed: 9m42s
2020-05-28 06:49:00.061089: Epoch: 1, Batch: 103, Loss: 0.6688, Elapsed: 7m25s
2020-05-28 06:56:41.425287: Epoch: 1, Batch: 104, Loss: 0.6494, Elapsed: 7m41s
2020-05-28 07:04:07.655253: Epoch: 1, Batch: 105, Loss: 0.6452, Elapsed: 7m26s
2020-05-28 07:15:56.713713: Epoch: 1, Batch: 106, Loss: 0.6610, Elapsed: 11m49s
2020-05-28 07:26:28.780966: Epoch: 1, Batch: 107, Loss: 0.6646, Elapsed: 10m32s
2020-05-28 07:37:22.632280: Epoch: 1, Batch: 108, Loss: 0.6701, Elapsed: 10m53s
2020-05-28 07:48:26.276633: Epoch: 1, Batch: 109, Loss: 0.6602, Elapsed: 11m3s
2020-05-28 07:53:28.618831: Epoch: 1, Batch: 110, Loss: 0.6352, Elapsed: 5m2s
2020-05-28 07:57:10.501874: Epoch: 1, Batch: 111, Loss: 0.6697, Elapsed: 3m41s
2020-05-28 08:06:21.670772: Epoch: 1, Batch: 112, Loss: 0.6478, Elapsed: 9m11s
2020-05-28 08:12:27.275603: Epoch: 1, Batch: 113, Loss: 0.6567, Elapsed: 6m5s
2020-05-28 08:17:34.144660: Epoch: 1, Batch: 114, Loss: 0.6685, Elapsed: 5m6s
2020-05-28 08:32:12.784409: Epoch: 1, Batch: 115, Loss: 0.6752, Elapsed: 14m38s
2020-05-28 08:41:38.325523: Epoch: 1, Batch: 116, Loss: 0.6562, Elapsed: 9m25s
2020-05-28 08:49:07.182505: Epoch: 1, Batch: 117, Loss: 0.6398, Elapsed: 7m28s
2020-05-28 08:59:43.243626: Epoch: 1, Batch: 118, Loss: 0.6588, Elapsed: 10m36s
2020-05-28 09:06:34.257010: Epoch: 1, Batch: 119, Loss: 0.6568, Elapsed: 6m50s
2020-05-28 09:17:45.732308: Epoch: 1, Batch: 120, Loss: 0.6622, Elapsed: 11m11s
2020-05-28 09:27:05.164869: Epoch: 1, Batch: 121, Loss: 0.6562, Elapsed: 9m19s
2020-05-28 09:38:03.590337: Epoch: 1, Batch: 122, Loss: 0.6472, Elapsed: 10m58s
2020-05-28 09:46:24.433602: Epoch: 1, Batch: 123, Loss: 0.6523, Elapsed: 8m20s
2020-05-28 09:54:21.370129: Epoch: 1, Batch: 124, Loss: 0.6529, Elapsed: 7m56s
2020-05-28 10:01:22.921312: Epoch: 1, Batch: 125, Loss: 0.6396, Elapsed: 7m1s
2020-05-28 10:06:51.257986: Epoch: 1, Batch: 126, Loss: 0.6612, Elapsed: 5m28s
2020-05-28 10:12:37.063149: Epoch: 1, Batch: 127, Loss: 0.6400, Elapsed: 5m45s
2020-05-28 10:23:22.347409: Epoch: 1, Batch: 128, Loss: 0.6630, Elapsed: 10m45s
2020-05-28 10:33:11.099608: Epoch: 1, Batch: 129, Loss: 0.6701, Elapsed: 9m48s
2020-05-28 10:39:46.936825: Epoch: 1, Batch: 130, Loss: 0.6388, Elapsed: 6m35s
2020-05-28 10:47:54.820328: Epoch: 1, Batch: 131, Loss: 0.6633, Elapsed: 8m7s
2020-05-28 10:54:43.655940: Epoch: 1, Batch: 132, Loss: 0.6499, Elapsed: 6m48s
2020-05-28 11:03:31.415594: Epoch: 1, Batch: 133, Loss: 0.6457, Elapsed: 8m47s
2020-05-28 11:12:46.936221: Epoch: 1, Batch: 134, Loss: 0.6394, Elapsed: 9m15s
2020-05-28 11:21:24.983092: Epoch: 1, Batch: 135, Loss: 0.6451, Elapsed: 8m38s
2020-05-28 11:32:55.395780: Epoch: 1, Batch: 136, Loss: 0.6521, Elapsed: 11m30s
2020-05-28 11:40:03.261750: Epoch: 1, Batch: 137, Loss: 0.6615, Elapsed: 7m7s
2020-05-28 11:45:27.331765: Epoch: 1, Batch: 138, Loss: 0.6336, Elapsed: 5m24s
2020-05-28 11:55:11.657794: Epoch: 1, Batch: 139, Loss: 0.6386, Elapsed: 9m44s
2020-05-28 12:06:03.874218: Epoch: 1, Batch: 140, Loss: 0.6628, Elapsed: 10m52s
2020-05-28 12:20:28.480328: Epoch: 1, Batch: 141, Loss: 0.6656, Elapsed: 14m24s
2020-05-28 12:27:32.693065: Epoch: 1, Batch: 142, Loss: 0.6486, Elapsed: 7m4s
2020-05-28 12:34:54.942635: Epoch: 1, Batch: 143, Loss: 0.6156, Elapsed: 7m22s
2020-05-28 12:41:52.086070: Epoch: 1, Batch: 144, Loss: 0.6170, Elapsed: 6m57s
2020-05-28 12:52:51.667916: Epoch: 1, Batch: 145, Loss: 0.6338, Elapsed: 10m59s
2020-05-28 13:01:51.031386: Epoch: 1, Batch: 146, Loss: 0.6278, Elapsed: 8m59s
2020-05-28 13:09:28.324652: Epoch: 1, Batch: 147, Loss: 0.6176, Elapsed: 7m37s
2020-05-28 13:16:38.182101: Epoch: 1, Batch: 148, Loss: 0.6197, Elapsed: 7m9s
2020-05-28 13:25:33.201037: Epoch: 1, Batch: 149, Loss: 0.6476, Elapsed: 8m55s
2020-05-28 13:38:32.792391: Epoch: 1, Batch: 150, Loss: 0.6405, Elapsed: 12m59s
Starting testing the validation set with 200 subgraphs!
2020-05-28 14:21:52.541742: Validation Test:  Loss: 0.6386,  Acc: 64.2723, AUC: 0.6911, Precision: 0.7263 -- Elapsed: 43m19s
2020-05-28 14:33:49.290476: Epoch: 1, Batch: 151, Loss: 0.6360, Elapsed: 11m56s
2020-05-28 14:43:53.484345: Epoch: 1, Batch: 152, Loss: 0.6294, Elapsed: 10m4s
2020-05-28 14:51:14.318624: Epoch: 1, Batch: 153, Loss: 0.6020, Elapsed: 7m20s
2020-05-28 14:59:10.597889: Epoch: 1, Batch: 154, Loss: 0.6207, Elapsed: 7m56s
2020-05-28 15:07:00.612307: Epoch: 1, Batch: 155, Loss: 0.6175, Elapsed: 7m50s
2020-05-28 15:14:38.191117: Epoch: 1, Batch: 156, Loss: 0.6268, Elapsed: 7m37s
2020-05-28 15:20:52.115276: Epoch: 1, Batch: 157, Loss: 0.6107, Elapsed: 6m13s
2020-05-28 15:28:00.172733: Epoch: 1, Batch: 158, Loss: 0.6373, Elapsed: 7m8s
2020-05-28 15:35:25.656265: Epoch: 1, Batch: 159, Loss: 0.5930, Elapsed: 7m25s
2020-05-28 15:46:37.012433: Epoch: 1, Batch: 160, Loss: 0.6226, Elapsed: 11m11s
2020-05-28 15:55:35.062494: Epoch: 1, Batch: 161, Loss: 0.6159, Elapsed: 8m58s
2020-05-28 16:09:43.921663: Epoch: 1, Batch: 162, Loss: 0.6690, Elapsed: 14m8s
2020-05-28 16:22:15.460324: Epoch: 1, Batch: 163, Loss: 0.6599, Elapsed: 12m31s
2020-05-28 16:33:59.299151: Epoch: 1, Batch: 164, Loss: 0.6307, Elapsed: 11m43s
2020-05-28 16:39:04.211428: Epoch: 1, Batch: 165, Loss: 0.5568, Elapsed: 5m4s
2020-05-28 16:46:10.603922: Epoch: 1, Batch: 166, Loss: 0.6128, Elapsed: 7m6s
2020-05-28 16:56:30.055680: Epoch: 1, Batch: 167, Loss: 0.6381, Elapsed: 10m19s
2020-05-28 17:02:34.551738: Epoch: 1, Batch: 168, Loss: 0.6016, Elapsed: 6m4s
2020-05-28 17:09:03.418907: Epoch: 1, Batch: 169, Loss: 0.6292, Elapsed: 6m28s
2020-05-28 17:17:24.926257: Epoch: 1, Batch: 170, Loss: 0.6513, Elapsed: 8m21s
2020-05-28 17:23:31.143680: Epoch: 1, Batch: 171, Loss: 0.6144, Elapsed: 6m6s
2020-05-28 17:36:58.438876: Epoch: 1, Batch: 172, Loss: 0.6593, Elapsed: 13m27s
2020-05-28 17:44:26.716468: Epoch: 1, Batch: 173, Loss: 0.5995, Elapsed: 7m28s
2020-05-28 17:55:59.351174: Epoch: 1, Batch: 174, Loss: 0.6067, Elapsed: 11m32s
2020-05-28 18:08:03.221167: Epoch: 1, Batch: 175, Loss: 0.6370, Elapsed: 12m3s
2020-05-28 18:16:44.973926: Epoch: 1, Batch: 176, Loss: 0.6346, Elapsed: 8m41s
2020-05-28 18:23:30.991937: Epoch: 1, Batch: 177, Loss: 0.6112, Elapsed: 6m46s
2020-05-28 18:31:53.946412: Epoch: 1, Batch: 178, Loss: 0.6309, Elapsed: 8m22s
2020-05-28 18:40:46.078403: Epoch: 1, Batch: 179, Loss: 0.6147, Elapsed: 8m52s
2020-05-28 18:47:45.065897: Epoch: 1, Batch: 180, Loss: 0.6086, Elapsed: 6m58s
2020-05-28 18:56:00.513036: Epoch: 1, Batch: 181, Loss: 0.6121, Elapsed: 8m15s
2020-05-28 19:03:40.185586: Epoch: 1, Batch: 182, Loss: 0.6008, Elapsed: 7m39s
2020-05-28 19:11:07.303784: Epoch: 1, Batch: 183, Loss: 0.6086, Elapsed: 7m27s
2020-05-28 19:21:47.552816: Epoch: 1, Batch: 184, Loss: 0.6149, Elapsed: 10m40s
2020-05-28 19:29:43.241023: Epoch: 1, Batch: 185, Loss: 0.5787, Elapsed: 7m55s
2020-05-28 19:34:10.574025: Epoch: 1, Batch: 186, Loss: 0.5476, Elapsed: 4m27s
2020-05-28 19:41:42.041915: Epoch: 1, Batch: 187, Loss: 0.6163, Elapsed: 7m31s
2020-05-28 19:49:17.067312: Epoch: 1, Batch: 188, Loss: 0.5965, Elapsed: 7m35s
2020-05-28 19:54:03.798858: Epoch: 1, Batch: 189, Loss: 0.6175, Elapsed: 4m46s
2020-05-28 20:01:57.397761: Epoch: 1, Batch: 190, Loss: 0.5914, Elapsed: 7m53s
2020-05-28 20:07:47.539720: Epoch: 1, Batch: 191, Loss: 0.5791, Elapsed: 5m50s
2020-05-28 20:20:22.511262: Epoch: 1, Batch: 192, Loss: 0.6323, Elapsed: 12m34s
2020-05-28 20:27:58.470612: Epoch: 1, Batch: 193, Loss: 0.5649, Elapsed: 7m35s
2020-05-28 20:41:15.186593: Epoch: 1, Batch: 194, Loss: 0.6346, Elapsed: 13m16s
2020-05-28 20:50:54.028880: Epoch: 1, Batch: 195, Loss: 0.6186, Elapsed: 9m38s
2020-05-28 20:57:19.208476: Epoch: 1, Batch: 196, Loss: 0.5727, Elapsed: 6m25s
2020-05-28 21:06:38.870587: Epoch: 1, Batch: 197, Loss: 0.6379, Elapsed: 9m19s
2020-05-28 21:10:30.068114: Epoch: 1, Batch: 198, Loss: 0.5442, Elapsed: 3m51s
2020-05-28 21:14:13.460181: Epoch: 1, Batch: 199, Loss: 0.5062, Elapsed: 3m43s
2020-05-28 21:24:55.883792: Epoch: 1, Batch: 200, Loss: 0.6382, Elapsed: 10m42s
Starting testing the validation set with 200 subgraphs!
2020-05-28 22:08:39.965684: Validation Test:  Loss: 0.6083,  Acc: 66.6491, AUC: 0.7255, Precision: 0.7818 -- Elapsed: 43m44s
2020-05-28 22:14:38.925064: Epoch: 1, Batch: 201, Loss: 0.6095, Elapsed: 5m58s
2020-05-28 22:23:27.045706: Epoch: 1, Batch: 202, Loss: 0.6182, Elapsed: 8m48s
2020-05-28 22:32:24.082596: Epoch: 1, Batch: 203, Loss: 0.6003, Elapsed: 8m57s
2020-05-28 22:39:35.442928: Epoch: 1, Batch: 204, Loss: 0.5792, Elapsed: 7m11s
2020-05-28 22:48:40.373647: Epoch: 1, Batch: 205, Loss: 0.6160, Elapsed: 9m4s
2020-05-28 22:55:28.309561: Epoch: 1, Batch: 206, Loss: 0.5648, Elapsed: 6m47s
2020-05-28 23:09:21.108928: Epoch: 1, Batch: 207, Loss: 0.6666, Elapsed: 13m52s
2020-05-28 23:18:23.486843: Epoch: 1, Batch: 208, Loss: 0.6345, Elapsed: 9m2s
2020-05-28 23:26:36.872458: Epoch: 1, Batch: 209, Loss: 0.6076, Elapsed: 8m13s
2020-05-28 23:33:43.300503: Epoch: 1, Batch: 210, Loss: 0.6053, Elapsed: 7m6s
2020-05-28 23:43:30.140814: Epoch: 1, Batch: 211, Loss: 0.6257, Elapsed: 9m46s
2020-05-28 23:49:19.466850: Epoch: 1, Batch: 212, Loss: 0.5762, Elapsed: 5m49s
2020-05-28 23:58:13.114463: Epoch: 1, Batch: 213, Loss: 0.6094, Elapsed: 8m53s
2020-05-29 00:06:48.655350: Epoch: 1, Batch: 214, Loss: 0.6294, Elapsed: 8m35s
2020-05-29 00:11:38.648111: Epoch: 1, Batch: 215, Loss: 0.5770, Elapsed: 4m49s
2020-05-29 00:24:17.178767: Epoch: 1, Batch: 216, Loss: 0.6490, Elapsed: 12m38s
2020-05-29 00:37:57.808366: Epoch: 1, Batch: 217, Loss: 0.6554, Elapsed: 13m40s
2020-05-29 00:44:53.482990: Epoch: 1, Batch: 218, Loss: 0.5750, Elapsed: 6m55s
2020-05-29 00:50:40.772876: Epoch: 1, Batch: 219, Loss: 0.6091, Elapsed: 5m47s
2020-05-29 01:00:39.913744: Epoch: 1, Batch: 220, Loss: 0.6381, Elapsed: 9m59s
2020-05-29 01:13:19.884427: Epoch: 1, Batch: 221, Loss: 0.6230, Elapsed: 12m39s
2020-05-29 01:21:03.062464: Epoch: 1, Batch: 222, Loss: 0.6054, Elapsed: 7m43s
2020-05-29 01:33:01.465891: Epoch: 1, Batch: 223, Loss: 0.6223, Elapsed: 11m58s
2020-05-29 01:42:48.920909: Epoch: 1, Batch: 224, Loss: 0.6265, Elapsed: 9m47s
2020-05-29 01:52:22.683426: Epoch: 1, Batch: 225, Loss: 0.6138, Elapsed: 9m33s
2020-05-29 01:57:39.897543: Epoch: 1, Batch: 226, Loss: 0.5545, Elapsed: 5m17s
2020-05-29 02:05:40.857426: Epoch: 1, Batch: 227, Loss: 0.6074, Elapsed: 8m0s
2020-05-29 02:12:08.347020: Epoch: 1, Batch: 228, Loss: 0.6019, Elapsed: 6m27s
2020-05-29 02:19:34.811407: Epoch: 1, Batch: 229, Loss: 0.5862, Elapsed: 7m26s
2020-05-29 02:30:10.112532: Epoch: 1, Batch: 230, Loss: 0.6560, Elapsed: 10m35s
2020-05-29 02:44:37.745423: Epoch: 1, Batch: 231, Loss: 0.6504, Elapsed: 14m27s
2020-05-29 02:54:49.114165: Epoch: 1, Batch: 232, Loss: 0.6415, Elapsed: 10m11s
2020-05-29 03:03:08.923429: Epoch: 1, Batch: 233, Loss: 0.6098, Elapsed: 8m19s
2020-05-29 03:17:07.786307: Epoch: 1, Batch: 234, Loss: 0.6415, Elapsed: 13m58s
2020-05-29 03:27:58.018392: Epoch: 1, Batch: 235, Loss: 0.5633, Elapsed: 10m50s
2020-05-29 03:38:53.641149: Epoch: 1, Batch: 236, Loss: 0.5755, Elapsed: 10m55s
2020-05-29 03:46:16.339619: Epoch: 1, Batch: 237, Loss: 0.5748, Elapsed: 7m22s
2020-05-29 03:55:54.449582: Epoch: 1, Batch: 238, Loss: 0.5913, Elapsed: 9m38s
2020-05-29 04:10:17.739227: Epoch: 1, Batch: 239, Loss: 0.6333, Elapsed: 14m23s
2020-05-29 04:16:33.851442: Epoch: 1, Batch: 240, Loss: 0.6017, Elapsed: 6m16s
2020-05-29 04:22:36.345595: Epoch: 1, Batch: 241, Loss: 0.6039, Elapsed: 6m2s
2020-05-29 04:28:04.894745: Epoch: 1, Batch: 242, Loss: 0.5857, Elapsed: 5m28s
2020-05-29 04:42:00.116352: Epoch: 1, Batch: 243, Loss: 0.6332, Elapsed: 13m55s
2020-05-29 04:49:32.729195: Epoch: 1, Batch: 244, Loss: 0.5788, Elapsed: 7m32s
2020-05-29 04:57:24.334566: Epoch: 1, Batch: 245, Loss: 0.5992, Elapsed: 7m51s
2020-05-29 05:07:12.133594: Epoch: 1, Batch: 246, Loss: 0.6143, Elapsed: 9m47s
2020-05-29 05:15:20.758505: Epoch: 1, Batch: 247, Loss: 0.6227, Elapsed: 8m8s
2020-05-29 05:25:32.051337: Epoch: 1, Batch: 248, Loss: 0.6131, Elapsed: 10m11s
2020-05-29 05:31:58.557207: Epoch: 1, Batch: 249, Loss: 0.5668, Elapsed: 6m26s
2020-05-29 05:43:02.629886: Epoch: 1, Batch: 250, Loss: 0.6519, Elapsed: 11m4s
Starting testing the validation set with 200 subgraphs!
2020-05-29 06:25:44.391285: Validation Test:  Loss: 0.6035,  Acc: 67.9191, AUC: 0.7460, Precision: 0.7971 -- Elapsed: 42m41s
2020-05-29 06:32:48.637089: Epoch: 1, Batch: 251, Loss: 0.5691, Elapsed: 7m4s
2020-05-29 06:38:45.900234: Epoch: 1, Batch: 252, Loss: 0.6177, Elapsed: 5m57s
2020-05-29 06:43:10.743283: Epoch: 1, Batch: 253, Loss: 0.5811, Elapsed: 4m24s
2020-05-29 06:52:27.776656: Epoch: 1, Batch: 254, Loss: 0.5731, Elapsed: 9m17s
2020-05-29 07:02:42.363686: Epoch: 1, Batch: 255, Loss: 0.6346, Elapsed: 10m14s
2020-05-29 07:12:01.135592: Epoch: 1, Batch: 256, Loss: 0.5817, Elapsed: 9m18s
2020-05-29 07:22:36.848599: Epoch: 1, Batch: 257, Loss: 0.6128, Elapsed: 10m35s
2020-05-29 07:30:00.270936: Epoch: 1, Batch: 258, Loss: 0.6368, Elapsed: 7m23s
2020-05-29 07:41:51.414710: Epoch: 1, Batch: 259, Loss: 0.5987, Elapsed: 11m51s
2020-05-29 07:50:11.394041: Epoch: 1, Batch: 260, Loss: 0.5851, Elapsed: 8m19s
2020-05-29 07:57:36.671445: Epoch: 1, Batch: 261, Loss: 0.6081, Elapsed: 7m25s
2020-05-29 08:08:20.435677: Epoch: 1, Batch: 262, Loss: 0.6174, Elapsed: 10m43s
2020-05-29 08:17:00.124961: Epoch: 1, Batch: 263, Loss: 0.5945, Elapsed: 8m39s
2020-05-29 08:27:11.670790: Epoch: 1, Batch: 264, Loss: 0.6245, Elapsed: 10m11s
2020-05-29 08:35:07.394832: Epoch: 1, Batch: 265, Loss: 0.5934, Elapsed: 7m55s
2020-05-29 08:43:33.023025: Epoch: 1, Batch: 266, Loss: 0.5669, Elapsed: 8m25s
2020-05-29 08:51:40.123035: Epoch: 1, Batch: 267, Loss: 0.5883, Elapsed: 8m7s
2020-05-29 09:00:43.629687: Epoch: 1, Batch: 268, Loss: 0.6196, Elapsed: 9m3s
2020-05-29 09:12:22.709528: Epoch: 1, Batch: 269, Loss: 0.5899, Elapsed: 11m39s
2020-05-29 09:23:36.394158: Epoch: 1, Batch: 270, Loss: 0.6017, Elapsed: 11m13s
2020-05-29 09:34:18.712342: Epoch: 1, Batch: 271, Loss: 0.5895, Elapsed: 10m42s
2020-05-29 09:43:52.403261: Epoch: 1, Batch: 272, Loss: 0.5986, Elapsed: 9m33s
2020-05-29 09:57:03.913794: Epoch: 1, Batch: 273, Loss: 0.6070, Elapsed: 13m11s
2020-05-29 10:03:52.839896: Epoch: 1, Batch: 274, Loss: 0.5455, Elapsed: 6m48s
2020-05-29 10:17:11.839984: Epoch: 1, Batch: 275, Loss: 0.5969, Elapsed: 13m18s
2020-05-29 10:25:49.613497: Epoch: 1, Batch: 276, Loss: 0.5826, Elapsed: 8m37s
2020-05-29 10:38:46.827129: Epoch: 1, Batch: 277, Loss: 0.6317, Elapsed: 12m57s
2020-05-29 10:58:45.194853: Epoch: 1, Batch: 278, Loss: 0.6327, Elapsed: 19m58s
2020-05-29 11:05:39.178412: Epoch: 1, Batch: 279, Loss: 0.5694, Elapsed: 6m53s
2020-05-29 11:13:37.060611: Epoch: 1, Batch: 280, Loss: 0.5744, Elapsed: 7m57s
2020-05-29 11:25:43.043384: Epoch: 1, Batch: 281, Loss: 0.6122, Elapsed: 12m5s
2020-05-29 11:32:24.690370: Epoch: 1, Batch: 282, Loss: 0.6061, Elapsed: 6m41s
2020-05-29 11:45:29.897226: Epoch: 1, Batch: 283, Loss: 0.6049, Elapsed: 13m5s
2020-05-29 11:52:17.767299: Epoch: 1, Batch: 284, Loss: 0.5674, Elapsed: 6m47s
2020-05-29 11:58:59.100664: Epoch: 1, Batch: 285, Loss: 0.5296, Elapsed: 6m41s
2020-05-29 12:04:26.807946: Epoch: 1, Batch: 286, Loss: 0.5608, Elapsed: 5m27s
2020-05-29 12:16:11.314034: Epoch: 1, Batch: 287, Loss: 0.6246, Elapsed: 11m44s
2020-05-29 12:20:06.185017: Epoch: 1, Batch: 288, Loss: 0.5267, Elapsed: 3m54s
2020-05-29 12:26:31.105931: Epoch: 1, Batch: 289, Loss: 0.6171, Elapsed: 6m24s
2020-05-29 12:35:16.078446: Epoch: 1, Batch: 290, Loss: 0.5908, Elapsed: 8m44s
2020-05-29 12:42:13.459277: Epoch: 1, Batch: 291, Loss: 0.5570, Elapsed: 6m57s
2020-05-29 12:47:58.386840: Epoch: 1, Batch: 292, Loss: 0.5472, Elapsed: 5m44s
2020-05-29 12:53:26.215895: Epoch: 1, Batch: 293, Loss: 0.5594, Elapsed: 5m27s
2020-05-29 12:59:58.858909: Epoch: 1, Batch: 294, Loss: 0.5819, Elapsed: 6m32s
2020-05-29 13:10:35.785307: Epoch: 1, Batch: 295, Loss: 0.6025, Elapsed: 10m36s
2020-05-29 13:20:14.383645: Epoch: 1, Batch: 296, Loss: 0.6339, Elapsed: 9m38s
2020-05-29 13:31:40.716468: Epoch: 1, Batch: 297, Loss: 0.6268, Elapsed: 11m26s
2020-05-29 13:42:03.354833: Epoch: 1, Batch: 298, Loss: 0.6103, Elapsed: 10m22s
2020-05-29 13:53:42.804073: Epoch: 1, Batch: 299, Loss: 0.6103, Elapsed: 11m39s
2020-05-29 14:00:44.874279: Epoch: 1, Batch: 300, Loss: 0.5759, Elapsed: 7m2s
Starting testing the validation set with 200 subgraphs!
2020-05-29 14:44:08.973245: Validation Test:  Loss: 0.5934,  Acc: 68.8128, AUC: 0.7568, Precision: 0.8094 -- Elapsed: 43m24s
2020-05-29 14:50:52.905869: Epoch: 1, Batch: 301, Loss: 0.5658, Elapsed: 6m43s
2020-05-29 14:58:25.988696: Epoch: 1, Batch: 302, Loss: 0.5782, Elapsed: 7m33s
2020-05-29 15:08:18.142720: Epoch: 1, Batch: 303, Loss: 0.5843, Elapsed: 9m52s
2020-05-29 15:18:39.760391: Epoch: 1, Batch: 304, Loss: 0.5902, Elapsed: 10m21s
2020-05-29 15:31:45.771071: Epoch: 1, Batch: 305, Loss: 0.6382, Elapsed: 13m6s
2020-05-29 15:39:26.360552: Epoch: 1, Batch: 306, Loss: 0.5937, Elapsed: 7m40s
2020-05-29 15:48:46.019420: Epoch: 1, Batch: 307, Loss: 0.5946, Elapsed: 9m19s
2020-05-29 15:58:18.201743: Epoch: 1, Batch: 308, Loss: 0.6025, Elapsed: 9m32s
2020-05-29 16:03:37.061145: Epoch: 1, Batch: 309, Loss: 0.5757, Elapsed: 5m18s
2020-05-29 16:13:44.351943: Epoch: 1, Batch: 310, Loss: 0.5912, Elapsed: 10m7s
2020-05-29 16:19:50.646952: Epoch: 1, Batch: 311, Loss: 0.6017, Elapsed: 6m6s
2020-05-29 16:28:15.118717: Epoch: 1, Batch: 312, Loss: 0.6311, Elapsed: 8m24s
2020-05-29 16:34:39.055962: Epoch: 1, Batch: 313, Loss: 0.5858, Elapsed: 6m23s
2020-05-29 16:45:34.296091: Epoch: 1, Batch: 314, Loss: 0.6148, Elapsed: 10m55s
2020-05-29 16:57:35.085905: Epoch: 1, Batch: 315, Loss: 0.6024, Elapsed: 12m0s
2020-05-29 17:09:54.577898: Epoch: 1, Batch: 316, Loss: 0.5857, Elapsed: 12m19s
2020-05-29 17:17:05.121706: Epoch: 1, Batch: 317, Loss: 0.5761, Elapsed: 7m10s
2020-05-29 17:29:15.400900: Epoch: 1, Batch: 318, Loss: 0.6048, Elapsed: 12m10s
2020-05-29 17:39:52.910064: Epoch: 1, Batch: 319, Loss: 0.5869, Elapsed: 10m37s
2020-05-29 17:47:45.304677: Epoch: 1, Batch: 320, Loss: 0.5774, Elapsed: 7m52s
2020-05-29 17:54:18.474872: Epoch: 1, Batch: 321, Loss: 0.5936, Elapsed: 6m33s
2020-05-29 18:04:05.349206: Epoch: 1, Batch: 322, Loss: 0.5974, Elapsed: 9m46s
2020-05-29 18:09:56.253648: Epoch: 1, Batch: 323, Loss: 0.5459, Elapsed: 5m50s
2020-05-29 18:17:55.645510: Epoch: 1, Batch: 324, Loss: 0.5924, Elapsed: 7m59s
2020-05-29 18:28:58.636439: Epoch: 1, Batch: 325, Loss: 0.6154, Elapsed: 11m2s
2020-05-29 18:39:31.571997: Epoch: 1, Batch: 326, Loss: 0.6180, Elapsed: 10m32s
2020-05-29 18:51:22.871744: Epoch: 1, Batch: 327, Loss: 0.6104, Elapsed: 11m51s
2020-05-29 18:57:24.150646: Epoch: 1, Batch: 328, Loss: 0.6162, Elapsed: 6m1s
2020-05-29 19:06:08.168962: Epoch: 1, Batch: 329, Loss: 0.5845, Elapsed: 8m44s
2020-05-29 19:14:00.214836: Epoch: 1, Batch: 330, Loss: 0.5897, Elapsed: 7m52s
2020-05-29 19:23:35.591182: Epoch: 1, Batch: 331, Loss: 0.6019, Elapsed: 9m35s
2020-05-29 19:34:14.227182: Epoch: 1, Batch: 332, Loss: 0.5887, Elapsed: 10m38s
2020-05-29 19:41:34.355082: Epoch: 1, Batch: 333, Loss: 0.5961, Elapsed: 7m20s
2020-05-29 19:49:59.132126: Epoch: 1, Batch: 334, Loss: 0.5873, Elapsed: 8m24s
2020-05-29 19:57:48.583902: Epoch: 1, Batch: 335, Loss: 0.6058, Elapsed: 7m49s
2020-05-29 20:08:21.359789: Epoch: 1, Batch: 336, Loss: 0.6300, Elapsed: 10m32s
2020-05-29 20:17:35.111101: Epoch: 1, Batch: 337, Loss: 0.5888, Elapsed: 9m13s
2020-05-29 20:23:59.080326: Epoch: 1, Batch: 338, Loss: 0.5331, Elapsed: 6m23s
2020-05-29 20:31:00.720921: Epoch: 1, Batch: 339, Loss: 0.5540, Elapsed: 7m1s
2020-05-29 20:40:04.026356: Epoch: 1, Batch: 340, Loss: 0.5861, Elapsed: 9m3s
2020-05-29 20:49:34.439468: Epoch: 1, Batch: 341, Loss: 0.6164, Elapsed: 9m30s
2020-05-29 21:02:28.177793: Epoch: 1, Batch: 342, Loss: 0.6117, Elapsed: 12m53s
2020-05-29 21:10:46.939314: Epoch: 1, Batch: 343, Loss: 0.5776, Elapsed: 8m18s
2020-05-29 21:21:37.262708: Epoch: 1, Batch: 344, Loss: 0.5896, Elapsed: 10m50s
2020-05-29 21:34:44.935867: Epoch: 1, Batch: 345, Loss: 0.5779, Elapsed: 13m7s
2020-05-29 21:41:05.008913: Epoch: 1, Batch: 346, Loss: 0.5757, Elapsed: 6m20s
2020-05-29 21:48:33.239185: Epoch: 1, Batch: 347, Loss: 0.5596, Elapsed: 7m28s
2020-05-29 22:04:48.079961: Epoch: 1, Batch: 348, Loss: 0.6188, Elapsed: 16m14s
2020-05-29 22:13:09.287052: Epoch: 1, Batch: 349, Loss: 0.5633, Elapsed: 8m21s
2020-05-29 22:25:20.566980: Epoch: 1, Batch: 350, Loss: 0.6102, Elapsed: 12m11s
Starting testing the validation set with 200 subgraphs!
2020-05-29 23:08:31.206645: Validation Test:  Loss: 0.5859,  Acc: 70.1711, AUC: 0.7683, Precision: 0.8025 -- Elapsed: 43m10s
2020-05-29 23:23:33.632423: Epoch: 1, Batch: 351, Loss: 0.6078, Elapsed: 15m2s
2020-05-29 23:29:49.786762: Epoch: 1, Batch: 352, Loss: 0.6197, Elapsed: 6m16s
2020-05-29 23:37:18.711056: Epoch: 1, Batch: 353, Loss: 0.5648, Elapsed: 7m28s
2020-05-29 23:46:48.017118: Epoch: 1, Batch: 354, Loss: 0.5621, Elapsed: 9m29s
2020-05-29 23:57:34.921340: Epoch: 1, Batch: 355, Loss: 0.5796, Elapsed: 10m46s
2020-05-30 00:13:14.712428: Epoch: 1, Batch: 356, Loss: 0.6310, Elapsed: 15m39s
2020-05-30 00:20:02.450547: Epoch: 1, Batch: 357, Loss: 0.5478, Elapsed: 6m47s
2020-05-30 00:24:13.987659: Epoch: 1, Batch: 358, Loss: 0.5350, Elapsed: 4m11s
2020-05-30 00:40:03.124724: Epoch: 1, Batch: 359, Loss: 0.5989, Elapsed: 15m49s
2020-05-30 00:47:48.366033: Epoch: 1, Batch: 360, Loss: 0.5742, Elapsed: 7m45s
2020-05-30 00:57:11.351625: Epoch: 1, Batch: 361, Loss: 0.5710, Elapsed: 9m22s
2020-05-30 01:01:50.271562: Epoch: 1, Batch: 362, Loss: 0.5846, Elapsed: 4m38s
2020-05-30 01:08:34.043955: Epoch: 1, Batch: 363, Loss: 0.5414, Elapsed: 6m43s
2020-05-30 01:11:59.381395: Epoch: 1, Batch: 364, Loss: 0.5011, Elapsed: 3m25s
2020-05-30 01:21:38.153223: Epoch: 1, Batch: 365, Loss: 0.5926, Elapsed: 9m38s
2020-05-30 01:37:58.092397: Epoch: 1, Batch: 366, Loss: 0.6133, Elapsed: 16m19s
2020-05-30 01:45:05.776037: Epoch: 1, Batch: 367, Loss: 0.5520, Elapsed: 7m7s
2020-05-30 01:50:15.433397: Epoch: 1, Batch: 368, Loss: 0.5431, Elapsed: 5m9s
2020-05-30 01:58:35.421141: Epoch: 1, Batch: 369, Loss: 0.5674, Elapsed: 8m19s
2020-05-30 02:04:27.456632: Epoch: 1, Batch: 370, Loss: 0.5709, Elapsed: 5m52s
2020-05-30 02:13:58.339020: Epoch: 1, Batch: 371, Loss: 0.5703, Elapsed: 9m30s
2020-05-30 02:18:07.453186: Epoch: 1, Batch: 372, Loss: 0.4603, Elapsed: 4m9s
2020-05-30 02:26:20.265609: Epoch: 1, Batch: 373, Loss: 0.5511, Elapsed: 8m12s
2020-05-30 02:35:56.042043: Epoch: 1, Batch: 374, Loss: 0.5770, Elapsed: 9m35s
2020-05-30 02:46:47.440470: Epoch: 1, Batch: 375, Loss: 0.5713, Elapsed: 10m51s
2020-05-30 02:55:11.151122: Epoch: 1, Batch: 376, Loss: 0.5704, Elapsed: 8m23s
2020-05-30 03:06:11.770002: Epoch: 1, Batch: 377, Loss: 0.5971, Elapsed: 11m0s
2020-05-30 03:16:16.498188: Epoch: 1, Batch: 378, Loss: 0.5927, Elapsed: 10m4s
2020-05-30 03:23:35.740074: Epoch: 1, Batch: 379, Loss: 0.5599, Elapsed: 7m19s
2020-05-30 03:31:47.647139: Epoch: 1, Batch: 380, Loss: 0.6075, Elapsed: 8m11s
2020-05-30 03:42:39.760134: Epoch: 1, Batch: 381, Loss: 0.5804, Elapsed: 10m52s
2020-05-30 03:49:02.073717: Epoch: 1, Batch: 382, Loss: 0.5026, Elapsed: 6m22s
2020-05-30 03:56:53.072513: Epoch: 1, Batch: 383, Loss: 0.5477, Elapsed: 7m50s
2020-05-30 04:04:03.273776: Epoch: 1, Batch: 384, Loss: 0.5697, Elapsed: 7m10s
2020-05-30 04:12:10.018544: Epoch: 1, Batch: 385, Loss: 0.5633, Elapsed: 8m6s
2020-05-30 04:20:21.770453: Epoch: 1, Batch: 386, Loss: 0.5558, Elapsed: 8m11s
2020-05-30 04:27:49.263588: Epoch: 1, Batch: 387, Loss: 0.5662, Elapsed: 7m27s
2020-05-30 04:32:31.438733: Epoch: 1, Batch: 388, Loss: 0.5084, Elapsed: 4m42s
2020-05-30 04:41:51.411847: Epoch: 1, Batch: 389, Loss: 0.5904, Elapsed: 9m19s
2020-05-30 04:48:22.252700: Epoch: 1, Batch: 390, Loss: 0.5672, Elapsed: 6m30s
2020-05-30 04:55:28.269305: Epoch: 1, Batch: 391, Loss: 0.5473, Elapsed: 7m6s
2020-05-30 05:03:10.136166: Epoch: 1, Batch: 392, Loss: 0.5451, Elapsed: 7m41s
2020-05-30 05:15:34.805337: Epoch: 1, Batch: 393, Loss: 0.6184, Elapsed: 12m24s
2020-05-30 05:24:31.741357: Epoch: 1, Batch: 394, Loss: 0.5846, Elapsed: 8m56s
2020-05-30 05:30:41.877431: Epoch: 1, Batch: 395, Loss: 0.5366, Elapsed: 6m10s
2020-05-30 05:47:05.709771: Epoch: 1, Batch: 396, Loss: 0.6095, Elapsed: 16m23s
2020-05-30 05:59:10.494381: Epoch: 1, Batch: 397, Loss: 0.5891, Elapsed: 12m4s
2020-05-30 06:07:55.375202: Epoch: 1, Batch: 398, Loss: 0.5459, Elapsed: 8m44s
2020-05-30 06:17:47.606681: Epoch: 1, Batch: 399, Loss: 0.5476, Elapsed: 9m52s
2020-05-30 06:26:30.760376: Epoch: 1, Batch: 400, Loss: 0.5873, Elapsed: 8m43s
Starting testing the validation set with 200 subgraphs!
2020-05-30 07:10:03.852476: Validation Test:  Loss: 0.5784,  Acc: 69.9916, AUC: 0.7655, Precision: 0.8017 -- Elapsed: 43m33s
2020-05-30 07:15:47.137573: Epoch: 1, Batch: 401, Loss: 0.5250, Elapsed: 5m43s
2020-05-30 07:27:19.199980: Epoch: 1, Batch: 402, Loss: 0.5907, Elapsed: 11m32s
2020-05-30 07:38:11.309477: Epoch: 1, Batch: 403, Loss: 0.6422, Elapsed: 10m52s
2020-05-30 07:50:21.229070: Epoch: 1, Batch: 404, Loss: 0.5834, Elapsed: 12m9s
2020-05-30 08:02:07.573149: Epoch: 1, Batch: 405, Loss: 0.6394, Elapsed: 11m46s
2020-05-30 08:09:57.967419: Epoch: 1, Batch: 406, Loss: 0.6353, Elapsed: 7m50s
2020-05-30 08:21:33.073887: Epoch: 1, Batch: 407, Loss: 0.5692, Elapsed: 11m35s
2020-05-30 08:30:53.202625: Epoch: 1, Batch: 408, Loss: 0.6065, Elapsed: 9m20s
2020-05-30 08:36:58.391847: Epoch: 1, Batch: 409, Loss: 0.5564, Elapsed: 6m5s
2020-05-30 08:47:39.694558: Epoch: 1, Batch: 410, Loss: 0.6115, Elapsed: 10m41s
2020-05-30 08:55:32.346551: Epoch: 1, Batch: 411, Loss: 0.5914, Elapsed: 7m52s
2020-05-30 09:09:03.055218: Epoch: 1, Batch: 412, Loss: 0.5734, Elapsed: 13m30s
2020-05-30 09:18:01.404617: Epoch: 1, Batch: 413, Loss: 0.5663, Elapsed: 8m58s
2020-05-30 09:27:11.773144: Epoch: 1, Batch: 414, Loss: 0.6055, Elapsed: 9m10s
2020-05-30 09:37:57.810225: Epoch: 1, Batch: 415, Loss: 0.6054, Elapsed: 10m46s
2020-05-30 09:49:59.702540: Epoch: 1, Batch: 416, Loss: 0.5279, Elapsed: 12m1s
2020-05-30 10:03:17.065610: Epoch: 1, Batch: 417, Loss: 0.6209, Elapsed: 13m17s
2020-05-30 10:10:19.734116: Epoch: 1, Batch: 418, Loss: 0.5621, Elapsed: 7m2s
2020-05-30 10:16:30.696412: Epoch: 1, Batch: 419, Loss: 0.5800, Elapsed: 6m10s
2020-05-30 10:26:04.571928: Epoch: 1, Batch: 420, Loss: 0.5895, Elapsed: 9m33s
2020-05-30 10:33:14.571596: Epoch: 1, Batch: 421, Loss: 0.5631, Elapsed: 7m9s
2020-05-30 10:42:59.822656: Epoch: 1, Batch: 422, Loss: 0.5836, Elapsed: 9m45s
2020-05-30 10:59:01.801703: Epoch: 1, Batch: 423, Loss: 0.5919, Elapsed: 16m1s
2020-05-30 11:06:56.022049: Epoch: 1, Batch: 424, Loss: 0.5741, Elapsed: 7m54s
2020-05-30 11:15:55.835994: Epoch: 1, Batch: 425, Loss: 0.5710, Elapsed: 8m59s
2020-05-30 11:21:26.350626: Epoch: 1, Batch: 426, Loss: 0.5362, Elapsed: 5m30s
2020-05-30 11:29:47.185572: Epoch: 1, Batch: 427, Loss: 0.5893, Elapsed: 8m20s
2020-05-30 11:37:46.494017: Epoch: 1, Batch: 428, Loss: 0.5473, Elapsed: 7m59s
2020-05-30 11:48:22.013027: Epoch: 1, Batch: 429, Loss: 0.5862, Elapsed: 10m35s
2020-05-30 11:54:40.075719: Epoch: 1, Batch: 430, Loss: 0.5375, Elapsed: 6m18s
2020-05-30 12:00:13.990976: Epoch: 1, Batch: 431, Loss: 0.5324, Elapsed: 5m33s
2020-05-30 12:08:43.859468: Epoch: 1, Batch: 432, Loss: 0.5921, Elapsed: 8m29s
2020-05-30 12:19:07.789925: Epoch: 1, Batch: 433, Loss: 0.5701, Elapsed: 10m23s
2020-05-30 12:29:36.382742: Epoch: 1, Batch: 434, Loss: 0.6089, Elapsed: 10m28s
2020-05-30 12:39:31.634205: Epoch: 1, Batch: 435, Loss: 0.5834, Elapsed: 9m55s
2020-05-30 12:46:33.059367: Epoch: 1, Batch: 436, Loss: 0.5619, Elapsed: 7m1s
2020-05-30 12:55:20.536906: Epoch: 1, Batch: 437, Loss: 0.5764, Elapsed: 8m47s
2020-05-30 13:04:31.731364: Epoch: 1, Batch: 438, Loss: 0.6085, Elapsed: 9m11s
2020-05-30 13:16:57.239293: Epoch: 1, Batch: 439, Loss: 0.5856, Elapsed: 12m25s
2020-05-30 13:30:24.635464: Epoch: 1, Batch: 440, Loss: 0.5714, Elapsed: 13m27s
2020-05-30 13:38:56.473529: Epoch: 1, Batch: 441, Loss: 0.5759, Elapsed: 8m31s
2020-05-30 13:48:15.596144: Epoch: 1, Batch: 442, Loss: 0.5604, Elapsed: 9m19s
2020-05-30 13:53:59.725536: Epoch: 1, Batch: 443, Loss: 0.5699, Elapsed: 5m44s
2020-05-30 14:01:23.419259: Epoch: 1, Batch: 444, Loss: 0.5294, Elapsed: 7m23s
2020-05-30 14:10:55.607697: Epoch: 1, Batch: 445, Loss: 0.5671, Elapsed: 9m32s
2020-05-30 14:16:56.252309: Epoch: 1, Batch: 446, Loss: 0.5226, Elapsed: 6m0s
2020-05-30 14:23:16.743444: Epoch: 1, Batch: 447, Loss: 0.5127, Elapsed: 6m20s
2020-05-30 14:35:33.477252: Epoch: 1, Batch: 448, Loss: 0.5847, Elapsed: 12m16s
2020-05-30 14:46:16.796348: Epoch: 1, Batch: 449, Loss: 0.5902, Elapsed: 10m43s
2020-05-30 14:59:50.304455: Epoch: 1, Batch: 450, Loss: 0.6051, Elapsed: 13m33s
Starting testing the validation set with 200 subgraphs!
2020-05-30 15:43:21.313981: Validation Test:  Loss: 0.5774,  Acc: 71.3945, AUC: 0.7753, Precision: 0.7990 -- Elapsed: 43m31s
2020-05-30 15:53:28.801702: Epoch: 1, Batch: 451, Loss: 0.5721, Elapsed: 10m7s
2020-05-30 16:00:44.351562: Epoch: 1, Batch: 452, Loss: 0.5245, Elapsed: 7m15s
2020-05-30 16:08:19.097512: Epoch: 1, Batch: 453, Loss: 0.5427, Elapsed: 7m34s
2020-05-30 16:16:26.736825: Epoch: 1, Batch: 454, Loss: 0.5940, Elapsed: 8m7s
2020-05-30 16:25:03.206417: Epoch: 1, Batch: 455, Loss: 0.5569, Elapsed: 8m36s
2020-05-30 16:30:39.072457: Epoch: 1, Batch: 456, Loss: 0.4876, Elapsed: 5m35s
2020-05-30 16:42:47.082841: Epoch: 1, Batch: 457, Loss: 0.5772, Elapsed: 12m8s
2020-05-30 16:52:34.532413: Epoch: 1, Batch: 458, Loss: 0.5647, Elapsed: 9m47s
2020-05-30 17:07:28.995251: Epoch: 1, Batch: 459, Loss: 0.5796, Elapsed: 14m54s
2020-05-30 17:15:08.354265: Epoch: 1, Batch: 460, Loss: 0.5604, Elapsed: 7m39s
2020-05-30 17:31:25.045020: Epoch: 1, Batch: 461, Loss: 0.5569, Elapsed: 16m16s
2020-05-30 17:41:10.295915: Epoch: 1, Batch: 462, Loss: 0.5809, Elapsed: 9m45s
2020-05-30 17:50:22.863121: Epoch: 1, Batch: 463, Loss: 0.5627, Elapsed: 9m12s
2020-05-30 17:58:41.221748: Epoch: 1, Batch: 464, Loss: 0.5495, Elapsed: 8m18s
2020-05-30 18:05:20.252472: Epoch: 1, Batch: 465, Loss: 0.5592, Elapsed: 6m39s
2020-05-30 18:11:31.262750: Epoch: 1, Batch: 466, Loss: 0.5185, Elapsed: 6m11s
2020-05-30 18:22:18.395630: Epoch: 1, Batch: 467, Loss: 0.5685, Elapsed: 10m47s
2020-05-30 18:29:36.362203: Epoch: 1, Batch: 468, Loss: 0.5412, Elapsed: 7m17s
2020-05-30 18:35:38.021527: Epoch: 1, Batch: 469, Loss: 0.5699, Elapsed: 6m1s
2020-05-30 18:42:00.561938: Epoch: 1, Batch: 470, Loss: 0.5611, Elapsed: 6m22s
2020-05-30 18:51:07.279231: Epoch: 1, Batch: 471, Loss: 0.5923, Elapsed: 9m6s
2020-05-30 19:00:12.500487: Epoch: 1, Batch: 472, Loss: 0.5671, Elapsed: 9m5s
2020-05-30 19:10:33.597435: Epoch: 1, Batch: 473, Loss: 0.5522, Elapsed: 10m21s
2020-05-30 19:25:06.557090: Epoch: 1, Batch: 474, Loss: 0.5970, Elapsed: 14m32s
2020-05-30 19:33:27.893841: Epoch: 1, Batch: 475, Loss: 0.5286, Elapsed: 8m21s
2020-05-30 19:43:32.695580: Epoch: 1, Batch: 476, Loss: 0.5528, Elapsed: 10m4s
2020-05-30 19:50:06.678132: Epoch: 1, Batch: 477, Loss: 0.5351, Elapsed: 6m33s
2020-05-30 19:59:55.328785: Epoch: 1, Batch: 478, Loss: 0.5594, Elapsed: 9m48s
2020-05-30 20:12:38.060868: Epoch: 1, Batch: 479, Loss: 0.5566, Elapsed: 12m42s
2020-05-30 20:18:57.457466: Epoch: 1, Batch: 480, Loss: 0.5372, Elapsed: 6m19s
2020-05-30 20:25:03.228733: Epoch: 1, Batch: 481, Loss: 0.5412, Elapsed: 6m5s
2020-05-30 20:31:29.628368: Epoch: 1, Batch: 482, Loss: 0.5039, Elapsed: 6m26s
2020-05-30 20:39:28.588036: Epoch: 1, Batch: 483, Loss: 0.5650, Elapsed: 7m58s
2020-05-30 20:47:05.795841: Epoch: 1, Batch: 484, Loss: 0.5568, Elapsed: 7m37s
2020-05-30 20:55:23.134355: Epoch: 1, Batch: 485, Loss: 0.5629, Elapsed: 8m17s
2020-05-30 21:03:35.276705: Epoch: 1, Batch: 486, Loss: 0.5523, Elapsed: 8m12s
2020-05-30 21:16:27.671547: Epoch: 1, Batch: 487, Loss: 0.5637, Elapsed: 12m52s
2020-05-30 21:28:40.394237: Epoch: 1, Batch: 488, Loss: 0.5683, Elapsed: 12m12s
2020-05-30 21:34:27.605931: Epoch: 1, Batch: 489, Loss: 0.5148, Elapsed: 5m47s
2020-05-30 21:43:46.700029: Epoch: 1, Batch: 490, Loss: 0.5911, Elapsed: 9m19s
2020-05-30 21:51:58.237737: Epoch: 1, Batch: 491, Loss: 0.5228, Elapsed: 8m11s
2020-05-30 22:00:06.678514: Epoch: 1, Batch: 492, Loss: 0.5139, Elapsed: 8m8s
2020-05-30 22:09:33.166651: Epoch: 1, Batch: 493, Loss: 0.5926, Elapsed: 9m26s
2020-05-30 22:16:09.135664: Epoch: 1, Batch: 494, Loss: 0.5502, Elapsed: 6m35s
2020-05-30 22:21:52.094997: Epoch: 1, Batch: 495, Loss: 0.5455, Elapsed: 5m42s
2020-05-30 22:30:37.704019: Epoch: 1, Batch: 496, Loss: 0.5452, Elapsed: 8m45s
2020-05-30 22:37:03.045965: Epoch: 1, Batch: 497, Loss: 0.5320, Elapsed: 6m25s
2020-05-30 22:45:15.717032: Epoch: 1, Batch: 498, Loss: 0.5342, Elapsed: 8m12s
2020-05-30 22:53:29.669372: Epoch: 1, Batch: 499, Loss: 0.5714, Elapsed: 8m13s
2020-05-30 23:07:09.638000: Epoch: 1, Batch: 500, Loss: 0.5849, Elapsed: 13m39s
Starting testing the validation set with 200 subgraphs!
2020-05-30 23:50:02.702573: Validation Test:  Loss: 0.5537,  Acc: 71.5944, AUC: 0.7867, Precision: 0.8309 -- Elapsed: 42m53s
2020-05-31 00:01:04.528345: Epoch: 1, Batch: 501, Loss: 0.5485, Elapsed: 11m1s
2020-05-31 00:18:45.269453: Epoch: 1, Batch: 502, Loss: 0.5974, Elapsed: 17m40s
2020-05-31 00:25:13.025003: Epoch: 1, Batch: 503, Loss: 0.5117, Elapsed: 6m27s
2020-05-31 00:37:05.556894: Epoch: 1, Batch: 504, Loss: 0.5801, Elapsed: 11m52s
2020-05-31 00:49:11.281571: Epoch: 1, Batch: 505, Loss: 0.5855, Elapsed: 12m5s
2020-05-31 00:55:44.812464: Epoch: 1, Batch: 506, Loss: 0.5244, Elapsed: 6m33s
2020-05-31 01:05:29.921038: Epoch: 1, Batch: 507, Loss: 0.5987, Elapsed: 9m45s
2020-05-31 01:19:07.203661: Epoch: 1, Batch: 508, Loss: 0.5867, Elapsed: 13m37s
2020-05-31 01:31:12.743897: Epoch: 1, Batch: 509, Loss: 0.5462, Elapsed: 12m5s
2020-05-31 01:36:03.318486: Epoch: 1, Batch: 510, Loss: 0.4669, Elapsed: 4m50s
2020-05-31 01:42:45.427371: Epoch: 1, Batch: 511, Loss: 0.5467, Elapsed: 6m42s
2020-05-31 01:51:30.734573: Epoch: 1, Batch: 512, Loss: 0.5792, Elapsed: 8m45s
2020-05-31 02:02:05.541565: Epoch: 1, Batch: 513, Loss: 0.5832, Elapsed: 10m34s
2020-05-31 02:16:57.799323: Epoch: 1, Batch: 514, Loss: 0.5773, Elapsed: 14m52s
2020-05-31 02:23:47.805895: Epoch: 1, Batch: 515, Loss: 0.5434, Elapsed: 6m49s
2020-05-31 02:39:07.917584: Epoch: 1, Batch: 516, Loss: 0.5971, Elapsed: 15m20s
2020-05-31 02:43:45.827625: Epoch: 1, Batch: 517, Loss: 0.4513, Elapsed: 4m37s
2020-05-31 02:50:16.913623: Epoch: 1, Batch: 518, Loss: 0.5623, Elapsed: 6m31s
2020-05-31 02:55:58.194237: Epoch: 1, Batch: 519, Loss: 0.5336, Elapsed: 5m41s
2020-05-31 03:06:36.932912: Epoch: 1, Batch: 520, Loss: 0.5763, Elapsed: 10m38s
2020-05-31 03:12:44.488144: Epoch: 1, Batch: 521, Loss: 0.5392, Elapsed: 6m7s
2020-05-31 03:20:42.005900: Epoch: 1, Batch: 522, Loss: 0.5015, Elapsed: 7m57s
2020-05-31 03:29:05.883339: Epoch: 1, Batch: 523, Loss: 0.5717, Elapsed: 8m23s
2020-05-31 03:38:46.634382: Epoch: 1, Batch: 524, Loss: 0.5530, Elapsed: 9m40s
2020-05-31 03:56:08.535973: Epoch: 1, Batch: 525, Loss: 0.6108, Elapsed: 17m21s
2020-05-31 04:08:16.328742: Epoch: 1, Batch: 526, Loss: 0.5611, Elapsed: 12m7s
2020-05-31 04:13:52.599738: Epoch: 1, Batch: 527, Loss: 0.5537, Elapsed: 5m36s
2020-05-31 04:23:56.912930: Epoch: 1, Batch: 528, Loss: 0.5390, Elapsed: 10m4s
2020-05-31 04:28:41.702883: Epoch: 1, Batch: 529, Loss: 0.5107, Elapsed: 4m44s
2020-05-31 04:36:41.232845: Epoch: 1, Batch: 530, Loss: 0.5586, Elapsed: 7m59s
2020-05-31 04:43:42.251942: Epoch: 1, Batch: 531, Loss: 0.4850, Elapsed: 7m1s
2020-05-31 04:54:53.100407: Epoch: 1, Batch: 532, Loss: 0.5691, Elapsed: 11m10s
2020-05-31 05:05:06.354379: Epoch: 1, Batch: 533, Loss: 0.5793, Elapsed: 10m13s
2020-05-31 05:23:40.614585: Epoch: 1, Batch: 534, Loss: 0.5847, Elapsed: 18m34s
2020-05-31 05:36:06.885267: Epoch: 1, Batch: 535, Loss: 0.5391, Elapsed: 12m26s
2020-05-31 05:47:18.205132: Epoch: 1, Batch: 536, Loss: 0.5672, Elapsed: 11m11s
2020-05-31 05:54:27.613568: Epoch: 1, Batch: 537, Loss: 0.5126, Elapsed: 7m9s
2020-05-31 06:01:28.566104: Epoch: 1, Batch: 538, Loss: 0.5013, Elapsed: 7m0s
2020-05-31 06:08:43.115261: Epoch: 1, Batch: 539, Loss: 0.5283, Elapsed: 7m14s
2020-05-31 06:23:26.476447: Epoch: 1, Batch: 540, Loss: 0.5629, Elapsed: 14m43s
2020-05-31 06:29:44.217900: Epoch: 1, Batch: 541, Loss: 0.5003, Elapsed: 6m17s
2020-05-31 06:39:27.677082: Epoch: 1, Batch: 542, Loss: 0.5918, Elapsed: 9m43s
2020-05-31 06:44:36.125548: Epoch: 1, Batch: 543, Loss: 0.4885, Elapsed: 5m8s
2020-05-31 06:54:18.671684: Epoch: 1, Batch: 544, Loss: 0.5556, Elapsed: 9m42s
2020-05-31 07:01:29.219568: Epoch: 1, Batch: 545, Loss: 0.5512, Elapsed: 7m10s
2020-05-31 07:09:06.310592: Epoch: 1, Batch: 546, Loss: 0.5590, Elapsed: 7m37s
2020-05-31 07:19:29.291883: Epoch: 1, Batch: 547, Loss: 0.5546, Elapsed: 10m22s
2020-05-31 07:28:22.974821: Epoch: 1, Batch: 548, Loss: 0.5063, Elapsed: 8m53s
2020-05-31 07:35:08.412225: Epoch: 1, Batch: 549, Loss: 0.4520, Elapsed: 6m45s
2020-05-31 07:42:15.563094: Epoch: 1, Batch: 550, Loss: 0.5078, Elapsed: 7m7s
Starting testing the validation set with 200 subgraphs!
2020-05-31 08:25:22.401828: Validation Test:  Loss: 0.5420,  Acc: 71.7861, AUC: 0.8007, Precision: 0.8506 -- Elapsed: 43m6s
2020-05-31 08:34:32.135430: Epoch: 1, Batch: 551, Loss: 0.5471, Elapsed: 9m9s
2020-05-31 08:42:19.343101: Epoch: 1, Batch: 552, Loss: 0.5190, Elapsed: 7m47s
2020-05-31 08:56:05.332338: Epoch: 1, Batch: 553, Loss: 0.5444, Elapsed: 13m45s
2020-05-31 09:03:03.752321: Epoch: 1, Batch: 554, Loss: 0.5236, Elapsed: 6m58s
2020-05-31 09:11:16.429870: Epoch: 1, Batch: 555, Loss: 0.5333, Elapsed: 8m12s
2020-05-31 09:22:14.825503: Epoch: 1, Batch: 556, Loss: 0.5485, Elapsed: 10m58s
2020-05-31 09:30:09.528601: Epoch: 1, Batch: 557, Loss: 0.5275, Elapsed: 7m54s
2020-05-31 09:36:22.549270: Epoch: 1, Batch: 558, Loss: 0.5166, Elapsed: 6m13s
2020-05-31 09:41:13.209814: Epoch: 1, Batch: 559, Loss: 0.4913, Elapsed: 4m50s
2020-05-31 09:46:58.256391: Epoch: 1, Batch: 560, Loss: 0.5230, Elapsed: 5m45s
2020-05-31 09:57:48.506418: Epoch: 1, Batch: 561, Loss: 0.5743, Elapsed: 10m50s
2020-05-31 10:03:11.612466: Epoch: 1, Batch: 562, Loss: 0.5186, Elapsed: 5m23s
2020-05-31 10:12:46.919433: Epoch: 1, Batch: 563, Loss: 0.5343, Elapsed: 9m35s
2020-05-31 10:20:14.808070: Epoch: 1, Batch: 564, Loss: 0.5279, Elapsed: 7m27s
2020-05-31 10:25:50.340251: Epoch: 1, Batch: 565, Loss: 0.5056, Elapsed: 5m35s
2020-05-31 10:35:54.245352: Epoch: 1, Batch: 566, Loss: 0.5493, Elapsed: 10m3s
2020-05-31 10:42:58.752693: Epoch: 1, Batch: 567, Loss: 0.5579, Elapsed: 7m4s
2020-05-31 10:55:45.638552: Epoch: 1, Batch: 568, Loss: 0.5910, Elapsed: 12m46s
2020-05-31 11:04:47.639645: Epoch: 1, Batch: 569, Loss: 0.5488, Elapsed: 9m1s
2020-05-31 11:16:35.078796: Epoch: 1, Batch: 570, Loss: 0.5476, Elapsed: 11m47s
2020-05-31 11:26:29.038915: Epoch: 1, Batch: 571, Loss: 0.5291, Elapsed: 9m53s
2020-05-31 11:31:29.822697: Epoch: 1, Batch: 572, Loss: 0.5026, Elapsed: 5m0s
2020-05-31 11:42:51.233898: Epoch: 1, Batch: 573, Loss: 0.5479, Elapsed: 11m21s
2020-05-31 11:50:32.768729: Epoch: 1, Batch: 574, Loss: 0.5459, Elapsed: 7m41s
2020-05-31 11:59:03.013380: Epoch: 1, Batch: 575, Loss: 0.5402, Elapsed: 8m30s
2020-05-31 12:08:40.991454: Epoch: 1, Batch: 576, Loss: 0.5469, Elapsed: 9m37s
2020-05-31 12:18:37.334760: Epoch: 1, Batch: 577, Loss: 0.5404, Elapsed: 9m56s
2020-05-31 12:31:14.565697: Epoch: 1, Batch: 578, Loss: 0.5508, Elapsed: 12m37s
2020-05-31 12:39:19.206992: Epoch: 1, Batch: 579, Loss: 0.5225, Elapsed: 8m4s
2020-05-31 12:47:26.110812: Epoch: 1, Batch: 580, Loss: 0.5207, Elapsed: 8m6s
2020-05-31 12:52:55.090553: Epoch: 1, Batch: 581, Loss: 0.4784, Elapsed: 5m28s
2020-05-31 12:59:46.604291: Epoch: 1, Batch: 582, Loss: 0.5290, Elapsed: 6m51s
2020-05-31 13:08:56.578566: Epoch: 1, Batch: 583, Loss: 0.5503, Elapsed: 9m9s
2020-05-31 13:20:59.715591: Epoch: 1, Batch: 584, Loss: 0.5755, Elapsed: 12m3s
2020-05-31 13:34:28.174708: Epoch: 1, Batch: 585, Loss: 0.5621, Elapsed: 13m28s
2020-05-31 13:44:36.713953: Epoch: 1, Batch: 586, Loss: 0.5684, Elapsed: 10m8s
2020-05-31 13:51:36.650908: Epoch: 1, Batch: 587, Loss: 0.5074, Elapsed: 6m59s
2020-05-31 14:04:53.154736: Epoch: 1, Batch: 588, Loss: 0.5516, Elapsed: 13m16s
2020-05-31 14:12:25.458181: Epoch: 1, Batch: 589, Loss: 0.4738, Elapsed: 7m32s
2020-05-31 14:21:09.196567: Epoch: 1, Batch: 590, Loss: 0.5278, Elapsed: 8m43s
2020-05-31 14:41:19.753910: Epoch: 1, Batch: 591, Loss: 0.5549, Elapsed: 20m10s
2020-05-31 14:49:42.730238: Epoch: 1, Batch: 592, Loss: 0.5134, Elapsed: 8m22s
2020-05-31 14:57:29.886799: Epoch: 1, Batch: 593, Loss: 0.5020, Elapsed: 7m47s
2020-05-31 15:04:47.831106: Epoch: 1, Batch: 594, Loss: 0.5390, Elapsed: 7m17s
2020-05-31 15:14:36.111128: Epoch: 1, Batch: 595, Loss: 0.5474, Elapsed: 9m48s
2020-05-31 15:23:04.442121: Epoch: 1, Batch: 596, Loss: 0.5255, Elapsed: 8m28s
2020-05-31 15:28:10.726255: Epoch: 1, Batch: 597, Loss: 0.5508, Elapsed: 5m6s
2020-05-31 15:35:11.154909: Epoch: 1, Batch: 598, Loss: 0.4954, Elapsed: 7m0s
2020-05-31 15:43:13.755492: Epoch: 1, Batch: 599, Loss: 0.5453, Elapsed: 8m2s
2020-05-31 15:49:41.078195: Epoch: 1, Batch: 600, Loss: 0.5330, Elapsed: 6m27s
Starting testing the validation set with 200 subgraphs!
2020-05-31 16:33:25.506330: Validation Test:  Loss: 0.5365,  Acc: 71.9555, AUC: 0.8060, Precision: 0.8581 -- Elapsed: 43m44s
2020-05-31 16:41:48.746933: Epoch: 1, Batch: 601, Loss: 0.5696, Elapsed: 8m23s
2020-05-31 16:45:30.293794: Epoch: 1, Batch: 602, Loss: 0.4677, Elapsed: 3m41s
2020-05-31 16:54:43.021923: Epoch: 1, Batch: 603, Loss: 0.5191, Elapsed: 9m12s
2020-05-31 17:03:16.621454: Epoch: 1, Batch: 604, Loss: 0.5108, Elapsed: 8m33s
2020-05-31 17:14:08.538540: Epoch: 1, Batch: 605, Loss: 0.5362, Elapsed: 10m51s
2020-05-31 17:22:20.699748: Epoch: 1, Batch: 606, Loss: 0.5367, Elapsed: 8m12s
2020-05-31 17:30:39.318216: Epoch: 1, Batch: 607, Loss: 0.4977, Elapsed: 8m18s
2020-05-31 17:39:22.219016: Epoch: 1, Batch: 608, Loss: 0.5452, Elapsed: 8m42s
2020-05-31 17:48:20.622082: Epoch: 1, Batch: 609, Loss: 0.5121, Elapsed: 8m58s
2020-05-31 17:58:16.013679: Epoch: 1, Batch: 610, Loss: 0.5236, Elapsed: 9m55s
2020-05-31 18:04:47.573518: Epoch: 1, Batch: 611, Loss: 0.4740, Elapsed: 6m31s
2020-05-31 18:14:54.716657: Epoch: 1, Batch: 612, Loss: 0.5208, Elapsed: 10m7s
2020-05-31 18:22:11.705077: Epoch: 1, Batch: 613, Loss: 0.5188, Elapsed: 7m16s
2020-05-31 18:28:22.021447: Epoch: 1, Batch: 614, Loss: 0.4481, Elapsed: 6m10s
2020-05-31 18:37:24.161658: Epoch: 1, Batch: 615, Loss: 0.5453, Elapsed: 9m2s
2020-05-31 18:52:14.318648: Epoch: 1, Batch: 616, Loss: 0.5625, Elapsed: 14m50s
2020-05-31 18:58:37.461983: Epoch: 1, Batch: 617, Loss: 0.4493, Elapsed: 6m23s
2020-05-31 19:15:04.635720: Epoch: 1, Batch: 618, Loss: 0.5635, Elapsed: 16m27s
2020-05-31 19:26:28.983093: Epoch: 1, Batch: 619, Loss: 0.5445, Elapsed: 11m24s
2020-05-31 19:31:15.763304: Epoch: 1, Batch: 620, Loss: 0.4881, Elapsed: 4m46s
2020-05-31 19:45:13.281743: Epoch: 1, Batch: 621, Loss: 0.5299, Elapsed: 13m57s
2020-05-31 19:53:32.218298: Epoch: 1, Batch: 622, Loss: 0.5114, Elapsed: 8m18s
2020-05-31 20:02:26.714340: Epoch: 1, Batch: 623, Loss: 0.5060, Elapsed: 8m54s
2020-05-31 20:13:35.983083: Epoch: 1, Batch: 624, Loss: 0.5396, Elapsed: 11m9s
2020-05-31 20:20:40.660825: Epoch: 1, Batch: 625, Loss: 0.5139, Elapsed: 7m4s
2020-05-31 20:27:16.021197: Epoch: 1, Batch: 626, Loss: 0.4559, Elapsed: 6m35s
2020-05-31 20:38:02.681370: Epoch: 1, Batch: 627, Loss: 0.5610, Elapsed: 10m46s
2020-05-31 20:55:28.098075: Epoch: 1, Batch: 628, Loss: 0.5547, Elapsed: 17m25s
2020-05-31 21:01:12.129947: Epoch: 1, Batch: 629, Loss: 0.4852, Elapsed: 5m44s
2020-05-31 21:08:42.924833: Epoch: 1, Batch: 630, Loss: 0.4825, Elapsed: 7m30s
2020-05-31 21:17:38.343824: Epoch: 1, Batch: 631, Loss: 0.5228, Elapsed: 8m55s
2020-05-31 21:26:04.701092: Epoch: 1, Batch: 632, Loss: 0.5489, Elapsed: 8m26s
2020-05-31 21:30:39.275826: Epoch: 1, Batch: 633, Loss: 0.4489, Elapsed: 4m34s
2020-05-31 21:35:45.229754: Epoch: 1, Batch: 634, Loss: 0.3929, Elapsed: 5m5s
2020-05-31 21:46:29.722493: Epoch: 1, Batch: 635, Loss: 0.5407, Elapsed: 10m44s
2020-05-31 21:58:25.966747: Epoch: 1, Batch: 636, Loss: 0.5637, Elapsed: 11m56s
2020-05-31 22:07:26.927450: Epoch: 1, Batch: 637, Loss: 0.5293, Elapsed: 9m0s
2020-05-31 22:19:34.918156: Epoch: 1, Batch: 638, Loss: 0.5411, Elapsed: 12m7s
2020-05-31 22:28:49.541360: Epoch: 1, Batch: 639, Loss: 0.5261, Elapsed: 9m14s
2020-05-31 22:35:15.154377: Epoch: 1, Batch: 640, Loss: 0.4794, Elapsed: 6m25s
2020-05-31 22:45:24.887039: Epoch: 1, Batch: 641, Loss: 0.5572, Elapsed: 10m9s
2020-05-31 22:54:00.736613: Epoch: 1, Batch: 642, Loss: 0.5141, Elapsed: 8m35s
2020-05-31 23:01:25.616593: Epoch: 1, Batch: 643, Loss: 0.5278, Elapsed: 7m24s
2020-05-31 23:12:05.248563: Epoch: 1, Batch: 644, Loss: 0.5055, Elapsed: 10m39s
2020-05-31 23:20:55.516289: Epoch: 1, Batch: 645, Loss: 0.5012, Elapsed: 8m50s
2020-05-31 23:27:18.405902: Epoch: 1, Batch: 646, Loss: 0.4864, Elapsed: 6m22s
2020-05-31 23:35:11.372147: Epoch: 1, Batch: 647, Loss: 0.5285, Elapsed: 7m52s
2020-05-31 23:42:17.691123: Epoch: 1, Batch: 648, Loss: 0.4875, Elapsed: 7m6s
2020-05-31 23:53:04.270550: Epoch: 1, Batch: 649, Loss: 0.5341, Elapsed: 10m46s
2020-06-01 00:02:44.121544: Epoch: 1, Batch: 650, Loss: 0.5403, Elapsed: 9m39s
Starting testing the validation set with 200 subgraphs!
2020-06-01 00:45:57.246059: Validation Test:  Loss: 0.5275,  Acc: 72.1219, AUC: 0.8133, Precision: 0.8668 -- Elapsed: 43m13s
2020-06-01 00:54:07.293619: Epoch: 1, Batch: 651, Loss: 0.5123, Elapsed: 8m10s
2020-06-01 01:00:46.993010: Epoch: 1, Batch: 652, Loss: 0.4900, Elapsed: 6m39s
2020-06-01 01:08:15.806637: Epoch: 1, Batch: 653, Loss: 0.5020, Elapsed: 7m28s
2020-06-01 01:17:23.560713: Epoch: 1, Batch: 654, Loss: 0.5342, Elapsed: 9m7s
2020-06-01 01:25:36.383158: Epoch: 1, Batch: 655, Loss: 0.4587, Elapsed: 8m12s
2020-06-01 01:31:46.111299: Epoch: 1, Batch: 656, Loss: 0.4909, Elapsed: 6m9s
2020-06-01 01:41:04.903724: Epoch: 1, Batch: 657, Loss: 0.5203, Elapsed: 9m18s
2020-06-01 01:49:33.782501: Epoch: 1, Batch: 658, Loss: 0.5398, Elapsed: 8m28s
2020-06-01 01:59:31.534255: Epoch: 1, Batch: 659, Loss: 0.5472, Elapsed: 9m57s
2020-06-01 02:08:01.530264: Epoch: 1, Batch: 660, Loss: 0.5141, Elapsed: 8m29s
2020-06-01 02:15:23.633087: Epoch: 1, Batch: 661, Loss: 0.4878, Elapsed: 7m22s
2020-06-01 02:26:23.359786: Epoch: 1, Batch: 662, Loss: 0.5429, Elapsed: 10m59s
2020-06-01 02:32:49.289113: Epoch: 1, Batch: 663, Loss: 0.4851, Elapsed: 6m25s
2020-06-01 02:38:48.719844: Epoch: 1, Batch: 664, Loss: 0.5140, Elapsed: 5m59s
2020-06-01 02:48:55.011806: Epoch: 1, Batch: 665, Loss: 0.5179, Elapsed: 10m6s
2020-06-01 02:55:14.511044: Epoch: 1, Batch: 666, Loss: 0.5102, Elapsed: 6m19s
2020-06-01 03:08:21.047641: Epoch: 1, Batch: 667, Loss: 0.5542, Elapsed: 13m6s
2020-06-01 03:13:31.647737: Epoch: 1, Batch: 668, Loss: 0.4176, Elapsed: 5m10s
2020-06-01 03:28:02.697782: Epoch: 1, Batch: 669, Loss: 0.6312, Elapsed: 14m31s
2020-06-01 03:37:30.860558: Epoch: 1, Batch: 670, Loss: 0.5429, Elapsed: 9m28s
2020-06-01 03:48:47.715207: Epoch: 1, Batch: 671, Loss: 0.5368, Elapsed: 11m16s
2020-06-01 03:58:30.446528: Epoch: 1, Batch: 672, Loss: 0.5828, Elapsed: 9m42s
2020-06-01 04:04:17.520927: Epoch: 1, Batch: 673, Loss: 0.5204, Elapsed: 5m47s
2020-06-01 04:18:25.460352: Epoch: 1, Batch: 674, Loss: 0.5516, Elapsed: 14m7s
2020-06-01 04:24:59.807854: Epoch: 1, Batch: 675, Loss: 0.5119, Elapsed: 6m34s
2020-06-01 04:35:52.569750: Epoch: 1, Batch: 676, Loss: 0.5511, Elapsed: 10m52s
2020-06-01 04:47:03.412650: Epoch: 1, Batch: 677, Loss: 0.5272, Elapsed: 11m10s
2020-06-01 04:58:21.386955: Epoch: 1, Batch: 678, Loss: 0.5562, Elapsed: 11m17s
2020-06-01 05:04:37.810822: Epoch: 1, Batch: 679, Loss: 0.4753, Elapsed: 6m16s
2020-06-01 05:16:38.738534: Epoch: 1, Batch: 680, Loss: 0.5496, Elapsed: 12m0s
2020-06-01 05:22:03.746444: Epoch: 1, Batch: 681, Loss: 0.4966, Elapsed: 5m24s
2020-06-01 05:31:13.062313: Epoch: 1, Batch: 682, Loss: 0.5315, Elapsed: 9m9s
2020-06-01 05:38:15.492571: Epoch: 1, Batch: 683, Loss: 0.5418, Elapsed: 7m2s
2020-06-01 05:46:19.694299: Epoch: 1, Batch: 684, Loss: 0.5400, Elapsed: 8m4s
2020-06-01 05:55:24.332961: Epoch: 1, Batch: 685, Loss: 0.5509, Elapsed: 9m4s
2020-06-01 06:09:29.195397: Epoch: 1, Batch: 686, Loss: 0.5650, Elapsed: 14m4s
2020-06-01 06:14:52.968799: Epoch: 1, Batch: 687, Loss: 0.5523, Elapsed: 5m23s
2020-06-01 06:25:05.399590: Epoch: 1, Batch: 688, Loss: 0.5165, Elapsed: 10m12s
2020-06-01 06:34:03.388798: Epoch: 1, Batch: 689, Loss: 0.5434, Elapsed: 8m57s
2020-06-01 06:47:06.055948: Epoch: 1, Batch: 690, Loss: 0.5793, Elapsed: 13m2s
2020-06-01 06:56:20.132387: Epoch: 1, Batch: 691, Loss: 0.5545, Elapsed: 9m14s
2020-06-01 07:04:44.211387: Epoch: 1, Batch: 692, Loss: 0.5280, Elapsed: 8m24s
2020-06-01 07:14:28.677051: Epoch: 1, Batch: 693, Loss: 0.5280, Elapsed: 9m44s
2020-06-01 07:22:47.238302: Epoch: 1, Batch: 694, Loss: 0.5362, Elapsed: 8m18s
2020-06-01 07:33:44.754468: Epoch: 1, Batch: 695, Loss: 0.5505, Elapsed: 10m57s
2020-06-01 07:44:52.196556: Epoch: 1, Batch: 696, Loss: 0.5312, Elapsed: 11m7s
2020-06-01 07:55:38.731529: Epoch: 1, Batch: 697, Loss: 0.5385, Elapsed: 10m46s
2020-06-01 08:07:08.124542: Epoch: 1, Batch: 698, Loss: 0.5639, Elapsed: 11m29s
2020-06-01 08:15:16.494232: Epoch: 1, Batch: 699, Loss: 0.4974, Elapsed: 8m8s
2020-06-01 08:34:27.771017: Epoch: 1, Batch: 700, Loss: 0.5511, Elapsed: 19m11s
Starting testing the validation set with 200 subgraphs!
2020-06-01 09:18:18.347121: Validation Test:  Loss: 0.5310,  Acc: 72.4779, AUC: 0.8147, Precision: 0.8677 -- Elapsed: 43m50s
2020-06-01 09:26:58.338310: Epoch: 1, Batch: 701, Loss: 0.5445, Elapsed: 8m39s
2020-06-01 09:36:12.489766: Epoch: 1, Batch: 702, Loss: 0.4898, Elapsed: 9m14s
2020-06-01 09:45:07.154553: Epoch: 1, Batch: 703, Loss: 0.5004, Elapsed: 8m54s
2020-06-01 09:53:26.934040: Epoch: 1, Batch: 704, Loss: 0.5376, Elapsed: 8m19s
2020-06-01 09:59:18.963802: Epoch: 1, Batch: 705, Loss: 0.4917, Elapsed: 5m52s
2020-06-01 10:03:42.447533: Epoch: 1, Batch: 706, Loss: 0.4968, Elapsed: 4m23s
2020-06-01 10:12:10.912308: Epoch: 1, Batch: 707, Loss: 0.4935, Elapsed: 8m28s
2020-06-01 10:23:19.801737: Epoch: 1, Batch: 708, Loss: 0.5548, Elapsed: 11m8s
2020-06-01 10:28:38.899136: Epoch: 1, Batch: 709, Loss: 0.4958, Elapsed: 5m19s
2020-06-01 10:36:16.442731: Epoch: 1, Batch: 710, Loss: 0.4269, Elapsed: 7m37s
2020-06-01 10:43:34.465012: Epoch: 1, Batch: 711, Loss: 0.5488, Elapsed: 7m18s
2020-06-01 10:56:27.914579: Epoch: 1, Batch: 712, Loss: 0.5608, Elapsed: 12m53s
2020-06-01 11:08:49.010791: Epoch: 1, Batch: 713, Loss: 0.5449, Elapsed: 12m21s
2020-06-01 11:16:02.459205: Epoch: 1, Batch: 714, Loss: 0.4245, Elapsed: 7m13s
2020-06-01 11:25:46.035654: Epoch: 1, Batch: 715, Loss: 0.5218, Elapsed: 9m43s
2020-06-01 11:31:08.203679: Epoch: 1, Batch: 716, Loss: 0.4793, Elapsed: 5m22s
2020-06-01 11:40:18.286066: Epoch: 1, Batch: 717, Loss: 0.5335, Elapsed: 9m10s
2020-06-01 11:48:24.085953: Epoch: 1, Batch: 718, Loss: 0.5437, Elapsed: 8m5s
2020-06-01 11:56:51.234746: Epoch: 1, Batch: 719, Loss: 0.5123, Elapsed: 8m27s
2020-06-01 12:07:27.343079: Epoch: 1, Batch: 720, Loss: 0.5430, Elapsed: 10m36s
2020-06-01 12:19:51.449909: Epoch: 1, Batch: 721, Loss: 0.5436, Elapsed: 12m24s
2020-06-01 12:36:09.292336: Epoch: 1, Batch: 722, Loss: 0.5635, Elapsed: 16m17s
2020-06-01 12:41:25.828384: Epoch: 1, Batch: 723, Loss: 0.4652, Elapsed: 5m16s
2020-06-01 12:50:36.932324: Epoch: 1, Batch: 724, Loss: 0.5431, Elapsed: 9m11s
2020-06-01 13:02:53.625971: Epoch: 1, Batch: 725, Loss: 0.5367, Elapsed: 12m16s
2020-06-01 13:10:23.933961: Epoch: 1, Batch: 726, Loss: 0.5120, Elapsed: 7m30s
2020-06-01 13:23:10.633127: Epoch: 1, Batch: 727, Loss: 0.5385, Elapsed: 12m46s
2020-06-01 13:29:49.852524: Epoch: 1, Batch: 728, Loss: 0.5099, Elapsed: 6m39s
2020-06-01 13:34:13.542334: Epoch: 1, Batch: 729, Loss: 0.3832, Elapsed: 4m23s
2020-06-01 13:45:14.815627: Epoch: 1, Batch: 730, Loss: 0.5591, Elapsed: 11m1s
2020-06-01 13:54:46.419748: Epoch: 1, Batch: 731, Loss: 0.5248, Elapsed: 9m31s
2020-06-01 14:02:58.869743: Epoch: 1, Batch: 732, Loss: 0.5589, Elapsed: 8m12s
2020-06-01 14:11:31.551566: Epoch: 1, Batch: 733, Loss: 0.5063, Elapsed: 8m32s
2020-06-01 14:16:53.637382: Epoch: 1, Batch: 734, Loss: 0.5177, Elapsed: 5m22s
2020-06-01 14:26:41.606919: Epoch: 1, Batch: 735, Loss: 0.5444, Elapsed: 9m47s
2020-06-01 14:38:16.079554: Epoch: 1, Batch: 736, Loss: 0.5279, Elapsed: 11m34s
2020-06-01 14:46:25.275939: Epoch: 1, Batch: 737, Loss: 0.4861, Elapsed: 8m9s
2020-06-01 14:56:38.505109: Epoch: 1, Batch: 738, Loss: 0.5637, Elapsed: 10m13s
2020-06-01 15:10:38.888435: Epoch: 1, Batch: 739, Loss: 0.5857, Elapsed: 14m0s
2020-06-01 15:18:33.068989: Epoch: 1, Batch: 740, Loss: 0.5145, Elapsed: 7m54s
2020-06-01 15:27:49.172419: Epoch: 1, Batch: 741, Loss: 0.5285, Elapsed: 9m16s
2020-06-01 15:37:58.822032: Epoch: 1, Batch: 742, Loss: 0.5617, Elapsed: 10m9s
2020-06-01 15:47:27.987367: Epoch: 1, Batch: 743, Loss: 0.5066, Elapsed: 9m29s
2020-06-01 15:55:50.638929: Epoch: 1, Batch: 744, Loss: 0.5261, Elapsed: 8m22s
2020-06-01 16:02:53.059273: Epoch: 1, Batch: 745, Loss: 0.4945, Elapsed: 7m2s
2020-06-01 16:10:08.036454: Epoch: 1, Batch: 746, Loss: 0.4719, Elapsed: 7m14s
2020-06-01 16:17:38.929785: Epoch: 1, Batch: 747, Loss: 0.5304, Elapsed: 7m30s
2020-06-01 16:27:17.738077: Epoch: 1, Batch: 748, Loss: 0.5213, Elapsed: 9m38s
2020-06-01 16:34:44.031627: Epoch: 1, Batch: 749, Loss: 0.5162, Elapsed: 7m26s
2020-06-01 16:45:40.439971: Epoch: 1, Batch: 750, Loss: 0.5215, Elapsed: 10m56s
Starting testing the validation set with 200 subgraphs!
2020-06-01 17:29:00.021345: Validation Test:  Loss: 0.5234,  Acc: 72.1219, AUC: 0.8142, Precision: 0.8689 -- Elapsed: 43m19s
2020-06-01 17:37:26.254132: Epoch: 1, Batch: 751, Loss: 0.5081, Elapsed: 8m26s
2020-06-01 17:47:21.057857: Epoch: 1, Batch: 752, Loss: 0.4998, Elapsed: 9m54s
2020-06-01 17:56:57.662506: Epoch: 1, Batch: 753, Loss: 0.5187, Elapsed: 9m36s
2020-06-01 18:10:50.783374: Epoch: 1, Batch: 754, Loss: 0.5961, Elapsed: 13m53s
2020-06-01 18:21:30.638700: Epoch: 1, Batch: 755, Loss: 0.5129, Elapsed: 10m39s
2020-06-01 18:28:45.343549: Epoch: 1, Batch: 756, Loss: 0.5299, Elapsed: 7m14s
2020-06-01 18:38:45.418197: Epoch: 1, Batch: 757, Loss: 0.5195, Elapsed: 10m0s
2020-06-01 18:45:19.399872: Epoch: 1, Batch: 758, Loss: 0.4638, Elapsed: 6m33s
2020-06-01 18:51:28.184674: Epoch: 1, Batch: 759, Loss: 0.4467, Elapsed: 6m8s
2020-06-01 18:57:07.022803: Epoch: 1, Batch: 760, Loss: 0.4685, Elapsed: 5m38s
2020-06-01 19:12:03.733896: Epoch: 1, Batch: 761, Loss: 0.5889, Elapsed: 14m56s
2020-06-01 19:22:38.634193: Epoch: 1, Batch: 762, Loss: 0.5364, Elapsed: 10m34s
2020-06-01 19:30:17.929473: Epoch: 1, Batch: 763, Loss: 0.5303, Elapsed: 7m39s
2020-06-01 19:38:15.656086: Epoch: 1, Batch: 764, Loss: 0.4844, Elapsed: 7m57s
2020-06-01 19:48:29.417591: Epoch: 1, Batch: 765, Loss: 0.5374, Elapsed: 10m13s
2020-06-01 19:59:33.181288: Epoch: 1, Batch: 766, Loss: 0.5320, Elapsed: 11m3s
2020-06-01 20:08:20.923898: Epoch: 1, Batch: 767, Loss: 0.5650, Elapsed: 8m47s
2020-06-01 20:14:59.479389: Epoch: 1, Batch: 768, Loss: 0.5009, Elapsed: 6m38s
2020-06-01 20:25:47.487672: Epoch: 1, Batch: 769, Loss: 0.5631, Elapsed: 10m47s
2020-06-01 20:33:56.109900: Epoch: 1, Batch: 770, Loss: 0.5513, Elapsed: 8m8s
2020-06-01 20:42:11.894035: Epoch: 1, Batch: 771, Loss: 0.5306, Elapsed: 8m15s
2020-06-01 20:50:18.903344: Epoch: 1, Batch: 772, Loss: 0.4656, Elapsed: 8m7s
2020-06-01 21:00:51.726024: Epoch: 1, Batch: 773, Loss: 0.5588, Elapsed: 10m32s
2020-06-01 21:06:49.782316: Epoch: 1, Batch: 774, Loss: 0.4330, Elapsed: 5m58s
2020-06-01 21:13:02.938291: Epoch: 1, Batch: 775, Loss: 0.4953, Elapsed: 6m13s
2020-06-01 21:20:48.726491: Epoch: 1, Batch: 776, Loss: 0.4602, Elapsed: 7m45s
2020-06-01 21:31:07.718476: Epoch: 1, Batch: 777, Loss: 0.5158, Elapsed: 10m18s
2020-06-01 21:40:36.459140: Epoch: 1, Batch: 778, Loss: 0.5279, Elapsed: 9m28s
2020-06-01 21:51:50.394644: Epoch: 1, Batch: 779, Loss: 0.5436, Elapsed: 11m13s
2020-06-01 21:57:59.644287: Epoch: 1, Batch: 780, Loss: 0.5369, Elapsed: 6m9s
2020-06-01 22:05:45.011468: Epoch: 1, Batch: 781, Loss: 0.5239, Elapsed: 7m45s
2020-06-01 22:41:03.900166: Epoch: 1, Batch: 782, Loss: 0.7522, Elapsed: 35m18s
2020-06-01 22:49:17.335518: Epoch: 1, Batch: 783, Loss: 0.5628, Elapsed: 8m13s
2020-06-01 22:56:43.598653: Epoch: 1, Batch: 784, Loss: 0.5370, Elapsed: 7m26s
2020-06-01 23:06:54.781070: Epoch: 1, Batch: 785, Loss: 0.6060, Elapsed: 10m11s
2020-06-01 23:16:23.058827: Epoch: 1, Batch: 786, Loss: 0.5991, Elapsed: 9m28s
2020-06-01 23:24:12.279788: Epoch: 1, Batch: 787, Loss: 0.5398, Elapsed: 7m49s
2020-06-01 23:33:04.182132: Epoch: 1, Batch: 788, Loss: 0.5705, Elapsed: 8m51s
2020-06-01 23:41:32.506729: Epoch: 1, Batch: 789, Loss: 0.5571, Elapsed: 8m28s
2020-06-01 23:48:53.654986: Epoch: 1, Batch: 790, Loss: 0.5683, Elapsed: 7m21s
2020-06-01 23:57:57.211333: Epoch: 1, Batch: 791, Loss: 0.6132, Elapsed: 9m3s
2020-06-02 00:09:39.684931: Epoch: 1, Batch: 792, Loss: 0.6685, Elapsed: 11m42s
2020-06-02 00:18:52.632656: Epoch: 1, Batch: 793, Loss: 0.5235, Elapsed: 9m12s
2020-06-02 00:25:03.153052: Epoch: 1, Batch: 794, Loss: 0.5528, Elapsed: 6m10s
2020-06-02 00:34:37.656374: Epoch: 1, Batch: 795, Loss: 0.5738, Elapsed: 9m34s
2020-06-02 00:42:44.972798: Epoch: 1, Batch: 796, Loss: 0.5655, Elapsed: 8m7s
2020-06-02 00:55:39.920157: Epoch: 1, Batch: 797, Loss: 0.6233, Elapsed: 12m54s
2020-06-02 01:04:36.082230: Epoch: 1, Batch: 798, Loss: 0.5981, Elapsed: 8m56s
2020-06-02 01:18:08.504329: Epoch: 1, Batch: 799, Loss: 0.5915, Elapsed: 13m32s
2020-06-02 01:27:17.956133: Epoch: 1, Batch: 800, Loss: 0.5489, Elapsed: 9m9s
Starting testing the validation set with 200 subgraphs!
2020-06-02 02:10:47.883757: Validation Test:  Loss: 0.5568,  Acc: 70.9664, AUC: 0.7887, Precision: 0.8441 -- Elapsed: 43m29s
2020-06-02 02:23:47.724719: Epoch: 1, Batch: 801, Loss: 0.6002, Elapsed: 12m59s
2020-06-02 02:32:02.909742: Epoch: 1, Batch: 802, Loss: 0.5427, Elapsed: 8m15s
2020-06-02 02:44:17.092272: Epoch: 1, Batch: 803, Loss: 0.6044, Elapsed: 12m14s
2020-06-02 02:53:11.997830: Epoch: 1, Batch: 804, Loss: 0.5749, Elapsed: 8m54s
2020-06-02 03:02:51.211235: Epoch: 1, Batch: 805, Loss: 0.5796, Elapsed: 9m39s
2020-06-02 03:09:56.935393: Epoch: 1, Batch: 806, Loss: 0.5785, Elapsed: 7m5s
2020-06-02 03:21:00.080233: Epoch: 1, Batch: 807, Loss: 0.5836, Elapsed: 11m3s
2020-06-02 03:28:19.175028: Epoch: 1, Batch: 808, Loss: 0.5359, Elapsed: 7m19s
2020-06-02 03:39:50.948007: Epoch: 1, Batch: 809, Loss: 0.5967, Elapsed: 11m31s
2020-06-02 03:51:33.290621: Epoch: 1, Batch: 810, Loss: 0.5851, Elapsed: 11m42s
2020-06-02 04:01:23.391011: Epoch: 1, Batch: 811, Loss: 0.5768, Elapsed: 9m50s
2020-06-02 04:11:07.806634: Epoch: 1, Batch: 812, Loss: 0.5401, Elapsed: 9m44s
2020-06-02 04:23:56.122957: Epoch: 1, Batch: 813, Loss: 0.5798, Elapsed: 12m48s
2020-06-02 04:36:53.262505: Epoch: 1, Batch: 814, Loss: 0.5644, Elapsed: 12m57s
2020-06-02 04:49:15.334772: Epoch: 1, Batch: 815, Loss: 0.5609, Elapsed: 12m22s
2020-06-02 04:56:29.265865: Epoch: 1, Batch: 816, Loss: 0.5592, Elapsed: 7m13s
2020-06-02 05:04:21.235940: Epoch: 1, Batch: 817, Loss: 0.5581, Elapsed: 7m51s
2020-06-02 05:14:52.345593: Epoch: 1, Batch: 818, Loss: 0.5486, Elapsed: 10m31s
2020-06-02 05:27:13.806095: Epoch: 1, Batch: 819, Loss: 0.5669, Elapsed: 12m21s
